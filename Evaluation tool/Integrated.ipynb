{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Recommendation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup, BertTokenizer, BertModel\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input necessary hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestnum = int(input(\"How many best? \"))\n",
    "\n",
    "diary_text = input(\"Input the diary contents: \")\n",
    "\n",
    "seed = input(\"Input seed(num): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_bracket(s):\n",
    "  pattern = r'\\([^)]*\\)'  # ()\n",
    "  s = re.sub(pattern=pattern, repl='', string=s)\n",
    "\n",
    "  pattern = r'\\[[^)]*\\]'  # []\n",
    "  s = re.sub(pattern=pattern, repl='', string=s)\n",
    "\n",
    "  pattern = r'\\<[^)]*\\>'  # <>\n",
    "  s = re.sub(pattern=pattern, repl='', string=s)\n",
    "\n",
    "  pattern = r'\\{[^)]*\\}'  # {}\n",
    "  s = re.sub(pattern=pattern, repl='', string=s)\n",
    "\n",
    "  return s\n",
    "\n",
    "def del_special_num(s):\n",
    "  pattern = r'[^a-zA-Z가-힣]'\n",
    "  s = re.sub(pattern=pattern, repl=' ', string=s)\n",
    "\n",
    "  return s\n",
    "\n",
    "def del_unit(s):\n",
    "  units = ['mm', 'cm', 'km', 'ml', 'kg', 'g']\n",
    "  for unit in units:\n",
    "    s = s.lower() # 대문자를 소문자로 변환\n",
    "    s = s.replace(unit, '')\n",
    "  return s\n",
    "\n",
    "def del_whitespace(s):\n",
    "  return \" \".join(s.split())\n",
    "  \n",
    "def del_stopwords(s):\n",
    "  stopwords = open(\"data/stopwords.txt\", 'r', encoding=\"utf-8\").read().split()\n",
    "  #print(stopwords)\n",
    "  s_o=s.split()\n",
    "  s_f=[]\n",
    "  for w in s_o:\n",
    "    if w.strip() not in stopwords:\n",
    "      s_f.append(w.strip())\n",
    "  return \" \".join(s_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1-4 ongoing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# importing model\n",
    "modelname = \"klue/bert-base\"\n",
    "model_path = 'models/fin_model_1.pt'  # replace with your actual path\n",
    "max_length = 64\n",
    "num_classes = 6\n",
    "\n",
    "# Use cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#BERT 모델 불러오기\n",
    "bertmodel = BertModel.from_pretrained(modelname)\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=num_classes,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = outputs[1]\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# assuming you have a function for tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "# Define a function to get the valid_length, attention_mask and segment_ids\n",
    "def get_inputs(tokens):\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    valid_length = len(tokens)\n",
    "    segment_ids = [0]*valid_length\n",
    "    attention_mask = [1]*valid_length\n",
    "\n",
    "    # Pad up to max length\n",
    "    if valid_length < max_length:\n",
    "        pad_length = max_length - valid_length\n",
    "        tokens.extend(['[PAD]' for _ in range(pad_length)])\n",
    "        attention_mask.extend([0]*pad_length)\n",
    "        segment_ids.extend([0]*pad_length)\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return torch.tensor([token_ids], dtype=torch.long), torch.tensor([valid_length], dtype=torch.long), torch.tensor([segment_ids], dtype=torch.long), torch.tensor([attention_mask], dtype=torch.long)\n",
    "\n",
    "# assuming you have a function for pre-processing\n",
    "def preprocess(text):\n",
    "    for t in text:\n",
    "        t=del_bracket(t)\n",
    "        t=del_special_num(t)\n",
    "        t=del_whitespace(t)\n",
    "        t=del_stopwords(t)\n",
    "    return text.lower()\n",
    "\n",
    "def temperature_scaled_softmax(output, temperature=1.0):\n",
    "    # Apply temperature scaling on logits\n",
    "    output = output / temperature\n",
    "\n",
    "    # Then apply softmax to convert to probabilities\n",
    "    probabilities = F.softmax(output, dim=-1)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Load the model\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.load(model_path)\n",
    "elif not torch.cuda.is_available():\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load the model and move to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def process_lyrics(lyrics, tokenizer, model):\n",
    "    # Preprocess and tokenize the lyrics\n",
    "    tokens = tokenizer.tokenize(preprocess(lyrics))\n",
    "    if len(tokens) > max_length-2: # Account for [CLS] and [SEP]\n",
    "        tokens = tokens[:max_length-2]\n",
    "\n",
    "    # Get inputs\n",
    "    token_ids, valid_length, segment_ids, attention_mask = get_inputs(tokens)\n",
    "\n",
    "    # Move all your tensors to the same device as your model\n",
    "    token_ids = token_ids.to(device)\n",
    "    valid_length = valid_length.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Ensure no gradient is calculated\n",
    "    with torch.no_grad():\n",
    "        sentiment_vector = model(token_ids, valid_length, segment_ids, attention_mask)\n",
    "\n",
    "    return sentiment_vector.detach().cpu().numpy()[0]  # Return as a 1-D numpy array\n",
    "\n",
    "# Function to compute cosine similarities and retrieve the top 10 songs\n",
    "def recommend_songs(diary_text):\n",
    "    # Process the diary text\n",
    "    diary_vector = process_lyrics(diary_text, tokenizer, model)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity([diary_vector], df_music['sentiment_vector'].to_list())\n",
    "\n",
    "    # Get the top 10 song indices\n",
    "    Best_N = bestnum\n",
    "    top_10_indices = similarities[0].argsort()[-Best_N:][::-1]\n",
    "\n",
    "    # Return the corresponding songs\n",
    "    return df_music.iloc[top_10_indices]\n",
    "\n",
    "# Load the music data\n",
    "df_music = pd.read_csv('library/music_library_model1.csv')\n",
    "\n",
    "# Convert the strings back to arrays\n",
    "df_music['sentiment_vector'] = df_music['sentiment_vector'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "recommend_songs(diary_text)[['title', 'artist']].to_csv(f\"model1{seed}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# importing model\n",
    "modelname = \"klue/bert-base\" \n",
    "model_path = 'models/fin_model_2.pt'  # replace with your actual path\n",
    "max_length = 64\n",
    "num_classes = 7\n",
    "\n",
    "# Use cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#BERT 모델 불러오기\n",
    "bertmodel = BertModel.from_pretrained(modelname)\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=num_classes,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = outputs[1]\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# assuming you have a function for tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "# Define a function to get the valid_length, attention_mask and segment_ids\n",
    "def get_inputs(tokens):\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    valid_length = len(tokens)\n",
    "    segment_ids = [0]*valid_length\n",
    "    attention_mask = [1]*valid_length\n",
    "\n",
    "    # Pad up to max length\n",
    "    if valid_length < max_length:\n",
    "        pad_length = max_length - valid_length\n",
    "        tokens.extend(['[PAD]' for _ in range(pad_length)])\n",
    "        attention_mask.extend([0]*pad_length)\n",
    "        segment_ids.extend([0]*pad_length)\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return torch.tensor([token_ids], dtype=torch.long), torch.tensor([valid_length], dtype=torch.long), torch.tensor([segment_ids], dtype=torch.long), torch.tensor([attention_mask], dtype=torch.long)\n",
    "\n",
    "# assuming you have a function for pre-processing\n",
    "def preprocess(text):\n",
    "    for t in text:\n",
    "        t=del_bracket(t)\n",
    "        t=del_special_num(t)\n",
    "        t=del_whitespace(t)\n",
    "        t=del_stopwords(t)\n",
    "    return text.lower()\n",
    "\n",
    "def temperature_scaled_softmax(output, temperature=1.0):\n",
    "    # Apply temperature scaling on logits\n",
    "    output = output / temperature\n",
    "\n",
    "    # Then apply softmax to convert to probabilities\n",
    "    probabilities = F.softmax(output, dim=-1)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Load the model\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.load(model_path)\n",
    "elif not torch.cuda.is_available():\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load the model and move to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def process_lyrics(lyrics, tokenizer, model):\n",
    "    # Preprocess and tokenize the lyrics\n",
    "    tokens = tokenizer.tokenize(preprocess(lyrics))\n",
    "    if len(tokens) > max_length-2: # Account for [CLS] and [SEP]\n",
    "        tokens = tokens[:max_length-2]\n",
    "\n",
    "    # Get inputs\n",
    "    token_ids, valid_length, segment_ids, attention_mask = get_inputs(tokens)\n",
    "\n",
    "    # Move all your tensors to the same device as your model\n",
    "    token_ids = token_ids.to(device)\n",
    "    valid_length = valid_length.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Ensure no gradient is calculated\n",
    "    with torch.no_grad():\n",
    "        sentiment_vector = model(token_ids, valid_length, segment_ids, attention_mask)\n",
    "\n",
    "    return sentiment_vector.detach().cpu().numpy()[0]  # Return as a 1-D numpy array\n",
    "\n",
    "# Function to compute cosine similarities and retrieve the top 10 songs\n",
    "def recommend_songs(diary_text):\n",
    "    # Process the diary text\n",
    "    diary_vector = process_lyrics(diary_text, tokenizer, model)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity([diary_vector], df_music['sentiment_vector'].to_list())\n",
    "\n",
    "    # Get the top 10 song indices\n",
    "    Best_N = bestnum\n",
    "    top_10_indices = similarities[0].argsort()[-Best_N:][::-1]\n",
    "\n",
    "    # Return the corresponding songs\n",
    "    return df_music.iloc[top_10_indices]\n",
    "\n",
    "# Function to print out the recommended songs\n",
    "def print_recommended_songs(diary_text):\n",
    "    recommended_songs = recommend_songs(diary_text)\n",
    "    print(\"Top 10 similar songs:\\n\")\n",
    "    print(\"Rank\\tSimilarity\\tSong Name - Artist\")\n",
    "    for i, song in enumerate(recommended_songs.iterrows(), start=1):\n",
    "        index, data = song\n",
    "        similarity = cosine_similarity([process_lyrics(diary_text, tokenizer, model)], [data['sentiment_vector']])[0][0]\n",
    "        print(f\"{i}st\\t{similarity*100:.2f}% similar\\t{data['title']} - {data['artist']}\")\n",
    "\n",
    "# Load the music data\n",
    "df_music = pd.read_csv('library/music_library_model2.csv')\n",
    "\n",
    "# Convert the strings back to arrays\n",
    "df_music['sentiment_vector'] = df_music['sentiment_vector'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "recommend_songs(diary_text)[['title', 'artist']].to_csv(f\"model2{seed}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# importing model\n",
    "modelname = \"kykim/bert-kor-base\" \n",
    "model_path = 'models/fin_model_3.pt'  # replace with your actual path\n",
    "max_length = 64\n",
    "num_classes = 9\n",
    "\n",
    "# Use cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#BERT 모델 불러오기\n",
    "bertmodel = BertModel.from_pretrained(modelname)\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=num_classes,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = outputs[1]\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# assuming you have a function for tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "# Define a function to get the valid_length, attention_mask and segment_ids\n",
    "def get_inputs(tokens):\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    valid_length = len(tokens)\n",
    "    segment_ids = [0]*valid_length\n",
    "    attention_mask = [1]*valid_length\n",
    "\n",
    "    # Pad up to max length\n",
    "    if valid_length < max_length:\n",
    "        pad_length = max_length - valid_length\n",
    "        tokens.extend(['[PAD]' for _ in range(pad_length)])\n",
    "        attention_mask.extend([0]*pad_length)\n",
    "        segment_ids.extend([0]*pad_length)\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return torch.tensor([token_ids], dtype=torch.long), torch.tensor([valid_length], dtype=torch.long), torch.tensor([segment_ids], dtype=torch.long), torch.tensor([attention_mask], dtype=torch.long)\n",
    "\n",
    "# assuming you have a function for pre-processing\n",
    "def preprocess(text):\n",
    "    for t in text:\n",
    "        t=del_bracket(t)\n",
    "        t=del_special_num(t)\n",
    "        t=del_whitespace(t)\n",
    "        t=del_stopwords(t)\n",
    "    return text.lower()\n",
    "\n",
    "def temperature_scaled_softmax(output, temperature=1.0):\n",
    "    # Apply temperature scaling on logits\n",
    "    output = output / temperature\n",
    "\n",
    "    # Then apply softmax to convert to probabilities\n",
    "    probabilities = F.softmax(output, dim=-1)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Load the model\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.load(model_path)\n",
    "elif not torch.cuda.is_available():\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load the model and move to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def process_lyrics(lyrics, tokenizer, model):\n",
    "    # Preprocess and tokenize the lyrics\n",
    "    tokens = tokenizer.tokenize(preprocess(lyrics))\n",
    "    if len(tokens) > max_length-2: # Account for [CLS] and [SEP]\n",
    "        tokens = tokens[:max_length-2]\n",
    "\n",
    "    # Get inputs\n",
    "    token_ids, valid_length, segment_ids, attention_mask = get_inputs(tokens)\n",
    "\n",
    "    # Move all your tensors to the same device as your model\n",
    "    token_ids = token_ids.to(device)\n",
    "    valid_length = valid_length.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Ensure no gradient is calculated\n",
    "    with torch.no_grad():\n",
    "        sentiment_vector = model(token_ids, valid_length, segment_ids, attention_mask)\n",
    "\n",
    "    return sentiment_vector.detach().cpu().numpy()[0]  # Return as a 1-D numpy array\n",
    "\n",
    "# Function to compute cosine similarities and retrieve the top 10 songs\n",
    "def recommend_songs(diary_text):\n",
    "    # Process the diary text\n",
    "    diary_vector = process_lyrics(diary_text, tokenizer, model)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity([diary_vector], df_music['sentiment_vector'].to_list())\n",
    "\n",
    "    # Get the top 10 song indices\n",
    "    Best_N = bestnum\n",
    "    top_10_indices = similarities[0].argsort()[-Best_N:][::-1]\n",
    "\n",
    "    # Return the corresponding songs\n",
    "    return df_music.iloc[top_10_indices]\n",
    "\n",
    "# Function to print out the recommended songs\n",
    "def print_recommended_songs(diary_text):\n",
    "    recommended_songs = recommend_songs(diary_text)\n",
    "    print(\"Top 10 similar songs:\\n\")\n",
    "    print(\"Rank\\tSimilarity\\tSong Name - Artist\")\n",
    "    for i, song in enumerate(recommended_songs.iterrows(), start=1):\n",
    "        index, data = song\n",
    "        similarity = cosine_similarity([process_lyrics(diary_text, tokenizer, model)], [data['sentiment_vector']])[0][0]\n",
    "        print(f\"{i}st\\t{similarity*100:.2f}% similar\\t{data['title']} - {data['artist']}\")\n",
    "\n",
    "# Load the music data\n",
    "df_music = pd.read_csv('library/music_library_model3.csv')\n",
    "\n",
    "# Convert the strings back to arrays\n",
    "df_music['sentiment_vector'] = df_music['sentiment_vector'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "recommend_songs(diary_text)[['title', 'artist']].to_csv(f\"model3{seed}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved FastText model\n",
    "loaded_fasttext_model = FastText.load(\"data/trained_fasttext_model\")\n",
    "\n",
    "# Function to compute average FastText vector for a text\n",
    "def text_to_avg_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return None\n",
    "    \n",
    "    return sum(word_vectors) / len(word_vectors)\n",
    "\n",
    "# Load lyrics from the CSV file\n",
    "lyrics_df = pd.read_csv('library/music_library.csv')\n",
    "\n",
    "# Compute average FastText vectors for lyrics and store them with song identifier\n",
    "song_vectors = {}\n",
    "for index, row in lyrics_df.iterrows():\n",
    "    lyrics_vector = text_to_avg_vector(row['lyrics'], loaded_fasttext_model)\n",
    "    if lyrics_vector is not None:\n",
    "        song_identifier = row['title'] + ' - ' + row['artist']\n",
    "        song_vectors[song_identifier] = lyrics_vector\n",
    "\n",
    "# Make system to get manual input\n",
    "diary_entry = diary_text\n",
    "\n",
    "# Compute the average FastText vector for the diary entry\n",
    "diary_vector = text_to_avg_vector(diary_entry, loaded_fasttext_model)\n",
    "\n",
    "# Compute cosine similarity between diary entry and song lyrics\n",
    "similarities = {}\n",
    "for song_identifier, lyrics_vector in song_vectors.items():\n",
    "    similarity = cosine_similarity(diary_vector.reshape(1, -1), lyrics_vector.reshape(1, -1))\n",
    "    similarities[song_identifier] = similarity[0][0]\n",
    "\n",
    "# Rank songs based on similarity and recommend top N songs\n",
    "N = bestnum\n",
    "# N = 5\n",
    "recommended_songs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "songinfo = [sublist[0] for sublist in recommended_songs]\n",
    "\n",
    "# Split each string in the list into a \"songname\" and an \"artist\"\n",
    "songs_split = [song.split(' - ') for song in songinfo]\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(songs_split)\n",
    "\n",
    "# Specify the file path (or name if the file is in the same directory as your script)\n",
    "file_path = f'model4{seed}.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file without header and index\n",
    "df.to_csv(file_path, index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match & Shuffle process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = input(\"Input seed(num): \")\n",
    "\n",
    "# Load the songs from the CSV files\n",
    "model1 = pd.read_csv(f'model1{seed}.csv', header=None)\n",
    "model1['model'] = 'model1'\n",
    "\n",
    "model2 = pd.read_csv(f'model2{seed}.csv', header=None)\n",
    "model2['model'] = 'model2'\n",
    "\n",
    "model3 = pd.read_csv(f'model3{seed}.csv', header=None)\n",
    "model3['model'] = 'model3'\n",
    "\n",
    "model4 = pd.read_csv(f'model4{seed}.csv', header=None)\n",
    "model4['model'] = 'model4'\n",
    "\n",
    "# Concatenate into a single DataFrame\n",
    "all_songs = pd.concat([model1, model2, model3, model4], ignore_index=True)\n",
    "\n",
    "# Shuffle the songs\n",
    "all_songs = shuffle(all_songs, random_state=1)\n",
    "\n",
    "# Assign each song a unique ID\n",
    "all_songs['id'] = range(1, len(all_songs) + 1)\n",
    "\n",
    "# Now, all_songs is your list of 40 songs, with randomly ordered with id of 01 to 40\n",
    "\n",
    "def evaluate_selection(selection):\n",
    "    # Find the model each selected song came from\n",
    "    selected_models = all_songs[all_songs['id'].isin(selection)]['model']\n",
    "    \n",
    "    # Count how many songs from each model were selected\n",
    "    model_counts = selected_models.groupby(selected_models).size()\n",
    "    \n",
    "    # Reindex to specify the order and include models with zero count\n",
    "    model_counts = model_counts.reindex(['model1', 'model2', 'model3', 'model4'], fill_value=0)\n",
    "\n",
    "    # Total number of selections\n",
    "    total_selections = len(selection)\n",
    "    \n",
    "    # Prepare the output string\n",
    "    output = f\"For your selection, {selection}\\nFollowing scored....\\n\"\n",
    "    \n",
    "    max_model = \"\"\n",
    "    max_count = 0\n",
    "    for model, count in model_counts.items():\n",
    "        percentage = (count / total_selections) * 100\n",
    "        output += f\"{model.capitalize()}: {count} ({percentage:.2f}%)\\n\"\n",
    "\n",
    "        if count > max_count:\n",
    "            max_model = model\n",
    "            max_count = count\n",
    "\n",
    "    output += f\"{max_model.capitalize()} will be the best.\"\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "############################## Shuffled song list ##############################\n",
      "################################################################################\n",
      " id                                                    0                                          1\n",
      "  1                      내게만 일어나는 일 (Feat. MC 메타 Of 가리온)                                        이승환\n",
      "  2              사랑엔 조건이 없습니다 (법무부 'Good-Bye 학교폭력' 캠페인송) 부활, 걸스데이, 배기성, 백청강, 손진영, 이성욱, 이태권, 정단, 최재훈\n",
      "  3 내가 너의 오아시스가 되어줄께 5 (괴물소년) (Feat. HEX Of Vanila CIty)                                         팻두\n",
      "  4                                       지난날(Feat.재주소년)                                        김윤희\n",
      "  5                                              나의 매일에게                                     안녕하신가영\n",
      "  6                                              27살의 고백                                        윤딴딴\n",
      "  7                          BENZ (Feat. Jibin of Y2K92)     짱유 (JJANGYOU), Jflow, HYPNOSIS THERAPY\n",
      "  8                                      Falling In Love                                         청안\n",
      "  9                            Find My True Self Part II                        엠씨더맥스 (M.C the MAX)\n",
      " 10                                            가장 쉬운 이야기                                       옥상달빛\n",
      " 11                                            너를 만나는 계절                                    단칸방 로맨스\n",
      " 12                                                텅 빈 방                                        정유빈\n",
      " 13                       나와 같은 사랑을 원했던 게 아냐 (Feat. 현신영)                                         준교\n",
      " 14                                                  실패담                                        김제형\n",
      " 15                             Melody (2021 Remastered)                                        김동률\n",
      " 16                                                  하얀비                     행주, 스텔라장 (Stella Jang)\n",
      " 17                                                  중2병                                YoBoy (강요셉)\n",
      " 18                                                   추락                                BrokenTeeth\n",
      " 19                                     마지막 인사 (Goodbye)                                        정재호\n",
      " 20                                              서로를 안으며                                  겸 (GYE0M)\n",
      " 21                                                나의 여름                                        이예린\n",
      " 22                                                  자유인                  원 모어 찬스 (one more chance)\n",
      " 23                                                  반대편                                         이적\n",
      " 24                                             사랑했던 경우엔                                     9와 숫자들\n",
      " 25                                                이상한 애                                        정유빈\n",
      " 26                                           LAST DANCE                               BIGBANG (빅뱅)\n",
      " 27  Be My Own Man (Song by Uzuhan, Brother Su, Sam Ock)                                    KozyPop\n",
      " 28                                                  출항사                                        뱃사공\n",
      " 29                                        Kiss And Tell                                       검정치마\n",
      " 30                                                  저글링                                       국카스텐\n",
      " 31                                       공감 (Feat. 서영은)                                        정준영\n",
      " 32                                             내 사랑 못난이                                        윤종신\n",
      " 33                                       잠자리 (feat. 예은)                           oceanfromtheblue\n",
      " 34                               어떤 이의 꿈 (원곡가수 봄여름가을겨울)                                        김건모\n",
      " 35                   인생네컷2 (Song By Tamiz) (Prod. SUDI)                                    KozyPop\n",
      " 36                           Empty Station (Feat. Chan)                                 고래 (GORAE)\n",
      " 37                                                 겨울 끝                                   반하나, 하준석\n",
      " 38                                     Still loving you                           주예인, 새봄 (saevom)\n",
      " 39                                                살고 싶어                                   오창석, 서하준\n",
      " 40                                 하나되어 (Original Ver.)                            Various Artists\n",
      "################################################################################\n",
      "Your Selection(id) is: [3, 17, 27, 36, 28, 18, 34, 12, 25, 9, 4, 7, 22, 15, 23, 14, 40, 30]\n",
      "################################################################################\n",
      "For your selection, [3, 17, 27, 36, 28, 18, 34, 12, 25, 9, 4, 7, 22, 15, 23, 14, 40, 30]\n",
      "Following scored....\n",
      "Model1: 3 (16.67%)\n",
      "Model2: 7 (38.89%)\n",
      "Model3: 3 (16.67%)\n",
      "Model4: 5 (27.78%)\n",
      "Model2 will be the best.\n"
     ]
    }
   ],
   "source": [
    "# Display the songs and their IDs\n",
    "print('#'*120)\n",
    "print('#'*50 + \" Shuffled song list \" + '#'*50)\n",
    "print('#'*120)\n",
    "print(all_songs[['id', 0, 1]].to_string(index=False))\n",
    "print('#'*80)\n",
    "\n",
    "try:\n",
    "    selection = input(\"Input best 10: \").split(',')\n",
    "    selection = [int(s) for s in selection]\n",
    "    print(\"Your Selection(id) is:\",selection)\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Test the function with a selection\n",
    "    print(evaluate_selection(selection))\n",
    "except:\n",
    "    print(\"canceled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
