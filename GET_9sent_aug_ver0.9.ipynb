{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UgNIl_H7qzDZ"
      },
      "source": [
        "# KoBERT 사용하기(HuggingFace Library)\n",
        "- KoBERT(SKT)도 있고 KorBERT(ETRI)도 있고 KoBART, KoGPT도 있는 것 같다. 어떤 걸 써야 하지?! \n",
        "- ETRI를 활용한 KorBertSum \n",
        "  - https://velog.io/@raqoon886/KorBertSum-SummaryBot \n",
        "  - https://github.com/raqoon886/KorBertSum\n",
        "- KoBART를 이용한 요약\n",
        "  - http://blog.ju-ing.co.kr/posts/KoBART-summarization/ \n",
        "  - https://younghwani.github.io/posts/kobart-summary-3/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Direct Access를 위한 data preprocess functions 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def del_bracket(s):\n",
        "  pattern = r'\\([^)]*\\)'  # ()\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\[[^)]*\\]'  # []\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\<[^)]*\\>'  # <>\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\{[^)]*\\}'  # {}\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  return s\n",
        "\n",
        "def del_special_num(s):\n",
        "  pattern = r'[^a-zA-Z가-힣]'\n",
        "  s = re.sub(pattern=pattern, repl=' ', string=s)\n",
        "\n",
        "  return s\n",
        "\n",
        "def del_unit(s):\n",
        "  units = ['mm', 'cm', 'km', 'ml', 'kg', 'g']\n",
        "  for unit in units:\n",
        "    s = s.lower() # 대문자를 소문자로 변환\n",
        "    s = s.replace(unit, '')\n",
        "  return s\n",
        "\n",
        "def del_whitespace(s):\n",
        "  return \" \".join(s.split())\n",
        "\n",
        "import io \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "def del_stopwords(s):\n",
        "  stopwords = open(\"data/stopwords.txt\", 'r', encoding=\"utf-8\").read().split()\n",
        "  #print(stopwords)\n",
        "  s_o=s.split()\n",
        "  s_f=[]\n",
        "  for w in s_o:\n",
        "    if w.strip() not in stopwords:\n",
        "      s_f.append(w.strip())\n",
        "  return \" \".join(s_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#modelname = \"beomi/KcELECTRA-base\"\n",
        "modelname = \"klue/bert-base\"\n",
        "#modelname = \"beomi/kcbert-base\"\n",
        "#modelname = \"monologg/kobert\"\n",
        "#modelname = \"kakaobrain/kogpt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbrYG0EawSjl",
        "outputId": "853ff6ae-3a25-4ac8-fcc0-f5aa957afb4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup, BertTokenizer, BertModel\n",
        "\n",
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "#BERT 모델 불러오기\n",
        "bertmodel = BertModel.from_pretrained(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I57c2QR5wjVW",
        "outputId": "061153f4-d2c4-43b9-e8b3-4c842aa48292"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#filter_values = ['기쁨', '슬픔', '분노', '불안']\n",
        "#filter_values = ['기쁨', '불안', '슬픔', '분노', '상처']\n",
        "\n",
        "train=pd.read_excel('data/Training.xlsx')\n",
        "train=pd.DataFrame({'s1': train['감정_대분류'], 's2': train['감정_소분류'], 't1': train['사람문장1']})\n",
        "#train = train[train['s1'].isin(filter_values)]\n",
        "train.head()\n",
        "\n",
        "val=pd.read_excel('data/Validation.xlsx')\n",
        "val=pd.DataFrame({'s1': val['감정_대분류'], 's2': val['감정_소분류'], 't1': val['사람문장1']})\n",
        "#val = val[val['s1'].isin(filter_values)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using other source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_1_path = \"data/single.xlsx\"\n",
        "data_2_path = \"data/continuous.xlsx\"\n",
        "# filter_values = ['공포', '분노', '슬픔', '행복', '혐오']\n",
        "filter_values = ['공포', '놀람', '분노', '슬픔', '행복', '혐오', '중립']\n",
        "\n",
        "data_1 = pd.read_excel(data_1_path)\n",
        "data_1 = pd.DataFrame({'t1': data_1['Sentence'], 's1': data_1['Emotion']})\n",
        "data_1 = data_1[data_1['s1'].isin(filter_values)] # filter out neutral/other typos\n",
        "\n",
        "data_2 = pd.read_excel(data_2_path, usecols=\"B:C\", skiprows=2, header=None, names=[\"t1\", \"s1\"])\n",
        "data_2 = data_2[data_2['s1'].isin(filter_values)] # filter out neutral/other typos\n",
        "\n",
        "# print(data_2[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXDroTv_wxOG",
        "outputId": "a9afe4a9-bb78-407c-ca8e-ec15be7c913f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['일은 끝이 없을까 화가 난다', 5], ['달에 급여가 깎였어 물가는 오르는데 월급만 자꾸 깎이니까 너무 화가', 5], ['회사에 신입이 들어왔는데 말투가 거슬려 애를 매일 봐야 한다고 생각하니까 스트레스 받아', 5], ['직장에서 막내라는 이유로 나에게만 온갖 심부름을 시켜 일도 데 정말 분하고 섭섭해', 5], ['전 입사한 신입사원이 나를 무시하는 같아서 너무 화가', 5], ['직장에 다니고 있지만 시간만 버리는 거 같아 진지하게 진로에 대한 고민이 생겨', 5], ['성인인데도 진로를 아직도 못 정했다고 부모님이 노여워하셔 나도 섭섭해', 5], ['퇴사한 지 됐지만 천천히 직장을 구해보려고', 1], ['졸업반이라서 취업을 생각해야 하는데 지금 너무 느긋해서 이래도 되나 싶어', 2], ['요즘 직장생활이 너무 편하고 좋은 같아', 1]]\n"
          ]
        }
      ],
      "source": [
        "#기쁨 불안 당황 슬픔 분노 상처 \n",
        "s12label={'기쁨':1, '불안': 2, '당황': 3, '슬픔': 4, '분노': 5, '상처': 6}\n",
        "# s12label={'기쁨':0, '불안': 1, '슬픔': 1, '분노': 1, '상처':1}\n",
        "train_l=[]\n",
        "\n",
        "for t, s in zip(train['t1'], train['s1']):\n",
        "  t=del_bracket(t)\n",
        "  t=del_special_num(t)\n",
        "  t=del_whitespace(t)\n",
        "  t=del_stopwords(t)\n",
        "  train_l.append([t, s12label[s.strip()]])\n",
        "\n",
        "for t, s in zip(val['t1'], val['s1']):\n",
        "  t=del_bracket(t)\n",
        "  t=del_special_num(t)\n",
        "  t=del_whitespace(t)\n",
        "  t=del_stopwords(t)\n",
        "  train_l.append([t, s12label[s]])\n",
        "\n",
        "# for additional data: 공포 놀람 분노 슬픔 행복 혐오 중립\n",
        "s12label = {'중립': 0, '행복': 1, '공포' : 2, '슬픔': 4, '분노': 5, '놀람': 7, '혐오': 8}\n",
        "# s12label = {'행복': 0, '공포': 1, '분노': 1, '슬픔': 1, '혐오': 1}\n",
        "for t, s in zip(data_1['t1'], data_1['s1']):\n",
        "  t=del_bracket(t)\n",
        "  t=del_special_num(t)\n",
        "  t=del_whitespace(t)\n",
        "  t=del_stopwords(t)\n",
        "  train_l.append([t, s12label[s]])\n",
        "\n",
        "for t, s in zip(data_2['t1'], data_2['s1']):\n",
        "  t=del_bracket(t)\n",
        "  t=del_special_num(t)\n",
        "  t=del_whitespace(t)\n",
        "  t=del_stopwords(t)\n",
        "  train_l.append([t, s12label[s]])\n",
        "\n",
        "print(train_l[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48618 0.29631000863877577 0.3290756509934592 0.20165370850302358 0.35721337776132295 0.4054259739191246 0.2087704142498663 0.22139948167345427 0.11619153399975318\n"
          ]
        }
      ],
      "source": [
        "data_augment_flag = [1, 5, 5, 9, 5, 4, 9, 9, 9]\n",
        "data_augment_limit = [150000, 150000, 150000, 150000, 150000, 150000, 150000, 150000, 150000]\n",
        "# Separate the data by class\n",
        "data_flag_0 = [sample for sample in train_l if sample[1] == 0]\n",
        "data_flag_1 = [sample for sample in train_l if sample[1] == 1]\n",
        "data_flag_2 = [sample for sample in train_l if sample[1] == 2]\n",
        "data_flag_3 = [sample for sample in train_l if sample[1] == 3]\n",
        "data_flag_4 = [sample for sample in train_l if sample[1] == 4]\n",
        "data_flag_5 = [sample for sample in train_l if sample[1] == 5]\n",
        "data_flag_6 = [sample for sample in train_l if sample[1] == 6]\n",
        "data_flag_7 = [sample for sample in train_l if sample[1] == 7]\n",
        "data_flag_8 = [sample for sample in train_l if sample[1] == 8]\n",
        "data_flag = [data_flag_0, data_flag_1, data_flag_2, data_flag_3, data_flag_4, data_flag_5, data_flag_6, data_flag_7, data_flag_8]\n",
        "print(len(data_flag_0),len(data_flag_1)/len(data_flag_0),len(data_flag_2)/len(data_flag_0),len(data_flag_3)/len(data_flag_0),len(data_flag_4)/len(data_flag_0),len(data_flag_5)/len(data_flag_0),len(data_flag_6)/len(data_flag_0),len(data_flag_7)/len(data_flag_0),len(data_flag_8)/len(data_flag_0))\n",
        "augmented_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training was done. used memory 1.387 Gbmory 1.146 Gb\n",
            "all cohesion probabilities was computed. # words = 3508\n",
            "all branching entropies was computed # words = 66604\n",
            "all accessor variety was computed # words = 66604\n"
          ]
        }
      ],
      "source": [
        "# For unbalanced-class\n",
        "import random\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "# Initialize and train WordExtractor\n",
        "word_extractor = WordExtractor(min_frequency=100,\n",
        "                               min_cohesion_forward=0.05, \n",
        "                               min_right_branching_entropy=0.0)\n",
        "corpus = [sample[0] for sample in train_l]\n",
        "word_extractor.train(corpus)\n",
        "words = word_extractor.extract()\n",
        "\n",
        "# Extract nouns from the words\n",
        "nouns = list(words.keys())\n",
        "\n",
        "# Now we'll define our text augmentation function using these nouns\n",
        "def augment_korean_text(text, nouns, num_aug_samples=1):\n",
        "    augmented_samples = [text]  # Start with the original text\n",
        "    tokenizer = LTokenizer()\n",
        "\n",
        "    for _ in range(num_aug_samples):\n",
        "        # Replace some nouns with other random nouns\n",
        "        words = tokenizer.tokenize(text)\n",
        "        augmented_words = [\n",
        "            word if word not in nouns else random.choice(nouns)\n",
        "            for word in words\n",
        "        ]\n",
        "        augmented_samples.append(\" \".join(augmented_words))\n",
        "\n",
        "    return augmented_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for datasets in data_flag:\n",
        "    count_sample = 0\n",
        "    for sample in datasets:\n",
        "        text = sample[0]\n",
        "        label = sample[1]\n",
        "        augmented_texts = augment_korean_text(text, nouns, num_aug_samples=data_augment_flag[i])\n",
        "        for augmented_text in augmented_texts:\n",
        "            if count_sample < data_augment_limit[i]:\n",
        "                augmented_data.append([augmented_text, label])\n",
        "                count_sample += 1\n",
        "    i += 1\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(augmented_data)\n",
        "\n",
        "# Convert the data to a DataFrame and save it to a new Excel file\n",
        "df_augmented = pd.DataFrame(augmented_data, columns=[\"document\", \"label\"])\n",
        "df_augmented.to_excel(\"data/train_l_augmentedplus_9sent.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the data from the Excel file and convert it back to a list\n",
        "df_augmented = pd.read_excel(\"data/train_l_augmented_9sent.xlsx\")\n",
        "train_l = df_augmented.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4a724Z1AyfCN"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len):\n",
        "        self.sentences = [tokenizer.encode_plus(i[sent_idx], add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.tensor(self.sentences[idx]['input_ids'], dtype=torch.long)\n",
        "        valid_length = torch.tensor(len(self.sentences[idx]['input_ids']), dtype=torch.long)\n",
        "        segment_ids = torch.tensor(self.sentences[idx]['token_type_ids'], dtype=torch.long)\n",
        "        attention_mask = torch.tensor(self.sentences[idx]['attention_mask'], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return token_ids, valid_length, segment_ids, attention_mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MHn8vxqhyjEi"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 50\n",
        "max_grad_norm = 1\n",
        "log_interval = 10\n",
        "learning_rate =  1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "317228 79307\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preparing the dataset\n",
        "dataset_train, dataset_test = train_test_split(train_l, test_size=0.2, random_state=0)\n",
        "print(len(dataset_train), len(dataset_test))\n",
        "len_data = len(dataset_train) + len(dataset_test)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
        "\n",
        "# Ensure all data is a string\n",
        "dataset_train = [[str(i[0]), i[1]] for i in dataset_train]\n",
        "dataset_test = [[str(i[0]), i[1]] for i in dataset_test]\n",
        "\n",
        "# Creating instances of the BERTDataset class for train and test sets\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2M9tl9zBE5",
        "outputId": "1b7f9cda-5451-4e4b-86a4-db4be2023e1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([    2,   850,  2052,  1408,  2315, 19655,  1201,  2660,  2116,   721,\n",
              "         22757,  1513,  2075,  2182,     3,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor(64),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(2, dtype=torch.int32))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "05rNHKrOzDmS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "omVeUrTWzJRD"
      },
      "outputs": [],
      "source": [
        "# Set num_of_classes\n",
        "num_classes = 9\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=num_classes,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        pooler = outputs[1]\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1240/1240 [00:02<00:00, 423.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({5: 11795, 4: 10293, 0: 9683, 2: 9658, 7: 8749, 1: 8592, 6: 8169, 3: 7897, 8: 4471})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "labels = []\n",
        "for _, (_, _, _, _, label) in enumerate(tqdm(test_dataloader)):\n",
        "    labels.extend(label.numpy())\n",
        "print(Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0hvnfJRAzOgv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Chiwoong\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x29d01028670>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.7).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(out, label):\n",
        "    predicted = out.argmax(1)\n",
        "    correct = (predicted == label).sum().item()\n",
        "    total = label.size(0)\n",
        "    accuracy = correct / total\n",
        "    # print(f'Predicted: {predicted}, Label: {label}, Correct: {correct}, Total: {total}, Accuracy: {accuracy}')\n",
        "    return accuracy\n",
        "\n",
        "    \n",
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For time-based tensorboard store\n",
        "num_augmentations=\"134\"\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "log_dir = f'./logs/{timestamp}-model{modelname}-data{len_data}/a{num_augmentations}-class{num_classes}-batch{batch_size}-lr{learning_rate}-epoch{num_epochs}-maxlen{max_len}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZYcqio-_yNGy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a36392dcfb04404caac7fc1535c8da14",
            "2b4daa450c6c40dea1dea0073949a217",
            "c51180644662452a8f70dc47210190b3",
            "999d407a2c45438c8ba048caf6ec7af7",
            "9b4aa1105f6e4f35a88caac2a20d305a",
            "537d419eea514f0e8547f51ff2d289ba",
            "a88d7de22bc44d5c9be9e448ae189230",
            "26a21eebafe44d85b73db64a07a2bbf7",
            "a1f26aef797447b09f502a6bdccc5438",
            "89fa0a48474f4af9be7f69681df0a470",
            "71d8404cdac54ffcaf6854ffd9f99d1b",
            "42645fc5984743d0b835c24f83bcd008",
            "b22b471bfe91493082160211f8ddfcb2",
            "c0579052144147a780734481b5560bdd",
            "9fa6e171beea4283a53be22dd3b22cb5",
            "64bf220824874e4b847f1b4d44d1e2df",
            "685b49164dba4e8ca2f11c6049b75b4a",
            "931dd9c46f6148879b234414a490edc7",
            "115a8cc8a6bd451aa3b0ad4c31fa10d9",
            "69f4c7d6f7ce440eafc4a682d07663c1",
            "9c259640dde74b108880ff64bb3af738",
            "060777b420c541a79d53f68296b169b4",
            "d582464602794bb3b4e928179bce33ca",
            "61023530b3db4eb7a5c3d0a1586b5e59",
            "4c1444173e0d41a7bc6f2aa133155114",
            "27bbe64e53d647f3b873c21292000da2",
            "322446e27fb94835b0c0a6763f7052e7",
            "d927883cf9984bcb87a51bb608b2bd13",
            "221e081afee145a7a7b614839ca435e3",
            "f23fae093596484ebf3a2fc299f79a8b",
            "145bf764473440c8a3cea3dfd67521b8",
            "13fde8cee5d44843bd9577f6de9a04b2",
            "260fbad624e4481f9791898787b06e1e",
            "953cc4dbe6cc4a64b68444c42f25302c",
            "8a5efa2c7bfa477884a62780a66adfb6",
            "668b6eb8baee4dbfa0f18a2eea34bd38",
            "44cb42f2b4504329abcfd6afd8975691",
            "b15d2d72797b4c71912a093e77a870f1",
            "31d433deb855466cb715d998392a60dc",
            "c862a11a18144793a2ec8e74a1df9762",
            "7ec2c3186b8541fcbb60655f93ec8f3d",
            "19fcce698cc44c4596176e6cb014edee",
            "cca81033bcf04ecaac09970d2c5b1f1b",
            "5fbae9dcc0814cfdb639fa319ece472f",
            "71a1bb516a6e43a1b51878278a948276",
            "2888b901d070495ebc440e421aef9c71",
            "d780da5a1359436aa043cdac58b379cd",
            "ad91e66b25fd42f3bb1a28a12940aab8",
            "8cb49115ff534fd1b7697ce7eef7cf1a",
            "bef23c7fb87646f6848929f6c1fecf4c",
            "a4d09608760b48deb7934871ee6e11b9",
            "b4b135938593488a8a494680f39e4969",
            "e48a352d042b4683ba6c4ad766de9d46",
            "204ffa7119ac4179be97d4c8cfea73a3",
            "7fc285808c19458092dc068dc1ef1e8a",
            "e0e275aee5d043058a3615df49beac56",
            "4634fc0487b943f9b51caf2f365ed763",
            "e99987e52ddd48bdbf58dfa49d1f7337",
            "c26cfffbd68840409f70786b2d22d077",
            "89b5ddb8e66d4c3a82b7b75b12bfcde4",
            "b6520da42b254a2bbb7f192726defc78",
            "49d5a7383754464cac5b16198bfc0bff",
            "b417e386209a4fb980706841d902f6ae",
            "dcd8f3b4901e46ab901f9481033f3d70",
            "f24418eefbf549daba2b960e27d0da6e",
            "9967b64021b5495e8a55931695d60911",
            "344321b6b58d42f4b8b1772f4bd64aa9",
            "dea7f410ea664215b26150ccff543916",
            "36ae51fcb62841d393f2746328568aee",
            "088d5ca1d8274bc8b0ac7dc5d32bf4e3",
            "37768cb504674277aa33155bea00f088",
            "d06ef49d80284d4e857d3de70ab4e42a",
            "35b45ccb7cd94303a159515b413d5d4e",
            "5d472783fc3b43dab350fa544300db32",
            "6ec9a6e2f5934a16a916ccf914d92b82",
            "db4b16e660014d98beca81dd935f2f7d",
            "b20e948a56144e63b563668d0311c016",
            "d99b28d2748049bcbbf35e2bd991ac3b",
            "215620ac6e684ef9a436f1f390331893",
            "eb25f5fe42824db28e5e8734c743aaf0",
            "5c88e00526db4fa8889f0c25391cb11f",
            "c4ef2485141948978983839d1c4995bb",
            "d9ba025ca762447f8e1108d8a8ac21ce",
            "e1481d88aaec476bae81d700176ff07e",
            "e04f46bc714640eaab9d03d2fd585da6",
            "f0db2b185dd94e86b8f62dcdbe71c852",
            "fc2671081e90437380ef473b5149d1b7",
            "43151f3d4d524ced8e0f879d993e827e",
            "d5600738377440608359b54f941d8eed",
            "a39b87a9637f4f2fa93af837b1d660fd",
            "ec508519c15c43baa87ba078c7624704",
            "7af5b13480bf4d7db24296421c8b1231",
            "dbbe53da775b47c699360b5c6b703b6d",
            "fbdc19ce8d794af88c1cab00c755bdd3",
            "97c8baf0686b4609a3e8020bfa0b723e",
            "2af37427a877432794a60b060b4db831",
            "0ea405aae134456b8af80bdbfbd7d995",
            "d8c89cd3e549455b865c974364af1fbb",
            "3120ef27193e4f8ea5795a7e2430a6b7",
            "e65cbf2d6c7649a984d2a9e646eaf63f",
            "90c2f76aaa5447188a4f7c5ad012baac",
            "76dd588e3d4e4bfa90ccf8b8acfa4b25",
            "1f654c9f2c6e431d8f875633bc99c861",
            "aff6af6e08a04e86a5da811a805e818a",
            "a8f219d8ca104cae888b1c524037f93b",
            "6619eeed7b9e42a18e0b3cf0e01c4c8b",
            "e665328c115742b7b49b18631f133bb9",
            "a9ad0cc9c942408bb0f769dc7f44156f",
            "e41ea261063947a29ccd7eda19a830a9",
            "2d45ac4bb92847059142ceabfc8796ad",
            "4e2fdc91aa9d462caae6ec9d7dec4757",
            "2751e6becf1e48b492c9015b375d660c",
            "5dc5b2dfa64e40e49d4cbf2f1864ff9d",
            "06c7cdfeb0ba45ef85ea6b45a8c44720",
            "93977f619a844e6db041b44aeca987b4",
            "f4e0009d37454db8b1a615adf92f7238",
            "16ae5377841c49d5aea7933488075eb8",
            "f787c6345cac4988878c348e839d4f32",
            "87482c46c88441339045f9782ffff869",
            "565961d186404d9a937922a3f403842a",
            "f480cc1a318d454191e333c259c637ce",
            "9f784b2777e341d587d67f6b1f1bfdb3",
            "757370b677b940b7a6e7cc5aed1d806f",
            "96e3d768b14c4ee4987df3c201b35812",
            "58ec95d0eb2f4855bc8701f4e66be41a",
            "50f71735ccfc4d49b68a94ed64a6f0db",
            "5983abd90c1547f995c60d718ad3a6df",
            "11737bff28844599822a9a007a4b26a8",
            "77f2a62ec87542de93972988ce42b981",
            "ec0fd14d74f04df4b8c73cfa70623886",
            "6578b120201544ee8a66bed3feb0bbe2",
            "f299f57658da45d99d5d9fa6995dec8d",
            "ef61101d2e7b4bb19bdbad8b80aedb6d",
            "3626d0bdd8e14904bd70784f6fadde7c",
            "af23384ffcc54386b3839964f1df01d2",
            "2571eaecd6b74be8b0f40160a23de443",
            "7334a11d35514608b188159e69717f9c",
            "8bcbf5353ddb41f58b3498e00fe31bd9",
            "d45e7cefc7ec4cdaa7661439a3ad88ad",
            "b24b64b59829474e8649d7745e135197",
            "8b11f7768d2048c281c84c222d505b5a",
            "68402970bb9245f1bc6ac074203782c8",
            "c105ca9f1cc0450092ae65a59d3138d6",
            "df3620ce9be44df89fbb87108b03ba33",
            "a6a790caa05149e68918d29e1264c2ad",
            "df6cc5f83a74495ea09ae03eae66b719",
            "32debaf7b0844c718d44d376322ca725",
            "73ea1317f8d34abcb0320fd0ef5705a1",
            "97e2d0233d3c48428038fac4d94edf38",
            "28aac017ffca4c1b92fef83a9827c7f9",
            "c4f781dadcd34ec0990956ad12ac629d",
            "54a2659b81a8425b9b83b6c27c1988c3",
            "a6cafc71727e41f6a8e3e6f49969479d",
            "b37530d30f7b46dba8d8105decca6d8b",
            "efb36c4dc9a543e5906770a0593e0b4a",
            "8a4c9a1807224276b6e4c1aee2ae7a84",
            "a09804bf03934a5cac3cb70369d2864a",
            "0ad034620c0a48a085a8cffd1f8f5d5b",
            "b37560efa87a4499ab4f3a1f85b562b4",
            "67eead90e3c94d4fba4d02aec123fe6c",
            "4080035014494caeb4159afd14728437",
            "6075a088e8ec4a32a9779a35ec7d1fa2",
            "4080cc07e9d64f43943cefd02a079fa8",
            "7193a948f3c249e2950a20fcc635d86b",
            "9ef86f8d144f48148270f3cea7aefab0",
            "8a896aa4d40146eb916942590c14681e",
            "1d8c302064b147aaa863ae401d4a0ccd",
            "ddaf61393fc74048b08c5ed8e6169ba3",
            "f9e89d1611b74a16bbce7d1681e7b59a",
            "1af356108c274cc4b412c2cb54de0a03",
            "8501b75f202642a0aa57db3f7d0396ef",
            "466a8ec9ba054731b8cc7c1d462485ff",
            "c2fa5988320b4d34815a88cb959ff23a",
            "c5d8e1ab5cdb4f1992d6442964f151cc",
            "358e3a615e8a43148f2356c9c74ae4ff",
            "6a26142236414fca9cb7c1c94785bf47",
            "d5873c57c49d40819ec57bbb98fdc939",
            "de065bc6691b4e8d9d9e78c73b696f25",
            "74b7bae2715b44a781690941156269e0",
            "4ded12fce21f479480314fcbe50304ef",
            "f919bda4e2b64ae4a8d79a493146afdd",
            "26383e1a367846dd935553dd05a22c6f",
            "333c3c2696ba4686aed6f5337325e5da",
            "3b905aaa4ba145429cd5a54775380173",
            "3ec3f6b1ed1e444bb5adeb8b1893d9ef",
            "e05885faad1441448e813b8a6a1502ef",
            "64165ae28252483497538155a866b8de",
            "e4ebdcdf2de94bcfa079eac6007bddd6",
            "fc876d00857742649445719641ae234d",
            "b66068b5d54d4898a6dbcf1a203cff38",
            "1cb24bf2ffc944df959d9fde0dcc1a00",
            "37a971f7ffc14fb68b26b89160d64f32",
            "0c5c64d2e05247a88eef8c050e44ee39",
            "4cef287096734ab9be76f2f099fddb90",
            "291d5237a3b7419fabea1f7cadf554f2",
            "d97c4522f72246bab603477177652ebd",
            "b51a078097f446738f377756597ee609",
            "c453ad3af78143a4b9d8900c54ec0d9f",
            "e12cf108a5e34631a7c5dc406cdf8713",
            "a74c3e5a57364e5495f775469fd76fc7",
            "0c709a5117ae413c84915bf6de80df9b",
            "e3c078ad1d2f479e9efed2fbcca62beb",
            "b14f4f87d55042399421a54eaf026b16",
            "abac16e958ff42cfb023719e8e310a17",
            "0e05e3d70f3b4bfab0f92e8ff08aecb6",
            "c5e1719cb7b945918d0d40a245464735",
            "26e7ea617980414e90e62ede2c2b7da2",
            "7c8adb0232cc42488afb3b895d5ce172",
            "5acd05b9556243719a4ef6174391aff0",
            "609a0c954f4e4664a78d41c861908dbf",
            "1c5c7a9b73644a61ae0a4d35f9ef3b5d",
            "d0c63e1c791f4875afa05fbafb7bf6b9",
            "ae95688d91d54d19a1f8d3ebfe3b3468",
            "9653cd35b2c242b8a3805fe278f8f4f2",
            "640f306faa694e85ad3158761d1d1e98",
            "ab97790aca584fb6bcb88cd9143c2567",
            "3bd67a7d339f4d3d808952704a6de9df",
            "2f854ed5b43e43ab883b3e95bb64f79a",
            "6671902ae8264b9e88336a2b31ac5620",
            "bc64a6d8fd4c4356b7785c1544d25248"
          ]
        },
        "id": "M7-vbScXzULP",
        "outputId": "1e7dca78-f191-449a-fc8d-3d6428d3ee4f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2d7c4f994e5419d924e9aaf223a30b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1 loss 2.3819549083709717 train acc 0.078125\n",
            "epoch 1 batch id 11 loss 2.4706594944000244 train acc 0.11789772727272728\n",
            "epoch 1 batch id 21 loss 2.3957042694091797 train acc 0.12872023809523808\n",
            "epoch 1 batch id 31 loss 2.481553554534912 train acc 0.125\n",
            "epoch 1 batch id 41 loss 2.2396457195281982 train acc 0.12233231707317073\n",
            "epoch 1 batch id 51 loss 2.3417234420776367 train acc 0.12193627450980392\n",
            "epoch 1 batch id 61 loss 2.209585666656494 train acc 0.12320696721311475\n",
            "epoch 1 batch id 71 loss 2.2701315879821777 train acc 0.11971830985915492\n",
            "epoch 1 batch id 81 loss 2.363008499145508 train acc 0.12017746913580248\n",
            "epoch 1 batch id 91 loss 2.394219160079956 train acc 0.11916208791208792\n",
            "epoch 1 batch id 101 loss 2.445164442062378 train acc 0.11834777227722772\n",
            "epoch 1 batch id 111 loss 2.274477958679199 train acc 0.11866554054054054\n",
            "epoch 1 batch id 121 loss 2.3957669734954834 train acc 0.11854338842975207\n",
            "epoch 1 batch id 131 loss 2.3108887672424316 train acc 0.11820133587786259\n",
            "epoch 1 batch id 141 loss 2.351228952407837 train acc 0.11768617021276596\n",
            "epoch 1 batch id 151 loss 2.2809500694274902 train acc 0.1181705298013245\n",
            "epoch 1 batch id 161 loss 2.4470903873443604 train acc 0.1171389751552795\n",
            "epoch 1 batch id 171 loss 2.2614359855651855 train acc 0.11851242690058479\n",
            "epoch 1 batch id 181 loss 2.401279926300049 train acc 0.11697168508287292\n",
            "epoch 1 batch id 191 loss 2.4419679641723633 train acc 0.11665575916230367\n",
            "epoch 1 batch id 201 loss 2.3870935440063477 train acc 0.11606032338308457\n",
            "epoch 1 batch id 211 loss 2.417318344116211 train acc 0.11626184834123222\n",
            "epoch 1 batch id 221 loss 2.3272674083709717 train acc 0.11602092760180996\n",
            "epoch 1 batch id 231 loss 2.31964111328125 train acc 0.11715367965367965\n",
            "epoch 1 batch id 241 loss 2.345188856124878 train acc 0.11663641078838174\n",
            "epoch 1 batch id 251 loss 2.304551362991333 train acc 0.11815239043824702\n",
            "epoch 1 batch id 261 loss 2.3576364517211914 train acc 0.1176963601532567\n",
            "epoch 1 batch id 271 loss 2.2204232215881348 train acc 0.117619926199262\n",
            "epoch 1 batch id 281 loss 2.208587646484375 train acc 0.11743772241992882\n",
            "epoch 1 batch id 291 loss 2.2612807750701904 train acc 0.1172680412371134\n",
            "epoch 1 batch id 301 loss 2.3231117725372314 train acc 0.1170577242524917\n",
            "epoch 1 batch id 311 loss 2.2364749908447266 train acc 0.11741358520900322\n",
            "epoch 1 batch id 321 loss 2.266014814376831 train acc 0.11799065420560748\n",
            "epoch 1 batch id 331 loss 2.4188778400421143 train acc 0.11773036253776435\n",
            "epoch 1 batch id 341 loss 2.3577041625976562 train acc 0.11785190615835778\n",
            "epoch 1 batch id 351 loss 2.3777453899383545 train acc 0.11792200854700854\n",
            "epoch 1 batch id 361 loss 2.3538241386413574 train acc 0.11798822714681441\n",
            "epoch 1 batch id 371 loss 2.143263578414917 train acc 0.11842991913746631\n",
            "epoch 1 batch id 381 loss 2.3709988594055176 train acc 0.11843832020997375\n",
            "epoch 1 batch id 391 loss 2.294870138168335 train acc 0.11840632992327366\n",
            "epoch 1 batch id 401 loss 2.3587875366210938 train acc 0.1180252493765586\n",
            "epoch 1 batch id 411 loss 2.3050436973571777 train acc 0.11739659367396593\n",
            "epoch 1 batch id 421 loss 2.3092570304870605 train acc 0.11768853919239905\n",
            "epoch 1 batch id 431 loss 2.296482801437378 train acc 0.11803944315545244\n",
            "epoch 1 batch id 441 loss 2.4170479774475098 train acc 0.11791383219954649\n",
            "epoch 1 batch id 451 loss 2.399930953979492 train acc 0.11838276053215077\n",
            "epoch 1 batch id 461 loss 2.2745344638824463 train acc 0.11856019522776573\n",
            "epoch 1 batch id 471 loss 2.4223787784576416 train acc 0.11856422505307855\n",
            "epoch 1 batch id 481 loss 2.2815194129943848 train acc 0.11899038461538461\n",
            "epoch 1 batch id 491 loss 2.3187031745910645 train acc 0.11892184317718942\n",
            "epoch 1 batch id 501 loss 2.3454909324645996 train acc 0.1187000998003992\n",
            "epoch 1 batch id 511 loss 2.392782211303711 train acc 0.1190068493150685\n",
            "epoch 1 batch id 521 loss 2.3881454467773438 train acc 0.11879198656429943\n",
            "epoch 1 batch id 531 loss 2.2927534580230713 train acc 0.1189677495291902\n",
            "epoch 1 batch id 541 loss 2.268908739089966 train acc 0.11928142329020333\n",
            "epoch 1 batch id 551 loss 2.3864331245422363 train acc 0.11947028130671507\n",
            "epoch 1 batch id 561 loss 2.2981903553009033 train acc 0.1196524064171123\n",
            "epoch 1 batch id 571 loss 2.331780195236206 train acc 0.11963660245183888\n",
            "epoch 1 batch id 581 loss 2.145453453063965 train acc 0.12002474182444062\n",
            "epoch 1 batch id 591 loss 2.332488775253296 train acc 0.12055837563451777\n",
            "epoch 1 batch id 601 loss 2.2605631351470947 train acc 0.12042429284525791\n",
            "epoch 1 batch id 611 loss 2.227010488510132 train acc 0.1207037643207856\n",
            "epoch 1 batch id 621 loss 2.2768852710723877 train acc 0.1209993961352657\n",
            "epoch 1 batch id 631 loss 2.327498197555542 train acc 0.12096374801901744\n",
            "epoch 1 batch id 641 loss 2.2219088077545166 train acc 0.12112421996879875\n",
            "epoch 1 batch id 651 loss 2.222878932952881 train acc 0.12139976958525346\n",
            "epoch 1 batch id 661 loss 2.420421838760376 train acc 0.12114693645990923\n",
            "epoch 1 batch id 671 loss 2.389530658721924 train acc 0.12141393442622951\n",
            "epoch 1 batch id 681 loss 2.298072338104248 train acc 0.12178781204111601\n",
            "epoch 1 batch id 691 loss 2.3536148071289062 train acc 0.1219247467438495\n",
            "epoch 1 batch id 701 loss 2.2402241230010986 train acc 0.12201319543509273\n",
            "epoch 1 batch id 711 loss 2.197582960128784 train acc 0.12236286919831224\n",
            "epoch 1 batch id 721 loss 2.307391881942749 train acc 0.12229108876560332\n",
            "epoch 1 batch id 731 loss 2.3158109188079834 train acc 0.12264876880984953\n",
            "epoch 1 batch id 741 loss 2.3117079734802246 train acc 0.12297570850202429\n",
            "epoch 1 batch id 751 loss 2.3361387252807617 train acc 0.12348119174434088\n",
            "epoch 1 batch id 761 loss 2.3516104221343994 train acc 0.1236654073587385\n",
            "epoch 1 batch id 771 loss 2.329948902130127 train acc 0.12386511024643321\n",
            "epoch 1 batch id 781 loss 2.311863422393799 train acc 0.12385963508322663\n",
            "epoch 1 batch id 791 loss 2.191030502319336 train acc 0.12401232616940581\n",
            "epoch 1 batch id 801 loss 2.23222279548645 train acc 0.12410268414481898\n",
            "epoch 1 batch id 811 loss 2.2225515842437744 train acc 0.12432567817509248\n",
            "epoch 1 batch id 821 loss 2.2793591022491455 train acc 0.1243338915956151\n",
            "epoch 1 batch id 831 loss 2.320049524307251 train acc 0.12452993381468111\n",
            "epoch 1 batch id 841 loss 2.318049907684326 train acc 0.12475847205707491\n",
            "epoch 1 batch id 851 loss 2.2600698471069336 train acc 0.12468786721504113\n",
            "epoch 1 batch id 861 loss 2.209367513656616 train acc 0.125181475029036\n",
            "epoch 1 batch id 871 loss 2.231328010559082 train acc 0.12535878300803674\n",
            "epoch 1 batch id 881 loss 2.2107410430908203 train acc 0.12540791713961408\n",
            "epoch 1 batch id 891 loss 2.25213360786438 train acc 0.1255611672278339\n",
            "epoch 1 batch id 901 loss 2.2829391956329346 train acc 0.1257630410654828\n",
            "epoch 1 batch id 911 loss 2.237365245819092 train acc 0.12578896816684962\n",
            "epoch 1 batch id 921 loss 2.334028959274292 train acc 0.1259500542888165\n",
            "epoch 1 batch id 931 loss 2.2380313873291016 train acc 0.12619159505907626\n",
            "epoch 1 batch id 941 loss 2.2823007106781006 train acc 0.12614572263549415\n",
            "epoch 1 batch id 951 loss 2.2420878410339355 train acc 0.12633083596214512\n",
            "epoch 1 batch id 961 loss 2.335474967956543 train acc 0.12639828303850156\n",
            "epoch 1 batch id 971 loss 2.394911527633667 train acc 0.1267378990731205\n",
            "epoch 1 batch id 981 loss 2.2419705390930176 train acc 0.12691131498470948\n",
            "epoch 1 batch id 991 loss 2.3004300594329834 train acc 0.12692356205852673\n",
            "epoch 1 batch id 1001 loss 2.26261305809021 train acc 0.12709165834165834\n",
            "epoch 1 batch id 1011 loss 2.198909282684326 train acc 0.12727188427299704\n",
            "epoch 1 batch id 1021 loss 2.356431245803833 train acc 0.12732615083251714\n",
            "epoch 1 batch id 1031 loss 2.2443716526031494 train acc 0.12771277885548013\n",
            "epoch 1 batch id 1041 loss 2.3288052082061768 train acc 0.1277767771373679\n",
            "epoch 1 batch id 1051 loss 2.2205865383148193 train acc 0.12798822549952427\n",
            "epoch 1 batch id 1061 loss 2.20766019821167 train acc 0.12807787464655984\n",
            "epoch 1 batch id 1071 loss 2.2010140419006348 train acc 0.12831174136321194\n",
            "epoch 1 batch id 1081 loss 2.0925662517547607 train acc 0.12817992599444958\n",
            "epoch 1 batch id 1091 loss 2.226750135421753 train acc 0.12842289184234648\n",
            "epoch 1 batch id 1101 loss 2.2216897010803223 train acc 0.1285621026339691\n",
            "epoch 1 batch id 1111 loss 2.0729551315307617 train acc 0.12861442394239425\n",
            "epoch 1 batch id 1121 loss 2.3224265575408936 train acc 0.12890276538804638\n",
            "epoch 1 batch id 1131 loss 2.198596239089966 train acc 0.12921363837312114\n",
            "epoch 1 batch id 1141 loss 2.231431484222412 train acc 0.12934103856266432\n",
            "epoch 1 batch id 1151 loss 2.2602219581604004 train acc 0.12939834926151172\n",
            "epoch 1 batch id 1161 loss 2.228095531463623 train acc 0.1296161714039621\n",
            "epoch 1 batch id 1171 loss 2.2015609741210938 train acc 0.12997704953031597\n",
            "epoch 1 batch id 1181 loss 2.201289415359497 train acc 0.12997459779847587\n",
            "epoch 1 batch id 1191 loss 2.3789594173431396 train acc 0.1300771410579345\n",
            "epoch 1 batch id 1201 loss 2.1435048580169678 train acc 0.1303210865945046\n",
            "epoch 1 batch id 1211 loss 2.2463462352752686 train acc 0.13056100330305534\n",
            "epoch 1 batch id 1221 loss 2.2251641750335693 train acc 0.1306562244062244\n",
            "epoch 1 batch id 1231 loss 2.127807378768921 train acc 0.13091490658001625\n",
            "epoch 1 batch id 1241 loss 2.2746694087982178 train acc 0.13105610394842868\n",
            "epoch 1 batch id 1251 loss 2.2191691398620605 train acc 0.13129496402877697\n",
            "epoch 1 batch id 1261 loss 2.195909261703491 train acc 0.13161677240285488\n",
            "epoch 1 batch id 1271 loss 2.197251796722412 train acc 0.13178599527930762\n",
            "epoch 1 batch id 1281 loss 2.286153793334961 train acc 0.13192818110850899\n",
            "epoch 1 batch id 1291 loss 2.1106796264648438 train acc 0.13205606119287375\n",
            "epoch 1 batch id 1301 loss 2.2189157009124756 train acc 0.1322900653343582\n",
            "epoch 1 batch id 1311 loss 2.208632707595825 train acc 0.13237747902364608\n",
            "epoch 1 batch id 1321 loss 2.2738113403320312 train acc 0.1324635692657078\n",
            "epoch 1 batch id 1331 loss 2.1612653732299805 train acc 0.13264228024042074\n",
            "epoch 1 batch id 1341 loss 2.1517274379730225 train acc 0.1327484153616704\n",
            "epoch 1 batch id 1351 loss 2.072575569152832 train acc 0.13296863434492967\n",
            "epoch 1 batch id 1361 loss 2.232102394104004 train acc 0.1332774614254225\n",
            "epoch 1 batch id 1371 loss 2.2826151847839355 train acc 0.13349060904449306\n",
            "epoch 1 batch id 1381 loss 2.135591506958008 train acc 0.13368935553946415\n",
            "epoch 1 batch id 1391 loss 2.1686668395996094 train acc 0.1337504493170381\n",
            "epoch 1 batch id 1401 loss 2.3575918674468994 train acc 0.1340671841541756\n",
            "epoch 1 batch id 1411 loss 2.041374683380127 train acc 0.13422439759036145\n",
            "epoch 1 batch id 1421 loss 2.1293864250183105 train acc 0.13453333919774807\n",
            "epoch 1 batch id 1431 loss 2.1767430305480957 train acc 0.13469601677148846\n",
            "epoch 1 batch id 1441 loss 2.217522144317627 train acc 0.13488896599583622\n",
            "epoch 1 batch id 1451 loss 2.1144843101501465 train acc 0.13516540317022743\n",
            "epoch 1 batch id 1461 loss 2.1714398860931396 train acc 0.13535249828884327\n",
            "epoch 1 batch id 1471 loss 2.121213436126709 train acc 0.13561140380693407\n",
            "epoch 1 batch id 1481 loss 2.1698696613311768 train acc 0.13597231600270088\n",
            "epoch 1 batch id 1491 loss 2.347167491912842 train acc 0.13622359154929578\n",
            "epoch 1 batch id 1501 loss 2.1312124729156494 train acc 0.13634660226515657\n",
            "epoch 1 batch id 1511 loss 2.236961603164673 train acc 0.13658173395102582\n",
            "epoch 1 batch id 1521 loss 2.046295404434204 train acc 0.13666995397764628\n",
            "epoch 1 batch id 1531 loss 2.1773362159729004 train acc 0.1369917537557152\n",
            "epoch 1 batch id 1541 loss 2.1557157039642334 train acc 0.13727895846852692\n",
            "epoch 1 batch id 1551 loss 2.0380985736846924 train acc 0.13760275628626692\n",
            "epoch 1 batch id 1561 loss 2.102695941925049 train acc 0.1379724535554132\n",
            "epoch 1 batch id 1571 loss 2.226604461669922 train acc 0.13827776893698282\n",
            "epoch 1 batch id 1581 loss 2.265592336654663 train acc 0.1385989879822897\n",
            "epoch 1 batch id 1591 loss 2.1530771255493164 train acc 0.1389358108108108\n",
            "epoch 1 batch id 1601 loss 2.213405132293701 train acc 0.13907323547782635\n",
            "epoch 1 batch id 1611 loss 2.1463606357574463 train acc 0.1390828677839851\n",
            "epoch 1 batch id 1621 loss 2.1133065223693848 train acc 0.13949722393584207\n",
            "epoch 1 batch id 1631 loss 2.1423943042755127 train acc 0.13961909871244635\n",
            "epoch 1 batch id 1641 loss 2.111405372619629 train acc 0.13985374771480805\n",
            "epoch 1 batch id 1651 loss 2.1686036586761475 train acc 0.14002877044215628\n",
            "epoch 1 batch id 1661 loss 2.080836057662964 train acc 0.1402487206502107\n",
            "epoch 1 batch id 1671 loss 2.0850396156311035 train acc 0.1404473369239976\n",
            "epoch 1 batch id 1681 loss 2.1371443271636963 train acc 0.14055063950029745\n",
            "epoch 1 batch id 1691 loss 2.0708847045898438 train acc 0.14084676227084567\n",
            "epoch 1 batch id 1701 loss 2.0962791442871094 train acc 0.14117614638447973\n",
            "epoch 1 batch id 1711 loss 2.1445765495300293 train acc 0.14130077440093514\n",
            "epoch 1 batch id 1721 loss 2.126309633255005 train acc 0.14166908773968623\n",
            "epoch 1 batch id 1731 loss 2.124052047729492 train acc 0.14195190641247835\n",
            "epoch 1 batch id 1741 loss 2.190361738204956 train acc 0.14215967834577828\n",
            "epoch 1 batch id 1751 loss 2.0513110160827637 train acc 0.14241861793260993\n",
            "epoch 1 batch id 1761 loss 2.1463160514831543 train acc 0.14269236229415105\n",
            "epoch 1 batch id 1771 loss 2.0649170875549316 train acc 0.14298948334274422\n",
            "epoch 1 batch id 1781 loss 2.117551803588867 train acc 0.14332713363279057\n",
            "epoch 1 batch id 1791 loss 2.173011302947998 train acc 0.14368718592964824\n",
            "epoch 1 batch id 1801 loss 2.0676090717315674 train acc 0.143982509716824\n",
            "epoch 1 batch id 1811 loss 2.10361385345459 train acc 0.1441451546107123\n",
            "epoch 1 batch id 1821 loss 1.9405035972595215 train acc 0.1445548462383306\n",
            "epoch 1 batch id 1831 loss 2.044588327407837 train acc 0.14478085745494265\n",
            "epoch 1 batch id 1841 loss 2.0567710399627686 train acc 0.14506382400869092\n",
            "epoch 1 batch id 1851 loss 2.174002170562744 train acc 0.14530996758508913\n",
            "epoch 1 batch id 1861 loss 2.0290677547454834 train acc 0.14554506985491672\n",
            "epoch 1 batch id 1871 loss 1.981479525566101 train acc 0.14593633083912347\n",
            "epoch 1 batch id 1881 loss 2.106473207473755 train acc 0.1461656034024455\n",
            "epoch 1 batch id 1891 loss 2.106351375579834 train acc 0.1463841882601798\n",
            "epoch 1 batch id 1901 loss 2.1428065299987793 train acc 0.14665800894266176\n",
            "epoch 1 batch id 1911 loss 2.098890781402588 train acc 0.14693714024071167\n",
            "epoch 1 batch id 1921 loss 2.0016605854034424 train acc 0.14726216814159293\n",
            "epoch 1 batch id 1931 loss 1.9892604351043701 train acc 0.14777802951838426\n",
            "epoch 1 batch id 1941 loss 2.035997152328491 train acc 0.14807927614631633\n",
            "epoch 1 batch id 1951 loss 2.0344772338867188 train acc 0.14840946950281908\n",
            "epoch 1 batch id 1961 loss 1.9925603866577148 train acc 0.14866458439571648\n",
            "epoch 1 batch id 1971 loss 1.9415122270584106 train acc 0.14894089294774226\n",
            "epoch 1 batch id 1981 loss 2.0278117656707764 train acc 0.14917497476022212\n",
            "epoch 1 batch id 1991 loss 2.0386404991149902 train acc 0.14954011803114012\n",
            "epoch 1 batch id 2001 loss 2.0538196563720703 train acc 0.14994065467266368\n",
            "epoch 1 batch id 2011 loss 2.1542599201202393 train acc 0.1501429636996519\n",
            "epoch 1 batch id 2021 loss 2.0084121227264404 train acc 0.15031234537357743\n",
            "epoch 1 batch id 2031 loss 1.9741814136505127 train acc 0.15065700393894632\n",
            "epoch 1 batch id 2041 loss 2.0605406761169434 train acc 0.15086814061734444\n",
            "epoch 1 batch id 2051 loss 2.0714895725250244 train acc 0.1512829107752316\n",
            "epoch 1 batch id 2061 loss 2.0308122634887695 train acc 0.1514813803978651\n",
            "epoch 1 batch id 2071 loss 2.0823111534118652 train acc 0.1516779333655239\n",
            "epoch 1 batch id 2081 loss 2.0609045028686523 train acc 0.15181253003363768\n",
            "epoch 1 batch id 2091 loss 2.020174503326416 train acc 0.1521924318507891\n",
            "epoch 1 batch id 2101 loss 2.159327507019043 train acc 0.15236048310328415\n",
            "epoch 1 batch id 2111 loss 1.9722203016281128 train acc 0.15275639507342492\n",
            "epoch 1 batch id 2121 loss 2.0318970680236816 train acc 0.15308227251296558\n",
            "epoch 1 batch id 2131 loss 1.9361820220947266 train acc 0.15341242374472078\n",
            "epoch 1 batch id 2141 loss 1.9954203367233276 train acc 0.1537467888836992\n",
            "epoch 1 batch id 2151 loss 1.9994784593582153 train acc 0.15409983728498372\n",
            "epoch 1 batch id 2161 loss 1.97086501121521 train acc 0.15436285284590467\n",
            "epoch 1 batch id 2171 loss 2.0290305614471436 train acc 0.154601853984339\n",
            "epoch 1 batch id 2181 loss 2.1011147499084473 train acc 0.1548100068775791\n",
            "epoch 1 batch id 2191 loss 2.150780439376831 train acc 0.15509470561387495\n",
            "epoch 1 batch id 2201 loss 1.9574389457702637 train acc 0.15556849159472966\n",
            "epoch 1 batch id 2211 loss 1.977113127708435 train acc 0.15595318860244234\n",
            "epoch 1 batch id 2221 loss 2.043696880340576 train acc 0.15634849167041873\n",
            "epoch 1 batch id 2231 loss 1.9476593732833862 train acc 0.15667721873599283\n",
            "epoch 1 batch id 2241 loss 2.1494481563568115 train acc 0.15681475903614459\n",
            "epoch 1 batch id 2251 loss 1.9955049753189087 train acc 0.15724261439360285\n",
            "epoch 1 batch id 2261 loss 2.0460548400878906 train acc 0.1574593653250774\n",
            "epoch 1 batch id 2271 loss 1.9016133546829224 train acc 0.15773612945838839\n",
            "epoch 1 batch id 2281 loss 2.0464348793029785 train acc 0.15811321788689173\n",
            "epoch 1 batch id 2291 loss 1.976210117340088 train acc 0.1584870144041903\n",
            "epoch 1 batch id 2301 loss 2.024658441543579 train acc 0.1588236093003042\n",
            "epoch 1 batch id 2311 loss 2.11274790763855 train acc 0.15919785807009953\n",
            "epoch 1 batch id 2321 loss 1.8316173553466797 train acc 0.15960927401981903\n",
            "epoch 1 batch id 2331 loss 2.0934336185455322 train acc 0.15996353496353496\n",
            "epoch 1 batch id 2341 loss 1.925439715385437 train acc 0.1603548163178129\n",
            "epoch 1 batch id 2351 loss 1.8991518020629883 train acc 0.16070953849425776\n",
            "epoch 1 batch id 2361 loss 2.080249786376953 train acc 0.16115390724269377\n",
            "epoch 1 batch id 2371 loss 1.9533275365829468 train acc 0.16155498734711093\n",
            "epoch 1 batch id 2381 loss 1.9863860607147217 train acc 0.16201832213355732\n",
            "epoch 1 batch id 2391 loss 2.0306766033172607 train acc 0.16244510664993728\n",
            "epoch 1 batch id 2401 loss 2.0642638206481934 train acc 0.16288785922532278\n",
            "epoch 1 batch id 2411 loss 2.0366530418395996 train acc 0.16324268975528827\n",
            "epoch 1 batch id 2421 loss 2.019547939300537 train acc 0.16341387856257744\n",
            "epoch 1 batch id 2431 loss 1.9936645030975342 train acc 0.16369935211846975\n",
            "epoch 1 batch id 2441 loss 1.8356244564056396 train acc 0.16412331011880377\n",
            "epoch 1 batch id 2451 loss 1.9296586513519287 train acc 0.1645438086495308\n",
            "epoch 1 batch id 2461 loss 1.981214165687561 train acc 0.16489739943112555\n",
            "epoch 1 batch id 2471 loss 1.83970046043396 train acc 0.1653682719546742\n",
            "epoch 1 batch id 2481 loss 1.8327689170837402 train acc 0.16579126360338572\n",
            "epoch 1 batch id 2491 loss 1.8756669759750366 train acc 0.1661606784423926\n",
            "epoch 1 batch id 2501 loss 1.8919590711593628 train acc 0.16661460415833668\n",
            "epoch 1 batch id 2511 loss 1.9659411907196045 train acc 0.1671146953405018\n",
            "epoch 1 batch id 2521 loss 1.8728784322738647 train acc 0.16745587068623563\n",
            "epoch 1 batch id 2531 loss 1.928246259689331 train acc 0.1678066969577242\n",
            "epoch 1 batch id 2541 loss 1.8698922395706177 train acc 0.16830234159779614\n",
            "epoch 1 batch id 2551 loss 1.9874823093414307 train acc 0.16857972363778911\n",
            "epoch 1 batch id 2561 loss 1.9508070945739746 train acc 0.16897696212417024\n",
            "epoch 1 batch id 2571 loss 1.978855013847351 train acc 0.16945011668611434\n",
            "epoch 1 batch id 2581 loss 1.8017494678497314 train acc 0.16991355094924449\n",
            "epoch 1 batch id 2591 loss 1.7773268222808838 train acc 0.1702648591277499\n",
            "epoch 1 batch id 2601 loss 1.8865742683410645 train acc 0.17069756824298346\n",
            "epoch 1 batch id 2611 loss 1.757840633392334 train acc 0.17101924549980851\n",
            "epoch 1 batch id 2621 loss 1.8616087436676025 train acc 0.17137423693246853\n",
            "epoch 1 batch id 2631 loss 1.9054977893829346 train acc 0.17171465222348917\n",
            "epoch 1 batch id 2641 loss 1.8653260469436646 train acc 0.1720347406285498\n",
            "epoch 1 batch id 2651 loss 1.795527696609497 train acc 0.17231705016974727\n",
            "epoch 1 batch id 2661 loss 1.9149726629257202 train acc 0.17263834084930477\n",
            "epoch 1 batch id 2671 loss 1.9385044574737549 train acc 0.17300987457880942\n",
            "epoch 1 batch id 2681 loss 1.763067603111267 train acc 0.17348354158895934\n",
            "epoch 1 batch id 2691 loss 2.107421875 train acc 0.17387239873652918\n",
            "epoch 1 batch id 2701 loss 1.8420073986053467 train acc 0.17427573121066273\n",
            "epoch 1 batch id 2711 loss 1.938985824584961 train acc 0.17467032460346735\n",
            "epoch 1 batch id 2721 loss 1.756895661354065 train acc 0.17498736677692026\n",
            "epoch 1 batch id 2731 loss 1.710688829421997 train acc 0.1754336781398755\n",
            "epoch 1 batch id 2741 loss 1.8357980251312256 train acc 0.17579122582998905\n",
            "epoch 1 batch id 2751 loss 1.84890615940094 train acc 0.17607801708469648\n",
            "epoch 1 batch id 2761 loss 1.8155351877212524 train acc 0.17639668598333938\n",
            "epoch 1 batch id 2771 loss 1.7910064458847046 train acc 0.17683710754240348\n",
            "epoch 1 batch id 2781 loss 1.716042399406433 train acc 0.17721255843221861\n",
            "epoch 1 batch id 2791 loss 1.811631202697754 train acc 0.17767489251164456\n",
            "epoch 1 batch id 2801 loss 1.917359471321106 train acc 0.17800004462691896\n",
            "epoch 1 batch id 2811 loss 1.7593364715576172 train acc 0.17838958555674136\n",
            "epoch 1 batch id 2821 loss 1.880975604057312 train acc 0.17872097660404113\n",
            "epoch 1 batch id 2831 loss 1.9315518140792847 train acc 0.17911073825503357\n",
            "epoch 1 batch id 2841 loss 1.903752088546753 train acc 0.17960225272791272\n",
            "epoch 1 batch id 2851 loss 1.8034425973892212 train acc 0.1800519554542266\n",
            "epoch 1 batch id 2861 loss 1.879350185394287 train acc 0.1803346731911919\n",
            "epoch 1 batch id 2871 loss 1.7931047677993774 train acc 0.1807623650296064\n",
            "epoch 1 batch id 2881 loss 1.814457893371582 train acc 0.18125759284970497\n",
            "epoch 1 batch id 2891 loss 1.729096531867981 train acc 0.18168453822206848\n",
            "epoch 1 batch id 2901 loss 1.9377740621566772 train acc 0.18203313512581867\n",
            "epoch 1 batch id 2911 loss 1.8646082878112793 train acc 0.1824813208519409\n",
            "epoch 1 batch id 2921 loss 1.7978472709655762 train acc 0.18284619993153028\n",
            "epoch 1 batch id 2931 loss 1.8279913663864136 train acc 0.18315527976799728\n",
            "epoch 1 batch id 2941 loss 1.8313403129577637 train acc 0.18356320129207754\n",
            "epoch 1 batch id 2951 loss 1.868357539176941 train acc 0.183920704845815\n",
            "epoch 1 batch id 2961 loss 1.7807332277297974 train acc 0.18437077845322525\n",
            "epoch 1 batch id 2971 loss 1.837149977684021 train acc 0.18475997139010433\n",
            "epoch 1 batch id 2981 loss 1.9218026399612427 train acc 0.18506268869506876\n",
            "epoch 1 batch id 2991 loss 1.8260829448699951 train acc 0.18547830992978936\n",
            "epoch 1 batch id 3001 loss 1.723013162612915 train acc 0.18580785571476174\n",
            "epoch 1 batch id 3011 loss 2.0257575511932373 train acc 0.18622343075390235\n",
            "epoch 1 batch id 3021 loss 2.0159053802490234 train acc 0.18664142667990732\n",
            "epoch 1 batch id 3031 loss 1.8549590110778809 train acc 0.18699480369515012\n",
            "epoch 1 batch id 3041 loss 1.8136087656021118 train acc 0.18740237586320288\n",
            "epoch 1 batch id 3051 loss 1.9251714944839478 train acc 0.187766306129138\n",
            "epoch 1 batch id 3061 loss 1.6942636966705322 train acc 0.18816359032995753\n",
            "epoch 1 batch id 3071 loss 1.9130157232284546 train acc 0.18843617714099642\n",
            "epoch 1 batch id 3081 loss 1.8423192501068115 train acc 0.188777994157741\n",
            "epoch 1 batch id 3091 loss 1.7464914321899414 train acc 0.1892237544483986\n",
            "epoch 1 batch id 3101 loss 1.779245376586914 train acc 0.1895759432441148\n",
            "epoch 1 batch id 3111 loss 1.9002623558044434 train acc 0.18992084538733525\n",
            "epoch 1 batch id 3121 loss 1.4995310306549072 train acc 0.19038369112463954\n",
            "epoch 1 batch id 3131 loss 1.7402191162109375 train acc 0.19067390610028745\n",
            "epoch 1 batch id 3141 loss 1.982150912284851 train acc 0.1909722222222222\n",
            "epoch 1 batch id 3151 loss 1.8575209379196167 train acc 0.1912835211044113\n",
            "epoch 1 batch id 3161 loss 1.6919060945510864 train acc 0.19170159759569758\n",
            "epoch 1 batch id 3171 loss 1.9440077543258667 train acc 0.19207761747082938\n",
            "epoch 1 batch id 3181 loss 1.8494783639907837 train acc 0.1923628575919522\n",
            "epoch 1 batch id 3191 loss 1.8148934841156006 train acc 0.19280789721090566\n",
            "epoch 1 batch id 3201 loss 1.8288737535476685 train acc 0.1932062246173071\n",
            "epoch 1 batch id 3211 loss 1.7429200410842896 train acc 0.19363126751790719\n",
            "epoch 1 batch id 3221 loss 1.8795006275177002 train acc 0.1939178438373176\n",
            "epoch 1 batch id 3231 loss 1.7596684694290161 train acc 0.19425100588053235\n",
            "epoch 1 batch id 3241 loss 1.802602767944336 train acc 0.19459657513113238\n",
            "epoch 1 batch id 3251 loss 1.6183809041976929 train acc 0.19489676253460475\n",
            "epoch 1 batch id 3261 loss 1.8097381591796875 train acc 0.1952909383624655\n",
            "epoch 1 batch id 3271 loss 1.8474894762039185 train acc 0.1956349357994497\n",
            "epoch 1 batch id 3281 loss 1.899778962135315 train acc 0.1958863532459616\n",
            "epoch 1 batch id 3291 loss 1.7435626983642578 train acc 0.1962454421148587\n",
            "epoch 1 batch id 3301 loss 1.6748642921447754 train acc 0.19664022265980005\n",
            "epoch 1 batch id 3311 loss 1.787450909614563 train acc 0.19704205678042888\n",
            "epoch 1 batch id 3321 loss 1.7001886367797852 train acc 0.19729561878952123\n",
            "epoch 1 batch id 3331 loss 1.6186028718948364 train acc 0.1977212173521465\n",
            "epoch 1 batch id 3341 loss 1.6686474084854126 train acc 0.19810217749176892\n",
            "epoch 1 batch id 3351 loss 1.7274863719940186 train acc 0.19854148015517756\n",
            "epoch 1 batch id 3361 loss 1.6205281019210815 train acc 0.19891308390360013\n",
            "epoch 1 batch id 3371 loss 1.7890326976776123 train acc 0.19928248294274695\n",
            "epoch 1 batch id 3381 loss 1.758150339126587 train acc 0.1996173469387755\n",
            "epoch 1 batch id 3391 loss 1.6526302099227905 train acc 0.199876511353583\n",
            "epoch 1 batch id 3401 loss 2.0027413368225098 train acc 0.20029035577771243\n",
            "epoch 1 batch id 3411 loss 1.7536041736602783 train acc 0.2007613236587511\n",
            "epoch 1 batch id 3421 loss 1.6811333894729614 train acc 0.20105597778427362\n",
            "epoch 1 batch id 3431 loss 1.7776732444763184 train acc 0.20143544156222676\n",
            "epoch 1 batch id 3441 loss 1.7888129949569702 train acc 0.2019080572507992\n",
            "epoch 1 batch id 3451 loss 1.8102689981460571 train acc 0.20221946537235583\n",
            "epoch 1 batch id 3461 loss 1.849133014678955 train acc 0.20265096792834442\n",
            "epoch 1 batch id 3471 loss 1.7275314331054688 train acc 0.20314300633823107\n",
            "epoch 1 batch id 3481 loss 1.7255797386169434 train acc 0.20355142200517093\n",
            "epoch 1 batch id 3491 loss 1.7514094114303589 train acc 0.20386798195359496\n",
            "epoch 1 batch id 3501 loss 1.7846554517745972 train acc 0.2042095115681234\n",
            "epoch 1 batch id 3511 loss 1.5844022035598755 train acc 0.20463365138137282\n",
            "epoch 1 batch id 3521 loss 1.8153643608093262 train acc 0.20490450156205622\n",
            "epoch 1 batch id 3531 loss 1.845031976699829 train acc 0.20508974086661003\n",
            "epoch 1 batch id 3541 loss 1.6899477243423462 train acc 0.20550780146851172\n",
            "epoch 1 batch id 3551 loss 1.799128770828247 train acc 0.20577390171782597\n",
            "epoch 1 batch id 3561 loss 1.8157447576522827 train acc 0.2061745296265094\n",
            "epoch 1 batch id 3571 loss 1.900899887084961 train acc 0.2064679011481378\n",
            "epoch 1 batch id 3581 loss 1.788853406906128 train acc 0.2067770874057526\n",
            "epoch 1 batch id 3591 loss 1.785543441772461 train acc 0.20706279587858537\n",
            "epoch 1 batch id 3601 loss 1.6604799032211304 train acc 0.2075291585670647\n",
            "epoch 1 batch id 3611 loss 1.7466245889663696 train acc 0.20786312655774022\n",
            "epoch 1 batch id 3621 loss 1.651970624923706 train acc 0.2082599765258216\n",
            "epoch 1 batch id 3631 loss 1.5832005739212036 train acc 0.2085943954833379\n",
            "epoch 1 batch id 3641 loss 1.6978813409805298 train acc 0.20898705712716287\n",
            "epoch 1 batch id 3651 loss 1.8937878608703613 train acc 0.20928769515201315\n",
            "epoch 1 batch id 3661 loss 1.884441614151001 train acc 0.20966351406719475\n",
            "epoch 1 batch id 3671 loss 1.6967496871948242 train acc 0.20995215881231272\n",
            "epoch 1 batch id 3681 loss 1.7328449487686157 train acc 0.21022650095082857\n",
            "epoch 1 batch id 3691 loss 1.8724451065063477 train acc 0.21057132213492277\n",
            "epoch 1 batch id 3701 loss 1.6164559125900269 train acc 0.2109522764117806\n",
            "epoch 1 batch id 3711 loss 1.6221261024475098 train acc 0.2113269671247642\n",
            "epoch 1 batch id 3721 loss 1.6279085874557495 train acc 0.2117248387530234\n",
            "epoch 1 batch id 3731 loss 1.8560703992843628 train acc 0.21199912891986064\n",
            "epoch 1 batch id 3741 loss 1.7686067819595337 train acc 0.2122886594493451\n",
            "epoch 1 batch id 3751 loss 1.8251951932907104 train acc 0.2125933084510797\n",
            "epoch 1 batch id 3761 loss 1.8574755191802979 train acc 0.21293372773198618\n",
            "epoch 1 batch id 3771 loss 1.745962142944336 train acc 0.21327234155396446\n",
            "epoch 1 batch id 3781 loss 1.5677437782287598 train acc 0.21360089923300715\n",
            "epoch 1 batch id 3791 loss 1.7971062660217285 train acc 0.21387414270640992\n",
            "epoch 1 batch id 3801 loss 1.5675833225250244 train acc 0.21428571428571427\n",
            "epoch 1 batch id 3811 loss 1.5118756294250488 train acc 0.21460082655471005\n",
            "epoch 1 batch id 3821 loss 1.6697865724563599 train acc 0.21488157550379483\n",
            "epoch 1 batch id 3831 loss 1.8296263217926025 train acc 0.21523835160532498\n",
            "epoch 1 batch id 3841 loss 1.6733982563018799 train acc 0.21567056105180943\n",
            "epoch 1 batch id 3851 loss 1.8453516960144043 train acc 0.21597880420669957\n",
            "epoch 1 batch id 3861 loss 1.60780668258667 train acc 0.21628949753949753\n",
            "epoch 1 batch id 3871 loss 1.7209064960479736 train acc 0.2166147313355722\n",
            "epoch 1 batch id 3881 loss 1.6149061918258667 train acc 0.21691815897964442\n",
            "epoch 1 batch id 3891 loss 1.7063639163970947 train acc 0.2172079799537394\n",
            "epoch 1 batch id 3901 loss 1.6519230604171753 train acc 0.21749230966418867\n",
            "epoch 1 batch id 3911 loss 1.5562989711761475 train acc 0.2178630784965482\n",
            "epoch 1 batch id 3921 loss 1.736279010772705 train acc 0.21814030221882172\n",
            "epoch 1 batch id 3931 loss 1.6575474739074707 train acc 0.2184598384634953\n",
            "epoch 1 batch id 3941 loss 1.866287112236023 train acc 0.21880947094646028\n",
            "epoch 1 batch id 3951 loss 1.6984338760375977 train acc 0.21914942419640598\n",
            "epoch 1 batch id 3961 loss 1.707031488418579 train acc 0.21949949507700076\n",
            "epoch 1 batch id 3971 loss 1.7162352800369263 train acc 0.21986354192898513\n",
            "epoch 1 batch id 3981 loss 1.8292723894119263 train acc 0.22014726199447376\n",
            "epoch 1 batch id 3991 loss 1.802783489227295 train acc 0.22047654096717614\n",
            "epoch 1 batch id 4001 loss 1.6640379428863525 train acc 0.22075731067233192\n",
            "epoch 1 batch id 4011 loss 1.7869895696640015 train acc 0.2211379643480429\n",
            "epoch 1 batch id 4021 loss 1.734796404838562 train acc 0.2214934095996021\n",
            "epoch 1 batch id 4031 loss 1.7561109066009521 train acc 0.22181220540808733\n",
            "epoch 1 batch id 4041 loss 1.6110203266143799 train acc 0.22208689062113338\n",
            "epoch 1 batch id 4051 loss 1.5240535736083984 train acc 0.22238721920513455\n",
            "epoch 1 batch id 4061 loss 1.9121168851852417 train acc 0.2227514774686038\n",
            "epoch 1 batch id 4071 loss 1.6465985774993896 train acc 0.22311010808155243\n",
            "epoch 1 batch id 4081 loss 1.625638723373413 train acc 0.22339806419995098\n",
            "epoch 1 batch id 4091 loss 1.6573961973190308 train acc 0.2236693351258861\n",
            "epoch 1 batch id 4101 loss 1.8060520887374878 train acc 0.22397738356498414\n",
            "epoch 1 batch id 4111 loss 1.8117434978485107 train acc 0.22430293724154707\n",
            "epoch 1 batch id 4121 loss 1.742642879486084 train acc 0.2244904149478282\n",
            "epoch 1 batch id 4131 loss 1.6112189292907715 train acc 0.22477532679738563\n",
            "epoch 1 batch id 4141 loss 1.9500501155853271 train acc 0.22500226394590678\n",
            "epoch 1 batch id 4151 loss 1.722966194152832 train acc 0.22526198506384004\n",
            "epoch 1 batch id 4161 loss 1.72170889377594 train acc 0.22550168228791156\n",
            "epoch 1 batch id 4171 loss 1.5915942192077637 train acc 0.22581515224166868\n",
            "epoch 1 batch id 4181 loss 1.5786856412887573 train acc 0.226160756995934\n",
            "epoch 1 batch id 4191 loss 1.7388391494750977 train acc 0.22656436411357672\n",
            "epoch 1 batch id 4201 loss 1.6276953220367432 train acc 0.22688422399428707\n",
            "epoch 1 batch id 4211 loss 1.8562114238739014 train acc 0.22712093327000712\n",
            "epoch 1 batch id 4221 loss 1.673784613609314 train acc 0.22747867803837954\n",
            "epoch 1 batch id 4231 loss 1.5879865884780884 train acc 0.2278236528007563\n",
            "epoch 1 batch id 4241 loss 1.6481273174285889 train acc 0.22817068497995754\n",
            "epoch 1 batch id 4251 loss 1.8492494821548462 train acc 0.22851608445071747\n",
            "epoch 1 batch id 4261 loss 1.7594512701034546 train acc 0.22875718727998123\n",
            "epoch 1 batch id 4271 loss 1.6134084463119507 train acc 0.22907398735659096\n",
            "epoch 1 batch id 4281 loss 1.6987158060073853 train acc 0.22934550922681615\n",
            "epoch 1 batch id 4291 loss 1.4481966495513916 train acc 0.22969587508739223\n",
            "epoch 1 batch id 4301 loss 1.467843770980835 train acc 0.2300119158335271\n",
            "epoch 1 batch id 4311 loss 1.49366295337677 train acc 0.23028299698445837\n",
            "epoch 1 batch id 4321 loss 1.5815935134887695 train acc 0.23056367160379543\n",
            "epoch 1 batch id 4331 loss 1.6063237190246582 train acc 0.23089355806972986\n",
            "epoch 1 batch id 4341 loss 1.7233413457870483 train acc 0.23126871688551026\n",
            "epoch 1 batch id 4351 loss 1.7853327989578247 train acc 0.23154878188922087\n",
            "epoch 1 batch id 4361 loss 1.8081486225128174 train acc 0.23182039669800505\n",
            "epoch 1 batch id 4371 loss 1.8210090398788452 train acc 0.23211936627773966\n",
            "epoch 1 batch id 4381 loss 1.5409303903579712 train acc 0.23236347295138096\n",
            "epoch 1 batch id 4391 loss 1.7735944986343384 train acc 0.23267407765884765\n",
            "epoch 1 batch id 4401 loss 1.5706143379211426 train acc 0.23295841854124064\n",
            "epoch 1 batch id 4411 loss 1.7097383737564087 train acc 0.2332131319428701\n",
            "epoch 1 batch id 4421 loss 1.5409334897994995 train acc 0.23358332390861797\n",
            "epoch 1 batch id 4431 loss 1.5635255575180054 train acc 0.2339518449559919\n",
            "epoch 1 batch id 4441 loss 1.5298744440078735 train acc 0.23427648615176763\n",
            "epoch 1 batch id 4451 loss 1.508415699005127 train acc 0.2346137104021568\n",
            "epoch 1 batch id 4461 loss 1.5673710107803345 train acc 0.23490739184039452\n",
            "epoch 1 batch id 4471 loss 1.789595365524292 train acc 0.23518578058599865\n",
            "epoch 1 batch id 4481 loss 1.71876859664917 train acc 0.23542805735326935\n",
            "epoch 1 batch id 4491 loss 1.6835033893585205 train acc 0.23559967156535294\n",
            "epoch 1 batch id 4501 loss 1.7085340023040771 train acc 0.23591285269940013\n",
            "epoch 1 batch id 4511 loss 1.6377055644989014 train acc 0.2361934715140767\n",
            "epoch 1 batch id 4521 loss 1.7173941135406494 train acc 0.236445200176952\n",
            "epoch 1 batch id 4531 loss 1.6910693645477295 train acc 0.23670271463253145\n",
            "epoch 1 batch id 4541 loss 1.5598258972167969 train acc 0.23695565404096014\n",
            "epoch 1 batch id 4551 loss 1.613191843032837 train acc 0.23723494836299713\n",
            "epoch 1 batch id 4561 loss 1.643341064453125 train acc 0.23748218592413944\n",
            "epoch 1 batch id 4571 loss 1.7039726972579956 train acc 0.2377761977685408\n",
            "epoch 1 batch id 4581 loss 1.7317421436309814 train acc 0.23805869351669942\n",
            "epoch 1 batch id 4591 loss 1.5994826555252075 train acc 0.23827869745153563\n",
            "epoch 1 batch id 4601 loss 1.4937525987625122 train acc 0.23858264507715715\n",
            "epoch 1 batch id 4611 loss 1.6969057321548462 train acc 0.2388717198004771\n",
            "epoch 1 batch id 4621 loss 1.900076985359192 train acc 0.23905810430642718\n",
            "epoch 1 batch id 4631 loss 1.7365152835845947 train acc 0.23933815590585186\n",
            "epoch 1 batch id 4641 loss 1.5690865516662598 train acc 0.23971463585434175\n",
            "epoch 1 batch id 4651 loss 1.6275957822799683 train acc 0.24002566652332832\n",
            "epoch 1 batch id 4661 loss 1.7152624130249023 train acc 0.24026496459987126\n",
            "epoch 1 batch id 4671 loss 1.635908842086792 train acc 0.24057348533504602\n",
            "epoch 1 batch id 4681 loss 1.7191638946533203 train acc 0.24086066011535998\n",
            "epoch 1 batch id 4691 loss 1.3365404605865479 train acc 0.24112995629929654\n",
            "epoch 1 batch id 4701 loss 1.5619746446609497 train acc 0.24141472559029994\n",
            "epoch 1 batch id 4711 loss 1.6043082475662231 train acc 0.24170491933772023\n",
            "epoch 1 batch id 4721 loss 1.6813702583312988 train acc 0.24196078690955306\n",
            "epoch 1 batch id 4731 loss 1.6089097261428833 train acc 0.24216933523567957\n",
            "epoch 1 batch id 4741 loss 1.7382309436798096 train acc 0.24243962244252268\n",
            "epoch 1 batch id 4751 loss 1.7957419157028198 train acc 0.24267588402441592\n",
            "epoch 1 batch id 4761 loss 1.7424136400222778 train acc 0.2428881800042008\n",
            "epoch 1 batch id 4771 loss 1.6741001605987549 train acc 0.2431618109411025\n",
            "epoch 1 batch id 4781 loss 1.6999435424804688 train acc 0.24342122463919683\n",
            "epoch 1 batch id 4791 loss 1.7029914855957031 train acc 0.24364041953663118\n",
            "epoch 1 batch id 4801 loss 1.7114648818969727 train acc 0.24387171943345137\n",
            "epoch 1 batch id 4811 loss 1.6725821495056152 train acc 0.24417026086052795\n",
            "epoch 1 batch id 4821 loss 1.6276804208755493 train acc 0.2443897790914748\n",
            "epoch 1 batch id 4831 loss 1.750097632408142 train acc 0.2446116228524115\n",
            "epoch 1 batch id 4841 loss 1.7382928133010864 train acc 0.24482609481512085\n",
            "epoch 1 batch id 4851 loss 1.4558666944503784 train acc 0.24508799732014017\n",
            "epoch 1 batch id 4861 loss 1.6274375915527344 train acc 0.24539060892820408\n",
            "epoch 1 batch id 4871 loss 1.6665064096450806 train acc 0.24565348491069597\n",
            "epoch 1 batch id 4881 loss 1.5814648866653442 train acc 0.2459633015775456\n",
            "epoch 1 batch id 4891 loss 1.6001920700073242 train acc 0.2462111531384175\n",
            "epoch 1 batch id 4901 loss 1.5918519496917725 train acc 0.24652175576412977\n",
            "epoch 1 batch id 4911 loss 1.4387011528015137 train acc 0.24681836693137854\n",
            "epoch 1 batch id 4921 loss 1.7876852750778198 train acc 0.24701534241007925\n",
            "epoch 1 batch id 4931 loss 1.6983778476715088 train acc 0.24726221861691342\n",
            "epoch 1 batch id 4941 loss 1.5072239637374878 train acc 0.24750493321190042\n",
            "epoch 1 batch id 4951 loss 1.782946228981018 train acc 0.247771914764694\n",
            "epoch 1 train acc 0.2479582935976672\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c614c75a01e54a0b90846620ed7199b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 loss 1.7997475862503052 test acc 0.4208749541788856\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b277c14468547ec87a9ac3372eefcbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1 loss 1.6245043277740479 train acc 0.34375\n",
            "epoch 2 batch id 11 loss 1.524915099143982 train acc 0.3849431818181818\n",
            "epoch 2 batch id 21 loss 1.7063993215560913 train acc 0.3802083333333333\n",
            "epoch 2 batch id 31 loss 1.663312315940857 train acc 0.3684475806451613\n",
            "epoch 2 batch id 41 loss 1.4759918451309204 train acc 0.36928353658536583\n",
            "epoch 2 batch id 51 loss 1.6721407175064087 train acc 0.37224264705882354\n",
            "epoch 2 batch id 61 loss 1.7337164878845215 train acc 0.3706454918032787\n",
            "epoch 2 batch id 71 loss 1.731078863143921 train acc 0.37455985915492956\n",
            "epoch 2 batch id 81 loss 1.7263888120651245 train acc 0.37094907407407407\n",
            "epoch 2 batch id 91 loss 1.5698491334915161 train acc 0.3696771978021978\n",
            "epoch 2 batch id 101 loss 1.4811538457870483 train acc 0.37066831683168316\n",
            "epoch 2 batch id 111 loss 1.6425083875656128 train acc 0.3697916666666667\n",
            "epoch 2 batch id 121 loss 1.8334906101226807 train acc 0.368672520661157\n",
            "epoch 2 batch id 131 loss 1.6802432537078857 train acc 0.3680820610687023\n",
            "epoch 2 batch id 141 loss 1.639971375465393 train acc 0.368572695035461\n",
            "epoch 2 batch id 151 loss 1.4463577270507812 train acc 0.3695157284768212\n",
            "epoch 2 batch id 161 loss 1.8508937358856201 train acc 0.3690799689440994\n",
            "epoch 2 batch id 171 loss 1.709173560142517 train acc 0.3672331871345029\n",
            "epoch 2 batch id 181 loss 1.6901582479476929 train acc 0.3674896408839779\n",
            "epoch 2 batch id 191 loss 1.7100188732147217 train acc 0.3668193717277487\n",
            "epoch 2 batch id 201 loss 1.4369735717773438 train acc 0.36862562189054726\n",
            "epoch 2 batch id 211 loss 1.7841765880584717 train acc 0.36981635071090047\n",
            "epoch 2 batch id 221 loss 1.6111125946044922 train acc 0.3700509049773756\n",
            "epoch 2 batch id 231 loss 1.5864503383636475 train acc 0.3710091991341991\n",
            "epoch 2 batch id 241 loss 1.713975191116333 train acc 0.37188796680497926\n",
            "epoch 2 batch id 251 loss 1.5679913759231567 train acc 0.3712026892430279\n",
            "epoch 2 batch id 261 loss 1.4130291938781738 train acc 0.37308429118773945\n",
            "epoch 2 batch id 271 loss 1.53355073928833 train acc 0.3743657749077491\n",
            "epoch 2 batch id 281 loss 1.5576698780059814 train acc 0.3758896797153025\n",
            "epoch 2 batch id 291 loss 1.8398017883300781 train acc 0.37698668384879724\n",
            "epoch 2 batch id 301 loss 1.8226343393325806 train acc 0.37723214285714285\n",
            "epoch 2 batch id 311 loss 1.5876325368881226 train acc 0.376306270096463\n",
            "epoch 2 batch id 321 loss 1.8045852184295654 train acc 0.3767523364485981\n",
            "epoch 2 batch id 331 loss 1.610822319984436 train acc 0.3768882175226586\n",
            "epoch 2 batch id 341 loss 1.6906615495681763 train acc 0.3772452346041056\n",
            "epoch 2 batch id 351 loss 1.5241332054138184 train acc 0.3780715811965812\n",
            "epoch 2 batch id 361 loss 1.6573041677474976 train acc 0.3784193213296399\n",
            "epoch 2 batch id 371 loss 1.622509479522705 train acc 0.3795485175202156\n",
            "epoch 2 batch id 381 loss 1.4934779405593872 train acc 0.37996227034120733\n",
            "epoch 2 batch id 391 loss 1.7571885585784912 train acc 0.3805546675191816\n",
            "epoch 2 batch id 401 loss 1.5902018547058105 train acc 0.38010442643391523\n",
            "epoch 2 batch id 411 loss 1.5127990245819092 train acc 0.3802083333333333\n",
            "epoch 2 batch id 421 loss 1.5541367530822754 train acc 0.38052998812351546\n",
            "epoch 2 batch id 431 loss 1.806269884109497 train acc 0.3801116589327146\n",
            "epoch 2 batch id 441 loss 1.6565334796905518 train acc 0.3799248866213152\n",
            "epoch 2 batch id 451 loss 1.4895572662353516 train acc 0.38016213968957874\n",
            "epoch 2 batch id 461 loss 1.5940066576004028 train acc 0.38079582429501085\n",
            "epoch 2 batch id 471 loss 1.603682279586792 train acc 0.3812699044585987\n",
            "epoch 2 batch id 481 loss 1.6314575672149658 train acc 0.3814643970893971\n",
            "epoch 2 batch id 491 loss 1.3731917142868042 train acc 0.38136456211812625\n",
            "epoch 2 batch id 501 loss 2.063216209411621 train acc 0.3808944610778443\n",
            "epoch 2 batch id 511 loss 1.5578784942626953 train acc 0.3818187377690802\n",
            "epoch 2 batch id 521 loss 1.6629114151000977 train acc 0.3817478406909789\n",
            "epoch 2 batch id 531 loss 1.7500553131103516 train acc 0.381444209039548\n",
            "epoch 2 batch id 541 loss 1.644986629486084 train acc 0.3821626617375231\n",
            "epoch 2 batch id 551 loss 1.5966941118240356 train acc 0.38180580762250454\n",
            "epoch 2 batch id 561 loss 1.5390583276748657 train acc 0.38185160427807485\n",
            "epoch 2 batch id 571 loss 1.749021291732788 train acc 0.38178633975481613\n",
            "epoch 2 batch id 581 loss 1.671676516532898 train acc 0.3817502151462995\n",
            "epoch 2 batch id 591 loss 1.4699451923370361 train acc 0.38234983079526225\n",
            "epoch 2 batch id 601 loss 1.4089486598968506 train acc 0.3827215058236273\n",
            "epoch 2 batch id 611 loss 1.540285348892212 train acc 0.3831833060556465\n",
            "epoch 2 batch id 621 loss 1.5337531566619873 train acc 0.38418377616747185\n",
            "epoch 2 batch id 631 loss 1.5567461252212524 train acc 0.38403823296354994\n",
            "epoch 2 batch id 641 loss 1.6464991569519043 train acc 0.3837509750390016\n",
            "epoch 2 batch id 651 loss 1.4879192113876343 train acc 0.38368855606758834\n",
            "epoch 2 batch id 661 loss 1.6710233688354492 train acc 0.3838644099848714\n",
            "epoch 2 batch id 671 loss 1.7113982439041138 train acc 0.3837555886736215\n",
            "epoch 2 batch id 681 loss 1.4778136014938354 train acc 0.383764684287812\n",
            "epoch 2 batch id 691 loss 1.5113319158554077 train acc 0.38415792329956583\n",
            "epoch 2 batch id 701 loss 1.5354881286621094 train acc 0.38465139087018546\n",
            "epoch 2 batch id 711 loss 1.5701137781143188 train acc 0.38449367088607594\n",
            "epoch 2 batch id 721 loss 1.49876070022583 train acc 0.3843619972260749\n",
            "epoch 2 batch id 731 loss 1.3920613527297974 train acc 0.3850247948016416\n",
            "epoch 2 batch id 741 loss 1.7078955173492432 train acc 0.3853534075573549\n",
            "epoch 2 batch id 751 loss 1.8248945474624634 train acc 0.3850074900133156\n",
            "epoch 2 batch id 761 loss 1.4089349508285522 train acc 0.3856356767411301\n",
            "epoch 2 batch id 771 loss 1.6853976249694824 train acc 0.3857003891050584\n",
            "epoch 2 batch id 781 loss 1.5885906219482422 train acc 0.38598351472471193\n",
            "epoch 2 batch id 791 loss 1.4322185516357422 train acc 0.3858051517067004\n",
            "epoch 2 batch id 801 loss 1.4510060548782349 train acc 0.3861579275905119\n",
            "epoch 2 batch id 811 loss 1.6926273107528687 train acc 0.38603961159062883\n",
            "epoch 2 batch id 821 loss 1.4828660488128662 train acc 0.38632384287454324\n",
            "epoch 2 batch id 831 loss 1.713625192642212 train acc 0.38624398315282793\n",
            "epoch 2 batch id 841 loss 1.55532968044281 train acc 0.3862589179548157\n",
            "epoch 2 batch id 851 loss 1.5345838069915771 train acc 0.38614497649823737\n",
            "epoch 2 batch id 861 loss 1.6570138931274414 train acc 0.3860518292682927\n",
            "epoch 2 batch id 871 loss 1.4178887605667114 train acc 0.38597876004592424\n",
            "epoch 2 batch id 881 loss 1.7829378843307495 train acc 0.3860314982973893\n",
            "epoch 2 batch id 891 loss 1.4715397357940674 train acc 0.38639870931537595\n",
            "epoch 2 batch id 901 loss 1.6634494066238403 train acc 0.3864802996670366\n",
            "epoch 2 batch id 911 loss 1.6562808752059937 train acc 0.3866287047200878\n",
            "epoch 2 batch id 921 loss 1.74111008644104 train acc 0.38680781758957655\n",
            "epoch 2 batch id 931 loss 1.524727702140808 train acc 0.3872683941997852\n",
            "epoch 2 batch id 941 loss 1.3793405294418335 train acc 0.3873538788522848\n",
            "epoch 2 batch id 951 loss 1.5437955856323242 train acc 0.38745399579390116\n",
            "epoch 2 batch id 961 loss 1.5772004127502441 train acc 0.38725936524453697\n",
            "epoch 2 batch id 971 loss 1.7183003425598145 train acc 0.38739057672502575\n",
            "epoch 2 batch id 981 loss 1.4989391565322876 train acc 0.3879650866462793\n",
            "epoch 2 batch id 991 loss 1.5367498397827148 train acc 0.38810229566094856\n",
            "epoch 2 batch id 1001 loss 1.7612463235855103 train acc 0.3879714035964036\n",
            "epoch 2 batch id 1011 loss 1.615854024887085 train acc 0.38810583580613256\n",
            "epoch 2 batch id 1021 loss 1.56964111328125 train acc 0.38809990205680706\n",
            "epoch 2 batch id 1031 loss 1.5569446086883545 train acc 0.3881547041707081\n",
            "epoch 2 batch id 1041 loss 1.556912899017334 train acc 0.3878482228626321\n",
            "epoch 2 batch id 1051 loss 1.4286469221115112 train acc 0.388008444338725\n",
            "epoch 2 batch id 1061 loss 1.647228717803955 train acc 0.3880920122525919\n",
            "epoch 2 batch id 1071 loss 1.3559727668762207 train acc 0.3881886087768441\n",
            "epoch 2 batch id 1081 loss 1.5692408084869385 train acc 0.3882545097132285\n",
            "epoch 2 batch id 1091 loss 1.6981115341186523 train acc 0.38817598533455544\n",
            "epoch 2 batch id 1101 loss 1.6174501180648804 train acc 0.38855301998183467\n",
            "epoch 2 batch id 1111 loss 1.5247828960418701 train acc 0.38888107560756074\n",
            "epoch 2 batch id 1121 loss 1.6566485166549683 train acc 0.389049955396967\n",
            "epoch 2 batch id 1131 loss 1.592719554901123 train acc 0.3890086206896552\n",
            "epoch 2 batch id 1141 loss 1.5611863136291504 train acc 0.38913234005258546\n",
            "epoch 2 batch id 1151 loss 1.8570228815078735 train acc 0.3895389878366638\n",
            "epoch 2 batch id 1161 loss 1.5579060316085815 train acc 0.38977713178294576\n",
            "epoch 2 batch id 1171 loss 1.5782183408737183 train acc 0.3899711784799317\n",
            "epoch 2 batch id 1181 loss 1.4489097595214844 train acc 0.3899767146486029\n",
            "epoch 2 batch id 1191 loss 1.517987847328186 train acc 0.3902707808564232\n",
            "epoch 2 batch id 1201 loss 1.6607285737991333 train acc 0.39006557035803496\n",
            "epoch 2 batch id 1211 loss 1.3815267086029053 train acc 0.3901734104046243\n",
            "epoch 2 batch id 1221 loss 1.6182827949523926 train acc 0.39050982800982803\n",
            "epoch 2 batch id 1231 loss 1.7299200296401978 train acc 0.39045999187652314\n",
            "epoch 2 batch id 1241 loss 1.511891484260559 train acc 0.39054945608380337\n",
            "epoch 2 batch id 1251 loss 1.429219126701355 train acc 0.3908248401278977\n",
            "epoch 2 batch id 1261 loss 1.620597243309021 train acc 0.3909223830293418\n",
            "epoch 2 batch id 1271 loss 1.4958492517471313 train acc 0.3911413257277734\n",
            "epoch 2 batch id 1281 loss 1.656616449356079 train acc 0.3911250975800156\n",
            "epoch 2 batch id 1291 loss 1.659529685974121 train acc 0.3911575329202169\n",
            "epoch 2 batch id 1301 loss 1.6962881088256836 train acc 0.3909372598001537\n",
            "epoch 2 batch id 1311 loss 1.6951770782470703 train acc 0.390934877955759\n",
            "epoch 2 batch id 1321 loss 1.4175485372543335 train acc 0.3908024224072672\n",
            "epoch 2 batch id 1331 loss 1.5514905452728271 train acc 0.39082456799398946\n",
            "epoch 2 batch id 1341 loss 1.4875425100326538 train acc 0.3908463832960477\n",
            "epoch 2 batch id 1351 loss 1.4523378610610962 train acc 0.39097196521095484\n",
            "epoch 2 batch id 1361 loss 1.7562370300292969 train acc 0.3909808963997061\n",
            "epoch 2 batch id 1371 loss 1.5583100318908691 train acc 0.3910466812545587\n",
            "epoch 2 batch id 1381 loss 1.5542094707489014 train acc 0.3911454561911658\n",
            "epoch 2 batch id 1391 loss 1.300782322883606 train acc 0.39153486700215673\n",
            "epoch 2 batch id 1401 loss 1.4859654903411865 train acc 0.3917291220556745\n",
            "epoch 2 batch id 1411 loss 1.4841939210891724 train acc 0.3917766654854713\n",
            "epoch 2 batch id 1421 loss 1.5129964351654053 train acc 0.3918345355383533\n",
            "epoch 2 batch id 1431 loss 1.3968545198440552 train acc 0.3920662997903564\n",
            "epoch 2 batch id 1441 loss 1.500049352645874 train acc 0.3920237682165163\n",
            "epoch 2 batch id 1451 loss 1.5406036376953125 train acc 0.3924233287388008\n",
            "epoch 2 batch id 1461 loss 1.6180806159973145 train acc 0.3925072724161533\n",
            "epoch 2 batch id 1471 loss 1.625267744064331 train acc 0.39262194085656016\n",
            "epoch 2 batch id 1481 loss 1.4383760690689087 train acc 0.3928405638082377\n",
            "epoch 2 batch id 1491 loss 1.5063930749893188 train acc 0.3929514587525151\n",
            "epoch 2 batch id 1501 loss 1.472643494606018 train acc 0.39330029980013326\n",
            "epoch 2 batch id 1511 loss 1.4566336870193481 train acc 0.3933653209794838\n",
            "epoch 2 batch id 1521 loss 1.5739283561706543 train acc 0.39364521696252464\n",
            "epoch 2 batch id 1531 loss 1.4505681991577148 train acc 0.3936561071195297\n",
            "epoch 2 batch id 1541 loss 1.4968111515045166 train acc 0.39399132057105773\n",
            "epoch 2 batch id 1551 loss 1.5819896459579468 train acc 0.3940199871050935\n",
            "epoch 2 batch id 1561 loss 1.6078609228134155 train acc 0.39421844971172326\n",
            "epoch 2 batch id 1571 loss 1.5424842834472656 train acc 0.39445416931890515\n",
            "epoch 2 batch id 1581 loss 1.673646330833435 train acc 0.39459796015180265\n",
            "epoch 2 batch id 1591 loss 1.5828218460083008 train acc 0.39472030169704586\n",
            "epoch 2 batch id 1601 loss 1.6194417476654053 train acc 0.394743519675203\n",
            "epoch 2 batch id 1611 loss 1.494943380355835 train acc 0.39493133147113596\n",
            "epoch 2 batch id 1621 loss 1.4554314613342285 train acc 0.3950879086983344\n",
            "epoch 2 batch id 1631 loss 1.2790378332138062 train acc 0.39510844573881054\n",
            "epoch 2 batch id 1641 loss 1.6133805513381958 train acc 0.39537629494210846\n",
            "epoch 2 batch id 1651 loss 1.6605185270309448 train acc 0.39534751665657175\n",
            "epoch 2 batch id 1661 loss 1.4379231929779053 train acc 0.3955542594822396\n",
            "epoch 2 batch id 1671 loss 1.467287302017212 train acc 0.39565567025733095\n",
            "epoch 2 batch id 1681 loss 1.5994988679885864 train acc 0.39569080904223675\n",
            "epoch 2 batch id 1691 loss 1.5929962396621704 train acc 0.3959472945002957\n",
            "epoch 2 batch id 1701 loss 1.4658271074295044 train acc 0.39604460611405057\n",
            "epoch 2 batch id 1711 loss 1.487480878829956 train acc 0.39613164815897134\n",
            "epoch 2 batch id 1721 loss 1.5680968761444092 train acc 0.3961813625798954\n",
            "epoch 2 batch id 1731 loss 1.3880277872085571 train acc 0.3961763431542461\n",
            "epoch 2 batch id 1741 loss 1.463146448135376 train acc 0.39629702757036184\n",
            "epoch 2 batch id 1751 loss 1.344206690788269 train acc 0.39638956310679613\n",
            "epoch 2 batch id 1761 loss 1.150883436203003 train acc 0.3967738500851789\n",
            "epoch 2 batch id 1771 loss 1.4649412631988525 train acc 0.3970479249011858\n",
            "epoch 2 batch id 1781 loss 1.5343130826950073 train acc 0.39716977821448624\n",
            "epoch 2 batch id 1791 loss 1.6886067390441895 train acc 0.39723792573981015\n",
            "epoch 2 batch id 1801 loss 1.5155539512634277 train acc 0.39751353414769575\n",
            "epoch 2 batch id 1811 loss 1.4473823308944702 train acc 0.39763942573163996\n",
            "epoch 2 batch id 1821 loss 1.3739205598831177 train acc 0.397618066996156\n",
            "epoch 2 batch id 1831 loss 1.3291054964065552 train acc 0.397750546149645\n",
            "epoch 2 batch id 1841 loss 1.3679765462875366 train acc 0.39782217544812604\n",
            "epoch 2 batch id 1851 loss 1.6566420793533325 train acc 0.39782549972987574\n",
            "epoch 2 batch id 1861 loss 1.5206820964813232 train acc 0.39776162009672217\n",
            "epoch 2 batch id 1871 loss 1.410217046737671 train acc 0.39794895777659006\n",
            "epoch 2 batch id 1881 loss 1.4646191596984863 train acc 0.39801800903774587\n",
            "epoch 2 batch id 1891 loss 1.3513367176055908 train acc 0.3983011634056055\n",
            "epoch 2 batch id 1901 loss 1.5840972661972046 train acc 0.39826078379800106\n",
            "epoch 2 batch id 1911 loss 1.4507858753204346 train acc 0.3984252354788069\n",
            "epoch 2 batch id 1921 loss 1.5816452503204346 train acc 0.3983032925559604\n",
            "epoch 2 batch id 1931 loss 1.748835802078247 train acc 0.3986357457276023\n",
            "epoch 2 batch id 1941 loss 1.3969521522521973 train acc 0.39879572385368367\n",
            "epoch 2 batch id 1951 loss 1.4900643825531006 train acc 0.3989300358790364\n",
            "epoch 2 batch id 1961 loss 1.479519248008728 train acc 0.3990311065782764\n",
            "epoch 2 batch id 1971 loss 1.5908392667770386 train acc 0.39920249873160835\n",
            "epoch 2 batch id 1981 loss 1.4161980152130127 train acc 0.3995062468450278\n",
            "epoch 2 batch id 1991 loss 1.4802751541137695 train acc 0.39950087895529884\n",
            "epoch 2 batch id 2001 loss 1.6202037334442139 train acc 0.39964392803598203\n",
            "epoch 2 batch id 2011 loss 1.7295138835906982 train acc 0.39968454748881155\n",
            "epoch 2 batch id 2021 loss 1.4378936290740967 train acc 0.3998020781791192\n",
            "epoch 2 batch id 2031 loss 1.5699636936187744 train acc 0.3998415189561792\n",
            "epoch 2 batch id 2041 loss 1.3872851133346558 train acc 0.3998499510044096\n",
            "epoch 2 batch id 2051 loss 1.6388875246047974 train acc 0.40007922964407605\n",
            "epoch 2 batch id 2061 loss 1.634130597114563 train acc 0.4002304706453178\n",
            "epoch 2 batch id 2071 loss 1.6938968896865845 train acc 0.4002897151134718\n",
            "epoch 2 batch id 2081 loss 1.6544796228408813 train acc 0.40017569678039405\n",
            "epoch 2 batch id 2091 loss 1.479038119316101 train acc 0.40027947154471544\n",
            "epoch 2 batch id 2101 loss 1.46328604221344 train acc 0.400367384578772\n",
            "epoch 2 batch id 2111 loss 1.5523279905319214 train acc 0.4004100544765514\n",
            "epoch 2 batch id 2121 loss 1.3997292518615723 train acc 0.40054809052333806\n",
            "epoch 2 batch id 2131 loss 1.7812325954437256 train acc 0.4006994955419991\n",
            "epoch 2 batch id 2141 loss 1.5114318132400513 train acc 0.40094436011209716\n",
            "epoch 2 batch id 2151 loss 1.4611294269561768 train acc 0.4010997791724779\n",
            "epoch 2 batch id 2161 loss 1.5491406917572021 train acc 0.40105130726515503\n",
            "epoch 2 batch id 2171 loss 1.4381370544433594 train acc 0.401190407646246\n",
            "epoch 2 batch id 2181 loss 1.4284107685089111 train acc 0.40123509857863365\n",
            "epoch 2 batch id 2191 loss 1.7892385721206665 train acc 0.40149332496576906\n",
            "epoch 2 batch id 2201 loss 1.6796759366989136 train acc 0.4014368468877783\n",
            "epoch 2 batch id 2211 loss 1.5958755016326904 train acc 0.40159288783355945\n",
            "epoch 2 batch id 2221 loss 1.6909362077713013 train acc 0.4017686289959478\n",
            "epoch 2 batch id 2231 loss 1.5383561849594116 train acc 0.4019007731958763\n",
            "epoch 2 batch id 2241 loss 1.6403813362121582 train acc 0.401913208389112\n",
            "epoch 2 batch id 2251 loss 1.5006728172302246 train acc 0.40203659484673476\n",
            "epoch 2 batch id 2261 loss 1.2266889810562134 train acc 0.40220726448474126\n",
            "epoch 2 batch id 2271 loss 1.7253338098526 train acc 0.40239019154557465\n",
            "epoch 2 batch id 2281 loss 1.5745015144348145 train acc 0.402441363437089\n",
            "epoch 2 batch id 2291 loss 1.613088607788086 train acc 0.40283991706678307\n",
            "epoch 2 batch id 2301 loss 1.5563561916351318 train acc 0.4028954802259887\n",
            "epoch 2 batch id 2311 loss 1.509782075881958 train acc 0.40287618996105584\n",
            "epoch 2 batch id 2321 loss 1.2469189167022705 train acc 0.40308595433003014\n",
            "epoch 2 batch id 2331 loss 1.4101667404174805 train acc 0.40309282496782495\n",
            "epoch 2 batch id 2341 loss 1.7945293188095093 train acc 0.40309963690730455\n",
            "epoch 2 batch id 2351 loss 1.4382809400558472 train acc 0.4031130370055296\n",
            "epoch 2 batch id 2361 loss 1.6937975883483887 train acc 0.40315279542566707\n",
            "epoch 2 batch id 2371 loss 1.5309377908706665 train acc 0.4031460881484606\n",
            "epoch 2 batch id 2381 loss 1.4274574518203735 train acc 0.40323787274254513\n",
            "epoch 2 batch id 2391 loss 1.5493559837341309 train acc 0.40336156419907987\n",
            "epoch 2 batch id 2401 loss 1.6783322095870972 train acc 0.4035037484381508\n",
            "epoch 2 batch id 2411 loss 1.6501436233520508 train acc 0.4036965989216093\n",
            "epoch 2 batch id 2421 loss 1.5764031410217285 train acc 0.4038039549772821\n",
            "epoch 2 batch id 2431 loss 1.521084189414978 train acc 0.403826871657754\n",
            "epoch 2 batch id 2441 loss 1.4923405647277832 train acc 0.4039776218762802\n",
            "epoch 2 batch id 2451 loss 1.4006975889205933 train acc 0.40407614239086087\n",
            "epoch 2 batch id 2461 loss 1.5402085781097412 train acc 0.404148466070703\n",
            "epoch 2 batch id 2471 loss 1.423851728439331 train acc 0.40420123431808985\n",
            "epoch 2 batch id 2481 loss 1.490788221359253 train acc 0.40421579000403063\n",
            "epoch 2 batch id 2491 loss 1.6537363529205322 train acc 0.40420513849859496\n",
            "epoch 2 batch id 2501 loss 1.3136812448501587 train acc 0.40441323470611756\n",
            "epoch 2 batch id 2511 loss 1.5966068506240845 train acc 0.4045325567502987\n",
            "epoch 2 batch id 2521 loss 1.4926931858062744 train acc 0.4045145775485918\n",
            "epoch 2 batch id 2531 loss 1.6209254264831543 train acc 0.4045090873172659\n",
            "epoch 2 batch id 2541 loss 1.4881970882415771 train acc 0.4046143250688705\n",
            "epoch 2 batch id 2551 loss 1.6850401163101196 train acc 0.404577861622893\n",
            "epoch 2 batch id 2561 loss 1.619744062423706 train acc 0.4046026942600547\n",
            "epoch 2 batch id 2571 loss 1.3963316679000854 train acc 0.4048096557759627\n",
            "epoch 2 batch id 2581 loss 1.4524861574172974 train acc 0.40484550561797755\n",
            "epoch 2 batch id 2591 loss 1.2604553699493408 train acc 0.4049232921651872\n",
            "epoch 2 batch id 2601 loss 1.5155524015426636 train acc 0.4049344002306805\n",
            "epoch 2 batch id 2611 loss 1.418961763381958 train acc 0.4050471562619686\n",
            "epoch 2 batch id 2621 loss 1.528639793395996 train acc 0.40497424647081265\n",
            "epoch 2 batch id 2631 loss 1.6918712854385376 train acc 0.4050800551121247\n",
            "epoch 2 batch id 2641 loss 1.4903236627578735 train acc 0.40514364823930327\n",
            "epoch 2 batch id 2651 loss 1.4033516645431519 train acc 0.40520676159939645\n",
            "epoch 2 batch id 2661 loss 1.5610156059265137 train acc 0.40528114430665163\n",
            "epoch 2 batch id 2671 loss 1.4881864786148071 train acc 0.40529647135904157\n",
            "epoch 2 batch id 2681 loss 1.2540442943572998 train acc 0.40544572920552036\n",
            "epoch 2 batch id 2691 loss 1.751742959022522 train acc 0.4054603307320699\n",
            "epoch 2 batch id 2701 loss 1.5275276899337769 train acc 0.4055847371343947\n",
            "epoch 2 batch id 2711 loss 1.5100092887878418 train acc 0.4056621172998893\n",
            "epoch 2 batch id 2721 loss 1.492482304573059 train acc 0.4057733829474458\n",
            "epoch 2 batch id 2731 loss 1.4427602291107178 train acc 0.40578084950567556\n",
            "epoch 2 batch id 2741 loss 1.6589634418487549 train acc 0.40577686063480484\n",
            "epoch 2 batch id 2751 loss 1.2105381488800049 train acc 0.40598873137041075\n",
            "epoch 2 batch id 2761 loss 1.3416602611541748 train acc 0.40608588373777615\n",
            "epoch 2 batch id 2771 loss 1.5362908840179443 train acc 0.40610903103572715\n",
            "epoch 2 batch id 2781 loss 1.447169542312622 train acc 0.4062050521395182\n",
            "epoch 2 batch id 2791 loss 1.4406464099884033 train acc 0.4063227785739878\n",
            "epoch 2 batch id 2801 loss 1.5031752586364746 train acc 0.4065233398786148\n",
            "epoch 2 batch id 2811 loss 1.5592314004898071 train acc 0.4066891230878691\n",
            "epoch 2 batch id 2821 loss 1.544411301612854 train acc 0.40682049805033677\n",
            "epoch 2 batch id 2831 loss 1.329074740409851 train acc 0.4070392529141646\n",
            "epoch 2 batch id 2841 loss 1.3986446857452393 train acc 0.4072014695529743\n",
            "epoch 2 batch id 2851 loss 1.5955921411514282 train acc 0.4072255349000351\n",
            "epoch 2 batch id 2861 loss 1.407223105430603 train acc 0.4072985844110451\n",
            "epoch 2 batch id 2871 loss 1.2050682306289673 train acc 0.4074636450714037\n",
            "epoch 2 batch id 2881 loss 1.5307583808898926 train acc 0.40757874869836863\n",
            "epoch 2 batch id 2891 loss 1.4885467290878296 train acc 0.40772548426150124\n",
            "epoch 2 batch id 2901 loss 1.5839765071868896 train acc 0.4077688728024819\n",
            "epoch 2 batch id 2911 loss 1.4258372783660889 train acc 0.4078065956715905\n",
            "epoch 2 batch id 2921 loss 1.3334791660308838 train acc 0.4078815046217049\n",
            "epoch 2 batch id 2931 loss 1.4107896089553833 train acc 0.40795590242238144\n",
            "epoch 2 batch id 2941 loss 1.4346452951431274 train acc 0.408019168650119\n",
            "epoch 2 batch id 2951 loss 1.5614030361175537 train acc 0.40815613351406305\n",
            "epoch 2 batch id 2961 loss 1.3408674001693726 train acc 0.40833966565349544\n",
            "epoch 2 batch id 2971 loss 1.446873426437378 train acc 0.40838522383036013\n",
            "epoch 2 batch id 2981 loss 1.5025080442428589 train acc 0.40843571787990607\n",
            "epoch 2 batch id 2991 loss 1.377808928489685 train acc 0.40851199431628216\n",
            "epoch 2 batch id 3001 loss 1.462129831314087 train acc 0.4085981756081306\n",
            "epoch 2 batch id 3011 loss 1.6164520978927612 train acc 0.40867340584523415\n",
            "epoch 2 batch id 3021 loss 1.6838723421096802 train acc 0.4086860724925521\n",
            "epoch 2 batch id 3031 loss 1.3811050653457642 train acc 0.40865741504453973\n",
            "epoch 2 batch id 3041 loss 1.547262191772461 train acc 0.40856728872081555\n",
            "epoch 2 batch id 3051 loss 1.4333105087280273 train acc 0.4086313913470993\n",
            "epoch 2 batch id 3061 loss 1.3222694396972656 train acc 0.4087410160078406\n",
            "epoch 2 batch id 3071 loss 1.5910109281539917 train acc 0.4088295750569847\n",
            "epoch 2 batch id 3081 loss 1.488276720046997 train acc 0.40886684518013633\n",
            "epoch 2 batch id 3091 loss 1.6197154521942139 train acc 0.4088785991588483\n",
            "epoch 2 batch id 3101 loss 1.477146863937378 train acc 0.4090363995485327\n",
            "epoch 2 batch id 3111 loss 1.6225334405899048 train acc 0.4091379379620701\n",
            "epoch 2 batch id 3121 loss 1.292588472366333 train acc 0.4093239346363345\n",
            "epoch 2 batch id 3131 loss 1.3817007541656494 train acc 0.40944885819227084\n",
            "epoch 2 batch id 3141 loss 1.6384236812591553 train acc 0.40952324100604903\n",
            "epoch 2 batch id 3151 loss 1.5483266115188599 train acc 0.4095475642653126\n",
            "epoch 2 batch id 3161 loss 1.1677876710891724 train acc 0.4095915058525783\n",
            "epoch 2 batch id 3171 loss 1.5409114360809326 train acc 0.40973864711447494\n",
            "epoch 2 batch id 3181 loss 1.6087919473648071 train acc 0.4097669757937755\n",
            "epoch 2 batch id 3191 loss 1.472723126411438 train acc 0.40997630053274836\n",
            "epoch 2 batch id 3201 loss 1.621208906173706 train acc 0.40999394720399873\n",
            "epoch 2 batch id 3211 loss 1.3935575485229492 train acc 0.4101817969479913\n",
            "epoch 2 batch id 3221 loss 1.7023279666900635 train acc 0.4103102685501397\n",
            "epoch 2 batch id 3231 loss 1.2502180337905884 train acc 0.4104572887650882\n",
            "epoch 2 batch id 3241 loss 1.3771131038665771 train acc 0.41052144399876583\n",
            "epoch 2 batch id 3251 loss 1.2999157905578613 train acc 0.4106092356198093\n",
            "epoch 2 batch id 3261 loss 1.3994275331497192 train acc 0.4107108632321374\n",
            "epoch 2 batch id 3271 loss 1.5946943759918213 train acc 0.4107211097523693\n",
            "epoch 2 batch id 3281 loss 1.6061502695083618 train acc 0.41073129381286194\n",
            "epoch 2 batch id 3291 loss 1.350192904472351 train acc 0.41083637192342753\n",
            "epoch 2 batch id 3301 loss 1.4069041013717651 train acc 0.41095501363223264\n",
            "epoch 2 batch id 3311 loss 1.600364327430725 train acc 0.4109691180912111\n",
            "epoch 2 batch id 3321 loss 1.4034684896469116 train acc 0.41103959650707617\n",
            "epoch 2 batch id 3331 loss 1.4035532474517822 train acc 0.41117063194235964\n",
            "epoch 2 batch id 3341 loss 1.6141716241836548 train acc 0.4111746108949416\n",
            "epoch 2 batch id 3351 loss 1.352557897567749 train acc 0.41123451954640405\n",
            "epoch 2 batch id 3361 loss 1.4299641847610474 train acc 0.4113080184468908\n",
            "epoch 2 batch id 3371 loss 1.4517008066177368 train acc 0.41139962177395434\n",
            "epoch 2 batch id 3381 loss 1.6204605102539062 train acc 0.41149992605737945\n",
            "epoch 2 batch id 3391 loss 1.3233829736709595 train acc 0.41152130639929224\n",
            "epoch 2 batch id 3401 loss 1.5865272283554077 train acc 0.4115839091443693\n",
            "epoch 2 batch id 3411 loss 1.5320122241973877 train acc 0.41165988712987395\n",
            "epoch 2 batch id 3421 loss 1.4955238103866577 train acc 0.4117034492838351\n",
            "epoch 2 batch id 3431 loss 1.5606712102890015 train acc 0.4117148790440105\n",
            "epoch 2 batch id 3441 loss 1.5647754669189453 train acc 0.4118760897994769\n",
            "epoch 2 batch id 3451 loss 1.640763521194458 train acc 0.41191864676905243\n",
            "epoch 2 batch id 3461 loss 1.5090460777282715 train acc 0.4121189685062121\n",
            "epoch 2 batch id 3471 loss 1.3784921169281006 train acc 0.412286624891962\n",
            "epoch 2 batch id 3481 loss 1.6019322872161865 train acc 0.4123590563056593\n",
            "epoch 2 batch id 3491 loss 1.3525935411453247 train acc 0.41252954024634775\n",
            "epoch 2 batch id 3501 loss 1.6524924039840698 train acc 0.412578548986004\n",
            "epoch 2 batch id 3511 loss 1.2686307430267334 train acc 0.4127563372258616\n",
            "epoch 2 batch id 3521 loss 1.697940707206726 train acc 0.41287986367509233\n",
            "epoch 2 batch id 3531 loss 1.6074743270874023 train acc 0.41295401444350044\n",
            "epoch 2 batch id 3541 loss 1.4971312284469604 train acc 0.41311158571025136\n",
            "epoch 2 batch id 3551 loss 1.5017863512039185 train acc 0.41327266967051535\n",
            "epoch 2 batch id 3561 loss 1.4447681903839111 train acc 0.4133801951698961\n",
            "epoch 2 batch id 3571 loss 1.7507448196411133 train acc 0.4135046205544665\n",
            "epoch 2 batch id 3581 loss 1.389717698097229 train acc 0.413493088522759\n",
            "epoch 2 batch id 3591 loss 1.454192042350769 train acc 0.4135773461431356\n",
            "epoch 2 batch id 3601 loss 1.3999202251434326 train acc 0.4137349000277701\n",
            "epoch 2 batch id 3611 loss 1.3395895957946777 train acc 0.41380071309886457\n",
            "epoch 2 batch id 3621 loss 1.368590235710144 train acc 0.41381438138635734\n",
            "epoch 2 batch id 3631 loss 1.3493599891662598 train acc 0.41382797438722113\n",
            "epoch 2 batch id 3641 loss 1.5551397800445557 train acc 0.4139874004394397\n",
            "epoch 2 batch id 3651 loss 1.5937670469284058 train acc 0.41406891947411667\n",
            "epoch 2 batch id 3661 loss 1.6955267190933228 train acc 0.4141713329691341\n",
            "epoch 2 batch id 3671 loss 1.4051958322525024 train acc 0.41419231816943614\n",
            "epoch 2 batch id 3681 loss 1.560553789138794 train acc 0.4143362876935615\n",
            "epoch 2 batch id 3691 loss 1.8049002885818481 train acc 0.41436094554321323\n",
            "epoch 2 batch id 3701 loss 1.395509123802185 train acc 0.4143939138070792\n",
            "epoch 2 batch id 3711 loss 1.450705885887146 train acc 0.41453617623282135\n",
            "epoch 2 batch id 3721 loss 1.3602722883224487 train acc 0.41466927573233003\n",
            "epoch 2 batch id 3731 loss 1.3098468780517578 train acc 0.414780722326454\n",
            "epoch 2 batch id 3741 loss 1.4897795915603638 train acc 0.4148623362737236\n",
            "epoch 2 batch id 3751 loss 1.6976370811462402 train acc 0.4149560117302053\n",
            "epoch 2 batch id 3761 loss 1.422655701637268 train acc 0.4150658069662324\n",
            "epoch 2 batch id 3771 loss 1.3658090829849243 train acc 0.4150755767700875\n",
            "epoch 2 batch id 3781 loss 1.4254521131515503 train acc 0.4151555474742132\n",
            "epoch 2 batch id 3791 loss 1.2490315437316895 train acc 0.41520624505407544\n",
            "epoch 2 batch id 3801 loss 1.324151635169983 train acc 0.4153799986845567\n",
            "epoch 2 batch id 3811 loss 1.3140084743499756 train acc 0.4154380411965363\n",
            "epoch 2 batch id 3821 loss 1.4347807168960571 train acc 0.4155284938497775\n",
            "epoch 2 batch id 3831 loss 1.5642058849334717 train acc 0.4156388671365179\n",
            "epoch 2 batch id 3841 loss 1.4396785497665405 train acc 0.41567951054412916\n",
            "epoch 2 batch id 3851 loss 1.5025599002838135 train acc 0.4157848610750454\n",
            "epoch 2 batch id 3861 loss 1.3586372137069702 train acc 0.4158694314944315\n",
            "epoch 2 batch id 3871 loss 1.3774348497390747 train acc 0.41592934642211316\n",
            "epoch 2 batch id 3881 loss 1.4671772718429565 train acc 0.416005056686421\n",
            "epoch 2 batch id 3891 loss 1.5750869512557983 train acc 0.4160442367000771\n",
            "epoch 2 batch id 3901 loss 1.381408452987671 train acc 0.416095231991797\n",
            "epoch 2 batch id 3911 loss 1.3470455408096313 train acc 0.4162498401943237\n",
            "epoch 2 batch id 3921 loss 1.5765520334243774 train acc 0.4162442616679419\n",
            "epoch 2 batch id 3931 loss 1.4865325689315796 train acc 0.41635398117527345\n",
            "epoch 2 batch id 3941 loss 1.5129214525222778 train acc 0.4164512496828216\n",
            "epoch 2 batch id 3951 loss 1.5888452529907227 train acc 0.41652825234117946\n",
            "epoch 2 batch id 3961 loss 1.3999109268188477 train acc 0.416644313304721\n",
            "epoch 2 batch id 3971 loss 1.4171355962753296 train acc 0.41676372450264415\n",
            "epoch 2 batch id 3981 loss 1.5251702070236206 train acc 0.41685506154232604\n",
            "epoch 2 batch id 3991 loss 1.5087743997573853 train acc 0.4169302806314207\n",
            "epoch 2 batch id 4001 loss 1.4147881269454956 train acc 0.41696216570857286\n",
            "epoch 2 batch id 4011 loss 1.4862650632858276 train acc 0.41707959361755176\n",
            "epoch 2 batch id 4021 loss 1.5154001712799072 train acc 0.41716535065904004\n",
            "epoch 2 batch id 4031 loss 1.5391390323638916 train acc 0.4172778156784917\n",
            "epoch 2 batch id 4041 loss 1.487548828125 train acc 0.4172737255629795\n",
            "epoch 2 batch id 4051 loss 1.343552589416504 train acc 0.4174509380399901\n",
            "epoch 2 batch id 4061 loss 1.6710420846939087 train acc 0.41755032627431665\n",
            "epoch 2 batch id 4071 loss 1.58550226688385 train acc 0.41770679808400885\n",
            "epoch 2 batch id 4081 loss 1.374983787536621 train acc 0.4177820999754962\n",
            "epoch 2 batch id 4091 loss 1.4995923042297363 train acc 0.41779592397946713\n",
            "epoch 2 batch id 4101 loss 1.523710012435913 train acc 0.41786683126066815\n",
            "epoch 2 batch id 4111 loss 1.5286492109298706 train acc 0.4179868036973972\n",
            "epoch 2 batch id 4121 loss 1.653859257698059 train acc 0.41804173744236833\n",
            "epoch 2 batch id 4131 loss 1.3688277006149292 train acc 0.41810018760590656\n",
            "epoch 2 batch id 4141 loss 1.6207301616668701 train acc 0.41812062303791353\n",
            "epoch 2 batch id 4151 loss 1.389797329902649 train acc 0.41818989400144546\n",
            "epoch 2 batch id 4161 loss 1.5604058504104614 train acc 0.41824756669069935\n",
            "epoch 2 batch id 4171 loss 1.3969311714172363 train acc 0.4183836310237353\n",
            "epoch 2 batch id 4181 loss 1.295976996421814 train acc 0.41844056445826355\n",
            "epoch 2 batch id 4191 loss 1.6657633781433105 train acc 0.4185419649248389\n",
            "epoch 2 batch id 4201 loss 1.3136796951293945 train acc 0.41873214710783146\n",
            "epoch 2 batch id 4211 loss 1.5280978679656982 train acc 0.4188472156257421\n",
            "epoch 2 batch id 4221 loss 1.280836820602417 train acc 0.4189580371949775\n",
            "epoch 2 batch id 4231 loss 1.4013447761535645 train acc 0.41900555424249586\n",
            "epoch 2 batch id 4241 loss 1.510493516921997 train acc 0.41914495402027824\n",
            "epoch 2 batch id 4251 loss 1.5031036138534546 train acc 0.4192285638673253\n",
            "epoch 2 batch id 4261 loss 1.4540339708328247 train acc 0.41927877845576156\n",
            "epoch 2 batch id 4271 loss 1.3060321807861328 train acc 0.4194019257785062\n",
            "epoch 2 batch id 4281 loss 1.4300360679626465 train acc 0.41957924550338704\n",
            "epoch 2 batch id 4291 loss 1.2331292629241943 train acc 0.4196537811698905\n",
            "epoch 2 batch id 4301 loss 1.257077693939209 train acc 0.41972433736340387\n",
            "epoch 2 batch id 4311 loss 1.2416894435882568 train acc 0.41979456622593364\n",
            "epoch 2 batch id 4321 loss 1.355681300163269 train acc 0.4198536218467947\n",
            "epoch 2 batch id 4331 loss 1.3608862161636353 train acc 0.41993405102747633\n",
            "epoch 2 batch id 4341 loss 1.486220121383667 train acc 0.4199925132457959\n",
            "epoch 2 batch id 4351 loss 1.4813313484191895 train acc 0.42008302689037\n",
            "epoch 2 batch id 4361 loss 1.532539963722229 train acc 0.42017670832377896\n",
            "epoch 2 batch id 4371 loss 1.632902979850769 train acc 0.42029140928849235\n",
            "epoch 2 batch id 4381 loss 1.3112385272979736 train acc 0.42038418740013694\n",
            "epoch 2 batch id 4391 loss 1.2606217861175537 train acc 0.42051568549305396\n",
            "epoch 2 batch id 4401 loss 1.4332444667816162 train acc 0.4206181833674165\n",
            "epoch 2 batch id 4411 loss 1.4088585376739502 train acc 0.42068833597823624\n",
            "epoch 2 batch id 4421 loss 1.3002808094024658 train acc 0.4208041167156752\n",
            "epoch 2 batch id 4431 loss 1.3357751369476318 train acc 0.42090879598284814\n",
            "epoch 2 batch id 4441 loss 1.3231956958770752 train acc 0.4210376322900248\n",
            "epoch 2 batch id 4451 loss 1.3900896310806274 train acc 0.4211202538755336\n",
            "epoch 2 batch id 4461 loss 1.2939285039901733 train acc 0.4211744844205335\n",
            "epoch 2 batch id 4471 loss 1.49982488155365 train acc 0.42121099865801837\n",
            "epoch 2 batch id 4481 loss 1.434038758277893 train acc 0.42118807185896007\n",
            "epoch 2 batch id 4491 loss 1.4817848205566406 train acc 0.42119655978623916\n",
            "epoch 2 batch id 4501 loss 1.3686951398849487 train acc 0.4213195678738058\n",
            "epoch 2 batch id 4511 loss 1.3230031728744507 train acc 0.4213935380181778\n",
            "epoch 2 batch id 4521 loss 1.285486102104187 train acc 0.4214153395266534\n",
            "epoch 2 batch id 4531 loss 1.444387674331665 train acc 0.42146118406532773\n",
            "epoch 2 batch id 4541 loss 1.151172399520874 train acc 0.42150338581810176\n",
            "epoch 2 batch id 4551 loss 1.3816232681274414 train acc 0.42163123489343\n",
            "epoch 2 batch id 4561 loss 1.3225592374801636 train acc 0.42168315610611706\n",
            "epoch 2 batch id 4571 loss 1.515588641166687 train acc 0.4217792879019908\n",
            "epoch 2 batch id 4581 loss 1.4754655361175537 train acc 0.4218204267627156\n",
            "epoch 2 batch id 4591 loss 1.4002732038497925 train acc 0.42188180679590503\n",
            "epoch 2 batch id 4601 loss 1.3076481819152832 train acc 0.42201084003477507\n",
            "epoch 2 batch id 4611 loss 1.4168192148208618 train acc 0.4221291476903058\n",
            "epoch 2 batch id 4621 loss 1.4310166835784912 train acc 0.4221691733391041\n",
            "epoch 2 batch id 4631 loss 1.6199654340744019 train acc 0.4222596361477003\n",
            "epoch 2 batch id 4641 loss 1.462284803390503 train acc 0.4224002100840336\n",
            "epoch 2 batch id 4651 loss 1.3827060461044312 train acc 0.42247634917222104\n",
            "epoch 2 batch id 4661 loss 1.528844952583313 train acc 0.42254880926839733\n",
            "epoch 2 batch id 4671 loss 1.3185031414031982 train acc 0.4226778259473346\n",
            "epoch 2 batch id 4681 loss 1.5283524990081787 train acc 0.42275955992309333\n",
            "epoch 2 batch id 4691 loss 1.06365966796875 train acc 0.4228642613515242\n",
            "epoch 2 batch id 4701 loss 1.4850906133651733 train acc 0.42290868964050204\n",
            "epoch 2 batch id 4711 loss 1.2699450254440308 train acc 0.4230059966036935\n",
            "epoch 2 batch id 4721 loss 1.4500499963760376 train acc 0.4230631751747511\n",
            "epoch 2 batch id 4731 loss 1.3610053062438965 train acc 0.4230804798139928\n",
            "epoch 2 batch id 4741 loss 1.5148876905441284 train acc 0.42314385150812067\n",
            "epoch 2 batch id 4751 loss 1.4031634330749512 train acc 0.4232398442433172\n",
            "epoch 2 batch id 4761 loss 1.5468717813491821 train acc 0.4232829237555136\n",
            "epoch 2 batch id 4771 loss 1.265145182609558 train acc 0.4233552976315238\n",
            "epoch 2 batch id 4781 loss 1.4403083324432373 train acc 0.4234175643170885\n",
            "epoch 2 batch id 4791 loss 1.4963881969451904 train acc 0.42348609371738677\n",
            "epoch 2 batch id 4801 loss 1.3938226699829102 train acc 0.42356410122891064\n",
            "epoch 2 batch id 4811 loss 1.173764705657959 train acc 0.4236547755144461\n",
            "epoch 2 batch id 4821 loss 1.3856174945831299 train acc 0.4237191454055175\n",
            "epoch 2 batch id 4831 loss 1.4552005529403687 train acc 0.42383823225005174\n",
            "epoch 2 batch id 4841 loss 1.3633949756622314 train acc 0.42395682710183846\n",
            "epoch 2 batch id 4851 loss 1.2375606298446655 train acc 0.42397186147186144\n",
            "epoch 2 batch id 4861 loss 1.3726303577423096 train acc 0.42404469245011317\n",
            "epoch 2 batch id 4871 loss 1.342353105545044 train acc 0.4241011855881749\n",
            "epoch 2 batch id 4881 loss 1.4815882444381714 train acc 0.42418945912722805\n",
            "epoch 2 batch id 4891 loss 1.6114964485168457 train acc 0.42422306276835003\n",
            "epoch 2 batch id 4901 loss 1.4906195402145386 train acc 0.4242852224035911\n",
            "epoch 2 batch id 4911 loss 1.255893588066101 train acc 0.42439803502341683\n",
            "epoch 2 batch id 4921 loss 1.478108286857605 train acc 0.42444053546027233\n",
            "epoch 2 batch id 4931 loss 1.4907125234603882 train acc 0.4244511762320016\n",
            "epoch 2 batch id 4941 loss 1.3134514093399048 train acc 0.4245376695001012\n",
            "epoch 2 batch id 4951 loss 1.6746280193328857 train acc 0.4245827863057968\n",
            "epoch 2 train acc 0.42466060392099325\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dadfc72b14d4cf39b121f3eab4509cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 loss 1.3179408311843872 test acc 0.491497892228739\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5daeeeb0c38f47c58aebfa79236e61e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 batch id 1 loss 1.38813054561615 train acc 0.46875\n",
            "epoch 3 batch id 11 loss 1.308872103691101 train acc 0.46448863636363635\n",
            "epoch 3 batch id 21 loss 1.453298807144165 train acc 0.4732142857142857\n",
            "epoch 3 batch id 31 loss 1.5506757497787476 train acc 0.45514112903225806\n",
            "epoch 3 batch id 41 loss 1.1300585269927979 train acc 0.4653201219512195\n",
            "epoch 3 batch id 51 loss 1.6154149770736694 train acc 0.46047794117647056\n",
            "epoch 3 batch id 61 loss 1.476393222808838 train acc 0.4600409836065574\n",
            "epoch 3 batch id 71 loss 1.5789555311203003 train acc 0.46192781690140844\n",
            "epoch 3 batch id 81 loss 1.4925018548965454 train acc 0.46199845679012347\n",
            "epoch 3 batch id 91 loss 1.5062472820281982 train acc 0.46274038461538464\n",
            "epoch 3 batch id 101 loss 1.290747046470642 train acc 0.46163366336633666\n",
            "epoch 3 batch id 111 loss 1.582720398902893 train acc 0.46269707207207206\n",
            "epoch 3 batch id 121 loss 1.6509838104248047 train acc 0.4617768595041322\n",
            "epoch 3 batch id 131 loss 1.4422297477722168 train acc 0.461712786259542\n",
            "epoch 3 batch id 141 loss 1.404878854751587 train acc 0.4605496453900709\n",
            "epoch 3 batch id 151 loss 1.2913976907730103 train acc 0.460885761589404\n",
            "epoch 3 batch id 161 loss 1.55742347240448 train acc 0.46011257763975155\n",
            "epoch 3 batch id 171 loss 1.5060596466064453 train acc 0.4590643274853801\n",
            "epoch 3 batch id 181 loss 1.4317216873168945 train acc 0.45821823204419887\n",
            "epoch 3 batch id 191 loss 1.4699112176895142 train acc 0.4580333769633508\n",
            "epoch 3 batch id 201 loss 1.169155240058899 train acc 0.45949937810945274\n",
            "epoch 3 batch id 211 loss 1.549781084060669 train acc 0.4591232227488152\n",
            "epoch 3 batch id 221 loss 1.4924671649932861 train acc 0.4594174208144796\n",
            "epoch 3 batch id 231 loss 1.4822936058044434 train acc 0.45921266233766234\n",
            "epoch 3 batch id 241 loss 1.5748069286346436 train acc 0.4592842323651452\n",
            "epoch 3 batch id 251 loss 1.3544353246688843 train acc 0.4581050796812749\n",
            "epoch 3 batch id 261 loss 1.2648801803588867 train acc 0.46048850574712646\n",
            "epoch 3 batch id 271 loss 1.2689281702041626 train acc 0.4616005535055351\n",
            "epoch 3 batch id 281 loss 1.3535399436950684 train acc 0.4627446619217082\n",
            "epoch 3 batch id 291 loss 1.611954689025879 train acc 0.4638638316151203\n",
            "epoch 3 batch id 301 loss 1.6126375198364258 train acc 0.464078073089701\n",
            "epoch 3 batch id 311 loss 1.4670310020446777 train acc 0.46272106109324757\n",
            "epoch 3 batch id 321 loss 1.663199543952942 train acc 0.46213006230529596\n",
            "epoch 3 batch id 331 loss 1.5469334125518799 train acc 0.461952416918429\n",
            "epoch 3 batch id 341 loss 1.5518113374710083 train acc 0.46173936950146627\n",
            "epoch 3 batch id 351 loss 1.4981540441513062 train acc 0.4622952279202279\n",
            "epoch 3 batch id 361 loss 1.5010037422180176 train acc 0.4629934210526316\n",
            "epoch 3 batch id 371 loss 1.4311270713806152 train acc 0.4638645552560647\n",
            "epoch 3 batch id 381 loss 1.4253332614898682 train acc 0.46366469816272965\n",
            "epoch 3 batch id 391 loss 1.4646631479263306 train acc 0.4631553708439898\n",
            "epoch 3 batch id 401 loss 1.364707112312317 train acc 0.4625545511221945\n",
            "epoch 3 batch id 411 loss 1.2221457958221436 train acc 0.4625532238442822\n",
            "epoch 3 batch id 421 loss 1.4929227828979492 train acc 0.46229216152019004\n",
            "epoch 3 batch id 431 loss 1.5254777669906616 train acc 0.4622607308584687\n",
            "epoch 3 batch id 441 loss 1.4729868173599243 train acc 0.4623370181405896\n",
            "epoch 3 batch id 451 loss 1.326303243637085 train acc 0.4623059866962306\n",
            "epoch 3 batch id 461 loss 1.3676648139953613 train acc 0.46268302603036876\n",
            "epoch 3 batch id 471 loss 1.3923615217208862 train acc 0.463010881104034\n",
            "epoch 3 batch id 481 loss 1.5578163862228394 train acc 0.46326013513513514\n",
            "epoch 3 batch id 491 loss 1.250990390777588 train acc 0.46353105906313646\n",
            "epoch 3 batch id 501 loss 1.7522976398468018 train acc 0.4627619760479042\n",
            "epoch 3 batch id 511 loss 1.3828730583190918 train acc 0.4627874266144814\n",
            "epoch 3 batch id 521 loss 1.49717378616333 train acc 0.4626919385796545\n",
            "epoch 3 batch id 531 loss 1.6469194889068604 train acc 0.4624823446327684\n",
            "epoch 3 batch id 541 loss 1.4344252347946167 train acc 0.4630891866913124\n",
            "epoch 3 batch id 551 loss 1.3029388189315796 train acc 0.4629083484573503\n",
            "epoch 3 batch id 561 loss 1.237842321395874 train acc 0.4634302584670232\n",
            "epoch 3 batch id 571 loss 1.589302897453308 train acc 0.4637423380035026\n",
            "epoch 3 batch id 581 loss 1.4539902210235596 train acc 0.46342512908777966\n",
            "epoch 3 batch id 591 loss 1.315519094467163 train acc 0.46380604906937395\n",
            "epoch 3 batch id 601 loss 1.3473438024520874 train acc 0.46427828618968386\n",
            "epoch 3 batch id 611 loss 1.3126059770584106 train acc 0.4646327741407529\n",
            "epoch 3 batch id 621 loss 1.441246747970581 train acc 0.46520229468599034\n",
            "epoch 3 batch id 631 loss 1.5386030673980713 train acc 0.46530804278922344\n",
            "epoch 3 batch id 641 loss 1.4969358444213867 train acc 0.46519110764430577\n",
            "epoch 3 batch id 651 loss 1.3463420867919922 train acc 0.4648377496159754\n",
            "epoch 3 batch id 661 loss 1.4581353664398193 train acc 0.46522787443267777\n",
            "epoch 3 batch id 671 loss 1.5994282960891724 train acc 0.46542008196721313\n",
            "epoch 3 batch id 681 loss 1.4247283935546875 train acc 0.46517070484581496\n",
            "epoch 3 batch id 691 loss 1.239675521850586 train acc 0.4655842981186686\n",
            "epoch 3 batch id 701 loss 1.2752022743225098 train acc 0.4658969329529244\n",
            "epoch 3 batch id 711 loss 1.5004127025604248 train acc 0.46565137130801687\n",
            "epoch 3 batch id 721 loss 1.3404252529144287 train acc 0.46536927877947293\n",
            "epoch 3 batch id 731 loss 1.2088068723678589 train acc 0.46569339945280436\n",
            "epoch 3 batch id 741 loss 1.5537605285644531 train acc 0.4655448717948718\n",
            "epoch 3 batch id 751 loss 1.5193190574645996 train acc 0.4654211051930759\n",
            "epoch 3 batch id 761 loss 1.148158073425293 train acc 0.46603975032851513\n",
            "epoch 3 batch id 771 loss 1.7601957321166992 train acc 0.46593304150453957\n",
            "epoch 3 batch id 781 loss 1.4827755689620972 train acc 0.4659691101152369\n",
            "epoch 3 batch id 791 loss 1.2507693767547607 train acc 0.46624130847029077\n",
            "epoch 3 batch id 801 loss 1.408416748046875 train acc 0.46676029962546817\n",
            "epoch 3 batch id 811 loss 1.5392303466796875 train acc 0.46711236128236744\n",
            "epoch 3 batch id 821 loss 1.3920313119888306 train acc 0.4671894031668697\n",
            "epoch 3 batch id 831 loss 1.5525438785552979 train acc 0.46711416967509023\n",
            "epoch 3 batch id 841 loss 1.3129745721817017 train acc 0.46748662306777644\n",
            "epoch 3 batch id 851 loss 1.3708598613739014 train acc 0.4673729435957697\n",
            "epoch 3 batch id 861 loss 1.5589932203292847 train acc 0.467298199767712\n",
            "epoch 3 batch id 871 loss 1.3084299564361572 train acc 0.46731486796785304\n",
            "epoch 3 batch id 881 loss 1.4920055866241455 train acc 0.46718927355278095\n",
            "epoch 3 batch id 891 loss 1.295078992843628 train acc 0.46746983726150393\n",
            "epoch 3 batch id 901 loss 1.4407854080200195 train acc 0.46758809655937844\n",
            "epoch 3 batch id 911 loss 1.5405067205429077 train acc 0.46772091108671787\n",
            "epoch 3 batch id 921 loss 1.580204963684082 train acc 0.4676811889250814\n",
            "epoch 3 batch id 931 loss 1.3373925685882568 train acc 0.46774301825993553\n",
            "epoch 3 batch id 941 loss 1.2742937803268433 train acc 0.46777032412327313\n",
            "epoch 3 batch id 951 loss 1.4326921701431274 train acc 0.46789563617245006\n",
            "epoch 3 batch id 961 loss 1.3942736387252808 train acc 0.46796956295525494\n",
            "epoch 3 batch id 971 loss 1.4731382131576538 train acc 0.46833161688980435\n",
            "epoch 3 batch id 981 loss 1.4720135927200317 train acc 0.4684951580020387\n",
            "epoch 3 batch id 991 loss 1.2319427728652954 train acc 0.4684661957618567\n",
            "epoch 3 batch id 1001 loss 1.4318972826004028 train acc 0.46818806193806195\n",
            "epoch 3 batch id 1011 loss 1.4527498483657837 train acc 0.4684254451038576\n",
            "epoch 3 batch id 1021 loss 1.2649948596954346 train acc 0.46848983839373165\n",
            "epoch 3 batch id 1031 loss 1.4653326272964478 train acc 0.46862875848690594\n",
            "epoch 3 batch id 1041 loss 1.3744040727615356 train acc 0.4684948366954851\n",
            "epoch 3 batch id 1051 loss 1.3100262880325317 train acc 0.46836346336822077\n",
            "epoch 3 batch id 1061 loss 1.4874267578125 train acc 0.46848491988689916\n",
            "epoch 3 batch id 1071 loss 1.1243996620178223 train acc 0.46875\n",
            "epoch 3 batch id 1081 loss 1.3765350580215454 train acc 0.4686777289546716\n",
            "epoch 3 batch id 1091 loss 1.6295230388641357 train acc 0.46847788725939504\n",
            "epoch 3 batch id 1101 loss 1.4535876512527466 train acc 0.4688919164396004\n",
            "epoch 3 batch id 1111 loss 1.2134361267089844 train acc 0.4693266201620162\n",
            "epoch 3 batch id 1121 loss 1.4691815376281738 train acc 0.4694747992863515\n",
            "epoch 3 batch id 1131 loss 1.3792915344238281 train acc 0.46959272767462423\n",
            "epoch 3 batch id 1141 loss 1.4032648801803589 train acc 0.4697222830850131\n",
            "epoch 3 batch id 1151 loss 1.4868841171264648 train acc 0.4697817115551694\n",
            "epoch 3 batch id 1161 loss 1.460193157196045 train acc 0.4699074074074074\n",
            "epoch 3 batch id 1171 loss 1.3346494436264038 train acc 0.47016438941076005\n",
            "epoch 3 batch id 1181 loss 1.4106156826019287 train acc 0.4701127222692633\n",
            "epoch 3 batch id 1191 loss 1.396611213684082 train acc 0.4703374265323258\n",
            "epoch 3 batch id 1201 loss 1.5167241096496582 train acc 0.47057139883430477\n",
            "epoch 3 batch id 1211 loss 1.2653841972351074 train acc 0.47063377374071014\n",
            "epoch 3 batch id 1221 loss 1.450881838798523 train acc 0.4706055487305487\n",
            "epoch 3 batch id 1231 loss 1.3035035133361816 train acc 0.4707681762794476\n",
            "epoch 3 batch id 1241 loss 1.3199498653411865 train acc 0.4707896857373086\n",
            "epoch 3 batch id 1251 loss 1.1056312322616577 train acc 0.47083583133493206\n",
            "epoch 3 batch id 1261 loss 1.52275812625885 train acc 0.47088124504361617\n",
            "epoch 3 batch id 1271 loss 1.3440390825271606 train acc 0.4709751180173092\n",
            "epoch 3 batch id 1281 loss 1.5540958642959595 train acc 0.47103093286494924\n",
            "epoch 3 batch id 1291 loss 1.4924688339233398 train acc 0.47112219209914796\n",
            "epoch 3 batch id 1301 loss 1.5215883255004883 train acc 0.4709238086087625\n",
            "epoch 3 batch id 1311 loss 1.529551386833191 train acc 0.4708118802440885\n",
            "epoch 3 batch id 1321 loss 1.3205171823501587 train acc 0.4707962717638153\n",
            "epoch 3 batch id 1331 loss 1.3753070831298828 train acc 0.47089829075882794\n",
            "epoch 3 batch id 1341 loss 1.2606674432754517 train acc 0.4709754847129008\n",
            "epoch 3 batch id 1351 loss 1.195117712020874 train acc 0.4712365840118431\n",
            "epoch 3 batch id 1361 loss 1.464171051979065 train acc 0.4710231447465099\n",
            "epoch 3 batch id 1371 loss 1.358581781387329 train acc 0.4711205324580598\n",
            "epoch 3 batch id 1381 loss 1.5366051197052002 train acc 0.4711599384503983\n",
            "epoch 3 batch id 1391 loss 1.1194456815719604 train acc 0.47136727174694465\n",
            "epoch 3 batch id 1401 loss 1.3813461065292358 train acc 0.4714824232690935\n",
            "epoch 3 batch id 1411 loss 1.2248562574386597 train acc 0.4715737951807229\n",
            "epoch 3 batch id 1421 loss 1.3361111879348755 train acc 0.47178483462350457\n",
            "epoch 3 batch id 1431 loss 1.1559274196624756 train acc 0.47194924877707894\n",
            "epoch 3 batch id 1441 loss 1.3922768831253052 train acc 0.4718077723802915\n",
            "epoch 3 batch id 1451 loss 1.2929662466049194 train acc 0.4720559097174363\n",
            "epoch 3 batch id 1461 loss 1.3929225206375122 train acc 0.4720653661875428\n",
            "epoch 3 batch id 1471 loss 1.5265297889709473 train acc 0.47209593813732154\n",
            "epoch 3 batch id 1481 loss 1.3021266460418701 train acc 0.47230545239702904\n",
            "epoch 3 batch id 1491 loss 1.2603172063827515 train acc 0.4723025653923541\n",
            "epoch 3 batch id 1501 loss 1.3253318071365356 train acc 0.4724870919387075\n",
            "epoch 3 batch id 1511 loss 1.3384100198745728 train acc 0.4725864493712773\n",
            "epoch 3 batch id 1521 loss 1.336776614189148 train acc 0.4727872287968442\n",
            "epoch 3 batch id 1531 loss 1.4479167461395264 train acc 0.4728731221423906\n",
            "epoch 3 batch id 1541 loss 1.5249327421188354 train acc 0.47299845879299157\n",
            "epoch 3 batch id 1551 loss 1.3088006973266602 train acc 0.4728803997421019\n",
            "epoch 3 batch id 1561 loss 1.4589639902114868 train acc 0.4730941704035874\n",
            "epoch 3 batch id 1571 loss 1.3513555526733398 train acc 0.47330521960534694\n",
            "epoch 3 batch id 1581 loss 1.4983633756637573 train acc 0.4731874604680582\n",
            "epoch 3 batch id 1591 loss 1.3080928325653076 train acc 0.4733167033312382\n",
            "epoch 3 batch id 1601 loss 1.5186268091201782 train acc 0.47339553404122425\n",
            "epoch 3 batch id 1611 loss 1.300236463546753 train acc 0.47342489137181876\n",
            "epoch 3 batch id 1621 loss 1.215633511543274 train acc 0.4732900215916101\n",
            "epoch 3 batch id 1631 loss 1.2455168962478638 train acc 0.4733196658491723\n",
            "epoch 3 batch id 1641 loss 1.5727304220199585 train acc 0.47347273004265694\n",
            "epoch 3 batch id 1651 loss 1.5050886869430542 train acc 0.47357662023016356\n",
            "epoch 3 batch id 1661 loss 1.4679064750671387 train acc 0.4736604455147502\n",
            "epoch 3 batch id 1671 loss 1.3195010423660278 train acc 0.47381807301017353\n",
            "epoch 3 batch id 1681 loss 1.5428944826126099 train acc 0.47371356335514575\n",
            "epoch 3 batch id 1691 loss 1.3610605001449585 train acc 0.4738597723240686\n",
            "epoch 3 batch id 1701 loss 1.3498449325561523 train acc 0.4738297325102881\n",
            "epoch 3 batch id 1711 loss 1.3899760246276855 train acc 0.4738091759205143\n",
            "epoch 3 batch id 1721 loss 1.3386448621749878 train acc 0.4738705694363742\n",
            "epoch 3 batch id 1731 loss 1.3012397289276123 train acc 0.4738229347198151\n",
            "epoch 3 batch id 1741 loss 1.251436710357666 train acc 0.4740720132107984\n",
            "epoch 3 batch id 1751 loss 1.0431009531021118 train acc 0.4743985579668761\n",
            "epoch 3 batch id 1761 loss 1.0896753072738647 train acc 0.4746415388983532\n",
            "epoch 3 batch id 1771 loss 1.3371096849441528 train acc 0.4747229672501412\n",
            "epoch 3 batch id 1781 loss 1.3574024438858032 train acc 0.4747947080291971\n",
            "epoch 3 batch id 1791 loss 1.681861400604248 train acc 0.4747173366834171\n",
            "epoch 3 batch id 1801 loss 1.3971014022827148 train acc 0.47485771793448084\n",
            "epoch 3 batch id 1811 loss 1.225468397140503 train acc 0.47500517669795694\n",
            "epoch 3 batch id 1821 loss 1.2198467254638672 train acc 0.47497940691927515\n",
            "epoch 3 batch id 1831 loss 1.1369013786315918 train acc 0.4751075232113599\n",
            "epoch 3 batch id 1841 loss 1.2689409255981445 train acc 0.47523424769147204\n",
            "epoch 3 batch id 1851 loss 1.5468229055404663 train acc 0.47519921663965425\n",
            "epoch 3 batch id 1861 loss 1.3638856410980225 train acc 0.47514777001612035\n",
            "epoch 3 batch id 1871 loss 1.3590143918991089 train acc 0.4752889497594869\n",
            "epoch 3 batch id 1881 loss 1.392739176750183 train acc 0.4753538676236045\n",
            "epoch 3 batch id 1891 loss 1.2344671487808228 train acc 0.4756246694870439\n",
            "epoch 3 batch id 1901 loss 1.3130837678909302 train acc 0.4757775512887954\n",
            "epoch 3 batch id 1911 loss 1.257550597190857 train acc 0.4757980115122972\n",
            "epoch 3 batch id 1921 loss 1.312397837638855 train acc 0.47581012493492975\n",
            "epoch 3 batch id 1931 loss 1.4395549297332764 train acc 0.476097229414811\n",
            "epoch 3 batch id 1941 loss 1.2389625310897827 train acc 0.47619622617207624\n",
            "epoch 3 batch id 1951 loss 1.4108779430389404 train acc 0.4761740773962071\n",
            "epoch 3 batch id 1961 loss 1.3533731698989868 train acc 0.47632744773074964\n",
            "epoch 3 batch id 1971 loss 1.4159208536148071 train acc 0.4763603500761035\n",
            "epoch 3 batch id 1981 loss 1.325252890586853 train acc 0.4764560196870268\n",
            "epoch 3 batch id 1991 loss 1.2724279165267944 train acc 0.4764016197890507\n",
            "epoch 3 batch id 2001 loss 1.517524242401123 train acc 0.4765273613193403\n",
            "epoch 3 batch id 2011 loss 1.470446228981018 train acc 0.47651976628543014\n",
            "epoch 3 batch id 2021 loss 1.4254060983657837 train acc 0.4764735898070262\n",
            "epoch 3 batch id 2031 loss 1.3869527578353882 train acc 0.4764663343180699\n",
            "epoch 3 batch id 2041 loss 1.2649636268615723 train acc 0.4764514943655071\n",
            "epoch 3 batch id 2051 loss 1.359777808189392 train acc 0.4765967820575329\n",
            "epoch 3 batch id 2061 loss 1.3436521291732788 train acc 0.4766193595342067\n",
            "epoch 3 batch id 2071 loss 1.3534830808639526 train acc 0.4766492636407533\n",
            "epoch 3 batch id 2081 loss 1.4911198616027832 train acc 0.47655874579529073\n",
            "epoch 3 batch id 2091 loss 1.3780196905136108 train acc 0.4765961262553802\n",
            "epoch 3 batch id 2101 loss 1.2662243843078613 train acc 0.4766926463588767\n",
            "epoch 3 batch id 2111 loss 1.4219889640808105 train acc 0.4766994315490289\n",
            "epoch 3 batch id 2121 loss 1.1493867635726929 train acc 0.4768019212635549\n",
            "epoch 3 batch id 2131 loss 1.5419776439666748 train acc 0.4768887846081652\n",
            "epoch 3 batch id 2141 loss 1.470072627067566 train acc 0.4769529425502102\n",
            "epoch 3 batch id 2151 loss 1.3464471101760864 train acc 0.47709640864714087\n",
            "epoch 3 batch id 2161 loss 1.5146139860153198 train acc 0.47705055529847296\n",
            "epoch 3 batch id 2171 loss 1.3252134323120117 train acc 0.4771202786734224\n",
            "epoch 3 batch id 2181 loss 1.3096944093704224 train acc 0.47711772122879414\n",
            "epoch 3 batch id 2191 loss 1.5518028736114502 train acc 0.4773219990871748\n",
            "epoch 3 batch id 2201 loss 1.6450234651565552 train acc 0.4772901522035438\n",
            "epoch 3 batch id 2211 loss 1.4650957584381104 train acc 0.47733632971506107\n",
            "epoch 3 batch id 2221 loss 1.530896544456482 train acc 0.4773328455650608\n",
            "epoch 3 batch id 2231 loss 1.29532790184021 train acc 0.4775184894666069\n",
            "epoch 3 batch id 2241 loss 1.480579137802124 train acc 0.4774166108879964\n",
            "epoch 3 batch id 2251 loss 1.4220296144485474 train acc 0.47746834740115507\n",
            "epoch 3 batch id 2261 loss 1.0855249166488647 train acc 0.47747816231755863\n",
            "epoch 3 batch id 2271 loss 1.5093027353286743 train acc 0.47752229194187584\n",
            "epoch 3 batch id 2281 loss 1.4890213012695312 train acc 0.47759343489697503\n",
            "epoch 3 batch id 2291 loss 1.3928849697113037 train acc 0.47785492143168923\n",
            "epoch 3 batch id 2301 loss 1.4073175191879272 train acc 0.4778764667535854\n",
            "epoch 3 batch id 2311 loss 1.2465174198150635 train acc 0.47801276503678064\n",
            "epoch 3 batch id 2321 loss 1.0359723567962646 train acc 0.4781748168892719\n",
            "epoch 3 batch id 2331 loss 1.255151391029358 train acc 0.4781947125697126\n",
            "epoch 3 batch id 2341 loss 1.6737041473388672 train acc 0.4782678342588637\n",
            "epoch 3 batch id 2351 loss 1.41917085647583 train acc 0.4782871650361548\n",
            "epoch 3 batch id 2361 loss 1.5355584621429443 train acc 0.4783394218551461\n",
            "epoch 3 batch id 2371 loss 1.2840503454208374 train acc 0.4784966786166175\n",
            "epoch 3 batch id 2381 loss 1.3673923015594482 train acc 0.4786394897102058\n",
            "epoch 3 batch id 2391 loss 1.3452873229980469 train acc 0.47868308239230445\n",
            "epoch 3 batch id 2401 loss 1.495223879814148 train acc 0.47873281965847564\n",
            "epoch 3 batch id 2411 loss 1.4736298322677612 train acc 0.4788728743260058\n",
            "epoch 3 batch id 2421 loss 1.35427725315094 train acc 0.4788310615448162\n",
            "epoch 3 batch id 2431 loss 1.573868751525879 train acc 0.47885386672151375\n",
            "epoch 3 batch id 2441 loss 1.2904942035675049 train acc 0.47893409463334696\n",
            "epoch 3 batch id 2451 loss 1.230139970779419 train acc 0.4789499184006528\n",
            "epoch 3 batch id 2461 loss 1.4687066078186035 train acc 0.47909259447379116\n",
            "epoch 3 batch id 2471 loss 1.2895405292510986 train acc 0.4791392654795629\n",
            "epoch 3 batch id 2481 loss 1.3845752477645874 train acc 0.4791225816203144\n",
            "epoch 3 batch id 2491 loss 1.568751573562622 train acc 0.4790683962264151\n",
            "epoch 3 batch id 2501 loss 1.1359550952911377 train acc 0.4790771191523391\n",
            "epoch 3 batch id 2511 loss 1.3981311321258545 train acc 0.47920400238948624\n",
            "epoch 3 batch id 2521 loss 1.280288577079773 train acc 0.4791191491471638\n",
            "epoch 3 batch id 2531 loss 1.402596116065979 train acc 0.4791275681548795\n",
            "epoch 3 batch id 2541 loss 1.3152817487716675 train acc 0.47930809720582446\n",
            "epoch 3 batch id 2551 loss 1.5416555404663086 train acc 0.4791135829086633\n",
            "epoch 3 batch id 2561 loss 1.4051529169082642 train acc 0.47908531823506445\n",
            "epoch 3 batch id 2571 loss 1.2196866273880005 train acc 0.4792274406845585\n",
            "epoch 3 batch id 2581 loss 1.4256477355957031 train acc 0.47925343858969394\n",
            "epoch 3 batch id 2591 loss 1.1948790550231934 train acc 0.47925511385565417\n",
            "epoch 3 batch id 2601 loss 1.4423394203186035 train acc 0.4792868127643214\n",
            "epoch 3 batch id 2611 loss 1.3264811038970947 train acc 0.4792584258904634\n",
            "epoch 3 batch id 2621 loss 1.4034572839736938 train acc 0.4792421785578024\n",
            "epoch 3 batch id 2631 loss 1.4804884195327759 train acc 0.4791904218928164\n",
            "epoch 3 batch id 2641 loss 1.2449833154678345 train acc 0.4793402120408936\n",
            "epoch 3 batch id 2651 loss 1.2943238019943237 train acc 0.47938278008298757\n",
            "epoch 3 batch id 2661 loss 1.3697998523712158 train acc 0.47950723412251034\n",
            "epoch 3 batch id 2671 loss 1.3860398530960083 train acc 0.47953715836765254\n",
            "epoch 3 batch id 2681 loss 1.3460166454315186 train acc 0.47945612644535623\n",
            "epoch 3 batch id 2691 loss 1.488878846168518 train acc 0.47942795429208473\n",
            "epoch 3 batch id 2701 loss 1.3985438346862793 train acc 0.4794578396890041\n",
            "epoch 3 batch id 2711 loss 1.3305644989013672 train acc 0.4795451401696791\n",
            "epoch 3 batch id 2721 loss 1.2246352434158325 train acc 0.47962031422271223\n",
            "epoch 3 batch id 2731 loss 1.1592090129852295 train acc 0.4796606096667887\n",
            "epoch 3 batch id 2741 loss 1.536907434463501 train acc 0.4796550072966071\n",
            "epoch 3 batch id 2751 loss 1.1409666538238525 train acc 0.47963808615049075\n",
            "epoch 3 batch id 2761 loss 1.2062690258026123 train acc 0.47973447120608476\n",
            "epoch 3 batch id 2771 loss 1.4557722806930542 train acc 0.4796835528690004\n",
            "epoch 3 batch id 2781 loss 1.3944422006607056 train acc 0.47978469974829197\n",
            "epoch 3 batch id 2791 loss 1.377981424331665 train acc 0.4797899498387675\n",
            "epoch 3 batch id 2801 loss 1.2848869562149048 train acc 0.47987883791503033\n",
            "epoch 3 batch id 2811 loss 1.2386800050735474 train acc 0.4800838224831021\n",
            "epoch 3 batch id 2821 loss 1.311887264251709 train acc 0.4801267281105991\n",
            "epoch 3 batch id 2831 loss 1.1844536066055298 train acc 0.48038458141999296\n",
            "epoch 3 batch id 2841 loss 1.2266066074371338 train acc 0.48051412354804646\n",
            "epoch 3 batch id 2851 loss 1.4982494115829468 train acc 0.4804454577341284\n",
            "epoch 3 batch id 2861 loss 1.2275928258895874 train acc 0.48047011534428524\n",
            "epoch 3 batch id 2871 loss 1.0761085748672485 train acc 0.4805490247300592\n",
            "epoch 3 batch id 2881 loss 1.3906869888305664 train acc 0.48065450364456785\n",
            "epoch 3 batch id 2891 loss 1.3543248176574707 train acc 0.48069980110688343\n",
            "epoch 3 batch id 2901 loss 1.2439712285995483 train acc 0.48064245087900725\n",
            "epoch 3 batch id 2911 loss 1.1968345642089844 train acc 0.48073041909996567\n",
            "epoch 3 batch id 2921 loss 1.1649653911590576 train acc 0.48076429305032525\n",
            "epoch 3 batch id 2931 loss 1.256469488143921 train acc 0.48082459058341864\n",
            "epoch 3 batch id 2941 loss 1.3865309953689575 train acc 0.48083134988099285\n",
            "epoch 3 batch id 2951 loss 1.4999186992645264 train acc 0.48090689596746866\n",
            "epoch 3 batch id 2961 loss 1.227612018585205 train acc 0.48101359338061467\n",
            "epoch 3 batch id 2971 loss 1.3859587907791138 train acc 0.4809670565466173\n",
            "epoch 3 batch id 2981 loss 1.2685296535491943 train acc 0.4810151794699765\n",
            "epoch 3 batch id 2991 loss 1.2227731943130493 train acc 0.48114656469408223\n",
            "epoch 3 batch id 3001 loss 1.3686673641204834 train acc 0.4812250083305565\n",
            "epoch 3 batch id 3011 loss 1.3775981664657593 train acc 0.4812147127200266\n",
            "epoch 3 batch id 3021 loss 1.5127891302108765 train acc 0.4812458622972526\n",
            "epoch 3 batch id 3031 loss 1.19092857837677 train acc 0.4812716512702079\n",
            "epoch 3 batch id 3041 loss 1.5371426343917847 train acc 0.4812664419598816\n",
            "epoch 3 batch id 3051 loss 1.3357187509536743 train acc 0.4813124795149131\n",
            "epoch 3 batch id 3061 loss 1.149523138999939 train acc 0.4814041571381901\n",
            "epoch 3 batch id 3071 loss 1.361513376235962 train acc 0.48154102898078804\n",
            "epoch 3 batch id 3081 loss 1.4843101501464844 train acc 0.48157051282051283\n",
            "epoch 3 batch id 3091 loss 1.4421645402908325 train acc 0.4815694758977677\n",
            "epoch 3 batch id 3101 loss 1.337786316871643 train acc 0.4816389874234118\n",
            "epoch 3 batch id 3111 loss 1.409124732017517 train acc 0.48166284956605593\n",
            "epoch 3 batch id 3121 loss 1.2103478908538818 train acc 0.4818467638577379\n",
            "epoch 3 batch id 3131 loss 1.223768711090088 train acc 0.48191472373043753\n",
            "epoch 3 batch id 3141 loss 1.4399770498275757 train acc 0.4819524036930914\n",
            "epoch 3 batch id 3151 loss 1.3851382732391357 train acc 0.4819154633449699\n",
            "epoch 3 batch id 3161 loss 1.0410151481628418 train acc 0.4819825608984499\n",
            "epoch 3 batch id 3171 loss 1.3922209739685059 train acc 0.4821034374014507\n",
            "epoch 3 batch id 3181 loss 1.4670143127441406 train acc 0.48214496227601383\n",
            "epoch 3 batch id 3191 loss 1.2989978790283203 train acc 0.48227926198683796\n",
            "epoch 3 batch id 3201 loss 1.5849263668060303 train acc 0.48235902842861605\n",
            "epoch 3 batch id 3211 loss 1.322013020515442 train acc 0.482370172843351\n",
            "epoch 3 batch id 3221 loss 1.5584912300109863 train acc 0.4824055029493946\n",
            "epoch 3 batch id 3231 loss 1.1918398141860962 train acc 0.4824551222531724\n",
            "epoch 3 batch id 3241 loss 1.3250046968460083 train acc 0.482475509102129\n",
            "epoch 3 batch id 3251 loss 1.1187924146652222 train acc 0.4825294140264534\n",
            "epoch 3 batch id 3261 loss 1.3480440378189087 train acc 0.4826069457221711\n",
            "epoch 3 batch id 3271 loss 1.5886658430099487 train acc 0.48262190461632526\n",
            "epoch 3 batch id 3281 loss 1.4187277555465698 train acc 0.4826558213959159\n",
            "epoch 3 batch id 3291 loss 1.3096942901611328 train acc 0.48267528866605897\n",
            "epoch 3 batch id 3301 loss 1.1948386430740356 train acc 0.48274670554377463\n",
            "epoch 3 batch id 3311 loss 1.6310292482376099 train acc 0.4827469042585322\n",
            "epoch 3 batch id 3321 loss 1.1731921434402466 train acc 0.4826718232460102\n",
            "epoch 3 batch id 3331 loss 1.2939515113830566 train acc 0.4827097718402882\n",
            "epoch 3 batch id 3341 loss 1.4615626335144043 train acc 0.48271475606105957\n",
            "epoch 3 batch id 3351 loss 1.2917969226837158 train acc 0.48274302447030737\n",
            "epoch 3 batch id 3361 loss 1.28910493850708 train acc 0.48282226271942874\n",
            "epoch 3 batch id 3371 loss 1.3191665410995483 train acc 0.4827944230198754\n",
            "epoch 3 batch id 3381 loss 1.2312734127044678 train acc 0.48284531203785863\n",
            "epoch 3 batch id 3391 loss 1.280552625656128 train acc 0.4828129607785314\n",
            "epoch 3 batch id 3401 loss 1.4912558794021606 train acc 0.48300591737724197\n",
            "epoch 3 batch id 3411 loss 1.251059889793396 train acc 0.4830145118733509\n",
            "epoch 3 batch id 3421 loss 1.2612923383712769 train acc 0.48309613417129493\n",
            "epoch 3 batch id 3431 loss 1.4143791198730469 train acc 0.4831226318857476\n",
            "epoch 3 batch id 3441 loss 1.3813587427139282 train acc 0.48319438390002906\n",
            "epoch 3 batch id 3451 loss 1.424496054649353 train acc 0.4831751666183715\n",
            "epoch 3 batch id 3461 loss 1.286888599395752 train acc 0.4832508668015025\n",
            "epoch 3 batch id 3471 loss 1.3319023847579956 train acc 0.4833666450590608\n",
            "epoch 3 batch id 3481 loss 1.5017672777175903 train acc 0.48337403045101984\n",
            "epoch 3 batch id 3491 loss 1.1306407451629639 train acc 0.483399276711544\n",
            "epoch 3 batch id 3501 loss 1.3562846183776855 train acc 0.4834288417594973\n",
            "epoch 3 batch id 3511 loss 1.0615516901016235 train acc 0.4834893904870407\n",
            "epoch 3 batch id 3521 loss 1.4793816804885864 train acc 0.483536282306163\n",
            "epoch 3 batch id 3531 loss 1.516249179840088 train acc 0.4835121070518267\n",
            "epoch 3 batch id 3541 loss 1.325477123260498 train acc 0.48360279582038973\n",
            "epoch 3 batch id 3551 loss 1.3223917484283447 train acc 0.48368417347226134\n",
            "epoch 3 batch id 3561 loss 1.2848421335220337 train acc 0.4837694818871104\n",
            "epoch 3 batch id 3571 loss 1.5644444227218628 train acc 0.483810557266872\n",
            "epoch 3 batch id 3581 loss 1.2569928169250488 train acc 0.4838077701759285\n",
            "epoch 3 batch id 3591 loss 1.3919861316680908 train acc 0.4838528613199666\n",
            "epoch 3 batch id 3601 loss 1.270835280418396 train acc 0.4840061788392113\n",
            "epoch 3 batch id 3611 loss 1.3397020101547241 train acc 0.4840677790085849\n",
            "epoch 3 batch id 3621 loss 1.2693138122558594 train acc 0.4841376691521679\n",
            "epoch 3 batch id 3631 loss 1.100588083267212 train acc 0.4841641421096117\n",
            "epoch 3 batch id 3641 loss 1.3669593334197998 train acc 0.48421621807195825\n",
            "epoch 3 batch id 3651 loss 1.4189578294754028 train acc 0.48423377156943304\n",
            "epoch 3 batch id 3661 loss 1.5886871814727783 train acc 0.4842768369298006\n",
            "epoch 3 batch id 3671 loss 1.3122574090957642 train acc 0.48429412966494145\n",
            "epoch 3 batch id 3681 loss 1.4404908418655396 train acc 0.4843537761477859\n",
            "epoch 3 batch id 3691 loss 1.5698440074920654 train acc 0.484341133839068\n",
            "epoch 3 batch id 3701 loss 1.0809248685836792 train acc 0.484455214806809\n",
            "epoch 3 batch id 3711 loss 1.3894617557525635 train acc 0.4845392077607114\n",
            "epoch 3 batch id 3721 loss 1.168454885482788 train acc 0.48471093120129\n",
            "epoch 3 batch id 3731 loss 1.2081595659255981 train acc 0.48474353390511926\n",
            "epoch 3 batch id 3741 loss 1.3752186298370361 train acc 0.48473837209302323\n",
            "epoch 3 batch id 3751 loss 1.5429376363754272 train acc 0.48482071447613967\n",
            "epoch 3 batch id 3761 loss 1.3917360305786133 train acc 0.48486522866258974\n",
            "epoch 3 batch id 3771 loss 1.1492918729782104 train acc 0.4849053632988597\n",
            "epoch 3 batch id 3781 loss 1.2890585660934448 train acc 0.48488743057392225\n",
            "epoch 3 batch id 3791 loss 1.144227147102356 train acc 0.48491493009759956\n",
            "epoch 3 batch id 3801 loss 1.1876391172409058 train acc 0.48508205077611155\n",
            "epoch 3 batch id 3811 loss 1.2943261861801147 train acc 0.4851047953293099\n",
            "epoch 3 batch id 3821 loss 1.2178670167922974 train acc 0.48518875948704526\n",
            "epoch 3 batch id 3831 loss 1.5240728855133057 train acc 0.4852682067345341\n",
            "epoch 3 batch id 3841 loss 1.3012745380401611 train acc 0.48531062874251496\n",
            "epoch 3 batch id 3851 loss 1.3243533372879028 train acc 0.48537311737211114\n",
            "epoch 3 batch id 3861 loss 1.340879201889038 train acc 0.4854271885521885\n",
            "epoch 3 batch id 3871 loss 1.235586404800415 train acc 0.4854971260656161\n",
            "epoch 3 batch id 3881 loss 1.3821181058883667 train acc 0.48559085931460966\n",
            "epoch 3 batch id 3891 loss 1.5825785398483276 train acc 0.4855957658699563\n",
            "epoch 3 batch id 3901 loss 1.2719321250915527 train acc 0.48563269033581136\n",
            "epoch 3 batch id 3911 loss 1.0377929210662842 train acc 0.48570937739708514\n",
            "epoch 3 batch id 3921 loss 1.3517001867294312 train acc 0.48566213976026523\n",
            "epoch 3 batch id 3931 loss 1.382735013961792 train acc 0.48568668913762403\n",
            "epoch 3 batch id 3941 loss 1.3617037534713745 train acc 0.4856833608221264\n",
            "epoch 3 batch id 3951 loss 1.2922110557556152 train acc 0.485719596304733\n",
            "epoch 3 batch id 3961 loss 1.3798739910125732 train acc 0.4857398699823277\n",
            "epoch 3 batch id 3971 loss 1.2617149353027344 train acc 0.4858072588768572\n",
            "epoch 3 batch id 3981 loss 1.3886271715164185 train acc 0.4858546847525747\n",
            "epoch 3 batch id 3991 loss 1.4261822700500488 train acc 0.4859253633174643\n",
            "epoch 3 batch id 4001 loss 1.307800531387329 train acc 0.48589805673581604\n",
            "epoch 3 batch id 4011 loss 1.3345880508422852 train acc 0.48596048366990774\n",
            "epoch 3 batch id 4021 loss 1.2716701030731201 train acc 0.48606534444168115\n",
            "epoch 3 batch id 4031 loss 1.3218530416488647 train acc 0.4861231704291739\n",
            "epoch 3 batch id 4041 loss 1.2564393281936646 train acc 0.48611884434545904\n",
            "epoch 3 batch id 4051 loss 1.2868156433105469 train acc 0.486199395211059\n",
            "epoch 3 batch id 4061 loss 1.4543771743774414 train acc 0.48624492120167445\n",
            "epoch 3 batch id 4071 loss 1.6776340007781982 train acc 0.4863170903954802\n",
            "epoch 3 batch id 4081 loss 1.346827745437622 train acc 0.4863085028179368\n",
            "epoch 3 batch id 4091 loss 1.3316105604171753 train acc 0.4863648863358592\n",
            "epoch 3 batch id 4101 loss 1.3911027908325195 train acc 0.48637527432333577\n",
            "epoch 3 batch id 4111 loss 1.397843599319458 train acc 0.48638561177329115\n",
            "epoch 3 batch id 4121 loss 1.4617475271224976 train acc 0.48633144261101674\n",
            "epoch 3 batch id 4131 loss 1.2283681631088257 train acc 0.4863456184943113\n",
            "epoch 3 batch id 4141 loss 1.4415173530578613 train acc 0.4863974583433953\n",
            "epoch 3 batch id 4151 loss 1.3941640853881836 train acc 0.48640764273668996\n",
            "epoch 3 batch id 4161 loss 1.5062612295150757 train acc 0.4864590843547224\n",
            "epoch 3 batch id 4171 loss 1.1523123979568481 train acc 0.48652526372572524\n",
            "epoch 3 batch id 4181 loss 1.0146232843399048 train acc 0.48657991509208326\n",
            "epoch 3 batch id 4191 loss 1.5396313667297363 train acc 0.48668277260796944\n",
            "epoch 3 batch id 4201 loss 1.2826379537582397 train acc 0.4867814210902166\n",
            "epoch 3 batch id 4211 loss 1.2618682384490967 train acc 0.48686104844454997\n",
            "epoch 3 batch id 4221 loss 1.1258807182312012 train acc 0.4869291933191187\n",
            "epoch 3 batch id 4231 loss 1.3038393259048462 train acc 0.4869490073268731\n",
            "epoch 3 batch id 4241 loss 1.39891517162323 train acc 0.4870239919830229\n",
            "epoch 3 batch id 4251 loss 1.3198457956314087 train acc 0.4870214361326747\n",
            "epoch 3 batch id 4261 loss 1.2220312356948853 train acc 0.4870922318704529\n",
            "epoch 3 batch id 4271 loss 1.1651886701583862 train acc 0.4871956216342777\n",
            "epoch 3 batch id 4281 loss 1.368132472038269 train acc 0.48737152534454564\n",
            "epoch 3 batch id 4291 loss 1.0836634635925293 train acc 0.4874956303891867\n",
            "epoch 3 batch id 4301 loss 1.143662452697754 train acc 0.4875283364333876\n",
            "epoch 3 batch id 4311 loss 1.1372040510177612 train acc 0.487502899559267\n",
            "epoch 3 batch id 4321 loss 1.293622374534607 train acc 0.487567981948623\n",
            "epoch 3 batch id 4331 loss 1.173770785331726 train acc 0.4875966866774417\n",
            "epoch 3 batch id 4341 loss 1.38887619972229 train acc 0.48764325616217463\n",
            "epoch 3 batch id 4351 loss 1.4693758487701416 train acc 0.4876357446564008\n",
            "epoch 3 batch id 4361 loss 1.396741271018982 train acc 0.48771067415730335\n",
            "epoch 3 batch id 4371 loss 1.5025296211242676 train acc 0.48772449096316633\n",
            "epoch 3 batch id 4381 loss 1.1636805534362793 train acc 0.4877739100661949\n",
            "epoch 3 batch id 4391 loss 1.1015119552612305 train acc 0.48785868822591666\n",
            "epoch 3 batch id 4401 loss 1.2553704977035522 train acc 0.4879217791411043\n",
            "epoch 3 batch id 4411 loss 1.2814561128616333 train acc 0.48797395715257313\n",
            "epoch 3 batch id 4421 loss 1.1595431566238403 train acc 0.4880683103370278\n",
            "epoch 3 batch id 4431 loss 1.1699621677398682 train acc 0.4881728165199729\n",
            "epoch 3 batch id 4441 loss 1.0542091131210327 train acc 0.48821352172934024\n",
            "epoch 3 batch id 4451 loss 1.2010130882263184 train acc 0.48826808582341047\n",
            "epoch 3 batch id 4461 loss 1.1907100677490234 train acc 0.488294384667115\n",
            "epoch 3 batch id 4471 loss 1.303444504737854 train acc 0.4883450290762693\n",
            "epoch 3 batch id 4481 loss 1.3359572887420654 train acc 0.4883082738228074\n",
            "epoch 3 batch id 4491 loss 1.4017244577407837 train acc 0.48830995323981297\n",
            "epoch 3 batch id 4501 loss 1.0523360967636108 train acc 0.4883741113085981\n",
            "epoch 3 batch id 4511 loss 1.2626116275787354 train acc 0.48838602859676344\n",
            "epoch 3 batch id 4521 loss 1.0707091093063354 train acc 0.48844627847821276\n",
            "epoch 3 batch id 4531 loss 1.2537416219711304 train acc 0.4884441900242772\n",
            "epoch 3 batch id 4541 loss 1.065630555152893 train acc 0.488452433384717\n",
            "epoch 3 batch id 4551 loss 1.377333164215088 train acc 0.48848810700944845\n",
            "epoch 3 batch id 4561 loss 1.3088644742965698 train acc 0.4885441789081342\n",
            "epoch 3 batch id 4571 loss 1.3974354267120361 train acc 0.4886205152045504\n",
            "epoch 3 batch id 4581 loss 1.2558950185775757 train acc 0.488607836716874\n",
            "epoch 3 batch id 4591 loss 1.27928626537323 train acc 0.48863605423654977\n",
            "epoch 3 batch id 4601 loss 1.240619421005249 train acc 0.4886981091067159\n",
            "epoch 3 batch id 4611 loss 1.2216057777404785 train acc 0.488780226631967\n",
            "epoch 3 batch id 4621 loss 1.3094868659973145 train acc 0.4888078879030513\n",
            "epoch 3 batch id 4631 loss 1.3530532121658325 train acc 0.48892652774778667\n",
            "epoch 3 batch id 4641 loss 1.2988905906677246 train acc 0.4890076222796811\n",
            "epoch 3 batch id 4651 loss 1.376947045326233 train acc 0.4890816491077188\n",
            "epoch 3 batch id 4661 loss 1.3043849468231201 train acc 0.4890849603089466\n",
            "epoch 3 batch id 4671 loss 1.2790091037750244 train acc 0.48915181438664096\n",
            "epoch 3 batch id 4681 loss 1.5131256580352783 train acc 0.48920169301431315\n",
            "epoch 3 batch id 4691 loss 0.9249312877655029 train acc 0.48929465998720956\n",
            "epoch 3 batch id 4701 loss 1.38222074508667 train acc 0.4893174324611785\n",
            "epoch 3 batch id 4711 loss 1.0658316612243652 train acc 0.48940975907450646\n",
            "epoch 3 batch id 4721 loss 1.3876601457595825 train acc 0.48948845583562806\n",
            "epoch 3 batch id 4731 loss 1.1770814657211304 train acc 0.489497463538364\n",
            "epoch 3 batch id 4741 loss 1.3195245265960693 train acc 0.4895426861421641\n",
            "epoch 3 batch id 4751 loss 1.3092336654663086 train acc 0.4895811408124605\n",
            "epoch 3 batch id 4761 loss 1.4129992723464966 train acc 0.48955707834488554\n",
            "epoch 3 batch id 4771 loss 1.3649492263793945 train acc 0.48969686648501365\n",
            "epoch 3 batch id 4781 loss 1.2532856464385986 train acc 0.48969227149131983\n",
            "epoch 3 batch id 4791 loss 1.4125087261199951 train acc 0.48967138906282615\n",
            "epoch 3 batch id 4801 loss 1.1291369199752808 train acc 0.4897091751718392\n",
            "epoch 3 batch id 4811 loss 1.0864845514297485 train acc 0.48977278632300975\n",
            "epoch 3 batch id 4821 loss 1.4166191816329956 train acc 0.48978751814976146\n",
            "epoch 3 batch id 4831 loss 1.2868289947509766 train acc 0.4898248292279031\n",
            "epoch 3 batch id 4841 loss 1.3489446640014648 train acc 0.4898393926874613\n",
            "epoch 3 batch id 4851 loss 1.096827507019043 train acc 0.4898989898989899\n",
            "epoch 3 batch id 4861 loss 1.2337566614151 train acc 0.4898908403620654\n",
            "epoch 3 batch id 4871 loss 1.2916680574417114 train acc 0.48993404845001026\n",
            "epoch 3 batch id 4881 loss 1.3112459182739258 train acc 0.4900411032575292\n",
            "epoch 3 batch id 4891 loss 1.3239161968231201 train acc 0.4900966060110407\n",
            "epoch 3 batch id 4901 loss 1.2756427526474 train acc 0.4901455060191798\n",
            "epoch 3 batch id 4911 loss 1.1116822957992554 train acc 0.4902451130116066\n",
            "epoch 3 batch id 4921 loss 1.3295198678970337 train acc 0.49020143263564314\n",
            "epoch 3 batch id 4931 loss 1.3645998239517212 train acc 0.4901515919691746\n",
            "epoch 3 batch id 4941 loss 1.283393383026123 train acc 0.49013673851447076\n",
            "epoch 3 batch id 4951 loss 1.546229600906372 train acc 0.4901819076954151\n",
            "epoch 3 train acc 0.4902978111761146\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b9460ed6e3641faad13364f9a804633",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 loss 1.1953743696212769 test acc 0.5307975164956013\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc6840817b8f483c9a3e01bd357cf4fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 batch id 1 loss 1.2882955074310303 train acc 0.5625\n",
            "epoch 4 batch id 11 loss 1.216974139213562 train acc 0.5355113636363636\n",
            "epoch 4 batch id 21 loss 1.1545079946517944 train acc 0.5305059523809523\n",
            "epoch 4 batch id 31 loss 1.3625611066818237 train acc 0.5196572580645161\n",
            "epoch 4 batch id 41 loss 0.9188138246536255 train acc 0.5259146341463414\n",
            "epoch 4 batch id 51 loss 1.4364912509918213 train acc 0.5257352941176471\n",
            "epoch 4 batch id 61 loss 1.4515972137451172 train acc 0.5210040983606558\n",
            "epoch 4 batch id 71 loss 1.4068070650100708 train acc 0.518705985915493\n",
            "epoch 4 batch id 81 loss 1.357034683227539 train acc 0.5192901234567902\n",
            "epoch 4 batch id 91 loss 1.279360294342041 train acc 0.5199175824175825\n",
            "epoch 4 batch id 101 loss 1.0881986618041992 train acc 0.5168626237623762\n",
            "epoch 4 batch id 111 loss 1.5165468454360962 train acc 0.5144988738738738\n",
            "epoch 4 batch id 121 loss 1.547065258026123 train acc 0.5138171487603306\n",
            "epoch 4 batch id 131 loss 1.4358383417129517 train acc 0.5135973282442748\n",
            "epoch 4 batch id 141 loss 1.1965973377227783 train acc 0.5148492907801419\n",
            "epoch 4 batch id 151 loss 1.1960105895996094 train acc 0.515521523178808\n",
            "epoch 4 batch id 161 loss 1.5623046159744263 train acc 0.5141692546583851\n",
            "epoch 4 batch id 171 loss 1.4008464813232422 train acc 0.5125182748538012\n",
            "epoch 4 batch id 181 loss 1.2438243627548218 train acc 0.5118266574585635\n",
            "epoch 4 batch id 191 loss 1.345373272895813 train acc 0.5120255235602095\n",
            "epoch 4 batch id 201 loss 0.9711003303527832 train acc 0.513603855721393\n",
            "epoch 4 batch id 211 loss 1.5518113374710083 train acc 0.5123667061611374\n",
            "epoch 4 batch id 221 loss 1.4499276876449585 train acc 0.5120899321266968\n",
            "epoch 4 batch id 231 loss 1.371049165725708 train acc 0.5113636363636364\n",
            "epoch 4 batch id 241 loss 1.4243927001953125 train acc 0.51076244813278\n",
            "epoch 4 batch id 251 loss 1.2689491510391235 train acc 0.5100224103585658\n",
            "epoch 4 batch id 261 loss 1.2081406116485596 train acc 0.5120330459770115\n",
            "epoch 4 batch id 271 loss 1.061963438987732 train acc 0.5131457564575646\n",
            "epoch 4 batch id 281 loss 1.2349687814712524 train acc 0.513845640569395\n",
            "epoch 4 batch id 291 loss 1.5031684637069702 train acc 0.5146585051546392\n",
            "epoch 4 batch id 301 loss 1.491805076599121 train acc 0.5152616279069767\n",
            "epoch 4 batch id 311 loss 1.2937692403793335 train acc 0.5132636655948553\n",
            "epoch 4 batch id 321 loss 1.4898895025253296 train acc 0.5138239875389408\n",
            "epoch 4 batch id 331 loss 1.4133522510528564 train acc 0.5146336858006042\n",
            "epoch 4 batch id 341 loss 1.3463916778564453 train acc 0.5144336510263929\n",
            "epoch 4 batch id 351 loss 1.308086633682251 train acc 0.5146901709401709\n",
            "epoch 4 batch id 361 loss 1.360416293144226 train acc 0.5149324792243767\n",
            "epoch 4 batch id 371 loss 1.3677685260772705 train acc 0.5147826819407008\n",
            "epoch 4 batch id 381 loss 1.1486974954605103 train acc 0.515132874015748\n",
            "epoch 4 batch id 391 loss 1.4522318840026855 train acc 0.5146659207161125\n",
            "epoch 4 batch id 401 loss 1.160707712173462 train acc 0.51410536159601\n",
            "epoch 4 batch id 411 loss 1.1325702667236328 train acc 0.5142183698296837\n",
            "epoch 4 batch id 421 loss 1.248426079750061 train acc 0.5143260095011877\n",
            "epoch 4 batch id 431 loss 1.3761358261108398 train acc 0.513703596287703\n",
            "epoch 4 batch id 441 loss 1.40176260471344 train acc 0.5132157029478458\n",
            "epoch 4 batch id 451 loss 1.2736314535140991 train acc 0.5136155764966741\n",
            "epoch 4 batch id 461 loss 1.2230857610702515 train acc 0.5138625271149675\n",
            "epoch 4 batch id 471 loss 1.114253044128418 train acc 0.5150278662420382\n",
            "epoch 4 batch id 481 loss 1.5241061449050903 train acc 0.5152351871101871\n",
            "epoch 4 batch id 491 loss 1.157364845275879 train acc 0.5146384928716904\n",
            "epoch 4 batch id 501 loss 1.67723548412323 train acc 0.5138473053892215\n",
            "epoch 4 batch id 511 loss 1.2123289108276367 train acc 0.5136680528375733\n",
            "epoch 4 batch id 521 loss 1.4103955030441284 train acc 0.5131357965451055\n",
            "epoch 4 batch id 531 loss 1.4541113376617432 train acc 0.5127412900188324\n",
            "epoch 4 batch id 541 loss 1.278302788734436 train acc 0.5129101201478743\n",
            "epoch 4 batch id 551 loss 1.1626054048538208 train acc 0.5128459618874773\n",
            "epoch 4 batch id 561 loss 1.0522977113723755 train acc 0.512979055258467\n",
            "epoch 4 batch id 571 loss 1.4043891429901123 train acc 0.5130801225919439\n",
            "epoch 4 batch id 581 loss 1.3181016445159912 train acc 0.512854991394148\n",
            "epoch 4 batch id 591 loss 1.085115909576416 train acc 0.513298434856176\n",
            "epoch 4 batch id 601 loss 1.1185288429260254 train acc 0.5131551580698835\n",
            "epoch 4 batch id 611 loss 1.1405408382415771 train acc 0.5136558919803601\n",
            "epoch 4 batch id 621 loss 1.301572561264038 train acc 0.5145933977455717\n",
            "epoch 4 batch id 631 loss 1.4053136110305786 train acc 0.5145354595879557\n",
            "epoch 4 batch id 641 loss 1.2598928213119507 train acc 0.5143086973478939\n",
            "epoch 4 batch id 651 loss 1.1439173221588135 train acc 0.5140408986175116\n",
            "epoch 4 batch id 661 loss 1.400008201599121 train acc 0.5138993948562783\n",
            "epoch 4 batch id 671 loss 1.4036015272140503 train acc 0.5141346870342772\n",
            "epoch 4 batch id 681 loss 1.206001877784729 train acc 0.5142712922173275\n",
            "epoch 4 batch id 691 loss 1.1192569732666016 train acc 0.5149918596237337\n",
            "epoch 4 batch id 701 loss 1.1181780099868774 train acc 0.5154243937232525\n",
            "epoch 4 batch id 711 loss 1.3351233005523682 train acc 0.5154491912798875\n",
            "epoch 4 batch id 721 loss 1.250258207321167 train acc 0.5156033287101248\n",
            "epoch 4 batch id 731 loss 1.1694390773773193 train acc 0.5162234952120383\n",
            "epoch 4 batch id 741 loss 1.3640183210372925 train acc 0.5161310728744939\n",
            "epoch 4 batch id 751 loss 1.4161635637283325 train acc 0.5159786950732357\n",
            "epoch 4 batch id 761 loss 0.9480067491531372 train acc 0.516384691195795\n",
            "epoch 4 batch id 771 loss 1.6039248704910278 train acc 0.5160708495460441\n",
            "epoch 4 batch id 781 loss 1.3178209066390991 train acc 0.5161051536491678\n",
            "epoch 4 batch id 791 loss 1.184080958366394 train acc 0.5161780973451328\n",
            "epoch 4 batch id 801 loss 1.2706941366195679 train acc 0.5166003433208489\n",
            "epoch 4 batch id 811 loss 1.3210009336471558 train acc 0.5169543773119606\n",
            "epoch 4 batch id 821 loss 1.1846339702606201 train acc 0.517204628501827\n",
            "epoch 4 batch id 831 loss 1.5494561195373535 train acc 0.5170351985559567\n",
            "epoch 4 batch id 841 loss 1.1544594764709473 train acc 0.5175200653983353\n",
            "epoch 4 batch id 851 loss 1.295105218887329 train acc 0.5176079612220916\n",
            "epoch 4 batch id 861 loss 1.3761898279190063 train acc 0.5173671602787456\n",
            "epoch 4 batch id 871 loss 1.0668509006500244 train acc 0.5171857060849598\n",
            "epoch 4 batch id 881 loss 1.3582104444503784 train acc 0.5170438422247446\n",
            "epoch 4 batch id 891 loss 1.1586443185806274 train acc 0.5171682098765432\n",
            "epoch 4 batch id 901 loss 1.3276169300079346 train acc 0.5171337402885683\n",
            "epoch 4 batch id 911 loss 1.397065281867981 train acc 0.516568331503842\n",
            "epoch 4 batch id 921 loss 1.51008141040802 train acc 0.516439332247557\n",
            "epoch 4 batch id 931 loss 1.2584871053695679 train acc 0.5162459720730398\n",
            "epoch 4 batch id 941 loss 1.1282979249954224 train acc 0.5162559776833157\n",
            "epoch 4 batch id 951 loss 1.506791353225708 train acc 0.5163643533123028\n",
            "epoch 4 batch id 961 loss 1.242749571800232 train acc 0.5161615504682622\n",
            "epoch 4 batch id 971 loss 1.4250702857971191 train acc 0.5162847579814624\n",
            "epoch 4 batch id 981 loss 1.4239424467086792 train acc 0.5165965851172273\n",
            "epoch 4 batch id 991 loss 1.1327197551727295 train acc 0.5164921796165489\n",
            "epoch 4 batch id 1001 loss 1.273853063583374 train acc 0.5165771728271729\n",
            "epoch 4 batch id 1011 loss 1.3635848760604858 train acc 0.5166604846686449\n",
            "epoch 4 batch id 1021 loss 0.9826802015304565 train acc 0.5169870225269344\n",
            "epoch 4 batch id 1031 loss 1.3774257898330688 train acc 0.5168677255092143\n",
            "epoch 4 batch id 1041 loss 1.3052136898040771 train acc 0.5168557877041307\n",
            "epoch 4 batch id 1051 loss 1.1769311428070068 train acc 0.5168589438629876\n",
            "epoch 4 batch id 1061 loss 1.2741702795028687 train acc 0.5167736804901036\n",
            "epoch 4 batch id 1071 loss 1.0534281730651855 train acc 0.5168650793650794\n",
            "epoch 4 batch id 1081 loss 1.3152920007705688 train acc 0.5168246993524515\n",
            "epoch 4 batch id 1091 loss 1.5261958837509155 train acc 0.5165988771769019\n",
            "epoch 4 batch id 1101 loss 1.3188239336013794 train acc 0.5169732061762035\n",
            "epoch 4 batch id 1111 loss 1.0992070436477661 train acc 0.5173267326732673\n",
            "epoch 4 batch id 1121 loss 1.443629264831543 train acc 0.5174091213202497\n",
            "epoch 4 batch id 1131 loss 1.2701666355133057 train acc 0.5173657161803713\n",
            "epoch 4 batch id 1141 loss 1.354170560836792 train acc 0.5176517309377738\n",
            "epoch 4 batch id 1151 loss 1.4090098142623901 train acc 0.5178920503909644\n",
            "epoch 4 batch id 1161 loss 1.2816457748413086 train acc 0.5179398148148148\n",
            "epoch 4 batch id 1171 loss 1.1729390621185303 train acc 0.5179467335610589\n",
            "epoch 4 batch id 1181 loss 1.192331314086914 train acc 0.5177153895004234\n",
            "epoch 4 batch id 1191 loss 1.2197436094284058 train acc 0.5181176532325776\n",
            "epoch 4 batch id 1201 loss 1.4774792194366455 train acc 0.5182139883430474\n",
            "epoch 4 batch id 1211 loss 1.0397758483886719 train acc 0.5183861478117259\n",
            "epoch 4 batch id 1221 loss 1.2727835178375244 train acc 0.5183251433251433\n",
            "epoch 4 batch id 1231 loss 1.3073818683624268 train acc 0.5185443744922827\n",
            "epoch 4 batch id 1241 loss 1.4396227598190308 train acc 0.5184201248992748\n",
            "epoch 4 batch id 1251 loss 1.1668919324874878 train acc 0.5184477418065547\n",
            "epoch 4 batch id 1261 loss 1.467737078666687 train acc 0.5186360031720857\n",
            "epoch 4 batch id 1271 loss 1.2306121587753296 train acc 0.5186369000786782\n",
            "epoch 4 batch id 1281 loss 1.3478102684020996 train acc 0.5189427205308352\n",
            "epoch 4 batch id 1291 loss 1.403191328048706 train acc 0.5189896398140976\n",
            "epoch 4 batch id 1301 loss 1.4552886486053467 train acc 0.5187956379707916\n",
            "epoch 4 batch id 1311 loss 1.5179834365844727 train acc 0.5187237795575896\n",
            "epoch 4 batch id 1321 loss 1.2229808568954468 train acc 0.518747634367903\n",
            "epoch 4 batch id 1331 loss 1.206546664237976 train acc 0.5187946093163035\n",
            "epoch 4 batch id 1341 loss 1.3257215023040771 train acc 0.5189807046979866\n",
            "epoch 4 batch id 1351 loss 1.1383764743804932 train acc 0.5192565692079941\n",
            "epoch 4 batch id 1361 loss 1.3463325500488281 train acc 0.5192758082292432\n",
            "epoch 4 batch id 1371 loss 1.2634923458099365 train acc 0.5192035922684172\n",
            "epoch 4 batch id 1381 loss 1.426430344581604 train acc 0.5193134503982622\n",
            "epoch 4 batch id 1391 loss 0.9118576049804688 train acc 0.5195789899352984\n",
            "epoch 4 batch id 1401 loss 1.2927244901657104 train acc 0.5196399892933619\n",
            "epoch 4 batch id 1411 loss 1.180458426475525 train acc 0.5196779766123317\n",
            "epoch 4 batch id 1421 loss 1.2203941345214844 train acc 0.5197704081632653\n",
            "epoch 4 batch id 1431 loss 1.0842030048370361 train acc 0.5198943046820406\n",
            "epoch 4 batch id 1441 loss 1.1877241134643555 train acc 0.5197887751561415\n",
            "epoch 4 batch id 1451 loss 1.1766632795333862 train acc 0.520190816678153\n",
            "epoch 4 batch id 1461 loss 1.1576110124588013 train acc 0.5201916495550992\n",
            "epoch 4 batch id 1471 loss 1.4213274717330933 train acc 0.5202774473147519\n",
            "epoch 4 batch id 1481 loss 1.4006050825119019 train acc 0.5203831870357867\n",
            "epoch 4 batch id 1491 loss 1.1330838203430176 train acc 0.5205189470154259\n",
            "epoch 4 batch id 1501 loss 1.1926554441452026 train acc 0.5206320786142572\n",
            "epoch 4 batch id 1511 loss 1.1682909727096558 train acc 0.5206713269358041\n",
            "epoch 4 batch id 1521 loss 1.164501428604126 train acc 0.5209771531886916\n",
            "epoch 4 batch id 1531 loss 1.3376938104629517 train acc 0.5209728118876551\n",
            "epoch 4 batch id 1541 loss 1.4365472793579102 train acc 0.5209989454899416\n",
            "epoch 4 batch id 1551 loss 1.292950987815857 train acc 0.5207426660219213\n",
            "epoch 4 batch id 1561 loss 1.2682056427001953 train acc 0.5207198910954516\n",
            "epoch 4 batch id 1571 loss 1.0631433725357056 train acc 0.5209261616804584\n",
            "epoch 4 batch id 1581 loss 1.326261281967163 train acc 0.5210013440860215\n",
            "epoch 4 batch id 1591 loss 1.375559687614441 train acc 0.5209086266499057\n",
            "epoch 4 batch id 1601 loss 1.3428634405136108 train acc 0.5209341817613992\n",
            "epoch 4 batch id 1611 loss 1.22477388381958 train acc 0.5209400217256362\n",
            "epoch 4 batch id 1621 loss 1.1159087419509888 train acc 0.5209554287476866\n",
            "epoch 4 batch id 1631 loss 1.1537361145019531 train acc 0.5209802268546904\n",
            "epoch 4 batch id 1641 loss 1.42262864112854 train acc 0.5211094606946983\n",
            "epoch 4 batch id 1651 loss 1.2768971920013428 train acc 0.5210857056329498\n",
            "epoch 4 batch id 1661 loss 1.3009741306304932 train acc 0.521278597230584\n",
            "epoch 4 batch id 1671 loss 1.2197328805923462 train acc 0.5213195691202872\n",
            "epoch 4 batch id 1681 loss 1.3935563564300537 train acc 0.5212578078524688\n",
            "epoch 4 batch id 1691 loss 1.1967662572860718 train acc 0.5214462596096984\n",
            "epoch 4 batch id 1701 loss 1.2625325918197632 train acc 0.5215038947677837\n",
            "epoch 4 batch id 1711 loss 1.3331551551818848 train acc 0.5214056107539451\n",
            "epoch 4 batch id 1721 loss 1.2259501218795776 train acc 0.5215536025566531\n",
            "epoch 4 batch id 1731 loss 1.1350047588348389 train acc 0.5216005921432698\n",
            "epoch 4 batch id 1741 loss 1.160510778427124 train acc 0.5217367892016083\n",
            "epoch 4 batch id 1751 loss 1.0419615507125854 train acc 0.5218446601941747\n",
            "epoch 4 batch id 1761 loss 1.0034091472625732 train acc 0.522173126064736\n",
            "epoch 4 batch id 1771 loss 1.1682868003845215 train acc 0.5222067334839074\n",
            "epoch 4 batch id 1781 loss 1.2197597026824951 train acc 0.5222399635036497\n",
            "epoch 4 batch id 1791 loss 1.5857813358306885 train acc 0.522141959798995\n",
            "epoch 4 batch id 1801 loss 1.2717688083648682 train acc 0.5223486951693503\n",
            "epoch 4 batch id 1811 loss 1.1850411891937256 train acc 0.5223805908337935\n",
            "epoch 4 batch id 1821 loss 1.049062728881836 train acc 0.5225151015925316\n",
            "epoch 4 batch id 1831 loss 1.0391042232513428 train acc 0.5225542736209722\n",
            "epoch 4 batch id 1841 loss 1.0492383241653442 train acc 0.5225845328625747\n",
            "epoch 4 batch id 1851 loss 1.4135234355926514 train acc 0.5224794030253916\n",
            "epoch 4 batch id 1861 loss 1.217453122138977 train acc 0.5224845513164965\n",
            "epoch 4 batch id 1871 loss 1.2261316776275635 train acc 0.5225397514698022\n",
            "epoch 4 batch id 1881 loss 1.2158643007278442 train acc 0.5227272727272727\n",
            "epoch 4 batch id 1891 loss 1.0356698036193848 train acc 0.5229541248016922\n",
            "epoch 4 batch id 1901 loss 1.1252961158752441 train acc 0.5229402288269331\n",
            "epoch 4 batch id 1911 loss 1.162245750427246 train acc 0.5228774201988488\n",
            "epoch 4 batch id 1921 loss 1.200698971748352 train acc 0.5227501952108277\n",
            "epoch 4 batch id 1931 loss 1.3404152393341064 train acc 0.5229479544277577\n",
            "epoch 4 batch id 1941 loss 1.1492817401885986 train acc 0.5230551262235961\n",
            "epoch 4 batch id 1951 loss 1.340973973274231 train acc 0.5229449641209636\n",
            "epoch 4 batch id 1961 loss 1.1817935705184937 train acc 0.5229474757776644\n",
            "epoch 4 batch id 1971 loss 1.2526870965957642 train acc 0.5229578893962455\n",
            "epoch 4 batch id 1981 loss 1.2349450588226318 train acc 0.5229524230186774\n",
            "epoch 4 batch id 1991 loss 1.181659460067749 train acc 0.5229391637368157\n",
            "epoch 4 batch id 2001 loss 1.361115574836731 train acc 0.5230587831084458\n",
            "epoch 4 batch id 2011 loss 1.4039217233657837 train acc 0.5229674291397315\n",
            "epoch 4 batch id 2021 loss 1.2899454832077026 train acc 0.5230006803562592\n",
            "epoch 4 batch id 2031 loss 1.2118325233459473 train acc 0.5231028434268833\n",
            "epoch 4 batch id 2041 loss 1.1292845010757446 train acc 0.5230738608525233\n",
            "epoch 4 batch id 2051 loss 1.2466155290603638 train acc 0.5231061067771818\n",
            "epoch 4 batch id 2061 loss 1.1464115381240845 train acc 0.5231228772440563\n",
            "epoch 4 batch id 2071 loss 1.1973979473114014 train acc 0.5232224770642202\n",
            "epoch 4 batch id 2081 loss 1.5070545673370361 train acc 0.5232310187409899\n",
            "epoch 4 batch id 2091 loss 1.346051812171936 train acc 0.523306731229077\n",
            "epoch 4 batch id 2101 loss 1.1477864980697632 train acc 0.5234337815326036\n",
            "epoch 4 batch id 2111 loss 1.2214144468307495 train acc 0.5234930127901468\n",
            "epoch 4 batch id 2121 loss 1.0151050090789795 train acc 0.5236695544554455\n",
            "epoch 4 batch id 2131 loss 1.390071988105774 train acc 0.5237637846081652\n",
            "epoch 4 batch id 2141 loss 1.3880810737609863 train acc 0.5237330686595049\n",
            "epoch 4 batch id 2151 loss 1.33110773563385 train acc 0.5237026383077639\n",
            "epoch 4 batch id 2161 loss 1.3881911039352417 train acc 0.5236146459972235\n",
            "epoch 4 batch id 2171 loss 1.1308552026748657 train acc 0.5236929986181483\n",
            "epoch 4 batch id 2181 loss 1.1436259746551514 train acc 0.5236774988537368\n",
            "epoch 4 batch id 2191 loss 1.5401923656463623 train acc 0.5236050890004564\n",
            "epoch 4 batch id 2201 loss 1.463807463645935 train acc 0.5234481485688324\n",
            "epoch 4 batch id 2211 loss 1.356372356414795 train acc 0.5234622342831298\n",
            "epoch 4 batch id 2221 loss 1.3759164810180664 train acc 0.5235184038721297\n",
            "epoch 4 batch id 2231 loss 1.1826672554016113 train acc 0.523700134468848\n",
            "epoch 4 batch id 2241 loss 1.1828579902648926 train acc 0.523789602855868\n",
            "epoch 4 batch id 2251 loss 1.3369678258895874 train acc 0.5238158040870724\n",
            "epoch 4 batch id 2261 loss 1.0328457355499268 train acc 0.5238072202565237\n",
            "epoch 4 batch id 2271 loss 1.354813575744629 train acc 0.5239431968295905\n",
            "epoch 4 batch id 2281 loss 1.3636066913604736 train acc 0.5240505808855765\n",
            "epoch 4 batch id 2291 loss 1.3264447450637817 train acc 0.5242252291575731\n",
            "epoch 4 batch id 2301 loss 1.3476282358169556 train acc 0.5242761299435028\n",
            "epoch 4 batch id 2311 loss 1.2179994583129883 train acc 0.5243942016443098\n",
            "epoch 4 batch id 2321 loss 0.9415161609649658 train acc 0.5245516479965532\n",
            "epoch 4 batch id 2331 loss 1.2112549543380737 train acc 0.5245200557700558\n",
            "epoch 4 batch id 2341 loss 1.712764024734497 train acc 0.5245421294318667\n",
            "epoch 4 batch id 2351 loss 1.2228597402572632 train acc 0.5246769991492982\n",
            "epoch 4 batch id 2361 loss 1.3632631301879883 train acc 0.5246916031342651\n",
            "epoch 4 batch id 2371 loss 1.1576788425445557 train acc 0.5248049346267398\n",
            "epoch 4 batch id 2381 loss 1.2399547100067139 train acc 0.524897627047459\n",
            "epoch 4 batch id 2391 loss 1.2490367889404297 train acc 0.5248784504391468\n",
            "epoch 4 batch id 2401 loss 1.3605045080184937 train acc 0.5248724489795918\n",
            "epoch 4 batch id 2411 loss 1.3094209432601929 train acc 0.5249313044379925\n",
            "epoch 4 batch id 2421 loss 1.2885699272155762 train acc 0.5248412329615861\n",
            "epoch 4 batch id 2431 loss 1.4328902959823608 train acc 0.5246812011517894\n",
            "epoch 4 batch id 2441 loss 1.1321744918823242 train acc 0.5248169295370749\n",
            "epoch 4 batch id 2451 loss 1.1233274936676025 train acc 0.5248623011015912\n",
            "epoch 4 batch id 2461 loss 1.3901273012161255 train acc 0.5249009548963836\n",
            "epoch 4 batch id 2471 loss 1.2499788999557495 train acc 0.5249772359368676\n",
            "epoch 4 batch id 2481 loss 1.2961453199386597 train acc 0.5249332426440951\n",
            "epoch 4 batch id 2491 loss 1.5136549472808838 train acc 0.5248206041750301\n",
            "epoch 4 batch id 2501 loss 1.012956142425537 train acc 0.524858806477409\n",
            "epoch 4 batch id 2511 loss 1.2954740524291992 train acc 0.5250460473914775\n",
            "epoch 4 batch id 2521 loss 1.2208627462387085 train acc 0.5249652915509718\n",
            "epoch 4 batch id 2531 loss 1.2687307596206665 train acc 0.5250395100750691\n",
            "epoch 4 batch id 2541 loss 1.272984266281128 train acc 0.5251192935852027\n",
            "epoch 4 batch id 2551 loss 1.4845261573791504 train acc 0.5250024500196001\n",
            "epoch 4 batch id 2561 loss 1.365005373954773 train acc 0.5250207438500586\n",
            "epoch 4 batch id 2571 loss 1.0823074579238892 train acc 0.5251665208090237\n",
            "epoch 4 batch id 2581 loss 1.306592583656311 train acc 0.5252445757458349\n",
            "epoch 4 batch id 2591 loss 1.0445530414581299 train acc 0.5252617232728676\n",
            "epoch 4 batch id 2601 loss 1.2615712881088257 train acc 0.5253087754709727\n",
            "epoch 4 batch id 2611 loss 1.23725163936615 train acc 0.5253494829567216\n",
            "epoch 4 batch id 2621 loss 1.2643606662750244 train acc 0.5253541110263258\n",
            "epoch 4 batch id 2631 loss 1.3395419120788574 train acc 0.525340887495249\n",
            "epoch 4 batch id 2641 loss 1.208207368850708 train acc 0.5254875047330556\n",
            "epoch 4 batch id 2651 loss 1.1866822242736816 train acc 0.5254679837797057\n",
            "epoch 4 batch id 2661 loss 1.3029537200927734 train acc 0.5253957628711011\n",
            "epoch 4 batch id 2671 loss 1.2529000043869019 train acc 0.5253591819543242\n",
            "epoch 4 batch id 2681 loss 1.2494382858276367 train acc 0.5252820775829914\n",
            "epoch 4 batch id 2691 loss 1.3850611448287964 train acc 0.5252403846153846\n",
            "epoch 4 batch id 2701 loss 1.3277817964553833 train acc 0.5253436227323214\n",
            "epoch 4 batch id 2711 loss 1.1541485786437988 train acc 0.5253884636665437\n",
            "epoch 4 batch id 2721 loss 1.2424468994140625 train acc 0.5254502021315692\n",
            "epoch 4 batch id 2731 loss 1.1134592294692993 train acc 0.5254542749908458\n",
            "epoch 4 batch id 2741 loss 1.4328162670135498 train acc 0.5254982214520248\n",
            "epoch 4 batch id 2751 loss 1.0394799709320068 train acc 0.5255759269356598\n",
            "epoch 4 batch id 2761 loss 1.0515400171279907 train acc 0.5256643879029337\n",
            "epoch 4 batch id 2771 loss 1.3402577638626099 train acc 0.5256281577047998\n",
            "epoch 4 batch id 2781 loss 1.336613416671753 train acc 0.5257888349514563\n",
            "epoch 4 batch id 2791 loss 1.301531434059143 train acc 0.5258195987101397\n",
            "epoch 4 batch id 2801 loss 1.2326089143753052 train acc 0.5259561317386647\n",
            "epoch 4 batch id 2811 loss 1.160860538482666 train acc 0.5260916933475631\n",
            "epoch 4 batch id 2821 loss 1.2636563777923584 train acc 0.5261210563629919\n",
            "epoch 4 batch id 2831 loss 1.0716326236724854 train acc 0.5263544242317202\n",
            "epoch 4 batch id 2841 loss 1.2685515880584717 train acc 0.5265091517071454\n",
            "epoch 4 batch id 2851 loss 1.3958743810653687 train acc 0.526443572430726\n",
            "epoch 4 batch id 2861 loss 1.0846631526947021 train acc 0.5264439881160433\n",
            "epoch 4 batch id 2871 loss 0.9666057825088501 train acc 0.5265205938697318\n",
            "epoch 4 batch id 2881 loss 1.3274128437042236 train acc 0.5265424331829226\n",
            "epoch 4 batch id 2891 loss 1.156733751296997 train acc 0.5265857402282947\n",
            "epoch 4 batch id 2901 loss 1.150569200515747 train acc 0.5264833247156153\n",
            "epoch 4 batch id 2911 loss 1.0821360349655151 train acc 0.5265480075575404\n",
            "epoch 4 batch id 2921 loss 1.066792368888855 train acc 0.5264731684354673\n",
            "epoch 4 batch id 2931 loss 0.9545497298240662 train acc 0.5265267826680314\n",
            "epoch 4 batch id 2941 loss 1.2706882953643799 train acc 0.5265534682080925\n",
            "epoch 4 batch id 2951 loss 1.3067699670791626 train acc 0.526595857336496\n",
            "epoch 4 batch id 2961 loss 1.0395469665527344 train acc 0.5267382218844985\n",
            "epoch 4 batch id 2971 loss 1.2510945796966553 train acc 0.5267376304274655\n",
            "epoch 4 batch id 2981 loss 1.0999329090118408 train acc 0.526689869171419\n",
            "epoch 4 batch id 2991 loss 1.1017379760742188 train acc 0.5266998913406887\n",
            "epoch 4 batch id 3001 loss 1.1898759603500366 train acc 0.5267462929023659\n",
            "epoch 4 batch id 3011 loss 1.160873532295227 train acc 0.5268650365327134\n",
            "epoch 4 batch id 3021 loss 1.3266838788986206 train acc 0.5268795514730221\n",
            "epoch 4 batch id 3031 loss 1.1327240467071533 train acc 0.5269249010227648\n",
            "epoch 4 batch id 3041 loss 1.4079468250274658 train acc 0.5268517757316672\n",
            "epoch 4 batch id 3051 loss 1.2867419719696045 train acc 0.5267484021632252\n",
            "epoch 4 batch id 3061 loss 1.0570613145828247 train acc 0.5268243629532833\n",
            "epoch 4 batch id 3071 loss 1.286723256111145 train acc 0.5269100048844024\n",
            "epoch 4 batch id 3081 loss 1.3941771984100342 train acc 0.5269646624472574\n",
            "epoch 4 batch id 3091 loss 1.3570634126663208 train acc 0.5269532513749595\n",
            "epoch 4 batch id 3101 loss 1.26521897315979 train acc 0.5270376491454369\n",
            "epoch 4 batch id 3111 loss 1.3809850215911865 train acc 0.5270813243330119\n",
            "epoch 4 batch id 3121 loss 1.186385154724121 train acc 0.5272398670297982\n",
            "epoch 4 batch id 3131 loss 1.120341420173645 train acc 0.527292598211434\n",
            "epoch 4 batch id 3141 loss 1.2425165176391602 train acc 0.5272703756765361\n",
            "epoch 4 batch id 3151 loss 1.1755831241607666 train acc 0.5273226753411615\n",
            "epoch 4 batch id 3161 loss 0.9896962642669678 train acc 0.5274043024359379\n",
            "epoch 4 batch id 3171 loss 1.2218390703201294 train acc 0.5274854146956796\n",
            "epoch 4 batch id 3181 loss 1.2809035778045654 train acc 0.5274923373153096\n",
            "epoch 4 batch id 3191 loss 1.0916732549667358 train acc 0.5276412174866812\n",
            "epoch 4 batch id 3201 loss 1.4968323707580566 train acc 0.5277696422992815\n",
            "epoch 4 batch id 3211 loss 1.1742154359817505 train acc 0.5278194098411709\n",
            "epoch 4 batch id 3221 loss 1.496293067932129 train acc 0.5278494644520335\n",
            "epoch 4 batch id 3231 loss 1.0159021615982056 train acc 0.5279373645930052\n",
            "epoch 4 batch id 3241 loss 1.0845226049423218 train acc 0.5279620487503857\n",
            "epoch 4 batch id 3251 loss 0.982341468334198 train acc 0.5280442556136573\n",
            "epoch 4 batch id 3261 loss 1.1479758024215698 train acc 0.5280684605949095\n",
            "epoch 4 batch id 3271 loss 1.2590644359588623 train acc 0.5279969810455518\n",
            "epoch 4 batch id 3281 loss 1.2333966493606567 train acc 0.5280688052423042\n",
            "epoch 4 batch id 3291 loss 1.1313107013702393 train acc 0.5280452370100274\n",
            "epoch 4 batch id 3301 loss 1.0743616819381714 train acc 0.5280360118146017\n",
            "epoch 4 batch id 3311 loss 1.5261445045471191 train acc 0.5280929099969798\n",
            "epoch 4 batch id 3321 loss 1.2473922967910767 train acc 0.5280694820837097\n",
            "epoch 4 batch id 3331 loss 1.1900259256362915 train acc 0.5280837211047733\n",
            "epoch 4 batch id 3341 loss 1.3935753107070923 train acc 0.5280417539658785\n",
            "epoch 4 batch id 3351 loss 1.147334098815918 train acc 0.528149246493584\n",
            "epoch 4 batch id 3361 loss 1.1316813230514526 train acc 0.5282653972032133\n",
            "epoch 4 batch id 3371 loss 1.2774291038513184 train acc 0.5282696158409967\n",
            "epoch 4 batch id 3381 loss 1.1232491731643677 train acc 0.5282830523513753\n",
            "epoch 4 batch id 3391 loss 1.1677019596099854 train acc 0.5282687629017989\n",
            "epoch 4 batch id 3401 loss 1.4134894609451294 train acc 0.528433732725669\n",
            "epoch 4 batch id 3411 loss 1.2296922206878662 train acc 0.5285152814423922\n",
            "epoch 4 batch id 3421 loss 1.193698525428772 train acc 0.5285141406021631\n",
            "epoch 4 batch id 3431 loss 1.2064059972763062 train acc 0.5284720198192947\n",
            "epoch 4 batch id 3441 loss 1.1513605117797852 train acc 0.5285890729439117\n",
            "epoch 4 batch id 3451 loss 1.2930176258087158 train acc 0.528664698638076\n",
            "epoch 4 batch id 3461 loss 1.2622287273406982 train acc 0.5287895478185496\n",
            "epoch 4 batch id 3471 loss 1.0315288305282593 train acc 0.5289721982137713\n",
            "epoch 4 batch id 3481 loss 1.3018827438354492 train acc 0.5290370942257971\n",
            "epoch 4 batch id 3491 loss 1.0180631875991821 train acc 0.529070287883128\n",
            "epoch 4 batch id 3501 loss 1.2928330898284912 train acc 0.5291389960011426\n",
            "epoch 4 batch id 3511 loss 1.0141613483428955 train acc 0.5292251139276559\n",
            "epoch 4 batch id 3521 loss 1.3673462867736816 train acc 0.5293151803464925\n",
            "epoch 4 batch id 3531 loss 1.5313189029693604 train acc 0.5292675587652224\n",
            "epoch 4 batch id 3541 loss 1.2059682607650757 train acc 0.5293481714205027\n",
            "epoch 4 batch id 3551 loss 1.252870798110962 train acc 0.5294283300478738\n",
            "epoch 4 batch id 3561 loss 1.1301032304763794 train acc 0.5295255897219882\n",
            "epoch 4 batch id 3571 loss 1.4597021341323853 train acc 0.529491038924671\n",
            "epoch 4 batch id 3581 loss 1.3358877897262573 train acc 0.5294654077073443\n",
            "epoch 4 batch id 3591 loss 1.234858512878418 train acc 0.5295530492898914\n",
            "epoch 4 batch id 3601 loss 1.1603256464004517 train acc 0.5296315259650097\n",
            "epoch 4 batch id 3611 loss 1.1723746061325073 train acc 0.5296533162558847\n",
            "epoch 4 batch id 3621 loss 1.0644474029541016 train acc 0.5296188898094449\n",
            "epoch 4 batch id 3631 loss 1.05867338180542 train acc 0.5296793238777197\n",
            "epoch 4 batch id 3641 loss 1.3303416967391968 train acc 0.5297308431749519\n",
            "epoch 4 batch id 3651 loss 1.2938369512557983 train acc 0.5298377156943304\n",
            "epoch 4 batch id 3661 loss 1.4827160835266113 train acc 0.5299141286533734\n",
            "epoch 4 batch id 3671 loss 1.1509863138198853 train acc 0.5299347929719422\n",
            "epoch 4 batch id 3681 loss 1.3794505596160889 train acc 0.5299850584080413\n",
            "epoch 4 batch id 3691 loss 1.4716413021087646 train acc 0.5300181183960986\n",
            "epoch 4 batch id 3701 loss 1.022664189338684 train acc 0.5300847743853012\n",
            "epoch 4 batch id 3711 loss 1.2166850566864014 train acc 0.5302310697925088\n",
            "epoch 4 batch id 3721 loss 1.0605942010879517 train acc 0.530376578876646\n",
            "epoch 4 batch id 3731 loss 1.0089423656463623 train acc 0.5304375502546235\n",
            "epoch 4 batch id 3741 loss 1.313941478729248 train acc 0.5304021317829457\n",
            "epoch 4 batch id 3751 loss 1.406106948852539 train acc 0.5304585443881632\n",
            "epoch 4 batch id 3761 loss 1.2215697765350342 train acc 0.5305146570061154\n",
            "epoch 4 batch id 3771 loss 1.0488277673721313 train acc 0.5304917462211615\n",
            "epoch 4 batch id 3781 loss 1.117600679397583 train acc 0.5305061491668871\n",
            "epoch 4 batch id 3791 loss 1.0637863874435425 train acc 0.53049574650488\n",
            "epoch 4 batch id 3801 loss 1.0948498249053955 train acc 0.5306128321494343\n",
            "epoch 4 batch id 3811 loss 1.1432185173034668 train acc 0.530639103909735\n",
            "epoch 4 batch id 3821 loss 1.0596425533294678 train acc 0.5308083616854227\n",
            "epoch 4 batch id 3831 loss 1.3221648931503296 train acc 0.5308992430174889\n",
            "epoch 4 batch id 3841 loss 1.0733704566955566 train acc 0.530924563915647\n",
            "epoch 4 batch id 3851 loss 1.0966973304748535 train acc 0.5310065567385095\n",
            "epoch 4 batch id 3861 loss 1.2568732500076294 train acc 0.5310355154105154\n",
            "epoch 4 batch id 3871 loss 1.2268868684768677 train acc 0.5310602880392663\n",
            "epoch 4 batch id 3881 loss 1.2176536321640015 train acc 0.5310970110796187\n",
            "epoch 4 batch id 3891 loss 1.2420775890350342 train acc 0.5311295296838859\n",
            "epoch 4 batch id 3901 loss 1.1011590957641602 train acc 0.5311859138682389\n",
            "epoch 4 batch id 3911 loss 0.8962464332580566 train acc 0.5312779659933521\n",
            "epoch 4 batch id 3921 loss 1.2548952102661133 train acc 0.5312221053302729\n",
            "epoch 4 batch id 3931 loss 1.2321871519088745 train acc 0.5313016726023913\n",
            "epoch 4 batch id 3941 loss 1.0752156972885132 train acc 0.5313213651357523\n",
            "epoch 4 batch id 3951 loss 1.2051832675933838 train acc 0.5314240065806125\n",
            "epoch 4 batch id 3961 loss 1.3984878063201904 train acc 0.5313959543044686\n",
            "epoch 4 batch id 3971 loss 1.227112889289856 train acc 0.531462477965248\n",
            "epoch 4 batch id 3981 loss 1.3324521780014038 train acc 0.531532592313489\n",
            "epoch 4 batch id 3991 loss 1.4480386972427368 train acc 0.531531884239539\n",
            "epoch 4 batch id 4001 loss 1.2370822429656982 train acc 0.5314882216945763\n",
            "epoch 4 batch id 4011 loss 1.227500081062317 train acc 0.5314954188481675\n",
            "epoch 4 batch id 4021 loss 1.1794886589050293 train acc 0.5315336669982591\n",
            "epoch 4 batch id 4031 loss 1.2442340850830078 train acc 0.5315639729595634\n",
            "epoch 4 batch id 4041 loss 1.219198226928711 train acc 0.5315399962880475\n",
            "epoch 4 batch id 4051 loss 1.2768553495407104 train acc 0.5315585657862256\n",
            "epoch 4 batch id 4061 loss 1.3332204818725586 train acc 0.5315539583846344\n",
            "epoch 4 batch id 4071 loss 1.4924875497817993 train acc 0.5315800786047654\n",
            "epoch 4 batch id 4081 loss 1.0074187517166138 train acc 0.5316673303111983\n",
            "epoch 4 batch id 4091 loss 1.126894235610962 train acc 0.5316854069909558\n",
            "epoch 4 batch id 4101 loss 1.2891664505004883 train acc 0.5317262557912704\n",
            "epoch 4 batch id 4111 loss 1.2228569984436035 train acc 0.5318239175383118\n",
            "epoch 4 batch id 4121 loss 1.4195383787155151 train acc 0.5318566488716331\n",
            "epoch 4 batch id 4131 loss 1.0146377086639404 train acc 0.5318778746066328\n",
            "epoch 4 batch id 4141 loss 1.1422864198684692 train acc 0.5318989978266119\n",
            "epoch 4 batch id 4151 loss 1.1559580564498901 train acc 0.5319275475788966\n",
            "epoch 4 batch id 4161 loss 1.253933310508728 train acc 0.5319146539293439\n",
            "epoch 4 batch id 4171 loss 1.0144336223602295 train acc 0.5319804902900983\n",
            "epoch 4 batch id 4181 loss 0.9645720720291138 train acc 0.5320123774216694\n",
            "epoch 4 batch id 4191 loss 1.4385873079299927 train acc 0.5320925793366739\n",
            "epoch 4 batch id 4201 loss 1.206680178642273 train acc 0.5321500833134968\n",
            "epoch 4 batch id 4211 loss 1.2991547584533691 train acc 0.5321924720968891\n",
            "epoch 4 batch id 4221 loss 0.988074541091919 train acc 0.5322753790570954\n",
            "epoch 4 batch id 4231 loss 1.1617056131362915 train acc 0.5322803415268258\n",
            "epoch 4 batch id 4241 loss 1.3659924268722534 train acc 0.5323810716812073\n",
            "epoch 4 batch id 4251 loss 1.1770946979522705 train acc 0.5324298694424842\n",
            "epoch 4 batch id 4261 loss 1.112681269645691 train acc 0.532474771180474\n",
            "epoch 4 batch id 4271 loss 1.0219745635986328 train acc 0.532534096230391\n",
            "epoch 4 batch id 4281 loss 1.1857460737228394 train acc 0.5326442419995329\n",
            "epoch 4 batch id 4291 loss 0.9284359812736511 train acc 0.5327138196224657\n",
            "epoch 4 batch id 4301 loss 1.1160216331481934 train acc 0.5326777202976052\n",
            "epoch 4 batch id 4311 loss 1.0134196281433105 train acc 0.5327251507770819\n",
            "epoch 4 batch id 4321 loss 1.233330249786377 train acc 0.5327506653552418\n",
            "epoch 4 batch id 4331 loss 1.0511500835418701 train acc 0.5327941006695913\n",
            "epoch 4 batch id 4341 loss 1.4056023359298706 train acc 0.5327905436535361\n",
            "epoch 4 batch id 4351 loss 1.3651200532913208 train acc 0.5327295449321995\n",
            "epoch 4 batch id 4361 loss 1.3504421710968018 train acc 0.5327906443476267\n",
            "epoch 4 batch id 4371 loss 1.365132451057434 train acc 0.5328836364676276\n",
            "epoch 4 batch id 4381 loss 1.1098406314849854 train acc 0.5329512383017576\n",
            "epoch 4 batch id 4391 loss 1.0162278413772583 train acc 0.5330114153951264\n",
            "epoch 4 batch id 4401 loss 1.113694429397583 train acc 0.5330606680299932\n",
            "epoch 4 batch id 4411 loss 1.1503369808197021 train acc 0.5330246826116527\n",
            "epoch 4 batch id 4421 loss 1.0915957689285278 train acc 0.5331443677900928\n",
            "epoch 4 batch id 4431 loss 1.1432732343673706 train acc 0.5332317761227714\n",
            "epoch 4 batch id 4441 loss 0.9978080987930298 train acc 0.5332871256473767\n",
            "epoch 4 batch id 4451 loss 1.2679919004440308 train acc 0.5333422264659627\n",
            "epoch 4 batch id 4461 loss 1.049736738204956 train acc 0.5333900750952701\n",
            "epoch 4 batch id 4471 loss 1.298525333404541 train acc 0.5333713095504361\n",
            "epoch 4 batch id 4481 loss 1.2551685571670532 train acc 0.5333874972104441\n",
            "epoch 4 batch id 4491 loss 1.188223123550415 train acc 0.5333931752393676\n",
            "epoch 4 batch id 4501 loss 0.9962071776390076 train acc 0.5334508998000445\n",
            "epoch 4 batch id 4511 loss 1.1394944190979004 train acc 0.5334945134116604\n",
            "epoch 4 batch id 4521 loss 0.9985160827636719 train acc 0.5335068292413183\n",
            "epoch 4 batch id 4531 loss 1.2352218627929688 train acc 0.5334983999117192\n",
            "epoch 4 batch id 4541 loss 1.0008915662765503 train acc 0.533565706892755\n",
            "epoch 4 batch id 4551 loss 1.2499111890792847 train acc 0.5336018182816963\n",
            "epoch 4 batch id 4561 loss 1.1469125747680664 train acc 0.5336240681868012\n",
            "epoch 4 batch id 4571 loss 1.4735536575317383 train acc 0.5336838219208051\n",
            "epoch 4 batch id 4581 loss 1.2750014066696167 train acc 0.5336546332678455\n",
            "epoch 4 batch id 4591 loss 1.2068206071853638 train acc 0.5336289751688086\n",
            "epoch 4 batch id 4601 loss 1.1579593420028687 train acc 0.5337358726363834\n",
            "epoch 4 batch id 4611 loss 1.125755786895752 train acc 0.5337948655389286\n",
            "epoch 4 batch id 4621 loss 1.2160876989364624 train acc 0.5338502218134603\n",
            "epoch 4 batch id 4631 loss 1.23845374584198 train acc 0.5338952170157634\n",
            "epoch 4 batch id 4641 loss 1.2111928462982178 train acc 0.5339736856280974\n",
            "epoch 4 batch id 4651 loss 1.1398863792419434 train acc 0.5339947054396904\n",
            "epoch 4 batch id 4661 loss 1.1324350833892822 train acc 0.534075976185368\n",
            "epoch 4 batch id 4671 loss 1.1576976776123047 train acc 0.5341134125454935\n",
            "epoch 4 batch id 4681 loss 1.3798092603683472 train acc 0.534177392651143\n",
            "epoch 4 batch id 4691 loss 0.7400327324867249 train acc 0.5343177094436155\n",
            "epoch 4 batch id 4701 loss 1.2478479146957397 train acc 0.5343776590087216\n",
            "epoch 4 batch id 4711 loss 0.9552321434020996 train acc 0.534440670770537\n",
            "epoch 4 batch id 4721 loss 1.189664363861084 train acc 0.5344967962296123\n",
            "epoch 4 batch id 4731 loss 1.2459765672683716 train acc 0.5344998414711477\n",
            "epoch 4 batch id 4741 loss 1.2332804203033447 train acc 0.5345424224847078\n",
            "epoch 4 batch id 4751 loss 1.1383684873580933 train acc 0.5345683803409809\n",
            "epoch 4 batch id 4761 loss 1.2604912519454956 train acc 0.5345581285444234\n",
            "epoch 4 batch id 4771 loss 1.199415683746338 train acc 0.5346068696290086\n",
            "epoch 4 batch id 4781 loss 1.317590355873108 train acc 0.5345802394896465\n",
            "epoch 4 batch id 4791 loss 1.2070683240890503 train acc 0.5346319922771864\n",
            "epoch 4 batch id 4801 loss 1.2091188430786133 train acc 0.5346119298062904\n",
            "epoch 4 batch id 4811 loss 1.066354751586914 train acc 0.5346081895655789\n",
            "epoch 4 batch id 4821 loss 1.2362725734710693 train acc 0.534601223812487\n",
            "epoch 4 batch id 4831 loss 1.1947163343429565 train acc 0.5346233957772718\n",
            "epoch 4 batch id 4841 loss 1.2276209592819214 train acc 0.5346196550299525\n",
            "epoch 4 batch id 4851 loss 1.0558115243911743 train acc 0.5347222222222222\n",
            "epoch 4 batch id 4861 loss 1.2319244146347046 train acc 0.534766508948776\n",
            "epoch 4 batch id 4871 loss 1.4168552160263062 train acc 0.5347913672757134\n",
            "epoch 4 batch id 4881 loss 1.2374629974365234 train acc 0.5349025558287236\n",
            "epoch 4 batch id 4891 loss 1.2855361700057983 train acc 0.5349174504191372\n",
            "epoch 4 batch id 4901 loss 1.2485495805740356 train acc 0.5350087992246481\n",
            "epoch 4 batch id 4911 loss 1.132502794265747 train acc 0.5350488698839341\n",
            "epoch 4 batch id 4921 loss 1.174941062927246 train acc 0.5350475005080269\n",
            "epoch 4 batch id 4931 loss 1.3761924505233765 train acc 0.5350493054147232\n",
            "epoch 4 batch id 4941 loss 1.0726138353347778 train acc 0.5350605899615463\n",
            "epoch 4 batch id 4951 loss 1.4189120531082153 train acc 0.5351412593415472\n",
            "epoch 4 train acc 0.5352167848038587\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566cb7f639ca445a870b8fffe2ee9286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 loss 1.1667062044143677 test acc 0.5639983504398827\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b8ac473f61d4bc98684ff80cde3b646",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 batch id 1 loss 1.1386325359344482 train acc 0.578125\n",
            "epoch 5 batch id 11 loss 1.1377363204956055 train acc 0.5696022727272727\n",
            "epoch 5 batch id 21 loss 1.0097646713256836 train acc 0.5639880952380952\n",
            "epoch 5 batch id 31 loss 1.2135684490203857 train acc 0.5498991935483871\n",
            "epoch 5 batch id 41 loss 0.9378114342689514 train acc 0.5567835365853658\n",
            "epoch 5 batch id 51 loss 1.3118759393692017 train acc 0.5548406862745098\n",
            "epoch 5 batch id 61 loss 1.3182697296142578 train acc 0.5550717213114754\n",
            "epoch 5 batch id 71 loss 1.2981995344161987 train acc 0.5558978873239436\n",
            "epoch 5 batch id 81 loss 1.1507630348205566 train acc 0.5590277777777778\n",
            "epoch 5 batch id 91 loss 1.2711327075958252 train acc 0.5592376373626373\n",
            "epoch 5 batch id 101 loss 0.9472687244415283 train acc 0.5583230198019802\n",
            "epoch 5 batch id 111 loss 1.2740209102630615 train acc 0.5584177927927928\n",
            "epoch 5 batch id 121 loss 1.4821213483810425 train acc 0.5569473140495868\n",
            "epoch 5 batch id 131 loss 1.2929563522338867 train acc 0.5580868320610687\n",
            "epoch 5 batch id 141 loss 1.105323314666748 train acc 0.5585106382978723\n",
            "epoch 5 batch id 151 loss 1.0830039978027344 train acc 0.5597061258278145\n",
            "epoch 5 batch id 161 loss 1.3624906539916992 train acc 0.5609472049689441\n",
            "epoch 5 batch id 171 loss 1.2468721866607666 train acc 0.5608552631578947\n",
            "epoch 5 batch id 181 loss 1.1230931282043457 train acc 0.5606871546961326\n",
            "epoch 5 batch id 191 loss 1.2564722299575806 train acc 0.559800392670157\n",
            "epoch 5 batch id 201 loss 0.9234237670898438 train acc 0.5609452736318408\n",
            "epoch 5 batch id 211 loss 1.4018173217773438 train acc 0.5601303317535545\n",
            "epoch 5 batch id 221 loss 1.293977975845337 train acc 0.560237556561086\n",
            "epoch 5 batch id 231 loss 1.2290257215499878 train acc 0.5604031385281385\n",
            "epoch 5 batch id 241 loss 1.3956983089447021 train acc 0.5577671161825726\n",
            "epoch 5 batch id 251 loss 1.2565208673477173 train acc 0.5574576693227091\n",
            "epoch 5 batch id 261 loss 1.1612712144851685 train acc 0.5590876436781609\n",
            "epoch 5 batch id 271 loss 0.9726988077163696 train acc 0.5599630996309963\n",
            "epoch 5 batch id 281 loss 1.0901020765304565 train acc 0.5604982206405694\n",
            "epoch 5 batch id 291 loss 1.5402439832687378 train acc 0.5613724226804123\n",
            "epoch 5 batch id 301 loss 1.2531757354736328 train acc 0.5615656146179402\n",
            "epoch 5 batch id 311 loss 1.134706974029541 train acc 0.5606913183279743\n",
            "epoch 5 batch id 321 loss 1.3707902431488037 train acc 0.5601148753894081\n",
            "epoch 5 batch id 331 loss 1.2877906560897827 train acc 0.5598092900302115\n",
            "epoch 5 batch id 341 loss 1.2372443675994873 train acc 0.5596132697947214\n",
            "epoch 5 batch id 351 loss 1.2404197454452515 train acc 0.5599180911680912\n",
            "epoch 5 batch id 361 loss 1.3766953945159912 train acc 0.5600761772853186\n",
            "epoch 5 batch id 371 loss 1.2989450693130493 train acc 0.5605205525606469\n",
            "epoch 5 batch id 381 loss 1.0920193195343018 train acc 0.5609416010498688\n",
            "epoch 5 batch id 391 loss 1.3669341802597046 train acc 0.5602221867007673\n",
            "epoch 5 batch id 401 loss 0.9835900068283081 train acc 0.5596945137157108\n",
            "epoch 5 batch id 411 loss 1.0182126760482788 train acc 0.5597247566909975\n",
            "epoch 5 batch id 421 loss 1.1952637434005737 train acc 0.5594937648456056\n",
            "epoch 5 batch id 431 loss 1.2581806182861328 train acc 0.5594185034802784\n",
            "epoch 5 batch id 441 loss 1.330966830253601 train acc 0.5591695011337868\n",
            "epoch 5 batch id 451 loss 1.0871872901916504 train acc 0.5596937361419069\n",
            "epoch 5 batch id 461 loss 1.0862360000610352 train acc 0.5594834598698482\n",
            "epoch 5 batch id 471 loss 1.0178357362747192 train acc 0.5603105095541401\n",
            "epoch 5 batch id 481 loss 1.4525588750839233 train acc 0.5606808731808732\n",
            "epoch 5 batch id 491 loss 1.0836732387542725 train acc 0.560686099796334\n",
            "epoch 5 batch id 501 loss 1.468677282333374 train acc 0.5603480538922155\n",
            "epoch 5 batch id 511 loss 1.10124671459198 train acc 0.5608488258317026\n",
            "epoch 5 batch id 521 loss 1.315724492073059 train acc 0.5610604606525912\n",
            "epoch 5 batch id 531 loss 1.2310913801193237 train acc 0.560852165725047\n",
            "epoch 5 batch id 541 loss 1.1739143133163452 train acc 0.5612580868761553\n",
            "epoch 5 batch id 551 loss 1.019119143486023 train acc 0.5608269056261344\n",
            "epoch 5 batch id 561 loss 0.955200731754303 train acc 0.5611909536541889\n",
            "epoch 5 batch id 571 loss 1.2050635814666748 train acc 0.5610223292469352\n",
            "epoch 5 batch id 581 loss 1.2578167915344238 train acc 0.5607519363166954\n",
            "epoch 5 batch id 591 loss 0.9332350492477417 train acc 0.5610723350253807\n",
            "epoch 5 batch id 601 loss 1.0725066661834717 train acc 0.561174084858569\n",
            "epoch 5 batch id 611 loss 1.0817774534225464 train acc 0.5616560965630114\n",
            "epoch 5 batch id 621 loss 1.1283155679702759 train acc 0.5626509661835749\n",
            "epoch 5 batch id 631 loss 1.3307793140411377 train acc 0.5623019017432647\n",
            "epoch 5 batch id 641 loss 1.1466821432113647 train acc 0.561988104524181\n",
            "epoch 5 batch id 651 loss 1.0004671812057495 train acc 0.5618999615975423\n",
            "epoch 5 batch id 661 loss 1.3038270473480225 train acc 0.561838124054463\n",
            "epoch 5 batch id 671 loss 1.3859179019927979 train acc 0.5618712742175856\n",
            "epoch 5 batch id 681 loss 1.122997522354126 train acc 0.5618346182085169\n",
            "epoch 5 batch id 691 loss 0.9624844789505005 train acc 0.562206041968162\n",
            "epoch 5 batch id 701 loss 1.085270643234253 train acc 0.5623885520684736\n",
            "epoch 5 batch id 711 loss 1.1397783756256104 train acc 0.5625219760900141\n",
            "epoch 5 batch id 721 loss 1.0804665088653564 train acc 0.5625216712898752\n",
            "epoch 5 batch id 731 loss 1.0561360120773315 train acc 0.5628633720930233\n",
            "epoch 5 batch id 741 loss 1.2580513954162598 train acc 0.5632801956815114\n",
            "epoch 5 batch id 751 loss 1.328579306602478 train acc 0.5627080559254327\n",
            "epoch 5 batch id 761 loss 0.9142701625823975 train acc 0.5631775624178712\n",
            "epoch 5 batch id 771 loss 1.417627215385437 train acc 0.5626621271076524\n",
            "epoch 5 batch id 781 loss 1.119004487991333 train acc 0.5628201024327785\n",
            "epoch 5 batch id 791 loss 0.9644875526428223 train acc 0.562835809102402\n",
            "epoch 5 batch id 801 loss 1.1629595756530762 train acc 0.563455836454432\n",
            "epoch 5 batch id 811 loss 1.2280787229537964 train acc 0.5637908446362515\n",
            "epoch 5 batch id 821 loss 1.0404690504074097 train acc 0.5638702801461632\n",
            "epoch 5 batch id 831 loss 1.2757415771484375 train acc 0.5638537906137184\n",
            "epoch 5 batch id 841 loss 1.0975703001022339 train acc 0.5644879607609988\n",
            "epoch 5 batch id 851 loss 1.1462628841400146 train acc 0.5644462397179788\n",
            "epoch 5 batch id 861 loss 1.3717994689941406 train acc 0.5644780778164924\n",
            "epoch 5 batch id 871 loss 0.9666242003440857 train acc 0.5643118541905855\n",
            "epoch 5 batch id 881 loss 1.1659735441207886 train acc 0.5643267593643587\n",
            "epoch 5 batch id 891 loss 1.0640424489974976 train acc 0.5644991582491582\n",
            "epoch 5 batch id 901 loss 1.2841880321502686 train acc 0.5642862097669257\n",
            "epoch 5 batch id 911 loss 1.3625664710998535 train acc 0.5640607848518112\n",
            "epoch 5 batch id 921 loss 1.3869683742523193 train acc 0.5639929424538545\n",
            "epoch 5 batch id 931 loss 1.0430189371109009 train acc 0.5641615198711063\n",
            "epoch 5 batch id 941 loss 0.9981647729873657 train acc 0.5642102816153028\n",
            "epoch 5 batch id 951 loss 1.272358775138855 train acc 0.564077287066246\n",
            "epoch 5 batch id 961 loss 1.2284893989562988 train acc 0.5638657648283039\n",
            "epoch 5 batch id 971 loss 1.2206878662109375 train acc 0.5644792739443872\n",
            "epoch 5 batch id 981 loss 1.3384637832641602 train acc 0.5648413608562691\n",
            "epoch 5 batch id 991 loss 1.0688058137893677 train acc 0.5648177346115035\n",
            "epoch 5 batch id 1001 loss 1.3417268991470337 train acc 0.5648726273726273\n",
            "epoch 5 batch id 1011 loss 1.229319453239441 train acc 0.5649109792284867\n",
            "epoch 5 batch id 1021 loss 0.9577975869178772 train acc 0.5650557051909892\n",
            "epoch 5 batch id 1031 loss 1.322416067123413 train acc 0.5647884335596508\n",
            "epoch 5 batch id 1041 loss 1.2006884813308716 train acc 0.5646763928914506\n",
            "epoch 5 batch id 1051 loss 1.0618231296539307 train acc 0.5644326831588963\n",
            "epoch 5 batch id 1061 loss 1.125352382659912 train acc 0.5643555607917059\n",
            "epoch 5 batch id 1071 loss 0.9561027884483337 train acc 0.5643965919701214\n",
            "epoch 5 batch id 1081 loss 1.1575249433517456 train acc 0.5641911424606846\n",
            "epoch 5 batch id 1091 loss 1.395394206047058 train acc 0.5642902153987168\n",
            "epoch 5 batch id 1101 loss 1.199437141418457 train acc 0.5647138964577657\n",
            "epoch 5 batch id 1111 loss 1.1301038265228271 train acc 0.5648627362736274\n",
            "epoch 5 batch id 1121 loss 1.2342209815979004 train acc 0.5650786128456735\n",
            "epoch 5 batch id 1131 loss 1.253798246383667 train acc 0.5649176613616269\n",
            "epoch 5 batch id 1141 loss 1.0537383556365967 train acc 0.5651566608238388\n",
            "epoch 5 batch id 1151 loss 1.3328912258148193 train acc 0.565282906168549\n",
            "epoch 5 batch id 1161 loss 1.0940022468566895 train acc 0.5653666020671835\n",
            "epoch 5 batch id 1171 loss 1.0297839641571045 train acc 0.5655956447480786\n",
            "epoch 5 batch id 1181 loss 1.050991177558899 train acc 0.5654238992379339\n",
            "epoch 5 batch id 1191 loss 1.0718423128128052 train acc 0.5655961376994123\n",
            "epoch 5 batch id 1201 loss 1.3802247047424316 train acc 0.5657134679433805\n",
            "epoch 5 batch id 1211 loss 0.9787352681159973 train acc 0.5656611271676301\n",
            "epoch 5 batch id 1221 loss 1.111581563949585 train acc 0.5656480343980343\n",
            "epoch 5 batch id 1231 loss 1.182350754737854 train acc 0.5656732331437856\n",
            "epoch 5 batch id 1241 loss 1.3926345109939575 train acc 0.5655091659951652\n",
            "epoch 5 batch id 1251 loss 0.965986967086792 train acc 0.565472621902478\n",
            "epoch 5 batch id 1261 loss 1.3825616836547852 train acc 0.5654490483743061\n",
            "epoch 5 batch id 1271 loss 1.0987435579299927 train acc 0.5653520849724626\n",
            "epoch 5 batch id 1281 loss 1.230551838874817 train acc 0.5654152029664324\n",
            "epoch 5 batch id 1291 loss 1.2932978868484497 train acc 0.565525755228505\n",
            "epoch 5 batch id 1301 loss 1.2393217086791992 train acc 0.5653583781706379\n",
            "epoch 5 batch id 1311 loss 1.3744293451309204 train acc 0.5654200038138826\n",
            "epoch 5 batch id 1321 loss 1.026485800743103 train acc 0.565374242997729\n",
            "epoch 5 batch id 1331 loss 1.0736807584762573 train acc 0.565470041322314\n",
            "epoch 5 batch id 1341 loss 1.114314317703247 train acc 0.5655527591349739\n",
            "epoch 5 batch id 1351 loss 1.0365575551986694 train acc 0.5657152109548482\n",
            "epoch 5 batch id 1361 loss 1.3255685567855835 train acc 0.5655653012490816\n",
            "epoch 5 batch id 1371 loss 1.081596851348877 train acc 0.5654859591539022\n",
            "epoch 5 batch id 1381 loss 1.4032479524612427 train acc 0.5656793084721217\n",
            "epoch 5 batch id 1391 loss 0.7569082379341125 train acc 0.5660046728971962\n",
            "epoch 5 batch id 1401 loss 1.2238725423812866 train acc 0.566046573875803\n",
            "epoch 5 batch id 1411 loss 0.9877302050590515 train acc 0.5660214386959603\n",
            "epoch 5 batch id 1421 loss 1.1984038352966309 train acc 0.5661176108374384\n",
            "epoch 5 batch id 1431 loss 0.9381140470504761 train acc 0.566146925227114\n",
            "epoch 5 batch id 1441 loss 0.9835138320922852 train acc 0.5660782442748091\n",
            "epoch 5 batch id 1451 loss 1.0106911659240723 train acc 0.5663981736733288\n",
            "epoch 5 batch id 1461 loss 1.086482286453247 train acc 0.5663501026694046\n",
            "epoch 5 batch id 1471 loss 1.3426399230957031 train acc 0.5664832596872875\n",
            "epoch 5 batch id 1481 loss 1.1963117122650146 train acc 0.566561866981769\n",
            "epoch 5 batch id 1491 loss 1.1424239873886108 train acc 0.5665660630449363\n",
            "epoch 5 batch id 1501 loss 1.1257718801498413 train acc 0.5666847101932045\n",
            "epoch 5 batch id 1511 loss 1.0184862613677979 train acc 0.5668845135671741\n",
            "epoch 5 batch id 1521 loss 1.0431057214736938 train acc 0.5670200525969756\n",
            "epoch 5 batch id 1531 loss 1.1941089630126953 train acc 0.5669088830829523\n",
            "epoch 5 batch id 1541 loss 1.175082802772522 train acc 0.5669106911096691\n",
            "epoch 5 batch id 1551 loss 1.2278575897216797 train acc 0.5668218085106383\n",
            "epoch 5 batch id 1561 loss 1.2286509275436401 train acc 0.5668641896220371\n",
            "epoch 5 batch id 1571 loss 1.0189661979675293 train acc 0.5671546785486951\n",
            "epoch 5 batch id 1581 loss 1.1359754800796509 train acc 0.567293247944339\n",
            "epoch 5 batch id 1591 loss 1.1230305433273315 train acc 0.5672434789440604\n",
            "epoch 5 batch id 1601 loss 1.2260257005691528 train acc 0.5672431292941912\n",
            "epoch 5 batch id 1611 loss 1.0509206056594849 train acc 0.5672427839851024\n",
            "epoch 5 batch id 1621 loss 1.1116951704025269 train acc 0.56722316471314\n",
            "epoch 5 batch id 1631 loss 1.0278193950653076 train acc 0.5671367259350092\n",
            "epoch 5 batch id 1641 loss 1.1229748725891113 train acc 0.567298903107861\n",
            "epoch 5 batch id 1651 loss 1.27308988571167 train acc 0.5672319806178074\n",
            "epoch 5 batch id 1661 loss 1.1703038215637207 train acc 0.567485701384708\n",
            "epoch 5 batch id 1671 loss 1.14949369430542 train acc 0.5675213195691203\n",
            "epoch 5 batch id 1681 loss 1.3237546682357788 train acc 0.5674635633551457\n",
            "epoch 5 batch id 1691 loss 1.111562967300415 train acc 0.5677206534594914\n",
            "epoch 5 batch id 1701 loss 1.063978672027588 train acc 0.5677083333333334\n",
            "epoch 5 batch id 1711 loss 1.1904127597808838 train acc 0.5677418176504968\n",
            "epoch 5 batch id 1721 loss 1.1529570817947388 train acc 0.5678384660081348\n",
            "epoch 5 batch id 1731 loss 1.0140972137451172 train acc 0.5678617850953206\n",
            "epoch 5 batch id 1741 loss 1.1227144002914429 train acc 0.5680015077541642\n",
            "epoch 5 batch id 1751 loss 0.8381132483482361 train acc 0.5680503997715591\n",
            "epoch 5 batch id 1761 loss 0.9276174902915955 train acc 0.5681431005110733\n",
            "epoch 5 batch id 1771 loss 1.0332958698272705 train acc 0.5681553500846979\n",
            "epoch 5 batch id 1781 loss 1.026134967803955 train acc 0.5683692448062886\n",
            "epoch 5 batch id 1791 loss 1.5490695238113403 train acc 0.5684847850362926\n",
            "epoch 5 batch id 1801 loss 1.1109765768051147 train acc 0.5686424208772904\n",
            "epoch 5 batch id 1811 loss 0.9808072447776794 train acc 0.5687465488680287\n",
            "epoch 5 batch id 1821 loss 1.0150904655456543 train acc 0.56896107907743\n",
            "epoch 5 batch id 1831 loss 0.8784679174423218 train acc 0.5689428590933916\n",
            "epoch 5 batch id 1841 loss 0.9747041463851929 train acc 0.5690351711026616\n",
            "epoch 5 batch id 1851 loss 1.27068030834198 train acc 0.5689998649378715\n",
            "epoch 5 batch id 1861 loss 1.121079444885254 train acc 0.5690740865126276\n",
            "epoch 5 batch id 1871 loss 1.0710067749023438 train acc 0.5692477284874399\n",
            "epoch 5 batch id 1881 loss 1.1435035467147827 train acc 0.5692700026581605\n",
            "epoch 5 batch id 1891 loss 0.8389434218406677 train acc 0.5696390798519302\n",
            "epoch 5 batch id 1901 loss 0.9803867340087891 train acc 0.5696837190952131\n",
            "epoch 5 batch id 1911 loss 1.007826328277588 train acc 0.5696215986394558\n",
            "epoch 5 batch id 1921 loss 1.125623106956482 train acc 0.5695113222280063\n",
            "epoch 5 batch id 1931 loss 1.2415030002593994 train acc 0.5696449378560331\n",
            "epoch 5 batch id 1941 loss 1.1208198070526123 train acc 0.5696725270479135\n",
            "epoch 5 batch id 1951 loss 1.1798431873321533 train acc 0.5697238595592005\n",
            "epoch 5 batch id 1961 loss 0.9413972496986389 train acc 0.5697507649158593\n",
            "epoch 5 batch id 1971 loss 1.1337695121765137 train acc 0.5697615423642821\n",
            "epoch 5 batch id 1981 loss 1.0622265338897705 train acc 0.5698589727410399\n",
            "epoch 5 batch id 1991 loss 1.075888752937317 train acc 0.5698769462581618\n",
            "epoch 5 batch id 2001 loss 1.3652807474136353 train acc 0.5699806346826587\n",
            "epoch 5 batch id 2011 loss 1.2928446531295776 train acc 0.5699589756340129\n",
            "epoch 5 batch id 2021 loss 1.2523940801620483 train acc 0.5699529935675408\n",
            "epoch 5 batch id 2031 loss 1.063739538192749 train acc 0.5699701501723289\n",
            "epoch 5 batch id 2041 loss 1.0456782579421997 train acc 0.5700024497795199\n",
            "epoch 5 batch id 2051 loss 1.2900129556655884 train acc 0.5700496708922477\n",
            "epoch 5 batch id 2061 loss 1.0366743803024292 train acc 0.570088852498787\n",
            "epoch 5 batch id 2071 loss 1.0898869037628174 train acc 0.5701276557218735\n",
            "epoch 5 batch id 2081 loss 1.2887353897094727 train acc 0.5700684766938972\n",
            "epoch 5 batch id 2091 loss 1.1775919198989868 train acc 0.570181731229077\n",
            "epoch 5 batch id 2101 loss 1.0274688005447388 train acc 0.5702121013802951\n",
            "epoch 5 batch id 2111 loss 1.0512385368347168 train acc 0.5704050213169114\n",
            "epoch 5 batch id 2121 loss 0.8248946666717529 train acc 0.5704635195662423\n",
            "epoch 5 batch id 2131 loss 1.3018310070037842 train acc 0.5705581299859221\n",
            "epoch 5 batch id 2141 loss 1.2700717449188232 train acc 0.5706007706679121\n",
            "epoch 5 batch id 2151 loss 1.0803236961364746 train acc 0.5706720711297071\n",
            "epoch 5 batch id 2161 loss 1.3384621143341064 train acc 0.5705619504858862\n",
            "epoch 5 batch id 2171 loss 1.050978660583496 train acc 0.570632772915707\n",
            "epoch 5 batch id 2181 loss 1.1937553882598877 train acc 0.5705954837230628\n",
            "epoch 5 batch id 2191 loss 1.2941818237304688 train acc 0.5706726380648106\n",
            "epoch 5 batch id 2201 loss 1.290661096572876 train acc 0.5707348932303499\n",
            "epoch 5 batch id 2211 loss 1.1303859949111938 train acc 0.5706623134328358\n",
            "epoch 5 batch id 2221 loss 1.3110573291778564 train acc 0.5706537032868078\n",
            "epoch 5 batch id 2231 loss 1.0435707569122314 train acc 0.570855277902286\n",
            "epoch 5 batch id 2241 loss 0.9902826547622681 train acc 0.5708877175368139\n",
            "epoch 5 batch id 2251 loss 1.1803735494613647 train acc 0.5708296312749889\n",
            "epoch 5 batch id 2261 loss 0.9401713013648987 train acc 0.5708273440955329\n",
            "epoch 5 batch id 2271 loss 1.2365827560424805 train acc 0.571010843240863\n",
            "epoch 5 batch id 2281 loss 1.2935290336608887 train acc 0.5710488820692678\n",
            "epoch 5 batch id 2291 loss 1.1775535345077515 train acc 0.5712843736359668\n",
            "epoch 5 batch id 2301 loss 1.1212033033370972 train acc 0.571239406779661\n",
            "epoch 5 batch id 2311 loss 1.1452282667160034 train acc 0.5713300519255733\n",
            "epoch 5 batch id 2321 loss 0.7340298295021057 train acc 0.5714468440327445\n",
            "epoch 5 batch id 2331 loss 1.1030049324035645 train acc 0.571307915057915\n",
            "epoch 5 batch id 2341 loss 1.5267938375473022 train acc 0.5714304784280222\n",
            "epoch 5 batch id 2351 loss 0.9839938282966614 train acc 0.5713991386643982\n",
            "epoch 5 batch id 2361 loss 1.227754831314087 train acc 0.5714143900889453\n",
            "epoch 5 batch id 2371 loss 0.996250569820404 train acc 0.5714756431885281\n",
            "epoch 5 batch id 2381 loss 1.1997184753417969 train acc 0.5715363817723645\n",
            "epoch 5 batch id 2391 loss 1.1862423419952393 train acc 0.5714985884567126\n",
            "epoch 5 batch id 2401 loss 1.269525408744812 train acc 0.5715847563515202\n",
            "epoch 5 batch id 2411 loss 1.2096037864685059 train acc 0.5715859601824969\n",
            "epoch 5 batch id 2421 loss 1.0646220445632935 train acc 0.5716710553490293\n",
            "epoch 5 batch id 2431 loss 1.1966971158981323 train acc 0.5716911764705882\n",
            "epoch 5 batch id 2441 loss 1.166359543800354 train acc 0.571775143383859\n",
            "epoch 5 batch id 2451 loss 1.0348613262176514 train acc 0.5717883006935944\n",
            "epoch 5 batch id 2461 loss 1.3207091093063354 train acc 0.5717950020316944\n",
            "epoch 5 batch id 2471 loss 1.0787209272384644 train acc 0.5719091460946985\n",
            "epoch 5 batch id 2481 loss 1.184931993484497 train acc 0.5718775191455059\n",
            "epoch 5 batch id 2491 loss 1.3142656087875366 train acc 0.5717646025692493\n",
            "epoch 5 batch id 2501 loss 0.8290755152702332 train acc 0.5718212714914035\n",
            "epoch 5 batch id 2511 loss 1.2260489463806152 train acc 0.5720206093189965\n",
            "epoch 5 batch id 2521 loss 1.0842341184616089 train acc 0.5719146667988894\n",
            "epoch 5 batch id 2531 loss 1.1451054811477661 train acc 0.5720071118135125\n",
            "epoch 5 batch id 2541 loss 1.0935858488082886 train acc 0.5720557851239669\n",
            "epoch 5 batch id 2551 loss 1.3190932273864746 train acc 0.5719754508036065\n",
            "epoch 5 batch id 2561 loss 1.2615033388137817 train acc 0.5719140472471691\n",
            "epoch 5 batch id 2571 loss 1.0886814594268799 train acc 0.5721205270322831\n",
            "epoch 5 batch id 2581 loss 1.2073317766189575 train acc 0.5721619527314994\n",
            "epoch 5 batch id 2591 loss 1.016685962677002 train acc 0.5721548147433423\n",
            "epoch 5 batch id 2601 loss 1.1728360652923584 train acc 0.57217776816609\n",
            "epoch 5 batch id 2611 loss 1.0612637996673584 train acc 0.5721945614707009\n",
            "epoch 5 batch id 2621 loss 1.1344587802886963 train acc 0.5722291110263258\n",
            "epoch 5 batch id 2631 loss 1.2791142463684082 train acc 0.5721921322690992\n",
            "epoch 5 batch id 2641 loss 1.1110575199127197 train acc 0.5723565884134797\n",
            "epoch 5 batch id 2651 loss 1.098610758781433 train acc 0.5723842417955488\n",
            "epoch 5 batch id 2661 loss 1.2320399284362793 train acc 0.5723999436302142\n",
            "epoch 5 batch id 2671 loss 1.0447934865951538 train acc 0.5725793242231374\n",
            "epoch 5 batch id 2681 loss 1.094438910484314 train acc 0.572553384930996\n",
            "epoch 5 batch id 2691 loss 1.3418242931365967 train acc 0.5725218320327016\n",
            "epoch 5 batch id 2701 loss 1.1165624856948853 train acc 0.5726235653461681\n",
            "epoch 5 batch id 2711 loss 1.0620286464691162 train acc 0.5726150405754334\n",
            "epoch 5 batch id 2721 loss 1.0173590183258057 train acc 0.5726582598309445\n",
            "epoch 5 batch id 2731 loss 0.9180262684822083 train acc 0.572683998535335\n",
            "epoch 5 batch id 2741 loss 1.42707097530365 train acc 0.5727038489602335\n",
            "epoch 5 batch id 2751 loss 0.9611731767654419 train acc 0.5727689930934206\n",
            "epoch 5 batch id 2761 loss 1.0648934841156006 train acc 0.5727374592538935\n",
            "epoch 5 batch id 2771 loss 1.1141937971115112 train acc 0.5727117917719234\n",
            "epoch 5 batch id 2781 loss 1.2050663232803345 train acc 0.5728436263933837\n",
            "epoch 5 batch id 2791 loss 1.2350809574127197 train acc 0.573036098172698\n",
            "epoch 5 batch id 2801 loss 1.0970433950424194 train acc 0.5731769903605856\n",
            "epoch 5 batch id 2811 loss 1.0645347833633423 train acc 0.5733002045535397\n",
            "epoch 5 batch id 2821 loss 1.0357774496078491 train acc 0.5734114675646934\n",
            "epoch 5 batch id 2831 loss 1.003819465637207 train acc 0.5736323295655246\n",
            "epoch 5 batch id 2841 loss 1.1059212684631348 train acc 0.5737691393875396\n",
            "epoch 5 batch id 2851 loss 1.3154642581939697 train acc 0.5737241318835496\n",
            "epoch 5 batch id 2861 loss 0.9670650959014893 train acc 0.5737340527787487\n",
            "epoch 5 batch id 2871 loss 0.9135189056396484 train acc 0.5737003657262278\n",
            "epoch 5 batch id 2881 loss 1.1399463415145874 train acc 0.5737211471711211\n",
            "epoch 5 batch id 2891 loss 0.9526711702346802 train acc 0.573806641300588\n",
            "epoch 5 batch id 2901 loss 1.0586011409759521 train acc 0.5738269131334023\n",
            "epoch 5 batch id 2911 loss 0.8617620468139648 train acc 0.5739221916867056\n",
            "epoch 5 batch id 2921 loss 0.9187612533569336 train acc 0.5739954210886683\n",
            "epoch 5 batch id 2931 loss 0.9655598402023315 train acc 0.5740468270214943\n",
            "epoch 5 batch id 2941 loss 1.109967827796936 train acc 0.5740128782726963\n",
            "epoch 5 batch id 2951 loss 1.1218345165252686 train acc 0.5740215181294477\n",
            "epoch 5 batch id 2961 loss 0.9788305163383484 train acc 0.5741092536305302\n",
            "epoch 5 batch id 2971 loss 1.00704824924469 train acc 0.5741595843150454\n",
            "epoch 5 batch id 2981 loss 1.024991750717163 train acc 0.5742357849714861\n",
            "epoch 5 batch id 2991 loss 0.9350292086601257 train acc 0.5743114760949515\n",
            "epoch 5 batch id 3001 loss 0.9475777745246887 train acc 0.5744231089636788\n",
            "epoch 5 batch id 3011 loss 1.0910754203796387 train acc 0.5744302142145467\n",
            "epoch 5 batch id 3021 loss 1.226395606994629 train acc 0.5743803790135716\n",
            "epoch 5 batch id 3031 loss 1.0071126222610474 train acc 0.5744030435499835\n",
            "epoch 5 batch id 3041 loss 1.1962867975234985 train acc 0.57436390167708\n",
            "epoch 5 batch id 3051 loss 1.2366797924041748 train acc 0.5743147738446411\n",
            "epoch 5 batch id 3061 loss 0.8957879543304443 train acc 0.5744191032342372\n",
            "epoch 5 batch id 3071 loss 1.2503753900527954 train acc 0.5744718739824162\n",
            "epoch 5 batch id 3081 loss 1.322836995124817 train acc 0.5744076598506979\n",
            "epoch 5 batch id 3091 loss 1.3693848848342896 train acc 0.5743691362018765\n",
            "epoch 5 batch id 3101 loss 1.2239805459976196 train acc 0.5743762092873267\n",
            "epoch 5 batch id 3111 loss 1.167995810508728 train acc 0.5743832369013179\n",
            "epoch 5 batch id 3121 loss 1.0389171838760376 train acc 0.5745454181352131\n",
            "epoch 5 batch id 3131 loss 0.9917817115783691 train acc 0.5746117454487384\n",
            "epoch 5 batch id 3141 loss 1.1035186052322388 train acc 0.5746328796561605\n",
            "epoch 5 batch id 3151 loss 1.094120740890503 train acc 0.5746340447476992\n",
            "epoch 5 batch id 3161 loss 0.8600313067436218 train acc 0.5746203732995887\n",
            "epoch 5 batch id 3171 loss 1.022742509841919 train acc 0.5746807000946074\n",
            "epoch 5 batch id 3181 loss 1.1208584308624268 train acc 0.574701351776171\n",
            "epoch 5 batch id 3191 loss 0.8526942133903503 train acc 0.5748100125352554\n",
            "epoch 5 batch id 3201 loss 1.379999041557312 train acc 0.5748203686348016\n",
            "epoch 5 batch id 3211 loss 1.1284259557724 train acc 0.5748744549984428\n",
            "epoch 5 batch id 3221 loss 1.3480916023254395 train acc 0.5748505898789196\n",
            "epoch 5 batch id 3231 loss 0.9744171500205994 train acc 0.5748703961621789\n",
            "epoch 5 batch id 3241 loss 0.8656023144721985 train acc 0.5748997223079296\n",
            "epoch 5 batch id 3251 loss 0.9184319972991943 train acc 0.5749384804675485\n",
            "epoch 5 batch id 3261 loss 1.0967891216278076 train acc 0.5749961668199939\n",
            "epoch 5 batch id 3271 loss 1.0130535364151 train acc 0.5750152858453073\n",
            "epoch 5 batch id 3281 loss 1.2339147329330444 train acc 0.5750200015239256\n",
            "epoch 5 batch id 3291 loss 1.0748521089553833 train acc 0.5751006532968702\n",
            "epoch 5 batch id 3301 loss 0.8707732558250427 train acc 0.5751855498333838\n",
            "epoch 5 batch id 3311 loss 1.3062920570373535 train acc 0.5752133041377228\n",
            "epoch 5 batch id 3321 loss 1.0196208953857422 train acc 0.5751844323998796\n",
            "epoch 5 batch id 3331 loss 1.2065242528915405 train acc 0.5752589312518763\n",
            "epoch 5 batch id 3341 loss 1.1387968063354492 train acc 0.5751926818317868\n",
            "epoch 5 batch id 3351 loss 0.9579431414604187 train acc 0.5752620486421963\n",
            "epoch 5 batch id 3361 loss 1.0673850774765015 train acc 0.5753914385599525\n",
            "epoch 5 batch id 3371 loss 1.1957255601882935 train acc 0.5754690744586176\n",
            "epoch 5 batch id 3381 loss 0.9385089874267578 train acc 0.5755970866607513\n",
            "epoch 5 batch id 3391 loss 1.0523059368133545 train acc 0.5755999336478915\n",
            "epoch 5 batch id 3401 loss 1.205251932144165 train acc 0.5757176198177006\n",
            "epoch 5 batch id 3411 loss 1.0057693719863892 train acc 0.5757521621225447\n",
            "epoch 5 batch id 3421 loss 1.1191452741622925 train acc 0.5758778500438468\n",
            "epoch 5 batch id 3431 loss 1.036569595336914 train acc 0.5758525211308656\n",
            "epoch 5 batch id 3441 loss 1.0762344598770142 train acc 0.5759136152281313\n",
            "epoch 5 batch id 3451 loss 1.1035405397415161 train acc 0.5759743552593451\n",
            "epoch 5 batch id 3461 loss 1.1127744913101196 train acc 0.5761205215255707\n",
            "epoch 5 batch id 3471 loss 1.0447468757629395 train acc 0.5762838519158744\n",
            "epoch 5 batch id 3481 loss 1.1828742027282715 train acc 0.5763609594943981\n",
            "epoch 5 batch id 3491 loss 0.9793176651000977 train acc 0.5764241979375537\n",
            "epoch 5 batch id 3501 loss 1.2076326608657837 train acc 0.5764067409311625\n",
            "epoch 5 batch id 3511 loss 0.8787526488304138 train acc 0.576518442039305\n",
            "epoch 5 batch id 3521 loss 1.199900507926941 train acc 0.5766428216415791\n",
            "epoch 5 batch id 3531 loss 1.3909473419189453 train acc 0.5766735698102521\n",
            "epoch 5 batch id 3541 loss 1.0158448219299316 train acc 0.5767394450720136\n",
            "epoch 5 batch id 3551 loss 1.1676467657089233 train acc 0.5767785482962545\n",
            "epoch 5 batch id 3561 loss 0.9236277341842651 train acc 0.5769402906486942\n",
            "epoch 5 batch id 3571 loss 1.2756880521774292 train acc 0.5770048655838701\n",
            "epoch 5 batch id 3581 loss 1.0775020122528076 train acc 0.5770385367215861\n",
            "epoch 5 batch id 3591 loss 1.0542174577713013 train acc 0.5770633180172654\n",
            "epoch 5 batch id 3601 loss 1.1237612962722778 train acc 0.5771660649819494\n",
            "epoch 5 batch id 3611 loss 1.1749072074890137 train acc 0.5771990099695375\n",
            "epoch 5 batch id 3621 loss 0.9449377059936523 train acc 0.5771972521402927\n",
            "epoch 5 batch id 3631 loss 0.8887908458709717 train acc 0.5772213233269072\n",
            "epoch 5 batch id 3641 loss 1.1946669816970825 train acc 0.5772967591321065\n",
            "epoch 5 batch id 3651 loss 1.2271875143051147 train acc 0.5773247055601205\n",
            "epoch 5 batch id 3661 loss 1.2909135818481445 train acc 0.577407982791587\n",
            "epoch 5 batch id 3671 loss 1.0387828350067139 train acc 0.5774993189866522\n",
            "epoch 5 batch id 3681 loss 1.199103593826294 train acc 0.577598648465091\n",
            "epoch 5 batch id 3691 loss 1.3778588771820068 train acc 0.5775535085342726\n",
            "epoch 5 batch id 3701 loss 0.9205270409584045 train acc 0.5776648203188327\n",
            "epoch 5 batch id 3711 loss 0.9328002333641052 train acc 0.577775532201563\n",
            "epoch 5 batch id 3721 loss 0.8962512612342834 train acc 0.5779066447191615\n",
            "epoch 5 batch id 3731 loss 1.014852523803711 train acc 0.5779658603591531\n",
            "epoch 5 batch id 3741 loss 1.1802254915237427 train acc 0.5779245188452286\n",
            "epoch 5 batch id 3751 loss 1.3009917736053467 train acc 0.5779542122100774\n",
            "epoch 5 batch id 3761 loss 1.1163995265960693 train acc 0.5780252924754055\n",
            "epoch 5 batch id 3771 loss 0.8121797442436218 train acc 0.5780587045876425\n",
            "epoch 5 batch id 3781 loss 1.0219389200210571 train acc 0.5781291325046284\n",
            "epoch 5 batch id 3791 loss 1.0797935724258423 train acc 0.5781785808493801\n",
            "epoch 5 batch id 3801 loss 0.9706592559814453 train acc 0.5783140949750066\n",
            "epoch 5 batch id 3811 loss 1.0684419870376587 train acc 0.5783217987404881\n",
            "epoch 5 batch id 3821 loss 0.9813470244407654 train acc 0.5784603179795865\n",
            "epoch 5 batch id 3831 loss 1.1993086338043213 train acc 0.5785614069433568\n",
            "epoch 5 batch id 3841 loss 0.9424633383750916 train acc 0.5786416297839104\n",
            "epoch 5 batch id 3851 loss 0.9105023741722107 train acc 0.5787376655414178\n",
            "epoch 5 batch id 3861 loss 1.098733901977539 train acc 0.5787522662522663\n",
            "epoch 5 batch id 3871 loss 1.1099262237548828 train acc 0.57884348359597\n",
            "epoch 5 batch id 3881 loss 1.0479276180267334 train acc 0.5789503349652152\n",
            "epoch 5 batch id 3891 loss 1.0991963148117065 train acc 0.5790124646620406\n",
            "epoch 5 batch id 3901 loss 1.0039550065994263 train acc 0.5790141950781851\n",
            "epoch 5 batch id 3911 loss 0.7994427680969238 train acc 0.5791157951930452\n",
            "epoch 5 batch id 3921 loss 1.0994031429290771 train acc 0.5790415391481765\n",
            "epoch 5 batch id 3931 loss 1.093866229057312 train acc 0.5790471572119054\n",
            "epoch 5 batch id 3941 loss 0.9176595211029053 train acc 0.5791003235219487\n",
            "epoch 5 batch id 3951 loss 0.9867356419563293 train acc 0.5791413566185776\n",
            "epoch 5 batch id 3961 loss 1.2559894323349 train acc 0.5791545695531432\n",
            "epoch 5 batch id 3971 loss 0.9243817329406738 train acc 0.5792464114832536\n",
            "epoch 5 batch id 3981 loss 1.139794945716858 train acc 0.5793220924390856\n",
            "epoch 5 batch id 3991 loss 1.1631927490234375 train acc 0.5793464983713354\n",
            "epoch 5 batch id 4001 loss 1.0398800373077393 train acc 0.5793473506623344\n",
            "epoch 5 batch id 4011 loss 1.1004680395126343 train acc 0.5793988406881077\n",
            "epoch 5 batch id 4021 loss 1.043980360031128 train acc 0.5794306453618503\n",
            "epoch 5 batch id 4031 loss 1.0951623916625977 train acc 0.5794545398164227\n",
            "epoch 5 batch id 4041 loss 1.0715821981430054 train acc 0.5794473830734966\n",
            "epoch 5 batch id 4051 loss 1.0508629083633423 train acc 0.5795482596889657\n",
            "epoch 5 batch id 4061 loss 1.3368290662765503 train acc 0.5795409074119675\n",
            "epoch 5 batch id 4071 loss 1.4168806076049805 train acc 0.5795873249815771\n",
            "epoch 5 batch id 4081 loss 0.8658608198165894 train acc 0.579752205341828\n",
            "epoch 5 batch id 4091 loss 0.9265323281288147 train acc 0.5797405890980201\n",
            "epoch 5 batch id 4101 loss 1.1134319305419922 train acc 0.5797633199219703\n",
            "epoch 5 batch id 4111 loss 1.0682865381240845 train acc 0.5798125456093408\n",
            "epoch 5 batch id 4121 loss 1.0687369108200073 train acc 0.5797705350643048\n",
            "epoch 5 batch id 4131 loss 0.8366299271583557 train acc 0.5797930283224401\n",
            "epoch 5 batch id 4141 loss 1.0946998596191406 train acc 0.5798455988891572\n",
            "epoch 5 batch id 4151 loss 0.9825765490531921 train acc 0.579871567092267\n",
            "epoch 5 batch id 4161 loss 1.2278693914413452 train acc 0.5799049206921413\n",
            "epoch 5 batch id 4171 loss 0.8997644186019897 train acc 0.5799343682570127\n",
            "epoch 5 batch id 4181 loss 0.7725014686584473 train acc 0.5799935721119349\n",
            "epoch 5 batch id 4191 loss 1.2225594520568848 train acc 0.5800524934383202\n",
            "epoch 5 batch id 4201 loss 1.1875450611114502 train acc 0.5801148536062842\n",
            "epoch 5 batch id 4211 loss 1.1781096458435059 train acc 0.5801249703158394\n",
            "epoch 5 batch id 4221 loss 0.8555394411087036 train acc 0.580231284055911\n",
            "epoch 5 batch id 4231 loss 0.9751598834991455 train acc 0.5802521567005436\n",
            "epoch 5 batch id 4241 loss 1.2226054668426514 train acc 0.5803871433624145\n",
            "epoch 5 batch id 4251 loss 1.1451361179351807 train acc 0.5805214949423665\n",
            "epoch 5 batch id 4261 loss 0.9591744542121887 train acc 0.5806368810138465\n",
            "epoch 5 batch id 4271 loss 0.8676302433013916 train acc 0.5807261180051511\n",
            "epoch 5 batch id 4281 loss 1.0207653045654297 train acc 0.5808039885540761\n",
            "epoch 5 batch id 4291 loss 0.8700632452964783 train acc 0.5808887788394314\n",
            "epoch 5 batch id 4301 loss 0.8728894591331482 train acc 0.5809150488258544\n",
            "epoch 5 batch id 4311 loss 0.975727915763855 train acc 0.5809665680816516\n",
            "epoch 5 batch id 4321 loss 1.0595319271087646 train acc 0.5810178488775747\n",
            "epoch 5 batch id 4331 loss 0.8927452564239502 train acc 0.5810652851535442\n",
            "epoch 5 batch id 4341 loss 1.1860544681549072 train acc 0.5810657106657452\n",
            "epoch 5 batch id 4351 loss 1.0316152572631836 train acc 0.5811271834061136\n",
            "epoch 5 batch id 4361 loss 1.2176179885864258 train acc 0.5812062886952534\n",
            "epoch 5 batch id 4371 loss 1.2334208488464355 train acc 0.5812314115762983\n",
            "epoch 5 batch id 4381 loss 0.8600111603736877 train acc 0.5813134843643004\n",
            "epoch 5 batch id 4391 loss 0.8454803228378296 train acc 0.581402300159417\n",
            "epoch 5 batch id 4401 loss 1.01544189453125 train acc 0.5814552090433992\n",
            "epoch 5 batch id 4411 loss 0.9545920491218567 train acc 0.5814795397868964\n",
            "epoch 5 batch id 4421 loss 1.0353118181228638 train acc 0.5815850486315314\n",
            "epoch 5 batch id 4431 loss 1.1836109161376953 train acc 0.5816442394493342\n",
            "epoch 5 batch id 4441 loss 0.9361376762390137 train acc 0.5817066820535916\n",
            "epoch 5 batch id 4451 loss 1.02971613407135 train acc 0.5817442709503482\n",
            "epoch 5 batch id 4461 loss 0.9218569993972778 train acc 0.581802706792199\n",
            "epoch 5 batch id 4471 loss 1.183731198310852 train acc 0.5818014705882353\n",
            "epoch 5 batch id 4481 loss 1.2380101680755615 train acc 0.5818455701852265\n",
            "epoch 5 batch id 4491 loss 1.0283219814300537 train acc 0.5818790358494768\n",
            "epoch 5 batch id 4501 loss 0.868474006652832 train acc 0.5819158242612753\n",
            "epoch 5 batch id 4511 loss 1.1284396648406982 train acc 0.5819316670361339\n",
            "epoch 5 batch id 4521 loss 1.0523501634597778 train acc 0.5819025105065251\n",
            "epoch 5 batch id 4531 loss 0.9973410964012146 train acc 0.5819010704038844\n",
            "epoch 5 batch id 4541 loss 0.8667494654655457 train acc 0.5819718949570579\n",
            "epoch 5 batch id 4551 loss 1.1502830982208252 train acc 0.5820218083937596\n",
            "epoch 5 batch id 4561 loss 1.1394779682159424 train acc 0.5820338193378645\n",
            "epoch 5 batch id 4571 loss 1.2987560033798218 train acc 0.5820970520673813\n",
            "epoch 5 batch id 4581 loss 1.100448489189148 train acc 0.5821156679764243\n",
            "epoch 5 batch id 4591 loss 1.061996579170227 train acc 0.5821614299716837\n",
            "epoch 5 batch id 4601 loss 1.0339713096618652 train acc 0.5822613290589003\n",
            "epoch 5 batch id 4611 loss 0.975385844707489 train acc 0.5823540175666884\n",
            "epoch 5 batch id 4621 loss 1.0414279699325562 train acc 0.5824327797013633\n",
            "epoch 5 batch id 4631 loss 1.1519124507904053 train acc 0.5825618117037357\n",
            "epoch 5 batch id 4641 loss 1.0191339254379272 train acc 0.5826566203404439\n",
            "epoch 5 batch id 4651 loss 1.081154465675354 train acc 0.5827073478821759\n",
            "epoch 5 batch id 4661 loss 1.0868014097213745 train acc 0.5827913806050203\n",
            "epoch 5 batch id 4671 loss 0.977344810962677 train acc 0.5828181866837936\n",
            "epoch 5 batch id 4681 loss 1.2561371326446533 train acc 0.5829149754325998\n",
            "epoch 5 batch id 4691 loss 0.7180362343788147 train acc 0.5830213440630996\n",
            "epoch 5 batch id 4701 loss 1.0266778469085693 train acc 0.5830840512656882\n",
            "epoch 5 batch id 4711 loss 0.7722365856170654 train acc 0.5831730258968372\n",
            "epoch 5 batch id 4721 loss 1.099035382270813 train acc 0.5832781719974581\n",
            "epoch 5 batch id 4731 loss 1.0494921207427979 train acc 0.5833135172268019\n",
            "epoch 5 batch id 4741 loss 1.1143271923065186 train acc 0.5832827989875554\n",
            "epoch 5 batch id 4751 loss 1.1034682989120483 train acc 0.5833475847190065\n",
            "epoch 5 batch id 4761 loss 1.1475640535354614 train acc 0.5833628701953372\n",
            "epoch 5 batch id 4771 loss 1.0387096405029297 train acc 0.5834730664430937\n",
            "epoch 5 batch id 4781 loss 1.1269335746765137 train acc 0.5834782210834554\n",
            "epoch 5 batch id 4791 loss 1.1651349067687988 train acc 0.5835485806720935\n",
            "epoch 5 batch id 4801 loss 1.1916619539260864 train acc 0.5835698292022495\n",
            "epoch 5 batch id 4811 loss 0.9809772968292236 train acc 0.5836234670546664\n",
            "epoch 5 batch id 4821 loss 1.1281014680862427 train acc 0.583647713130056\n",
            "epoch 5 batch id 4831 loss 1.1150022745132446 train acc 0.5837236079486648\n",
            "epoch 5 batch id 4841 loss 1.0723921060562134 train acc 0.5837281811609172\n",
            "epoch 5 batch id 4851 loss 1.1064618825912476 train acc 0.5837327355184498\n",
            "epoch 5 batch id 4861 loss 0.9264829754829407 train acc 0.5837790578070355\n",
            "epoch 5 batch id 4871 loss 1.1178629398345947 train acc 0.5838508519811128\n",
            "epoch 5 batch id 4881 loss 1.0898737907409668 train acc 0.583960766236427\n",
            "epoch 5 batch id 4891 loss 1.1846439838409424 train acc 0.5840031435289307\n",
            "epoch 5 batch id 4901 loss 1.101492166519165 train acc 0.5840740410120384\n",
            "epoch 5 batch id 4911 loss 1.09246826171875 train acc 0.5841064701690083\n",
            "epoch 5 batch id 4921 loss 1.062935709953308 train acc 0.5841038406827881\n",
            "epoch 5 batch id 4931 loss 1.1691670417785645 train acc 0.5840758720340702\n",
            "epoch 5 batch id 4941 loss 1.000061273574829 train acc 0.5841713468933414\n",
            "epoch 5 batch id 4951 loss 1.30967116355896 train acc 0.5842380327206625\n",
            "epoch 5 train acc 0.5843148807013039\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54b998206cfd49048069376240b3b2fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 loss 0.9706096053123474 test acc 0.6016323771994135\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0a4343fe87447ba9ee090fce17ed2e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6 batch id 1 loss 1.113338589668274 train acc 0.609375\n",
            "epoch 6 batch id 11 loss 0.8861385583877563 train acc 0.6235795454545454\n",
            "epoch 6 batch id 21 loss 0.9205523729324341 train acc 0.6287202380952381\n",
            "epoch 6 batch id 31 loss 1.0267246961593628 train acc 0.6129032258064516\n",
            "epoch 6 batch id 41 loss 0.7909446954727173 train acc 0.6177591463414634\n",
            "epoch 6 batch id 51 loss 1.0546001195907593 train acc 0.6127450980392157\n",
            "epoch 6 batch id 61 loss 1.2343820333480835 train acc 0.6091188524590164\n",
            "epoch 6 batch id 71 loss 1.0939468145370483 train acc 0.6115757042253521\n",
            "epoch 6 batch id 81 loss 0.9909100532531738 train acc 0.6134259259259259\n",
            "epoch 6 batch id 91 loss 1.0499361753463745 train acc 0.614010989010989\n",
            "epoch 6 batch id 101 loss 0.8416646718978882 train acc 0.6147896039603961\n",
            "epoch 6 batch id 111 loss 1.3114279508590698 train acc 0.6138795045045045\n",
            "epoch 6 batch id 121 loss 1.2575178146362305 train acc 0.6140237603305785\n",
            "epoch 6 batch id 131 loss 1.0249438285827637 train acc 0.6146230916030534\n",
            "epoch 6 batch id 141 loss 0.8854554891586304 train acc 0.6146941489361702\n",
            "epoch 6 batch id 151 loss 1.0024548768997192 train acc 0.6136175496688742\n",
            "epoch 6 batch id 161 loss 1.1356903314590454 train acc 0.6122864906832298\n",
            "epoch 6 batch id 171 loss 1.1026352643966675 train acc 0.6115679824561403\n",
            "epoch 6 batch id 181 loss 0.9318026304244995 train acc 0.6112741712707183\n",
            "epoch 6 batch id 191 loss 1.208361029624939 train acc 0.6104384816753927\n",
            "epoch 6 batch id 201 loss 0.7514882683753967 train acc 0.611396144278607\n",
            "epoch 6 batch id 211 loss 1.2719990015029907 train acc 0.6097452606635071\n",
            "epoch 6 batch id 221 loss 1.2259089946746826 train acc 0.6098699095022625\n",
            "epoch 6 batch id 231 loss 1.1677122116088867 train acc 0.6092397186147186\n",
            "epoch 6 batch id 241 loss 1.101919174194336 train acc 0.6079486514522822\n",
            "epoch 6 batch id 251 loss 1.1474401950836182 train acc 0.6076319721115537\n",
            "epoch 6 batch id 261 loss 1.012604832649231 train acc 0.6094947318007663\n",
            "epoch 6 batch id 271 loss 0.8629357814788818 train acc 0.6100668819188192\n",
            "epoch 6 batch id 281 loss 0.9393550157546997 train acc 0.6109875444839857\n",
            "epoch 6 batch id 291 loss 1.2429043054580688 train acc 0.6124892611683849\n",
            "epoch 6 batch id 301 loss 1.1494596004486084 train acc 0.6130606312292359\n",
            "epoch 6 batch id 311 loss 1.0354071855545044 train acc 0.6129923633440515\n",
            "epoch 6 batch id 321 loss 1.1658899784088135 train acc 0.6129283489096573\n",
            "epoch 6 batch id 331 loss 1.1525613069534302 train acc 0.6126793806646526\n",
            "epoch 6 batch id 341 loss 1.2753719091415405 train acc 0.6127199413489736\n",
            "epoch 6 batch id 351 loss 1.120136022567749 train acc 0.6132478632478633\n",
            "epoch 6 batch id 361 loss 1.1625815629959106 train acc 0.613183864265928\n",
            "epoch 6 batch id 371 loss 1.2173985242843628 train acc 0.6129127358490566\n",
            "epoch 6 batch id 381 loss 0.9154921770095825 train acc 0.6132299868766404\n",
            "epoch 6 batch id 391 loss 1.308089017868042 train acc 0.6120124680306905\n",
            "epoch 6 batch id 401 loss 0.8474583625793457 train acc 0.6115570448877805\n",
            "epoch 6 batch id 411 loss 0.8197360634803772 train acc 0.6118080900243309\n",
            "epoch 6 batch id 421 loss 1.1374963521957397 train acc 0.6121956650831354\n",
            "epoch 6 batch id 431 loss 1.1325063705444336 train acc 0.611441415313225\n",
            "epoch 6 batch id 441 loss 1.192405104637146 train acc 0.6113236961451247\n",
            "epoch 6 batch id 451 loss 0.9495981335639954 train acc 0.6116269401330376\n",
            "epoch 6 batch id 461 loss 0.9774050712585449 train acc 0.6111713665943601\n",
            "epoch 6 batch id 471 loss 0.8681564927101135 train acc 0.6121616242038217\n",
            "epoch 6 batch id 481 loss 1.1503896713256836 train acc 0.6126234407484408\n",
            "epoch 6 batch id 491 loss 0.989133894443512 train acc 0.6121117617107943\n",
            "epoch 6 batch id 501 loss 1.411099910736084 train acc 0.6115893213572854\n",
            "epoch 6 batch id 511 loss 0.9837639927864075 train acc 0.6116071428571429\n",
            "epoch 6 batch id 521 loss 1.138830542564392 train acc 0.6116842610364683\n",
            "epoch 6 batch id 531 loss 1.1085054874420166 train acc 0.611787900188324\n",
            "epoch 6 batch id 541 loss 0.9966188669204712 train acc 0.6120609981515711\n",
            "epoch 6 batch id 551 loss 0.959163248538971 train acc 0.6119555353901996\n",
            "epoch 6 batch id 561 loss 0.8350963592529297 train acc 0.6118538324420677\n",
            "epoch 6 batch id 571 loss 1.1403095722198486 train acc 0.6118925131348512\n",
            "epoch 6 batch id 581 loss 1.0987350940704346 train acc 0.6116071428571429\n",
            "epoch 6 batch id 591 loss 0.821904718875885 train acc 0.6122303299492385\n",
            "epoch 6 batch id 601 loss 0.9192888140678406 train acc 0.6121568219633944\n",
            "epoch 6 batch id 611 loss 1.0297108888626099 train acc 0.6123414484451718\n",
            "epoch 6 batch id 621 loss 1.0343632698059082 train acc 0.6130736714975845\n",
            "epoch 6 batch id 631 loss 1.1438748836517334 train acc 0.6131141045958796\n",
            "epoch 6 batch id 641 loss 1.1025078296661377 train acc 0.6131776521060842\n",
            "epoch 6 batch id 651 loss 0.8650310039520264 train acc 0.6133832565284179\n",
            "epoch 6 batch id 661 loss 1.1656608581542969 train acc 0.6133698940998488\n",
            "epoch 6 batch id 671 loss 1.118693232536316 train acc 0.613263785394933\n",
            "epoch 6 batch id 681 loss 0.9253349304199219 train acc 0.613459067547724\n",
            "epoch 6 batch id 691 loss 0.8771912455558777 train acc 0.6139426555716353\n",
            "epoch 6 batch id 701 loss 0.9527152180671692 train acc 0.614100392296719\n",
            "epoch 6 batch id 711 loss 1.113567590713501 train acc 0.6139899789029536\n",
            "epoch 6 batch id 721 loss 0.9880061745643616 train acc 0.613990984743412\n",
            "epoch 6 batch id 731 loss 0.8549451231956482 train acc 0.6144408344733242\n",
            "epoch 6 batch id 741 loss 1.1071326732635498 train acc 0.6147309379217274\n",
            "epoch 6 batch id 751 loss 1.244202733039856 train acc 0.6144515645805593\n",
            "epoch 6 batch id 761 loss 0.7360767126083374 train acc 0.6149597568988173\n",
            "epoch 6 batch id 771 loss 1.2733752727508545 train acc 0.6149278534370947\n",
            "epoch 6 batch id 781 loss 0.9407408237457275 train acc 0.6149567861715749\n",
            "epoch 6 batch id 791 loss 0.8658831715583801 train acc 0.6152615360303414\n",
            "epoch 6 batch id 801 loss 1.0206290483474731 train acc 0.6156562109862672\n",
            "epoch 6 batch id 811 loss 1.1099767684936523 train acc 0.6160796855733662\n",
            "epoch 6 batch id 821 loss 0.9018675088882446 train acc 0.615902862362972\n",
            "epoch 6 batch id 831 loss 1.140769362449646 train acc 0.6158055054151624\n",
            "epoch 6 batch id 841 loss 0.9419224262237549 train acc 0.6164536266349584\n",
            "epoch 6 batch id 851 loss 1.0421680212020874 train acc 0.6163520857814336\n",
            "epoch 6 batch id 861 loss 1.1767972707748413 train acc 0.6160714285714286\n",
            "epoch 6 batch id 871 loss 0.8172897696495056 train acc 0.6160304247990815\n",
            "epoch 6 batch id 881 loss 0.9827474355697632 train acc 0.6157065834279228\n",
            "epoch 6 batch id 891 loss 0.8505563139915466 train acc 0.6159862514029181\n",
            "epoch 6 batch id 901 loss 1.1347849369049072 train acc 0.6160862930077692\n",
            "epoch 6 batch id 911 loss 1.218194842338562 train acc 0.6160640779363337\n",
            "epoch 6 batch id 921 loss 1.2578223943710327 train acc 0.6162119978284474\n",
            "epoch 6 batch id 931 loss 0.8252995610237122 train acc 0.6160882116004296\n",
            "epoch 6 batch id 941 loss 0.8828567266464233 train acc 0.6164485919234857\n",
            "epoch 6 batch id 951 loss 1.2439167499542236 train acc 0.6162756309148265\n",
            "epoch 6 batch id 961 loss 0.9667556285858154 train acc 0.6159436784599376\n",
            "epoch 6 batch id 971 loss 0.9889485239982605 train acc 0.6165840628218332\n",
            "epoch 6 batch id 981 loss 1.2639471292495728 train acc 0.6169246941896025\n",
            "epoch 6 batch id 991 loss 0.9003396034240723 train acc 0.6170534813319879\n",
            "epoch 6 batch id 1001 loss 1.0337400436401367 train acc 0.617086038961039\n",
            "epoch 6 batch id 1011 loss 1.0050116777420044 train acc 0.6172725024727992\n",
            "epoch 6 batch id 1021 loss 0.7698312401771545 train acc 0.6176389569049952\n",
            "epoch 6 batch id 1031 loss 1.1025582551956177 train acc 0.6176345780795345\n",
            "epoch 6 batch id 1041 loss 1.0699800252914429 train acc 0.6178404178674352\n",
            "epoch 6 batch id 1051 loss 0.8436492681503296 train acc 0.6177301379638439\n",
            "epoch 6 batch id 1061 loss 1.0008635520935059 train acc 0.6177250235626767\n",
            "epoch 6 batch id 1071 loss 0.7878468036651611 train acc 0.6179971988795518\n",
            "epoch 6 batch id 1081 loss 0.9967549443244934 train acc 0.6178596207215541\n",
            "epoch 6 batch id 1091 loss 1.2989072799682617 train acc 0.6178677818515124\n",
            "epoch 6 batch id 1101 loss 1.015640139579773 train acc 0.6180177111716622\n",
            "epoch 6 batch id 1111 loss 0.9420613050460815 train acc 0.618024302430243\n",
            "epoch 6 batch id 1121 loss 1.066652774810791 train acc 0.6180586529884032\n",
            "epoch 6 batch id 1131 loss 1.2088732719421387 train acc 0.6178022767462422\n",
            "epoch 6 batch id 1141 loss 0.9575155973434448 train acc 0.6178927475898335\n",
            "epoch 6 batch id 1151 loss 1.2723650932312012 train acc 0.6179816463944396\n",
            "epoch 6 batch id 1161 loss 1.057960867881775 train acc 0.6179748062015504\n",
            "epoch 6 batch id 1171 loss 0.8588336110115051 train acc 0.6181282023911187\n",
            "epoch 6 batch id 1181 loss 0.9621237516403198 train acc 0.6180011642675699\n",
            "epoch 6 batch id 1191 loss 1.0331133604049683 train acc 0.6181517632241813\n",
            "epoch 6 batch id 1201 loss 1.2504314184188843 train acc 0.6182608243130724\n",
            "epoch 6 batch id 1211 loss 0.7824707627296448 train acc 0.6185745251857968\n",
            "epoch 6 batch id 1221 loss 1.0099456310272217 train acc 0.6186527436527437\n",
            "epoch 6 batch id 1231 loss 1.1091978549957275 train acc 0.618577376116978\n",
            "epoch 6 batch id 1241 loss 1.1093863248825073 train acc 0.6184024979854955\n",
            "epoch 6 batch id 1251 loss 0.8779625296592712 train acc 0.6184802158273381\n",
            "epoch 6 batch id 1261 loss 1.2274750471115112 train acc 0.6184947462331483\n",
            "epoch 6 batch id 1271 loss 1.0431932210922241 train acc 0.6186688630999213\n",
            "epoch 6 batch id 1281 loss 1.1133918762207031 train acc 0.6188280640124902\n",
            "epoch 6 batch id 1291 loss 1.0252013206481934 train acc 0.61894848954299\n",
            "epoch 6 batch id 1301 loss 1.1836848258972168 train acc 0.6188749039200615\n",
            "epoch 6 batch id 1311 loss 1.138817310333252 train acc 0.618909706331045\n",
            "epoch 6 batch id 1321 loss 0.8785040378570557 train acc 0.6188966691900075\n",
            "epoch 6 batch id 1331 loss 0.9601898789405823 train acc 0.6190246994740797\n",
            "epoch 6 batch id 1341 loss 1.0043566226959229 train acc 0.6190226510067114\n",
            "epoch 6 batch id 1351 loss 0.9033129215240479 train acc 0.6189628053293856\n",
            "epoch 6 batch id 1361 loss 1.1097720861434937 train acc 0.6190071638501102\n",
            "epoch 6 batch id 1371 loss 0.9437429904937744 train acc 0.6191192560175055\n",
            "epoch 6 batch id 1381 loss 1.1297842264175415 train acc 0.6191505249818972\n",
            "epoch 6 batch id 1391 loss 0.5366331934928894 train acc 0.6196418943206327\n",
            "epoch 6 batch id 1401 loss 1.1135444641113281 train acc 0.6195016952177016\n",
            "epoch 6 batch id 1411 loss 0.8485397100448608 train acc 0.6195406626506024\n",
            "epoch 6 batch id 1421 loss 0.9522293210029602 train acc 0.6196780436312456\n",
            "epoch 6 batch id 1431 loss 0.8192175626754761 train acc 0.6197916666666666\n",
            "epoch 6 batch id 1441 loss 0.9287615418434143 train acc 0.6197735947258848\n",
            "epoch 6 batch id 1451 loss 0.7946494817733765 train acc 0.619992677463818\n",
            "epoch 6 batch id 1461 loss 0.9469215273857117 train acc 0.6198879192334018\n",
            "epoch 6 batch id 1471 loss 1.205262541770935 train acc 0.6199226716519375\n",
            "epoch 6 batch id 1481 loss 0.946549117565155 train acc 0.6200519074949359\n",
            "epoch 6 batch id 1491 loss 0.8886982202529907 train acc 0.6202737256874581\n",
            "epoch 6 batch id 1501 loss 0.9929121732711792 train acc 0.6203260326449034\n",
            "epoch 6 batch id 1511 loss 0.8586351275444031 train acc 0.6204603739245532\n",
            "epoch 6 batch id 1521 loss 0.8105403780937195 train acc 0.6206956771860618\n",
            "epoch 6 batch id 1531 loss 0.963787317276001 train acc 0.6208666721097322\n",
            "epoch 6 batch id 1541 loss 1.0527715682983398 train acc 0.6209543316028553\n",
            "epoch 6 batch id 1551 loss 0.9297832250595093 train acc 0.6209703417150225\n",
            "epoch 6 batch id 1561 loss 1.0561070442199707 train acc 0.6210161755285074\n",
            "epoch 6 batch id 1571 loss 0.8993077874183655 train acc 0.6212305060471037\n",
            "epoch 6 batch id 1581 loss 1.0252054929733276 train acc 0.6213927103099304\n",
            "epoch 6 batch id 1591 loss 0.9220065474510193 train acc 0.6214350251414205\n",
            "epoch 6 batch id 1601 loss 1.1506414413452148 train acc 0.6215158494690818\n",
            "epoch 6 batch id 1611 loss 0.8657116293907166 train acc 0.6215277777777778\n",
            "epoch 6 batch id 1621 loss 1.0852934122085571 train acc 0.621529919802591\n",
            "epoch 6 batch id 1631 loss 0.8871088027954102 train acc 0.6214841354996934\n",
            "epoch 6 batch id 1641 loss 0.9287135004997253 train acc 0.6216102985984155\n",
            "epoch 6 batch id 1651 loss 0.9847866296768188 train acc 0.6215740460327075\n",
            "epoch 6 batch id 1661 loss 1.127860188484192 train acc 0.6216699277543648\n",
            "epoch 6 batch id 1671 loss 0.9543840289115906 train acc 0.6217646618791143\n",
            "epoch 6 batch id 1681 loss 1.2351831197738647 train acc 0.6216073022010707\n",
            "epoch 6 batch id 1691 loss 0.9003223776817322 train acc 0.6217659668835009\n",
            "epoch 6 batch id 1701 loss 0.9411246180534363 train acc 0.6218400940623163\n",
            "epoch 6 batch id 1711 loss 1.0983811616897583 train acc 0.6219681472822911\n",
            "epoch 6 batch id 1721 loss 1.0568169355392456 train acc 0.6220765543288785\n",
            "epoch 6 batch id 1731 loss 0.8480029106140137 train acc 0.6221927354130561\n",
            "epoch 6 batch id 1741 loss 0.9976202249526978 train acc 0.6223434807581849\n",
            "epoch 6 batch id 1751 loss 0.7962000966072083 train acc 0.6225371216447744\n",
            "epoch 6 batch id 1761 loss 0.8495929837226868 train acc 0.6227019449176604\n",
            "epoch 6 batch id 1771 loss 0.823683500289917 train acc 0.622847261434218\n",
            "epoch 6 batch id 1781 loss 0.9262087345123291 train acc 0.6230699045480067\n",
            "epoch 6 batch id 1791 loss 1.4983911514282227 train acc 0.6231853713009492\n",
            "epoch 6 batch id 1801 loss 0.986016035079956 train acc 0.6233949888950583\n",
            "epoch 6 batch id 1811 loss 0.8842710256576538 train acc 0.6235160132523467\n",
            "epoch 6 batch id 1821 loss 0.7757660150527954 train acc 0.6236099670510709\n",
            "epoch 6 batch id 1831 loss 0.8387605547904968 train acc 0.6237626297105406\n",
            "epoch 6 batch id 1841 loss 0.8102737069129944 train acc 0.6238372487778381\n",
            "epoch 6 batch id 1851 loss 1.014573574066162 train acc 0.6239617098865479\n",
            "epoch 6 batch id 1861 loss 1.0616364479064941 train acc 0.6240680413756046\n",
            "epoch 6 batch id 1871 loss 1.0396467447280884 train acc 0.6241982896846606\n",
            "epoch 6 batch id 1881 loss 0.9713640213012695 train acc 0.6243271531100478\n",
            "epoch 6 batch id 1891 loss 0.9865477681159973 train acc 0.6245455446853516\n",
            "epoch 6 batch id 1901 loss 0.8795189261436462 train acc 0.6245725933719095\n",
            "epoch 6 batch id 1911 loss 0.83523029088974 train acc 0.6245748299319728\n",
            "epoch 6 batch id 1921 loss 1.0158637762069702 train acc 0.6245201067152525\n",
            "epoch 6 batch id 1931 loss 1.0692001581192017 train acc 0.6247491584671155\n",
            "epoch 6 batch id 1941 loss 0.9848717451095581 train acc 0.6247263008758372\n",
            "epoch 6 batch id 1951 loss 1.0742592811584473 train acc 0.624639607893388\n",
            "epoch 6 batch id 1961 loss 0.8657020330429077 train acc 0.6246573814380418\n",
            "epoch 6 batch id 1971 loss 0.9652493596076965 train acc 0.624730466768138\n",
            "epoch 6 batch id 1981 loss 1.0058684349060059 train acc 0.6247633770822817\n",
            "epoch 6 batch id 1991 loss 1.2116303443908691 train acc 0.624788108990457\n",
            "epoch 6 batch id 2001 loss 1.1605478525161743 train acc 0.6250078085957022\n",
            "epoch 6 batch id 2011 loss 1.1687922477722168 train acc 0.6249844604674292\n",
            "epoch 6 batch id 2021 loss 1.116598129272461 train acc 0.6250154626422563\n",
            "epoch 6 batch id 2031 loss 0.9089599251747131 train acc 0.6251077055637617\n",
            "epoch 6 batch id 2041 loss 0.8106029033660889 train acc 0.6252832557569818\n",
            "epoch 6 batch id 2051 loss 1.1698999404907227 train acc 0.6252742564602632\n",
            "epoch 6 batch id 2061 loss 1.0168311595916748 train acc 0.6252729257641921\n",
            "epoch 6 batch id 2071 loss 0.9828472137451172 train acc 0.6253093312409465\n",
            "epoch 6 batch id 2081 loss 1.2346770763397217 train acc 0.6252928279673234\n",
            "epoch 6 batch id 2091 loss 1.1805089712142944 train acc 0.6253661525585844\n",
            "epoch 6 batch id 2101 loss 0.9116178750991821 train acc 0.6254610899571632\n",
            "epoch 6 batch id 2111 loss 0.911590576171875 train acc 0.6254885125532923\n",
            "epoch 6 batch id 2121 loss 0.6857683658599854 train acc 0.6255451438000943\n",
            "epoch 6 batch id 2131 loss 1.1294066905975342 train acc 0.6256379047395589\n",
            "epoch 6 batch id 2141 loss 1.1249275207519531 train acc 0.6255692433442317\n",
            "epoch 6 batch id 2151 loss 0.9531663060188293 train acc 0.6257046141329614\n",
            "epoch 6 batch id 2161 loss 1.1116968393325806 train acc 0.6255856663581675\n",
            "epoch 6 batch id 2171 loss 1.0296787023544312 train acc 0.6256117572547213\n",
            "epoch 6 batch id 2181 loss 0.9092448353767395 train acc 0.6255731315910132\n",
            "epoch 6 batch id 2191 loss 0.8870934247970581 train acc 0.625741670470105\n",
            "epoch 6 batch id 2201 loss 1.1988238096237183 train acc 0.625731201726488\n",
            "epoch 6 batch id 2211 loss 1.07170832157135 train acc 0.6255865558570782\n",
            "epoch 6 batch id 2221 loss 1.2905176877975464 train acc 0.6256331607384061\n",
            "epoch 6 batch id 2231 loss 0.9964253902435303 train acc 0.6257703944419543\n",
            "epoch 6 batch id 2241 loss 0.7873308658599854 train acc 0.6258575970548862\n",
            "epoch 6 batch id 2251 loss 0.9774916172027588 train acc 0.6258746112838738\n",
            "epoch 6 batch id 2261 loss 0.7797040939331055 train acc 0.6259329389650597\n",
            "epoch 6 batch id 2271 loss 1.1953705549240112 train acc 0.6260389145750771\n",
            "epoch 6 batch id 2281 loss 1.3552626371383667 train acc 0.6261576611135466\n",
            "epoch 6 batch id 2291 loss 0.9825652241706848 train acc 0.6263572130074203\n",
            "epoch 6 batch id 2301 loss 1.0152360200881958 train acc 0.6264667535853976\n",
            "epoch 6 batch id 2311 loss 1.0211564302444458 train acc 0.6265618238857638\n",
            "epoch 6 batch id 2321 loss 0.6479166150093079 train acc 0.6265618267987936\n",
            "epoch 6 batch id 2331 loss 0.9836991429328918 train acc 0.6264880952380952\n",
            "epoch 6 batch id 2341 loss 1.3042548894882202 train acc 0.6265551580521145\n",
            "epoch 6 batch id 2351 loss 0.8600711822509766 train acc 0.6267213419821352\n",
            "epoch 6 batch id 2361 loss 1.147556185722351 train acc 0.6268199385853452\n",
            "epoch 6 batch id 2371 loss 0.8715527057647705 train acc 0.626970423871784\n",
            "epoch 6 batch id 2381 loss 1.0912997722625732 train acc 0.6270343343133137\n",
            "epoch 6 batch id 2391 loss 0.9863547086715698 train acc 0.6271303847762443\n",
            "epoch 6 batch id 2401 loss 0.9433241486549377 train acc 0.6271475426905456\n",
            "epoch 6 batch id 2411 loss 0.9709677696228027 train acc 0.6271840004147656\n",
            "epoch 6 batch id 2421 loss 1.0118361711502075 train acc 0.6272846964064436\n",
            "epoch 6 batch id 2431 loss 1.1346544027328491 train acc 0.6272752982311806\n",
            "epoch 6 batch id 2441 loss 0.8833354711532593 train acc 0.6274196026218762\n",
            "epoch 6 batch id 2451 loss 0.865407407283783 train acc 0.627422480620155\n",
            "epoch 6 batch id 2461 loss 1.2209957838058472 train acc 0.627482476635514\n",
            "epoch 6 batch id 2471 loss 1.0305882692337036 train acc 0.6274850768919465\n",
            "epoch 6 batch id 2481 loss 1.1341687440872192 train acc 0.6274876561870214\n",
            "epoch 6 batch id 2491 loss 1.1212443113327026 train acc 0.6274274889602569\n",
            "epoch 6 batch id 2501 loss 0.8194077610969543 train acc 0.6274740103958416\n",
            "epoch 6 batch id 2511 loss 1.1644271612167358 train acc 0.6277068399044206\n",
            "epoch 6 batch id 2521 loss 1.0008699893951416 train acc 0.6276093316144388\n",
            "epoch 6 batch id 2531 loss 0.8938366174697876 train acc 0.6277039707625445\n",
            "epoch 6 batch id 2541 loss 0.9490024447441101 train acc 0.6279085497835498\n",
            "epoch 6 batch id 2551 loss 1.1296451091766357 train acc 0.6277562720501764\n",
            "epoch 6 batch id 2561 loss 1.053252935409546 train acc 0.6277760152284264\n",
            "epoch 6 batch id 2571 loss 0.8473967909812927 train acc 0.6279110754570206\n",
            "epoch 6 batch id 2581 loss 0.866069495677948 train acc 0.6280087659821775\n",
            "epoch 6 batch id 2591 loss 0.9652355313301086 train acc 0.6280755499807025\n",
            "epoch 6 batch id 2601 loss 0.9524806141853333 train acc 0.6281237985390234\n",
            "epoch 6 batch id 2611 loss 0.9257234334945679 train acc 0.6280519915741095\n",
            "epoch 6 batch id 2621 loss 0.8829763531684875 train acc 0.6280284242655475\n",
            "epoch 6 batch id 2631 loss 1.0595035552978516 train acc 0.6279634644621817\n",
            "epoch 6 batch id 2641 loss 0.8843125104904175 train acc 0.6280232393032942\n",
            "epoch 6 batch id 2651 loss 0.9189804792404175 train acc 0.6279470011316485\n",
            "epoch 6 batch id 2661 loss 1.0221219062805176 train acc 0.627918310785419\n",
            "epoch 6 batch id 2671 loss 0.9713448882102966 train acc 0.6280419318607263\n",
            "epoch 6 batch id 2681 loss 0.9731363654136658 train acc 0.6279373368146214\n",
            "epoch 6 batch id 2691 loss 1.2393289804458618 train acc 0.627955453363062\n",
            "epoch 6 batch id 2701 loss 1.0228856801986694 train acc 0.6279329415031469\n",
            "epoch 6 batch id 2711 loss 0.8812086582183838 train acc 0.627962467724087\n",
            "epoch 6 batch id 2721 loss 0.679427981376648 train acc 0.6280492006615215\n",
            "epoch 6 batch id 2731 loss 0.7458642721176147 train acc 0.6281066916880264\n",
            "epoch 6 batch id 2741 loss 1.101851224899292 train acc 0.628146661802262\n",
            "epoch 6 batch id 2751 loss 0.8260024189949036 train acc 0.6282374591057798\n",
            "epoch 6 batch id 2761 loss 0.8318493366241455 train acc 0.6283049619703006\n",
            "epoch 6 batch id 2771 loss 1.1009726524353027 train acc 0.6282648412125587\n",
            "epoch 6 batch id 2781 loss 1.0878108739852905 train acc 0.6283654710535779\n",
            "epoch 6 batch id 2791 loss 1.0009287595748901 train acc 0.6284149946255823\n",
            "epoch 6 batch id 2801 loss 0.9475314021110535 train acc 0.6285143698679043\n",
            "epoch 6 batch id 2811 loss 0.8668493032455444 train acc 0.6287130914265386\n",
            "epoch 6 batch id 2821 loss 1.0643268823623657 train acc 0.6287165455512229\n",
            "epoch 6 batch id 2831 loss 0.7712373733520508 train acc 0.628874514305899\n",
            "epoch 6 batch id 2841 loss 0.8624278903007507 train acc 0.6289873724040831\n",
            "epoch 6 batch id 2851 loss 1.180794596672058 train acc 0.6288911785338478\n",
            "epoch 6 batch id 2861 loss 0.700917661190033 train acc 0.628975882558546\n",
            "epoch 6 batch id 2871 loss 0.7079492807388306 train acc 0.6290273423894114\n",
            "epoch 6 batch id 2881 loss 0.9527361392974854 train acc 0.6291164092329052\n",
            "epoch 6 batch id 2891 loss 0.7652742862701416 train acc 0.6291562175717744\n",
            "epoch 6 batch id 2901 loss 0.9449029564857483 train acc 0.629211909686315\n",
            "epoch 6 batch id 2911 loss 0.8453957438468933 train acc 0.6293584678804535\n",
            "epoch 6 batch id 2921 loss 0.8431233167648315 train acc 0.6293274991441288\n",
            "epoch 6 batch id 2931 loss 0.7846904397010803 train acc 0.6293553821221426\n",
            "epoch 6 batch id 2941 loss 1.047454595565796 train acc 0.6293246344780686\n",
            "epoch 6 batch id 2951 loss 0.938467800617218 train acc 0.6294105811589292\n",
            "epoch 6 batch id 2961 loss 0.7921592593193054 train acc 0.629580378250591\n",
            "epoch 6 batch id 2971 loss 0.9201351404190063 train acc 0.6296122938404578\n",
            "epoch 6 batch id 2981 loss 0.8369050621986389 train acc 0.6296702029520295\n",
            "epoch 6 batch id 2991 loss 0.8898931741714478 train acc 0.6297016048144434\n",
            "epoch 6 batch id 3001 loss 0.8701667785644531 train acc 0.629857755748084\n",
            "epoch 6 batch id 3011 loss 1.040787935256958 train acc 0.6298156758551976\n",
            "epoch 6 batch id 3021 loss 1.0011321306228638 train acc 0.6297945630585898\n",
            "epoch 6 batch id 3031 loss 0.8620454668998718 train acc 0.6298148300890795\n",
            "epoch 6 batch id 3041 loss 1.0503153800964355 train acc 0.6297116491285761\n",
            "epoch 6 batch id 3051 loss 1.0534801483154297 train acc 0.629665478531629\n",
            "epoch 6 batch id 3061 loss 0.8106147050857544 train acc 0.6297114913426984\n",
            "epoch 6 batch id 3071 loss 1.0524897575378418 train acc 0.6297165011396939\n",
            "epoch 6 batch id 3081 loss 1.1561206579208374 train acc 0.6297366926322623\n",
            "epoch 6 batch id 3091 loss 0.9842521548271179 train acc 0.6297415884826917\n",
            "epoch 6 batch id 3101 loss 1.1478016376495361 train acc 0.6297716462431474\n",
            "epoch 6 batch id 3111 loss 1.13106369972229 train acc 0.6297713757634201\n",
            "epoch 6 batch id 3121 loss 1.0125423669815063 train acc 0.6298812479974367\n",
            "epoch 6 batch id 3131 loss 0.8346790671348572 train acc 0.6299604758862983\n",
            "epoch 6 batch id 3141 loss 1.0815422534942627 train acc 0.6299198105698822\n",
            "epoch 6 batch id 3151 loss 0.9816002249717712 train acc 0.6299438670263409\n",
            "epoch 6 batch id 3161 loss 0.7769585847854614 train acc 0.6299381129389434\n",
            "epoch 6 batch id 3171 loss 0.9154567718505859 train acc 0.6299422500788395\n",
            "epoch 6 batch id 3181 loss 0.9635859727859497 train acc 0.6299807450487268\n",
            "epoch 6 batch id 3191 loss 0.7666435837745667 train acc 0.6300826543403322\n",
            "epoch 6 batch id 3201 loss 1.132393717765808 train acc 0.6301204701655733\n",
            "epoch 6 batch id 3211 loss 0.9240319132804871 train acc 0.6301921130488944\n",
            "epoch 6 batch id 3221 loss 1.1040273904800415 train acc 0.6301905464141571\n",
            "epoch 6 batch id 3231 loss 0.8829838633537292 train acc 0.6302034973692355\n",
            "epoch 6 batch id 3241 loss 0.8483182191848755 train acc 0.6302163684048133\n",
            "epoch 6 batch id 3251 loss 0.7814620733261108 train acc 0.6303060596739465\n",
            "epoch 6 batch id 3261 loss 0.9165893793106079 train acc 0.630356869058571\n",
            "epoch 6 batch id 3271 loss 0.9329516291618347 train acc 0.6303452690308774\n",
            "epoch 6 batch id 3281 loss 1.1914986371994019 train acc 0.6303623133191101\n",
            "epoch 6 batch id 3291 loss 0.9343274831771851 train acc 0.6304409753874203\n",
            "epoch 6 batch id 3301 loss 0.7234306335449219 train acc 0.630547561345047\n",
            "epoch 6 batch id 3311 loss 1.2047452926635742 train acc 0.6305827167019028\n",
            "epoch 6 batch id 3321 loss 0.9677317142486572 train acc 0.6305612014453478\n",
            "epoch 6 batch id 3331 loss 1.1019301414489746 train acc 0.6306758480936656\n",
            "epoch 6 batch id 3341 loss 1.146918773651123 train acc 0.6306822433403173\n",
            "epoch 6 batch id 3351 loss 0.8757169246673584 train acc 0.6308331468218442\n",
            "epoch 6 batch id 3361 loss 0.8918216228485107 train acc 0.6309180675394228\n",
            "epoch 6 batch id 3371 loss 0.9567632079124451 train acc 0.6310349302877485\n",
            "epoch 6 batch id 3381 loss 0.8014518618583679 train acc 0.6311326160899142\n",
            "epoch 6 batch id 3391 loss 0.9642736911773682 train acc 0.6310638454733117\n",
            "epoch 6 batch id 3401 loss 1.1286333799362183 train acc 0.6311608718024111\n",
            "epoch 6 batch id 3411 loss 0.9198940992355347 train acc 0.6312619099970683\n",
            "epoch 6 batch id 3421 loss 0.9499873518943787 train acc 0.631344087985969\n",
            "epoch 6 batch id 3431 loss 1.0129302740097046 train acc 0.6313164893617021\n",
            "epoch 6 batch id 3441 loss 0.9586122632026672 train acc 0.6314161944202267\n",
            "epoch 6 batch id 3451 loss 0.8471419811248779 train acc 0.6315153216458997\n",
            "epoch 6 batch id 3461 loss 0.8321910500526428 train acc 0.63166353655013\n",
            "epoch 6 batch id 3471 loss 0.9167316555976868 train acc 0.6318334053586863\n",
            "epoch 6 batch id 3481 loss 1.0797674655914307 train acc 0.6319619003160012\n",
            "epoch 6 batch id 3491 loss 0.8934518694877625 train acc 0.6320404253795474\n",
            "epoch 6 batch id 3501 loss 1.1143659353256226 train acc 0.6321318908883177\n",
            "epoch 6 batch id 3511 loss 0.7349347472190857 train acc 0.6322050341782968\n",
            "epoch 6 batch id 3521 loss 1.1474446058273315 train acc 0.6322733243396762\n",
            "epoch 6 batch id 3531 loss 1.4210768938064575 train acc 0.6322261753044464\n",
            "epoch 6 batch id 3541 loss 0.8378658890724182 train acc 0.6323513837898899\n",
            "epoch 6 batch id 3551 loss 1.0107073783874512 train acc 0.6324362855533653\n",
            "epoch 6 batch id 3561 loss 0.791752278804779 train acc 0.6325777520359449\n",
            "epoch 6 batch id 3571 loss 1.1848275661468506 train acc 0.6326265401848222\n",
            "epoch 6 batch id 3581 loss 1.075750708580017 train acc 0.632653239318626\n",
            "epoch 6 batch id 3591 loss 1.0022530555725098 train acc 0.6327058966861598\n",
            "epoch 6 batch id 3601 loss 0.9910799860954285 train acc 0.6327973132463205\n",
            "epoch 6 batch id 3611 loss 0.9006902575492859 train acc 0.6328579340902797\n",
            "epoch 6 batch id 3621 loss 0.7909052968025208 train acc 0.6329570560618614\n",
            "epoch 6 batch id 3631 loss 0.8142581582069397 train acc 0.6330125998347562\n",
            "epoch 6 batch id 3641 loss 1.1563878059387207 train acc 0.6330549642955232\n",
            "epoch 6 batch id 3651 loss 0.972348690032959 train acc 0.6331099356340728\n",
            "epoch 6 batch id 3661 loss 1.229512095451355 train acc 0.6331304629882546\n",
            "epoch 6 batch id 3671 loss 0.9404609799385071 train acc 0.6331849291746118\n",
            "epoch 6 batch id 3681 loss 1.147562026977539 train acc 0.6332263651181744\n",
            "epoch 6 batch id 3691 loss 1.1933976411819458 train acc 0.6332887428881062\n",
            "epoch 6 batch id 3701 loss 0.8521462678909302 train acc 0.6333718927316941\n",
            "epoch 6 batch id 3711 loss 0.9472673535346985 train acc 0.633500909458367\n",
            "epoch 6 batch id 3721 loss 0.7770391702651978 train acc 0.6336460292932008\n",
            "epoch 6 batch id 3731 loss 0.847625732421875 train acc 0.6336731104261593\n",
            "epoch 6 batch id 3741 loss 1.0554755926132202 train acc 0.6337125768511094\n",
            "epoch 6 batch id 3751 loss 1.04483163356781 train acc 0.6337601639562783\n",
            "epoch 6 batch id 3761 loss 0.7396885752677917 train acc 0.6338781241691039\n",
            "epoch 6 batch id 3771 loss 0.8304851651191711 train acc 0.633891872182445\n",
            "epoch 6 batch id 3781 loss 0.8933535814285278 train acc 0.6339262099973552\n",
            "epoch 6 batch id 3791 loss 0.7308502793312073 train acc 0.6339603666578739\n",
            "epoch 6 batch id 3801 loss 0.935192883014679 train acc 0.6341217771639043\n",
            "epoch 6 batch id 3811 loss 0.9520919919013977 train acc 0.6341880411965364\n",
            "epoch 6 batch id 3821 loss 0.9870195388793945 train acc 0.6343112077990055\n",
            "epoch 6 batch id 3831 loss 0.964658260345459 train acc 0.6343807099973897\n",
            "epoch 6 batch id 3841 loss 0.7477083206176758 train acc 0.6343562874251497\n",
            "epoch 6 batch id 3851 loss 0.8159114122390747 train acc 0.6344942871981304\n",
            "epoch 6 batch id 3861 loss 0.9082427024841309 train acc 0.6345384939134939\n",
            "epoch 6 batch id 3871 loss 1.0067121982574463 train acc 0.6346349457504521\n",
            "epoch 6 batch id 3881 loss 0.9508787989616394 train acc 0.6347631087348622\n",
            "epoch 6 batch id 3891 loss 1.0909910202026367 train acc 0.6348183307633\n",
            "epoch 6 batch id 3901 loss 0.912782609462738 train acc 0.6348532427582672\n",
            "epoch 6 batch id 3911 loss 0.734757661819458 train acc 0.6349518984914344\n",
            "epoch 6 batch id 3921 loss 0.8548851013183594 train acc 0.63499824662076\n",
            "epoch 6 batch id 3931 loss 1.0591315031051636 train acc 0.6350562833884508\n",
            "epoch 6 batch id 3941 loss 0.8296958208084106 train acc 0.6351060961684851\n",
            "epoch 6 batch id 3951 loss 0.857418417930603 train acc 0.6351675208807896\n",
            "epoch 6 batch id 3961 loss 1.0268908739089966 train acc 0.6352444142893209\n",
            "epoch 6 batch id 3971 loss 0.8061971068382263 train acc 0.6353602681944095\n",
            "epoch 6 batch id 3981 loss 0.8849648237228394 train acc 0.6354637653855815\n",
            "epoch 6 batch id 3991 loss 1.1237705945968628 train acc 0.6355080180405913\n",
            "epoch 6 batch id 4001 loss 0.9226691126823425 train acc 0.6355090914771307\n",
            "epoch 6 batch id 4011 loss 0.9044550061225891 train acc 0.6355530104712042\n",
            "epoch 6 batch id 4021 loss 0.8506025671958923 train acc 0.6356044827157423\n",
            "epoch 6 batch id 4031 loss 0.9636889696121216 train acc 0.6356712044157777\n",
            "epoch 6 batch id 4041 loss 0.9065519571304321 train acc 0.6356718634001485\n",
            "epoch 6 batch id 4051 loss 0.9570844769477844 train acc 0.6357882313009133\n",
            "epoch 6 batch id 4061 loss 1.2117441892623901 train acc 0.6358578552080768\n",
            "epoch 6 batch id 4071 loss 1.239172101020813 train acc 0.6359041083271924\n",
            "epoch 6 batch id 4081 loss 0.6578861474990845 train acc 0.6360535101690762\n",
            "epoch 6 batch id 4091 loss 0.8558651208877563 train acc 0.6360799621119531\n",
            "epoch 6 batch id 4101 loss 1.0541080236434937 train acc 0.6361139051450866\n",
            "epoch 6 batch id 4111 loss 0.7938847541809082 train acc 0.6361058744830942\n",
            "epoch 6 batch id 4121 loss 1.0533506870269775 train acc 0.6361130490172289\n",
            "epoch 6 batch id 4131 loss 0.6999029517173767 train acc 0.636176924473493\n",
            "epoch 6 batch id 4141 loss 1.0636630058288574 train acc 0.6362593576430814\n",
            "epoch 6 batch id 4151 loss 0.8915417194366455 train acc 0.6363376294868707\n",
            "epoch 6 batch id 4161 loss 1.062881350517273 train acc 0.636362953616919\n",
            "epoch 6 batch id 4171 loss 0.7262976765632629 train acc 0.6364143790457923\n",
            "epoch 6 batch id 4181 loss 0.6768784523010254 train acc 0.6364917184883999\n",
            "epoch 6 batch id 4191 loss 1.0319422483444214 train acc 0.6365425912670007\n",
            "epoch 6 batch id 4201 loss 0.8567735552787781 train acc 0.6366043799095453\n",
            "epoch 6 batch id 4211 loss 0.9566168189048767 train acc 0.6366844276893849\n",
            "epoch 6 batch id 4221 loss 0.7388157844543457 train acc 0.636727078891258\n",
            "epoch 6 batch id 4231 loss 0.851218581199646 train acc 0.6367806074214134\n",
            "epoch 6 batch id 4241 loss 0.9328403472900391 train acc 0.6368854633341193\n",
            "epoch 6 batch id 4251 loss 0.837127685546875 train acc 0.6370045283462714\n",
            "epoch 6 batch id 4261 loss 0.8694981336593628 train acc 0.6370900316827036\n",
            "epoch 6 batch id 4271 loss 0.7465184330940247 train acc 0.6371824514165301\n",
            "epoch 6 batch id 4281 loss 0.7982504963874817 train acc 0.6373109378649848\n",
            "epoch 6 batch id 4291 loss 0.7527585029602051 train acc 0.63746067350268\n",
            "epoch 6 batch id 4301 loss 0.7943841814994812 train acc 0.6375079923273658\n",
            "epoch 6 batch id 4311 loss 0.8471297025680542 train acc 0.6375405938297379\n",
            "epoch 6 batch id 4321 loss 0.9682968258857727 train acc 0.6375875086785466\n",
            "epoch 6 batch id 4331 loss 0.7571461200714111 train acc 0.6376486377280074\n",
            "epoch 6 batch id 4341 loss 0.9789561629295349 train acc 0.6376842893342548\n",
            "epoch 6 batch id 4351 loss 0.8707823753356934 train acc 0.6376766835210297\n",
            "epoch 6 batch id 4361 loss 1.205027461051941 train acc 0.6376941928456776\n",
            "epoch 6 batch id 4371 loss 1.0462523698806763 train acc 0.6378009894760924\n",
            "epoch 6 batch id 4381 loss 0.7921762466430664 train acc 0.6378645001141292\n",
            "epoch 6 batch id 4391 loss 0.757219135761261 train acc 0.6379099294010476\n",
            "epoch 6 batch id 4401 loss 0.9568875432014465 train acc 0.6379658032265394\n",
            "epoch 6 batch id 4411 loss 0.8171249628067017 train acc 0.6380214237134436\n",
            "epoch 6 batch id 4421 loss 0.8729152083396912 train acc 0.6381192038000453\n",
            "epoch 6 batch id 4431 loss 0.9886945486068726 train acc 0.6381742270367863\n",
            "epoch 6 batch id 4441 loss 0.7220506072044373 train acc 0.6382395575320874\n",
            "epoch 6 batch id 4451 loss 0.9807084798812866 train acc 0.6382659795551562\n",
            "epoch 6 batch id 4461 loss 0.9515000581741333 train acc 0.6383518269446312\n",
            "epoch 6 batch id 4471 loss 1.006554126739502 train acc 0.6383359427421158\n",
            "epoch 6 batch id 4481 loss 1.006464958190918 train acc 0.638330590270029\n",
            "epoch 6 batch id 4491 loss 0.9031906127929688 train acc 0.638370490981964\n",
            "epoch 6 batch id 4501 loss 0.7314882278442383 train acc 0.6384900577649412\n",
            "epoch 6 batch id 4511 loss 0.9487965106964111 train acc 0.6384843992462869\n",
            "epoch 6 batch id 4521 loss 0.8473825454711914 train acc 0.6384891340411414\n",
            "epoch 6 batch id 4531 loss 0.942585289478302 train acc 0.6384869510041933\n",
            "epoch 6 batch id 4541 loss 0.8163673877716064 train acc 0.6385845628716141\n",
            "epoch 6 batch id 4551 loss 1.1121457815170288 train acc 0.6386542792792793\n",
            "epoch 6 batch id 4561 loss 0.9134934544563293 train acc 0.6387031352773515\n",
            "epoch 6 batch id 4571 loss 1.091507911682129 train acc 0.6388201432946838\n",
            "epoch 6 batch id 4581 loss 0.9682177305221558 train acc 0.6388888888888888\n",
            "epoch 6 batch id 4591 loss 0.9292159080505371 train acc 0.6388994772380745\n",
            "epoch 6 batch id 4601 loss 0.9173815250396729 train acc 0.6390254835905238\n",
            "epoch 6 batch id 4611 loss 0.8037999868392944 train acc 0.6390967252222945\n",
            "epoch 6 batch id 4621 loss 0.9200405478477478 train acc 0.6391879463319627\n",
            "epoch 6 batch id 4631 loss 0.804269552230835 train acc 0.639295643489527\n",
            "epoch 6 batch id 4641 loss 0.9661610722541809 train acc 0.6393591090282267\n",
            "epoch 6 batch id 4651 loss 0.9760711193084717 train acc 0.639408863685229\n",
            "epoch 6 batch id 4661 loss 0.8542553186416626 train acc 0.6394919276979188\n",
            "epoch 6 batch id 4671 loss 0.8553729057312012 train acc 0.6395679458360094\n",
            "epoch 6 batch id 4681 loss 1.1618419885635376 train acc 0.6396236114078189\n",
            "epoch 6 batch id 4691 loss 0.5483506917953491 train acc 0.6397523182690258\n",
            "epoch 6 batch id 4701 loss 0.867886483669281 train acc 0.6397874122527122\n",
            "epoch 6 batch id 4711 loss 0.6017689108848572 train acc 0.6398853746550626\n",
            "epoch 6 batch id 4721 loss 0.9246910810470581 train acc 0.6399862317305656\n",
            "epoch 6 batch id 4731 loss 0.7486954927444458 train acc 0.640007398013105\n",
            "epoch 6 batch id 4741 loss 0.907233715057373 train acc 0.6400680236237081\n",
            "epoch 6 batch id 4751 loss 0.8020024299621582 train acc 0.6401481267101663\n",
            "epoch 6 batch id 4761 loss 1.0105677843093872 train acc 0.640211483931947\n",
            "epoch 6 batch id 4771 loss 0.9133390784263611 train acc 0.6403466254453993\n",
            "epoch 6 batch id 4781 loss 0.998874843120575 train acc 0.6403472076971345\n",
            "epoch 6 batch id 4791 loss 0.8815280199050903 train acc 0.640452149864329\n",
            "epoch 6 batch id 4801 loss 0.939363956451416 train acc 0.6404590189543845\n",
            "epoch 6 batch id 4811 loss 0.7217724919319153 train acc 0.6405535491581792\n",
            "epoch 6 batch id 4821 loss 1.0250446796417236 train acc 0.6405731435386849\n",
            "epoch 6 batch id 4831 loss 0.8026928305625916 train acc 0.6406993893603808\n",
            "epoch 6 batch id 4841 loss 0.9388142228126526 train acc 0.640725056806445\n",
            "epoch 6 batch id 4851 loss 0.933095395565033 train acc 0.6407989332096475\n",
            "epoch 6 batch id 4861 loss 0.9289729595184326 train acc 0.6408242902694918\n",
            "epoch 6 batch id 4871 loss 1.0532890558242798 train acc 0.6409072828987887\n",
            "epoch 6 batch id 4881 loss 0.8934939503669739 train acc 0.6409835330874821\n",
            "epoch 6 batch id 4891 loss 0.9519649744033813 train acc 0.6410658607646698\n",
            "epoch 6 batch id 4901 loss 0.8938380479812622 train acc 0.6411064068557437\n",
            "epoch 6 batch id 4911 loss 0.9930764436721802 train acc 0.6411690592547342\n",
            "epoch 6 batch id 4921 loss 0.9506129622459412 train acc 0.6411997053444422\n",
            "epoch 6 batch id 4931 loss 1.006560206413269 train acc 0.6411922023930238\n",
            "epoch 6 batch id 4941 loss 0.7701150178909302 train acc 0.6412574630641571\n",
            "epoch 6 batch id 4951 loss 1.192847490310669 train acc 0.6413319278933549\n",
            "epoch 6 train acc 0.6413975530471143\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55dd4705c6774cc3a1062ad529f335e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6 loss 0.9989722967147827 test acc 0.6409090909090909\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01bb694a806847ab8a76728bb3a65470",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7 batch id 1 loss 0.9411100745201111 train acc 0.609375\n",
            "epoch 7 batch id 11 loss 0.9006887078285217 train acc 0.6775568181818182\n",
            "epoch 7 batch id 21 loss 0.7687361240386963 train acc 0.6822916666666666\n",
            "epoch 7 batch id 31 loss 0.9835538864135742 train acc 0.6683467741935484\n",
            "epoch 7 batch id 41 loss 0.7137216925621033 train acc 0.671875\n",
            "epoch 7 batch id 51 loss 0.9838928580284119 train acc 0.6697303921568627\n",
            "epoch 7 batch id 61 loss 1.0078061819076538 train acc 0.6654713114754098\n",
            "epoch 7 batch id 71 loss 1.15940260887146 train acc 0.6643926056338029\n",
            "epoch 7 batch id 81 loss 1.0357837677001953 train acc 0.6657021604938271\n",
            "epoch 7 batch id 91 loss 0.9523221254348755 train acc 0.6665521978021978\n",
            "epoch 7 batch id 101 loss 0.7251912355422974 train acc 0.666305693069307\n",
            "epoch 7 batch id 111 loss 0.9083296060562134 train acc 0.665259009009009\n",
            "epoch 7 batch id 121 loss 1.1062426567077637 train acc 0.6650309917355371\n",
            "epoch 7 batch id 131 loss 0.872587263584137 train acc 0.6682967557251909\n",
            "epoch 7 batch id 141 loss 0.9586228132247925 train acc 0.6683289007092199\n",
            "epoch 7 batch id 151 loss 0.8513297438621521 train acc 0.6685637417218543\n",
            "epoch 7 batch id 161 loss 1.1032752990722656 train acc 0.6692546583850931\n",
            "epoch 7 batch id 171 loss 0.8808090686798096 train acc 0.6684027777777778\n",
            "epoch 7 batch id 181 loss 0.7593498229980469 train acc 0.6697168508287292\n",
            "epoch 7 batch id 191 loss 1.0443111658096313 train acc 0.6693390052356021\n",
            "epoch 7 batch id 201 loss 0.642936646938324 train acc 0.6710199004975125\n",
            "epoch 7 batch id 211 loss 1.0756757259368896 train acc 0.6703199052132701\n",
            "epoch 7 batch id 221 loss 1.0149811506271362 train acc 0.6704609728506787\n",
            "epoch 7 batch id 231 loss 0.868283212184906 train acc 0.6708603896103896\n",
            "epoch 7 batch id 241 loss 0.9877374768257141 train acc 0.6696058091286307\n",
            "epoch 7 batch id 251 loss 0.9987504482269287 train acc 0.6688247011952191\n",
            "epoch 7 batch id 261 loss 0.9141761064529419 train acc 0.6700191570881227\n",
            "epoch 7 batch id 271 loss 0.6467734575271606 train acc 0.6715290590405905\n",
            "epoch 7 batch id 281 loss 0.8096127510070801 train acc 0.6722086298932385\n",
            "epoch 7 batch id 291 loss 1.168271780014038 train acc 0.6738616838487973\n",
            "epoch 7 batch id 301 loss 1.1264375448226929 train acc 0.6742109634551495\n",
            "epoch 7 batch id 311 loss 0.8898658752441406 train acc 0.6732315112540193\n",
            "epoch 7 batch id 321 loss 1.076926589012146 train acc 0.6725077881619937\n",
            "epoch 7 batch id 331 loss 1.0773478746414185 train acc 0.672583081570997\n",
            "epoch 7 batch id 341 loss 1.1354318857192993 train acc 0.672608137829912\n",
            "epoch 7 batch id 351 loss 0.9397656321525574 train acc 0.6723646723646723\n",
            "epoch 7 batch id 361 loss 0.9791563749313354 train acc 0.6724376731301939\n",
            "epoch 7 batch id 371 loss 1.1923205852508545 train acc 0.6721276954177897\n",
            "epoch 7 batch id 381 loss 0.7873054146766663 train acc 0.6723261154855643\n",
            "epoch 7 batch id 391 loss 1.186569333076477 train acc 0.6722746163682864\n",
            "epoch 7 batch id 401 loss 0.6719935536384583 train acc 0.6721867206982544\n",
            "epoch 7 batch id 411 loss 0.7521437406539917 train acc 0.6727113746958637\n",
            "epoch 7 batch id 421 loss 1.0832533836364746 train acc 0.6725430522565321\n",
            "epoch 7 batch id 431 loss 0.9871740937232971 train acc 0.6722012761020881\n",
            "epoch 7 batch id 441 loss 0.960906982421875 train acc 0.6724418934240363\n",
            "epoch 7 batch id 451 loss 0.841691255569458 train acc 0.6730875831485588\n",
            "epoch 7 batch id 461 loss 0.7798579931259155 train acc 0.6726206616052061\n",
            "epoch 7 batch id 471 loss 0.7611752152442932 train acc 0.6735337048832272\n",
            "epoch 7 batch id 481 loss 1.155148983001709 train acc 0.6730119542619543\n",
            "epoch 7 batch id 491 loss 1.049347162246704 train acc 0.6724478105906314\n",
            "epoch 7 batch id 501 loss 1.3123208284378052 train acc 0.6722180638722555\n",
            "epoch 7 batch id 511 loss 0.7025982141494751 train acc 0.6723030821917808\n",
            "epoch 7 batch id 521 loss 1.0244097709655762 train acc 0.6724448176583493\n",
            "epoch 7 batch id 531 loss 0.8874556422233582 train acc 0.6725812146892656\n",
            "epoch 7 batch id 541 loss 0.8779788017272949 train acc 0.6728858595194085\n",
            "epoch 7 batch id 551 loss 0.8154005408287048 train acc 0.6723854355716878\n",
            "epoch 7 batch id 561 loss 0.6708145141601562 train acc 0.672320632798574\n",
            "epoch 7 batch id 571 loss 0.9571303725242615 train acc 0.6720391856392294\n",
            "epoch 7 batch id 581 loss 0.9502873420715332 train acc 0.6717405335628227\n",
            "epoch 7 batch id 591 loss 0.5967527627944946 train acc 0.6720071912013537\n",
            "epoch 7 batch id 601 loss 0.7703559994697571 train acc 0.6722129783693843\n",
            "epoch 7 batch id 611 loss 0.8199828863143921 train acc 0.6727189034369886\n",
            "epoch 7 batch id 621 loss 0.860712468624115 train acc 0.6735607890499195\n",
            "epoch 7 batch id 631 loss 0.9086956977844238 train acc 0.6735093106180665\n",
            "epoch 7 batch id 641 loss 0.9168959856033325 train acc 0.6734350624024961\n",
            "epoch 7 batch id 651 loss 0.7704248428344727 train acc 0.6738191244239631\n",
            "epoch 7 batch id 661 loss 0.9602764844894409 train acc 0.6740260968229954\n",
            "epoch 7 batch id 671 loss 1.0032367706298828 train acc 0.6738310357675111\n",
            "epoch 7 batch id 681 loss 0.815956175327301 train acc 0.6736646475770925\n",
            "epoch 7 batch id 691 loss 0.8562958836555481 train acc 0.6739100940665702\n",
            "epoch 7 batch id 701 loss 0.8460803031921387 train acc 0.6739702211126961\n",
            "epoch 7 batch id 711 loss 0.8443702459335327 train acc 0.6741385372714487\n",
            "epoch 7 batch id 721 loss 0.8969292640686035 train acc 0.6737820735090153\n",
            "epoch 7 batch id 731 loss 0.7285852432250977 train acc 0.6742689808481532\n",
            "epoch 7 batch id 741 loss 0.9596149325370789 train acc 0.6744264507422402\n",
            "epoch 7 batch id 751 loss 1.0497303009033203 train acc 0.6741011984021305\n",
            "epoch 7 batch id 761 loss 0.6197640895843506 train acc 0.6745031208935611\n",
            "epoch 7 batch id 771 loss 1.094018816947937 train acc 0.6744690337224384\n",
            "epoch 7 batch id 781 loss 0.8606218695640564 train acc 0.6743557938540333\n",
            "epoch 7 batch id 791 loss 0.7439537644386292 train acc 0.6744824589127687\n",
            "epoch 7 batch id 801 loss 0.9743448495864868 train acc 0.674605961298377\n",
            "epoch 7 batch id 811 loss 1.0139670372009277 train acc 0.6747842170160296\n",
            "epoch 7 batch id 821 loss 0.751832902431488 train acc 0.6747678136419001\n",
            "epoch 7 batch id 831 loss 0.9157414436340332 train acc 0.6748270156438027\n",
            "epoch 7 batch id 841 loss 0.8806079030036926 train acc 0.6753492865636147\n",
            "epoch 7 batch id 851 loss 0.9188291430473328 train acc 0.6753819036427732\n",
            "epoch 7 batch id 861 loss 1.0389372110366821 train acc 0.6750145180023229\n",
            "epoch 7 batch id 871 loss 0.6763774752616882 train acc 0.6750143513203215\n",
            "epoch 7 batch id 881 loss 0.8209248185157776 train acc 0.6749787173666288\n",
            "epoch 7 batch id 891 loss 0.73641437292099 train acc 0.6752595398428731\n",
            "epoch 7 batch id 901 loss 0.9833494424819946 train acc 0.6753780521642619\n",
            "epoch 7 batch id 911 loss 1.024081826210022 train acc 0.6751852360043907\n",
            "epoch 7 batch id 921 loss 0.9471222758293152 train acc 0.6750983984799132\n",
            "epoch 7 batch id 931 loss 0.7148872017860413 train acc 0.6751644736842105\n",
            "epoch 7 batch id 941 loss 0.7392433881759644 train acc 0.6756774707757705\n",
            "epoch 7 batch id 951 loss 1.0153778791427612 train acc 0.6756210567823344\n",
            "epoch 7 batch id 961 loss 0.8823772668838501 train acc 0.6753219302809573\n",
            "epoch 7 batch id 971 loss 1.0407915115356445 train acc 0.6756082646755922\n",
            "epoch 7 batch id 981 loss 0.923864483833313 train acc 0.6758569062181448\n",
            "epoch 7 batch id 991 loss 0.8412915468215942 train acc 0.6758955600403632\n",
            "epoch 7 batch id 1001 loss 0.9884201288223267 train acc 0.6758553946053946\n",
            "epoch 7 batch id 1011 loss 0.9601233005523682 train acc 0.6762023986152325\n",
            "epoch 7 batch id 1021 loss 0.6540482044219971 train acc 0.6767721596474046\n",
            "epoch 7 batch id 1031 loss 0.9514307975769043 train acc 0.6765731086323957\n",
            "epoch 7 batch id 1041 loss 0.8795160055160522 train acc 0.6766480547550432\n",
            "epoch 7 batch id 1051 loss 0.6934288740158081 train acc 0.6764688392007612\n",
            "epoch 7 batch id 1061 loss 0.8790958523750305 train acc 0.6767348020735156\n",
            "epoch 7 batch id 1071 loss 0.668209969997406 train acc 0.6768499066293184\n",
            "epoch 7 batch id 1081 loss 0.9645755887031555 train acc 0.6767171600370028\n",
            "epoch 7 batch id 1091 loss 1.1608198881149292 train acc 0.6765725252062328\n",
            "epoch 7 batch id 1101 loss 0.7598245739936829 train acc 0.6767001589464123\n",
            "epoch 7 batch id 1111 loss 0.868428111076355 train acc 0.6767692394239424\n",
            "epoch 7 batch id 1121 loss 0.7757449746131897 train acc 0.6767673951828724\n",
            "epoch 7 batch id 1131 loss 0.9976102709770203 train acc 0.6766826923076923\n",
            "epoch 7 batch id 1141 loss 0.9041054844856262 train acc 0.6769144390885189\n",
            "epoch 7 batch id 1151 loss 1.0941572189331055 train acc 0.6770064074717637\n",
            "epoch 7 batch id 1161 loss 0.7983540892601013 train acc 0.6768680017226529\n",
            "epoch 7 batch id 1171 loss 0.700034499168396 train acc 0.6772123185311699\n",
            "epoch 7 batch id 1181 loss 0.9112586975097656 train acc 0.6771671253175275\n",
            "epoch 7 batch id 1191 loss 0.8134162425994873 train acc 0.6772276448362721\n",
            "epoch 7 batch id 1201 loss 1.2419461011886597 train acc 0.6773652164862615\n",
            "epoch 7 batch id 1211 loss 0.7234203815460205 train acc 0.6773843930635838\n",
            "epoch 7 batch id 1221 loss 0.79102623462677 train acc 0.6775312244062244\n",
            "epoch 7 batch id 1231 loss 0.9661767482757568 train acc 0.6775106620633631\n",
            "epoch 7 batch id 1241 loss 0.907048225402832 train acc 0.6774526591458501\n",
            "epoch 7 batch id 1251 loss 0.726721465587616 train acc 0.6775579536370904\n",
            "epoch 7 batch id 1261 loss 0.9199581742286682 train acc 0.6776615781126091\n",
            "epoch 7 batch id 1271 loss 0.919995129108429 train acc 0.6776652242328874\n",
            "epoch 7 batch id 1281 loss 0.9691281318664551 train acc 0.6777541959406713\n",
            "epoch 7 batch id 1291 loss 0.9187496900558472 train acc 0.6779628195197521\n",
            "epoch 7 batch id 1301 loss 0.9673355221748352 train acc 0.6779280361260569\n",
            "epoch 7 batch id 1311 loss 1.111919641494751 train acc 0.6778937833714722\n",
            "epoch 7 batch id 1321 loss 0.798169732093811 train acc 0.6778482210446631\n",
            "epoch 7 batch id 1331 loss 0.7417582869529724 train acc 0.6780850864012021\n",
            "epoch 7 batch id 1341 loss 0.9640913605690002 train acc 0.6780154735272185\n",
            "epoch 7 batch id 1351 loss 0.6895554065704346 train acc 0.6782360288675056\n",
            "epoch 7 batch id 1361 loss 1.0541692972183228 train acc 0.678269654665687\n",
            "epoch 7 batch id 1371 loss 0.8461514115333557 train acc 0.6784053610503282\n",
            "epoch 7 batch id 1381 loss 0.99545818567276 train acc 0.6784599022447502\n",
            "epoch 7 batch id 1391 loss 0.46702462434768677 train acc 0.6787832494608196\n",
            "epoch 7 batch id 1401 loss 0.7782896161079407 train acc 0.6789012312633833\n",
            "epoch 7 batch id 1411 loss 0.5731876492500305 train acc 0.6789400248051027\n",
            "epoch 7 batch id 1421 loss 0.8130982518196106 train acc 0.678945285010556\n",
            "epoch 7 batch id 1431 loss 0.6387813687324524 train acc 0.6790924178895877\n",
            "epoch 7 batch id 1441 loss 0.9028255343437195 train acc 0.6791507633587787\n",
            "epoch 7 batch id 1451 loss 0.6967781186103821 train acc 0.6792190730530668\n",
            "epoch 7 batch id 1461 loss 0.7545062899589539 train acc 0.6791688056125941\n",
            "epoch 7 batch id 1471 loss 1.1996941566467285 train acc 0.6791404656696125\n",
            "epoch 7 batch id 1481 loss 0.8107665777206421 train acc 0.6792707629979743\n",
            "epoch 7 batch id 1491 loss 0.7290098071098328 train acc 0.6795879443326627\n",
            "epoch 7 batch id 1501 loss 0.8566566109657288 train acc 0.6796927048634244\n",
            "epoch 7 batch id 1511 loss 0.8127618432044983 train acc 0.6797753970880211\n",
            "epoch 7 batch id 1521 loss 0.7125949263572693 train acc 0.6797748191978962\n",
            "epoch 7 batch id 1531 loss 0.8863976001739502 train acc 0.6797742488569563\n",
            "epoch 7 batch id 1541 loss 0.8650297522544861 train acc 0.6797635463984426\n",
            "epoch 7 batch id 1551 loss 0.9191317558288574 train acc 0.6797227595099935\n",
            "epoch 7 batch id 1561 loss 0.8171179890632629 train acc 0.6798226297245356\n",
            "epoch 7 batch id 1571 loss 0.82469242811203 train acc 0.6799709579885423\n",
            "epoch 7 batch id 1581 loss 0.9347516894340515 train acc 0.6800284629981025\n",
            "epoch 7 batch id 1591 loss 0.7740164995193481 train acc 0.6798691860465116\n",
            "epoch 7 batch id 1601 loss 1.0016040802001953 train acc 0.6800046845721424\n",
            "epoch 7 batch id 1611 loss 0.7746160626411438 train acc 0.6800609093730602\n",
            "epoch 7 batch id 1621 loss 0.8127220869064331 train acc 0.6800296884639112\n",
            "epoch 7 batch id 1631 loss 0.7367124557495117 train acc 0.6801042305334151\n",
            "epoch 7 batch id 1641 loss 0.87350994348526 train acc 0.6802635588056063\n",
            "epoch 7 batch id 1651 loss 0.8239384889602661 train acc 0.6803168534221684\n",
            "epoch 7 batch id 1661 loss 1.033642292022705 train acc 0.6804259482239615\n",
            "epoch 7 batch id 1671 loss 0.948793888092041 train acc 0.6803934769599043\n",
            "epoch 7 batch id 1681 loss 1.1501346826553345 train acc 0.6800639500297442\n",
            "epoch 7 batch id 1691 loss 0.8964731693267822 train acc 0.6801356445890006\n",
            "epoch 7 batch id 1701 loss 0.8252376317977905 train acc 0.6802248677248677\n",
            "epoch 7 batch id 1711 loss 0.8634243607521057 train acc 0.6803952367036821\n",
            "epoch 7 batch id 1721 loss 0.8887600898742676 train acc 0.6806634950610111\n",
            "epoch 7 batch id 1731 loss 0.5558742880821228 train acc 0.6809015742345466\n",
            "epoch 7 batch id 1741 loss 0.8852174878120422 train acc 0.6809933228029867\n",
            "epoch 7 batch id 1751 loss 0.6442171931266785 train acc 0.6810215591090806\n",
            "epoch 7 batch id 1761 loss 0.6018454432487488 train acc 0.6811027115275412\n",
            "epoch 7 batch id 1771 loss 0.7206351161003113 train acc 0.6811388339920948\n",
            "epoch 7 batch id 1781 loss 0.9189179539680481 train acc 0.6811833239752948\n",
            "epoch 7 batch id 1791 loss 1.1458605527877808 train acc 0.681288386376326\n",
            "epoch 7 batch id 1801 loss 0.8134127855300903 train acc 0.6814530122154359\n",
            "epoch 7 batch id 1811 loss 0.7373042702674866 train acc 0.6815122860298177\n",
            "epoch 7 batch id 1821 loss 0.8033660054206848 train acc 0.6816223915431082\n",
            "epoch 7 batch id 1831 loss 0.6678190231323242 train acc 0.6816288913162206\n",
            "epoch 7 batch id 1841 loss 0.6336731910705566 train acc 0.6817456545355784\n",
            "epoch 7 batch id 1851 loss 0.9478231072425842 train acc 0.6817176526202053\n",
            "epoch 7 batch id 1861 loss 0.8471423387527466 train acc 0.6817823078989791\n",
            "epoch 7 batch id 1871 loss 0.7903597354888916 train acc 0.6819464858364511\n",
            "epoch 7 batch id 1881 loss 0.8535488247871399 train acc 0.6820424641148325\n",
            "epoch 7 batch id 1891 loss 0.7579451203346252 train acc 0.6821952670544685\n",
            "epoch 7 batch id 1901 loss 0.7592200040817261 train acc 0.6821820752235666\n",
            "epoch 7 batch id 1911 loss 0.7709715366363525 train acc 0.6823080193615908\n",
            "epoch 7 batch id 1921 loss 0.8490179777145386 train acc 0.6821398360229047\n",
            "epoch 7 batch id 1931 loss 0.9317189455032349 train acc 0.6823456110823407\n",
            "epoch 7 batch id 1941 loss 0.7744894623756409 train acc 0.6823963163317878\n",
            "epoch 7 batch id 1951 loss 0.8462620973587036 train acc 0.6823584059456689\n",
            "epoch 7 batch id 1961 loss 0.6334912180900574 train acc 0.682472271800102\n",
            "epoch 7 batch id 1971 loss 0.7306311130523682 train acc 0.6825849822425165\n",
            "epoch 7 batch id 1981 loss 0.7620827555656433 train acc 0.6825230312973246\n",
            "epoch 7 batch id 1991 loss 0.8206118941307068 train acc 0.6824852461074836\n",
            "epoch 7 batch id 2001 loss 0.9642991423606873 train acc 0.6825103073463268\n",
            "epoch 7 batch id 2011 loss 1.0328012704849243 train acc 0.6825351193436101\n",
            "epoch 7 batch id 2021 loss 1.0809674263000488 train acc 0.6825055665512123\n",
            "epoch 7 batch id 2031 loss 0.7595893740653992 train acc 0.682599396848843\n",
            "epoch 7 batch id 2041 loss 0.6563870310783386 train acc 0.6826463743263106\n",
            "epoch 7 batch id 2051 loss 0.8767814636230469 train acc 0.6827233666504144\n",
            "epoch 7 batch id 2061 loss 0.8803244829177856 train acc 0.6827465429403202\n",
            "epoch 7 batch id 2071 loss 0.8848345279693604 train acc 0.6827317720907774\n",
            "epoch 7 batch id 2081 loss 1.035284161567688 train acc 0.6826720927438732\n",
            "epoch 7 batch id 2091 loss 0.8472511172294617 train acc 0.6827474892395983\n",
            "epoch 7 batch id 2101 loss 0.7606996893882751 train acc 0.6828816634935745\n",
            "epoch 7 batch id 2111 loss 0.7910585403442383 train acc 0.6829405495026054\n",
            "epoch 7 batch id 2121 loss 0.5207974314689636 train acc 0.6830946487505893\n",
            "epoch 7 batch id 2131 loss 1.0319048166275024 train acc 0.6832692984514313\n",
            "epoch 7 batch id 2141 loss 1.0742015838623047 train acc 0.6832598668846334\n",
            "epoch 7 batch id 2151 loss 0.8716501593589783 train acc 0.6832577870757787\n",
            "epoch 7 batch id 2161 loss 1.0055276155471802 train acc 0.6831689611291069\n",
            "epoch 7 batch id 2171 loss 0.780870795249939 train acc 0.6832105020727776\n",
            "epoch 7 batch id 2181 loss 0.714289665222168 train acc 0.6832301696469509\n",
            "epoch 7 batch id 2191 loss 0.7800952792167664 train acc 0.6832567891373802\n",
            "epoch 7 batch id 2201 loss 0.9663227796554565 train acc 0.6832476715129486\n",
            "epoch 7 batch id 2211 loss 0.7199158072471619 train acc 0.6831750339213026\n",
            "epoch 7 batch id 2221 loss 1.1462838649749756 train acc 0.6831311909049977\n",
            "epoch 7 batch id 2231 loss 0.66300368309021 train acc 0.6833328664276109\n",
            "epoch 7 batch id 2241 loss 0.7316840887069702 train acc 0.6834769634091923\n",
            "epoch 7 batch id 2251 loss 0.8610132336616516 train acc 0.6835226010661928\n",
            "epoch 7 batch id 2261 loss 0.7302131652832031 train acc 0.6836023883237505\n",
            "epoch 7 batch id 2271 loss 0.9332007765769958 train acc 0.6836952333773668\n",
            "epoch 7 batch id 2281 loss 1.003932237625122 train acc 0.6839311157387111\n",
            "epoch 7 batch id 2291 loss 0.8626784682273865 train acc 0.6841649388913138\n",
            "epoch 7 batch id 2301 loss 0.8374871015548706 train acc 0.6842677096914385\n",
            "epoch 7 batch id 2311 loss 0.9265912175178528 train acc 0.684403396797923\n",
            "epoch 7 batch id 2321 loss 0.48210564255714417 train acc 0.6843628823782852\n",
            "epoch 7 batch id 2331 loss 0.8649685978889465 train acc 0.684242277992278\n",
            "epoch 7 batch id 2341 loss 0.911723256111145 train acc 0.6844030328919265\n",
            "epoch 7 batch id 2351 loss 0.8223575949668884 train acc 0.68443614419396\n",
            "epoch 7 batch id 2361 loss 0.9105914831161499 train acc 0.6845550084709868\n",
            "epoch 7 batch id 2371 loss 0.698889434337616 train acc 0.6846728700970055\n",
            "epoch 7 batch id 2381 loss 0.9313257336616516 train acc 0.6847569298614028\n",
            "epoch 7 batch id 2391 loss 0.812547504901886 train acc 0.6847880071099958\n",
            "epoch 7 batch id 2401 loss 0.8301824331283569 train acc 0.6848123177842566\n",
            "epoch 7 batch id 2411 loss 0.938926637172699 train acc 0.6848493882206553\n",
            "epoch 7 batch id 2421 loss 0.9578638672828674 train acc 0.6848732445270549\n",
            "epoch 7 batch id 2431 loss 0.8859054446220398 train acc 0.6848647675853559\n",
            "epoch 7 batch id 2441 loss 0.7441233396530151 train acc 0.684952376075379\n",
            "epoch 7 batch id 2451 loss 0.7498977184295654 train acc 0.684905395756834\n",
            "epoch 7 batch id 2461 loss 0.9698391556739807 train acc 0.6849095895977245\n",
            "epoch 7 batch id 2471 loss 0.9289562702178955 train acc 0.6848378692836908\n",
            "epoch 7 batch id 2481 loss 0.8981427550315857 train acc 0.6848234079000403\n",
            "epoch 7 batch id 2491 loss 0.9689068794250488 train acc 0.6846836109995985\n",
            "epoch 7 batch id 2501 loss 0.6379866003990173 train acc 0.6847635945621752\n",
            "epoch 7 batch id 2511 loss 0.9357622861862183 train acc 0.6849549482277977\n",
            "epoch 7 batch id 2521 loss 0.8350525498390198 train acc 0.6848720745735819\n",
            "epoch 7 batch id 2531 loss 0.7142704725265503 train acc 0.6849133247728171\n",
            "epoch 7 batch id 2541 loss 0.8451671004295349 train acc 0.6849112062180244\n",
            "epoch 7 batch id 2551 loss 1.0028389692306519 train acc 0.6849458545668365\n",
            "epoch 7 batch id 2561 loss 0.9974347352981567 train acc 0.6848887153455682\n",
            "epoch 7 batch id 2571 loss 0.8448140621185303 train acc 0.6850994262932711\n",
            "epoch 7 batch id 2581 loss 0.7691693902015686 train acc 0.6851934812088338\n",
            "epoch 7 batch id 2591 loss 0.8388546109199524 train acc 0.6852988710922424\n",
            "epoch 7 batch id 2601 loss 0.7578873634338379 train acc 0.6854094579008074\n",
            "epoch 7 batch id 2611 loss 0.7896687388420105 train acc 0.685393527384144\n",
            "epoch 7 batch id 2621 loss 0.8390848636627197 train acc 0.6854254101487982\n",
            "epoch 7 batch id 2631 loss 0.9311370253562927 train acc 0.6854511117445838\n",
            "epoch 7 batch id 2641 loss 0.7812042236328125 train acc 0.6855121166224915\n",
            "epoch 7 batch id 2651 loss 0.9616968035697937 train acc 0.6854901452282157\n",
            "epoch 7 batch id 2661 loss 0.9043592810630798 train acc 0.6855035700864337\n",
            "epoch 7 batch id 2671 loss 0.9602184295654297 train acc 0.6855636933732684\n",
            "epoch 7 batch id 2681 loss 0.8150796890258789 train acc 0.6855650876538605\n",
            "epoch 7 batch id 2691 loss 0.9728499054908752 train acc 0.6855896971386102\n",
            "epoch 7 batch id 2701 loss 0.846470832824707 train acc 0.6856893280266568\n",
            "epoch 7 batch id 2711 loss 0.800432562828064 train acc 0.6857997510143858\n",
            "epoch 7 batch id 2721 loss 0.6469739079475403 train acc 0.685874908122014\n",
            "epoch 7 batch id 2731 loss 0.634827196598053 train acc 0.6858980227023068\n",
            "epoch 7 batch id 2741 loss 0.9502071142196655 train acc 0.685926669098869\n",
            "epoch 7 batch id 2751 loss 0.7590153217315674 train acc 0.6860062250090876\n",
            "epoch 7 batch id 2761 loss 0.8294357061386108 train acc 0.6861248189061934\n",
            "epoch 7 batch id 2771 loss 0.8075019717216492 train acc 0.6861297816672681\n",
            "epoch 7 batch id 2781 loss 1.1065679788589478 train acc 0.6863088816972313\n",
            "epoch 7 batch id 2791 loss 0.8706942200660706 train acc 0.6863691329272662\n",
            "epoch 7 batch id 2801 loss 0.868457555770874 train acc 0.686440110674759\n",
            "epoch 7 batch id 2811 loss 0.7770450711250305 train acc 0.6865550515830665\n",
            "epoch 7 batch id 2821 loss 0.8484898805618286 train acc 0.6865639400921659\n",
            "epoch 7 batch id 2831 loss 0.8763234615325928 train acc 0.6866665930766513\n",
            "epoch 7 batch id 2841 loss 0.9384692907333374 train acc 0.6868180218233016\n",
            "epoch 7 batch id 2851 loss 0.8335981965065002 train acc 0.6867875306909856\n",
            "epoch 7 batch id 2861 loss 0.631534218788147 train acc 0.6868500961202377\n",
            "epoch 7 batch id 2871 loss 0.7767046689987183 train acc 0.6868850139324277\n",
            "epoch 7 batch id 2881 loss 0.7655736207962036 train acc 0.6869088424158278\n",
            "epoch 7 batch id 2891 loss 0.6078424453735352 train acc 0.6869216966447595\n",
            "epoch 7 batch id 2901 loss 0.9195877909660339 train acc 0.6870098672871424\n",
            "epoch 7 batch id 2911 loss 0.7015169262886047 train acc 0.6871511078667125\n",
            "epoch 7 batch id 2921 loss 0.6017374992370605 train acc 0.6871736990756591\n",
            "epoch 7 batch id 2931 loss 0.6322420239448547 train acc 0.6872121289662231\n",
            "epoch 7 batch id 2941 loss 0.7953017354011536 train acc 0.6871652924175451\n",
            "epoch 7 batch id 2951 loss 0.8702795505523682 train acc 0.6872246696035242\n",
            "epoch 7 batch id 2961 loss 0.6941601037979126 train acc 0.6873100303951368\n",
            "epoch 7 batch id 2971 loss 0.8295118808746338 train acc 0.6874053349040727\n",
            "epoch 7 batch id 2981 loss 0.8323666453361511 train acc 0.6875\n",
            "epoch 7 batch id 2991 loss 0.8297263383865356 train acc 0.6875679120695419\n",
            "epoch 7 batch id 3001 loss 0.6756239533424377 train acc 0.6876666111296235\n",
            "epoch 7 batch id 3011 loss 0.7165883183479309 train acc 0.6877179508468947\n",
            "epoch 7 batch id 3021 loss 0.8809471726417542 train acc 0.6877327457795432\n",
            "epoch 7 batch id 3031 loss 0.7392783761024475 train acc 0.687829924117453\n",
            "epoch 7 batch id 3041 loss 0.917737603187561 train acc 0.6877723199605393\n",
            "epoch 7 batch id 3051 loss 0.9469952583312988 train acc 0.6877202146837103\n",
            "epoch 7 batch id 3061 loss 0.6517258286476135 train acc 0.6877552270499837\n",
            "epoch 7 batch id 3071 loss 0.8790220618247986 train acc 0.68777983555845\n",
            "epoch 7 batch id 3081 loss 0.9608367681503296 train acc 0.6877992129178838\n",
            "epoch 7 batch id 3091 loss 0.9156836867332458 train acc 0.6878336298932385\n",
            "epoch 7 batch id 3101 loss 0.9130037426948547 train acc 0.6878930183811673\n",
            "epoch 7 batch id 3111 loss 0.9925363063812256 train acc 0.6878817100610736\n",
            "epoch 7 batch id 3121 loss 0.7605761289596558 train acc 0.6880206664530599\n",
            "epoch 7 batch id 3131 loss 0.7662657499313354 train acc 0.6880988502076014\n",
            "epoch 7 batch id 3141 loss 1.140028715133667 train acc 0.6880621219356893\n",
            "epoch 7 batch id 3151 loss 0.6382119059562683 train acc 0.6881099254205014\n",
            "epoch 7 batch id 3161 loss 0.6685784459114075 train acc 0.688142597279342\n",
            "epoch 7 batch id 3171 loss 0.7657711505889893 train acc 0.6881405707978555\n",
            "epoch 7 batch id 3181 loss 0.7038642764091492 train acc 0.6881925888085507\n",
            "epoch 7 batch id 3191 loss 0.5107049942016602 train acc 0.6883275227201504\n",
            "epoch 7 batch id 3201 loss 0.8672532439231873 train acc 0.6883298188066229\n",
            "epoch 7 batch id 3211 loss 0.7994736433029175 train acc 0.6883515649330426\n",
            "epoch 7 batch id 3221 loss 0.9181675910949707 train acc 0.688339219186588\n",
            "epoch 7 batch id 3231 loss 0.6350171566009521 train acc 0.6883946533580935\n",
            "epoch 7 batch id 3241 loss 0.7886685729026794 train acc 0.6884401033631595\n",
            "epoch 7 batch id 3251 loss 0.6837159991264343 train acc 0.688557366964011\n",
            "epoch 7 batch id 3261 loss 0.747776210308075 train acc 0.6886403710518246\n",
            "epoch 7 batch id 3271 loss 0.6370279788970947 train acc 0.6886321079180678\n",
            "epoch 7 batch id 3281 loss 0.9738364815711975 train acc 0.6886953291679366\n",
            "epoch 7 batch id 3291 loss 0.7462944984436035 train acc 0.6888198875721665\n",
            "epoch 7 batch id 3301 loss 0.6131001114845276 train acc 0.6888726900939109\n",
            "epoch 7 batch id 3311 loss 0.9105387926101685 train acc 0.6889440501359106\n",
            "epoch 7 batch id 3321 loss 0.8522233963012695 train acc 0.688916177356218\n",
            "epoch 7 batch id 3331 loss 1.1240748167037964 train acc 0.6889400705493846\n",
            "epoch 7 batch id 3341 loss 0.9269993305206299 train acc 0.6888609323555822\n",
            "epoch 7 batch id 3351 loss 0.7058281898498535 train acc 0.6889547896150403\n",
            "epoch 7 batch id 3361 loss 0.7318798303604126 train acc 0.6890945775066945\n",
            "epoch 7 batch id 3371 loss 0.8797427415847778 train acc 0.6890805769801246\n",
            "epoch 7 batch id 3381 loss 0.5902284383773804 train acc 0.6890990091688849\n",
            "epoch 7 batch id 3391 loss 0.8441736102104187 train acc 0.6891495871424359\n",
            "epoch 7 batch id 3401 loss 1.0265913009643555 train acc 0.689250404292855\n",
            "epoch 7 batch id 3411 loss 0.7530035376548767 train acc 0.6893460495455878\n",
            "epoch 7 batch id 3421 loss 0.8252907991409302 train acc 0.6894548377667349\n",
            "epoch 7 batch id 3431 loss 0.8609820008277893 train acc 0.6894946808510638\n",
            "epoch 7 batch id 3441 loss 0.7694414854049683 train acc 0.6895569965126417\n",
            "epoch 7 batch id 3451 loss 0.7350695133209229 train acc 0.6896234787018256\n",
            "epoch 7 batch id 3461 loss 0.8467501997947693 train acc 0.6897934123085814\n",
            "epoch 7 batch id 3471 loss 0.8054333329200745 train acc 0.6898948429847306\n",
            "epoch 7 batch id 3481 loss 0.8129468560218811 train acc 0.6899104064923872\n",
            "epoch 7 batch id 3491 loss 0.6322498321533203 train acc 0.6899751145803494\n",
            "epoch 7 batch id 3501 loss 0.9220501780509949 train acc 0.6900349900028563\n",
            "epoch 7 batch id 3511 loss 0.6859477758407593 train acc 0.6901034249501566\n",
            "epoch 7 batch id 3521 loss 0.9923264980316162 train acc 0.6901936594717409\n",
            "epoch 7 batch id 3531 loss 1.1299238204956055 train acc 0.6902214316057774\n",
            "epoch 7 batch id 3541 loss 0.6594060063362122 train acc 0.6903725995481502\n",
            "epoch 7 batch id 3551 loss 0.7457495331764221 train acc 0.690500915235145\n",
            "epoch 7 batch id 3561 loss 0.627759575843811 train acc 0.690685551811289\n",
            "epoch 7 batch id 3571 loss 1.0342873334884644 train acc 0.6907335130215626\n",
            "epoch 7 batch id 3581 loss 1.0132288932800293 train acc 0.6907419366098855\n",
            "epoch 7 batch id 3591 loss 0.8830462098121643 train acc 0.6907198551935394\n",
            "epoch 7 batch id 3601 loss 0.8426783084869385 train acc 0.6907933560122188\n",
            "epoch 7 batch id 3611 loss 0.834503710269928 train acc 0.6908577956244808\n",
            "epoch 7 batch id 3621 loss 0.6028077602386475 train acc 0.6908226318696493\n",
            "epoch 7 batch id 3631 loss 0.699597954750061 train acc 0.6908134811346737\n",
            "epoch 7 batch id 3641 loss 1.0136841535568237 train acc 0.6908558775061796\n",
            "epoch 7 batch id 3651 loss 1.0051977634429932 train acc 0.6909023212818406\n",
            "epoch 7 batch id 3661 loss 0.9095541834831238 train acc 0.6909826550122917\n",
            "epoch 7 batch id 3671 loss 0.7952075600624084 train acc 0.6910795764096976\n",
            "epoch 7 batch id 3681 loss 0.8242679238319397 train acc 0.691069851942407\n",
            "epoch 7 batch id 3691 loss 0.960790753364563 train acc 0.6911279124898402\n",
            "epoch 7 batch id 3701 loss 0.5338775515556335 train acc 0.6911687719535261\n",
            "epoch 7 batch id 3711 loss 0.7938758134841919 train acc 0.6912851994071679\n",
            "epoch 7 batch id 3721 loss 0.7146047353744507 train acc 0.6914387933351249\n",
            "epoch 7 batch id 3731 loss 0.6127486824989319 train acc 0.6915664366121683\n",
            "epoch 7 batch id 3741 loss 0.7890487909317017 train acc 0.6915179764768778\n",
            "epoch 7 batch id 3751 loss 0.8343663811683655 train acc 0.6916239002932552\n",
            "epoch 7 batch id 3761 loss 0.8247798085212708 train acc 0.6916918705131614\n",
            "epoch 7 batch id 3771 loss 0.5918349027633667 train acc 0.6917677671705118\n",
            "epoch 7 batch id 3781 loss 0.7921883463859558 train acc 0.6918597923829675\n",
            "epoch 7 batch id 3791 loss 0.7376968264579773 train acc 0.6918359271959905\n",
            "epoch 7 batch id 3801 loss 0.7743443250656128 train acc 0.691968396474612\n",
            "epoch 7 batch id 3811 loss 0.7845888137817383 train acc 0.6919771713461034\n",
            "epoch 7 batch id 3821 loss 0.8014718294143677 train acc 0.6920635959172992\n",
            "epoch 7 batch id 3831 loss 0.902446985244751 train acc 0.6920435264943879\n",
            "epoch 7 batch id 3841 loss 0.6480468511581421 train acc 0.6920927167404322\n",
            "epoch 7 batch id 3851 loss 0.7156577706336975 train acc 0.692157881069852\n",
            "epoch 7 batch id 3861 loss 0.6939634084701538 train acc 0.692149864024864\n",
            "epoch 7 batch id 3871 loss 0.7430831789970398 train acc 0.6922266533195557\n",
            "epoch 7 batch id 3881 loss 0.9230368137359619 train acc 0.6922788907498068\n",
            "epoch 7 batch id 3891 loss 0.8951165676116943 train acc 0.6922907029041377\n",
            "epoch 7 batch id 3901 loss 0.6722667217254639 train acc 0.6922463791335555\n",
            "epoch 7 batch id 3911 loss 0.6072552800178528 train acc 0.6923181411403733\n",
            "epoch 7 batch id 3921 loss 0.7465780377388 train acc 0.6923058530986993\n",
            "epoch 7 batch id 3931 loss 0.7302703261375427 train acc 0.6923770987026202\n",
            "epoch 7 batch id 3941 loss 0.5954371094703674 train acc 0.6924202296371479\n",
            "epoch 7 batch id 3951 loss 0.7719029188156128 train acc 0.6924987344975956\n",
            "epoch 7 batch id 3961 loss 1.0250226259231567 train acc 0.692513727594042\n",
            "epoch 7 batch id 3971 loss 0.753081738948822 train acc 0.6925758625031478\n",
            "epoch 7 batch id 3981 loss 0.6809544563293457 train acc 0.6926298354684752\n",
            "epoch 7 batch id 3991 loss 0.969454288482666 train acc 0.6926874530192935\n",
            "epoch 7 batch id 4001 loss 0.7344803810119629 train acc 0.6927525931017245\n",
            "epoch 7 batch id 4011 loss 0.9010652899742126 train acc 0.6927550797806034\n",
            "epoch 7 batch id 4021 loss 0.7439073324203491 train acc 0.6927847550360606\n",
            "epoch 7 batch id 4031 loss 0.8600035309791565 train acc 0.6928879310344828\n",
            "epoch 7 batch id 4041 loss 0.860105574131012 train acc 0.6929287305122495\n",
            "epoch 7 batch id 4051 loss 0.7953276634216309 train acc 0.6930811836583559\n",
            "epoch 7 batch id 4061 loss 1.0596530437469482 train acc 0.6931443917754248\n",
            "epoch 7 batch id 4071 loss 1.130936861038208 train acc 0.6931573937607467\n",
            "epoch 7 batch id 4081 loss 0.5590378642082214 train acc 0.69324307767704\n",
            "epoch 7 batch id 4091 loss 0.7122198939323425 train acc 0.6933054265460767\n",
            "epoch 7 batch id 4101 loss 0.800777792930603 train acc 0.6933408010241404\n",
            "epoch 7 batch id 4111 loss 0.7021237015724182 train acc 0.6933874057406957\n",
            "epoch 7 batch id 4121 loss 0.831298291683197 train acc 0.6933996602766319\n",
            "epoch 7 batch id 4131 loss 0.6323419213294983 train acc 0.6934458968772694\n",
            "epoch 7 batch id 4141 loss 0.7421820759773254 train acc 0.6935258693552282\n",
            "epoch 7 batch id 4151 loss 0.7404788732528687 train acc 0.6936092206697181\n",
            "epoch 7 batch id 4161 loss 0.9572232365608215 train acc 0.6936245794280221\n",
            "epoch 7 batch id 4171 loss 0.7327443361282349 train acc 0.6936773255813954\n",
            "epoch 7 batch id 4181 loss 0.5835169553756714 train acc 0.693733556565415\n",
            "epoch 7 batch id 4191 loss 0.9220144152641296 train acc 0.6937932474349797\n",
            "epoch 7 batch id 4201 loss 0.8259473443031311 train acc 0.6938749702451797\n",
            "epoch 7 batch id 4211 loss 0.7810565829277039 train acc 0.6938820945143671\n",
            "epoch 7 batch id 4221 loss 0.5935121178627014 train acc 0.6939113954039328\n",
            "epoch 7 batch id 4231 loss 0.7635670900344849 train acc 0.6939479437485228\n",
            "epoch 7 batch id 4241 loss 0.7269555926322937 train acc 0.6940469523697241\n",
            "epoch 7 batch id 4251 loss 0.7989469766616821 train acc 0.694108739120207\n",
            "epoch 7 batch id 4261 loss 0.8514233827590942 train acc 0.6941335660643042\n",
            "epoch 7 batch id 4271 loss 0.5652947425842285 train acc 0.6941948606883633\n",
            "epoch 7 batch id 4281 loss 0.704357385635376 train acc 0.6942595188040177\n",
            "epoch 7 batch id 4291 loss 0.6722148656845093 train acc 0.6943784956886506\n",
            "epoch 7 batch id 4301 loss 0.554240882396698 train acc 0.6943915659149035\n",
            "epoch 7 batch id 4311 loss 0.6310626268386841 train acc 0.6945459290187892\n",
            "epoch 7 batch id 4321 loss 0.7354035377502441 train acc 0.6946200242999305\n",
            "epoch 7 batch id 4331 loss 0.6436586976051331 train acc 0.6947046005541445\n",
            "epoch 7 batch id 4341 loss 0.7650099992752075 train acc 0.694745594333103\n",
            "epoch 7 batch id 4351 loss 0.8337999582290649 train acc 0.6947684440358538\n",
            "epoch 7 batch id 4361 loss 1.1043909788131714 train acc 0.6948306007796377\n",
            "epoch 7 batch id 4371 loss 0.8265030980110168 train acc 0.6949139212994738\n",
            "epoch 7 batch id 4381 loss 0.7480615377426147 train acc 0.694950496461995\n",
            "epoch 7 batch id 4391 loss 0.6535817384719849 train acc 0.6950260475973582\n",
            "epoch 7 batch id 4401 loss 0.9219748377799988 train acc 0.6950302488070893\n",
            "epoch 7 batch id 4411 loss 0.7557891607284546 train acc 0.6950733960553163\n",
            "epoch 7 batch id 4421 loss 0.6690371632575989 train acc 0.6951870334765891\n",
            "epoch 7 batch id 4431 loss 0.8991080522537231 train acc 0.6952895791017829\n",
            "epoch 7 batch id 4441 loss 0.6568785905838013 train acc 0.6953388876379194\n",
            "epoch 7 batch id 4451 loss 0.7599830627441406 train acc 0.6953458492473601\n",
            "epoch 7 batch id 4461 loss 0.8117327094078064 train acc 0.6953843028468953\n",
            "epoch 7 batch id 4471 loss 0.8438960909843445 train acc 0.6953911317378663\n",
            "epoch 7 batch id 4481 loss 0.8468112945556641 train acc 0.695432799598304\n",
            "epoch 7 batch id 4491 loss 0.6650099158287048 train acc 0.6954708027165442\n",
            "epoch 7 batch id 4501 loss 0.6279284954071045 train acc 0.6955398800266608\n",
            "epoch 7 batch id 4511 loss 0.8963608145713806 train acc 0.6955774772777654\n",
            "epoch 7 batch id 4521 loss 0.7802804708480835 train acc 0.6955907155496571\n",
            "epoch 7 batch id 4531 loss 0.7153462767601013 train acc 0.6955797561244759\n",
            "epoch 7 batch id 4541 loss 0.7228221893310547 train acc 0.6956686302576525\n",
            "epoch 7 batch id 4551 loss 0.9511352777481079 train acc 0.6957159140848165\n",
            "epoch 7 batch id 4561 loss 0.8203768730163574 train acc 0.6957903968427976\n",
            "epoch 7 batch id 4571 loss 0.9382444024085999 train acc 0.6959158280463793\n",
            "epoch 7 batch id 4581 loss 0.6841247081756592 train acc 0.6959554409517572\n",
            "epoch 7 batch id 4591 loss 0.8565624952316284 train acc 0.6959948812894794\n",
            "epoch 7 batch id 4601 loss 0.7776920199394226 train acc 0.6960647141925669\n",
            "epoch 7 batch id 4611 loss 0.7047467231750488 train acc 0.6961816851008458\n",
            "epoch 7 batch id 4621 loss 0.7396350502967834 train acc 0.6963015310538845\n",
            "epoch 7 batch id 4631 loss 0.6181260943412781 train acc 0.6963871194126539\n",
            "epoch 7 batch id 4641 loss 0.7704836130142212 train acc 0.6964555052790347\n",
            "epoch 7 batch id 4651 loss 0.8173477649688721 train acc 0.6964900021500753\n",
            "epoch 7 batch id 4661 loss 0.7874578833580017 train acc 0.6965545215618966\n",
            "epoch 7 batch id 4671 loss 0.7198258638381958 train acc 0.6965853136373368\n",
            "epoch 7 batch id 4681 loss 0.8026065230369568 train acc 0.6966960852381969\n",
            "epoch 7 batch id 4691 loss 0.5467883944511414 train acc 0.6967597527179706\n",
            "epoch 7 batch id 4701 loss 0.7410256862640381 train acc 0.6968198255690279\n",
            "epoch 7 batch id 4711 loss 0.4939577877521515 train acc 0.6969061770324771\n",
            "epoch 7 batch id 4721 loss 0.7919020652770996 train acc 0.6969723045964838\n",
            "epoch 7 batch id 4731 loss 0.5547552704811096 train acc 0.6969952177129571\n",
            "epoch 7 batch id 4741 loss 0.8002245426177979 train acc 0.6970246256064121\n",
            "epoch 7 batch id 4751 loss 0.6820579767227173 train acc 0.6971361292359504\n",
            "epoch 7 batch id 4761 loss 0.7860211133956909 train acc 0.6971848088636841\n",
            "epoch 7 batch id 4771 loss 0.828536868095398 train acc 0.697266034374345\n",
            "epoch 7 batch id 4781 loss 0.8053540587425232 train acc 0.6973175067977411\n",
            "epoch 7 batch id 4791 loss 0.8924170732498169 train acc 0.6973883322897099\n",
            "epoch 7 batch id 4801 loss 0.7559570074081421 train acc 0.6973807540095813\n",
            "epoch 7 batch id 4811 loss 0.7111097574234009 train acc 0.697431667013095\n",
            "epoch 7 batch id 4821 loss 0.8295244574546814 train acc 0.6974272713130056\n",
            "epoch 7 batch id 4831 loss 0.7766491174697876 train acc 0.6975166890912855\n",
            "epoch 7 batch id 4841 loss 0.807206392288208 train acc 0.6975476399504235\n",
            "epoch 7 batch id 4851 loss 0.7499108910560608 train acc 0.6976332199546486\n",
            "epoch 7 batch id 4861 loss 0.7603705525398254 train acc 0.6976991616951245\n",
            "epoch 7 batch id 4871 loss 0.9305775165557861 train acc 0.6977776637240813\n",
            "epoch 7 batch id 4881 loss 0.7299291491508484 train acc 0.6978462405244827\n",
            "epoch 7 batch id 4891 loss 0.7915375828742981 train acc 0.697908147618074\n",
            "epoch 7 batch id 4901 loss 0.8256560564041138 train acc 0.6979666139563354\n",
            "epoch 7 batch id 4911 loss 0.8864002823829651 train acc 0.6979612095296274\n",
            "epoch 7 batch id 4921 loss 0.8742374777793884 train acc 0.6979717029059135\n",
            "epoch 7 batch id 4931 loss 0.8494591116905212 train acc 0.6979663100790915\n",
            "epoch 7 batch id 4941 loss 0.678216278553009 train acc 0.698014698441611\n",
            "epoch 7 batch id 4951 loss 1.1538195610046387 train acc 0.6980692031912745\n",
            "epoch 7 train acc 0.6981569910319658\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9a7c8b3c4574ae79995724691db7fbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7 loss 0.8229501843452454 test acc 0.67746403042522\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdef1dadd86c4147834459566d64131d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8 batch id 1 loss 0.850211501121521 train acc 0.65625\n",
            "epoch 8 batch id 11 loss 0.7228322625160217 train acc 0.7357954545454546\n",
            "epoch 8 batch id 21 loss 0.593003511428833 train acc 0.7492559523809523\n",
            "epoch 8 batch id 31 loss 0.7333821058273315 train acc 0.734375\n",
            "epoch 8 batch id 41 loss 0.5579578876495361 train acc 0.7286585365853658\n",
            "epoch 8 batch id 51 loss 0.7043616771697998 train acc 0.7285539215686274\n",
            "epoch 8 batch id 61 loss 0.9427680373191833 train acc 0.7248975409836066\n",
            "epoch 8 batch id 71 loss 0.9460086822509766 train acc 0.7262323943661971\n",
            "epoch 8 batch id 81 loss 0.7839606404304504 train acc 0.7245370370370371\n",
            "epoch 8 batch id 91 loss 0.7348083257675171 train acc 0.7259615384615384\n",
            "epoch 8 batch id 101 loss 0.6542656421661377 train acc 0.7269492574257426\n",
            "epoch 8 batch id 111 loss 0.7592546343803406 train acc 0.7281813063063063\n",
            "epoch 8 batch id 121 loss 0.9939382672309875 train acc 0.7258522727272727\n",
            "epoch 8 batch id 131 loss 0.6826257109642029 train acc 0.7275763358778626\n",
            "epoch 8 batch id 141 loss 0.7044993042945862 train acc 0.727061170212766\n",
            "epoch 8 batch id 151 loss 0.6391339898109436 train acc 0.7273385761589404\n",
            "epoch 8 batch id 161 loss 0.9638728499412537 train acc 0.7291343167701864\n",
            "epoch 8 batch id 171 loss 0.8131222128868103 train acc 0.7281615497076024\n",
            "epoch 8 batch id 181 loss 0.5351867079734802 train acc 0.7286774861878453\n",
            "epoch 8 batch id 191 loss 0.9142076373100281 train acc 0.7279123036649214\n",
            "epoch 8 batch id 201 loss 0.6457754373550415 train acc 0.7293998756218906\n",
            "epoch 8 batch id 211 loss 0.8522900938987732 train acc 0.7274140995260664\n",
            "epoch 8 batch id 221 loss 0.8999710083007812 train acc 0.7273048642533937\n",
            "epoch 8 batch id 231 loss 0.746438205242157 train acc 0.7277462121212122\n",
            "epoch 8 batch id 241 loss 0.9091346263885498 train acc 0.7255575726141079\n",
            "epoch 8 batch id 251 loss 0.7611925601959229 train acc 0.7252863545816733\n",
            "epoch 8 batch id 261 loss 0.8455774188041687 train acc 0.7260536398467433\n",
            "epoch 8 batch id 271 loss 0.5767583847045898 train acc 0.7271102398523985\n",
            "epoch 8 batch id 281 loss 0.7331135272979736 train acc 0.7269795373665481\n",
            "epoch 8 batch id 291 loss 1.1021814346313477 train acc 0.7270189003436426\n",
            "epoch 8 batch id 301 loss 0.8328007459640503 train acc 0.7275747508305648\n",
            "epoch 8 batch id 311 loss 0.8263722658157349 train acc 0.727491961414791\n",
            "epoch 8 batch id 321 loss 0.8408873081207275 train acc 0.726927570093458\n",
            "epoch 8 batch id 331 loss 0.9192550182342529 train acc 0.7272469788519638\n",
            "epoch 8 batch id 341 loss 0.9682210087776184 train acc 0.7275476539589443\n",
            "epoch 8 batch id 351 loss 0.968038022518158 train acc 0.7274750712250713\n",
            "epoch 8 batch id 361 loss 0.7445026636123657 train acc 0.7271900969529086\n",
            "epoch 8 batch id 371 loss 0.9324659109115601 train acc 0.7274679919137467\n",
            "epoch 8 batch id 381 loss 0.5922966003417969 train acc 0.7271161417322834\n",
            "epoch 8 batch id 391 loss 1.0377777814865112 train acc 0.7267023657289002\n",
            "epoch 8 batch id 401 loss 0.5877222418785095 train acc 0.7264650872817955\n",
            "epoch 8 batch id 411 loss 0.698228120803833 train acc 0.7268476277372263\n",
            "epoch 8 batch id 421 loss 0.8842365145683289 train acc 0.7267666270783848\n",
            "epoch 8 batch id 431 loss 0.7515645623207092 train acc 0.726435614849188\n",
            "epoch 8 batch id 441 loss 0.84416264295578 train acc 0.7265802154195011\n",
            "epoch 8 batch id 451 loss 0.6133002042770386 train acc 0.7270995011086474\n",
            "epoch 8 batch id 461 loss 0.6383517384529114 train acc 0.7268167028199566\n",
            "epoch 8 batch id 471 loss 0.674778163433075 train acc 0.7277070063694268\n",
            "epoch 8 batch id 481 loss 0.9745761752128601 train acc 0.7267736486486487\n",
            "epoch 8 batch id 491 loss 0.7437665462493896 train acc 0.7266420570264766\n",
            "epoch 8 batch id 501 loss 1.2639573812484741 train acc 0.7259543413173652\n",
            "epoch 8 batch id 511 loss 0.6446401476860046 train acc 0.7260579745596869\n",
            "epoch 8 batch id 521 loss 0.7737518548965454 train acc 0.7263675623800384\n",
            "epoch 8 batch id 531 loss 0.7505365014076233 train acc 0.7264300847457628\n",
            "epoch 8 batch id 541 loss 0.7645547986030579 train acc 0.7269812846580407\n",
            "epoch 8 batch id 551 loss 0.6820896863937378 train acc 0.7265483212341198\n",
            "epoch 8 batch id 561 loss 0.6671958565711975 train acc 0.7269663547237076\n",
            "epoch 8 batch id 571 loss 0.8874340653419495 train acc 0.727260288966725\n",
            "epoch 8 batch id 581 loss 0.7435692548751831 train acc 0.7274365318416524\n",
            "epoch 8 batch id 591 loss 0.593743085861206 train acc 0.7276332487309645\n",
            "epoch 8 batch id 601 loss 0.5566142797470093 train acc 0.7277714226289518\n",
            "epoch 8 batch id 611 loss 0.6664016246795654 train acc 0.7281863747954174\n",
            "epoch 8 batch id 621 loss 0.8132758140563965 train acc 0.7284621578099839\n",
            "epoch 8 batch id 631 loss 0.8847153186798096 train acc 0.7282587163232963\n",
            "epoch 8 batch id 641 loss 0.757127583026886 train acc 0.7281347503900156\n",
            "epoch 8 batch id 651 loss 0.5711156725883484 train acc 0.7280865975422427\n",
            "epoch 8 batch id 661 loss 0.7686257362365723 train acc 0.7285835854765507\n",
            "epoch 8 batch id 671 loss 0.7879669666290283 train acc 0.7287630402384501\n",
            "epoch 8 batch id 681 loss 0.6409209966659546 train acc 0.7287766152716594\n",
            "epoch 8 batch id 691 loss 0.6214131116867065 train acc 0.7291063675832128\n",
            "epoch 8 batch id 701 loss 0.7964493632316589 train acc 0.7292706847360912\n",
            "epoch 8 batch id 711 loss 0.7062878012657166 train acc 0.7293424753867792\n",
            "epoch 8 batch id 721 loss 0.6768321394920349 train acc 0.729130547850208\n",
            "epoch 8 batch id 731 loss 0.6590165495872498 train acc 0.7293305403556771\n",
            "epoch 8 batch id 741 loss 0.8463163375854492 train acc 0.7295251349527665\n",
            "epoch 8 batch id 751 loss 0.7092785239219666 train acc 0.7295272969374168\n",
            "epoch 8 batch id 761 loss 0.6454755663871765 train acc 0.7296936596583443\n",
            "epoch 8 batch id 771 loss 0.8918077945709229 train acc 0.7296125162127107\n",
            "epoch 8 batch id 781 loss 0.8653833270072937 train acc 0.7295334507042254\n",
            "epoch 8 batch id 791 loss 0.6047547459602356 train acc 0.7293181099873578\n",
            "epoch 8 batch id 801 loss 0.8369927406311035 train acc 0.7295372971285893\n",
            "epoch 8 batch id 811 loss 0.7212817072868347 train acc 0.7297510789149199\n",
            "epoch 8 batch id 821 loss 0.5868063569068909 train acc 0.7299025578562729\n",
            "epoch 8 batch id 831 loss 0.774259090423584 train acc 0.7302008122743683\n",
            "epoch 8 batch id 841 loss 0.5760551691055298 train acc 0.7305105529131986\n",
            "epoch 8 batch id 851 loss 0.7861764430999756 train acc 0.7301153055229143\n",
            "epoch 8 batch id 861 loss 0.9007502794265747 train acc 0.7298199767711963\n",
            "epoch 8 batch id 871 loss 0.5718346238136292 train acc 0.730087543053961\n",
            "epoch 8 batch id 881 loss 0.6323718428611755 train acc 0.7301716799091941\n",
            "epoch 8 batch id 891 loss 0.7079484462738037 train acc 0.7303766835016835\n",
            "epoch 8 batch id 901 loss 0.8975324630737305 train acc 0.7304210599334073\n",
            "epoch 8 batch id 911 loss 0.9037570357322693 train acc 0.7303787047200878\n",
            "epoch 8 batch id 921 loss 0.9598876237869263 train acc 0.7301336862106406\n",
            "epoch 8 batch id 931 loss 0.5811128616333008 train acc 0.7299610633727175\n",
            "epoch 8 batch id 941 loss 0.6201790571212769 train acc 0.7303732731137088\n",
            "epoch 8 batch id 951 loss 0.9485000967979431 train acc 0.7301689011566772\n",
            "epoch 8 batch id 961 loss 0.7526920437812805 train acc 0.7300663371488033\n",
            "epoch 8 batch id 971 loss 0.9247663021087646 train acc 0.7301589855818743\n",
            "epoch 8 batch id 981 loss 0.752457320690155 train acc 0.730440876656473\n",
            "epoch 8 batch id 991 loss 0.615212082862854 train acc 0.7304805751765893\n",
            "epoch 8 batch id 1001 loss 0.8411068916320801 train acc 0.7304726523476524\n",
            "epoch 8 batch id 1011 loss 0.8299950361251831 train acc 0.7306503461918892\n",
            "epoch 8 batch id 1021 loss 0.5575428009033203 train acc 0.730855166503428\n",
            "epoch 8 batch id 1031 loss 0.8979626893997192 train acc 0.7306922890397672\n",
            "epoch 8 batch id 1041 loss 0.7531787753105164 train acc 0.7310578770413064\n",
            "epoch 8 batch id 1051 loss 0.6013398766517639 train acc 0.7310002378686965\n",
            "epoch 8 batch id 1061 loss 0.8569848537445068 train acc 0.7311056786050896\n",
            "epoch 8 batch id 1071 loss 0.6200408339500427 train acc 0.7311070261437909\n",
            "epoch 8 batch id 1081 loss 0.7319902777671814 train acc 0.7308915356151712\n",
            "epoch 8 batch id 1091 loss 0.9944308400154114 train acc 0.7308088909257562\n",
            "epoch 8 batch id 1101 loss 0.6984274983406067 train acc 0.730855472297911\n",
            "epoch 8 batch id 1111 loss 0.7169957756996155 train acc 0.730676192619262\n",
            "epoch 8 batch id 1121 loss 0.5274600982666016 train acc 0.7307091882247992\n",
            "epoch 8 batch id 1131 loss 0.9130060076713562 train acc 0.7306587091069849\n",
            "epoch 8 batch id 1141 loss 0.7127019166946411 train acc 0.7308008326029798\n",
            "epoch 8 batch id 1151 loss 0.8422145247459412 train acc 0.7310355125977411\n",
            "epoch 8 batch id 1161 loss 0.760499119758606 train acc 0.7310777347114557\n",
            "epoch 8 batch id 1171 loss 0.6308620572090149 train acc 0.7314261315115286\n",
            "epoch 8 batch id 1181 loss 0.742599368095398 train acc 0.7313584885690093\n",
            "epoch 8 batch id 1191 loss 0.6516211628913879 train acc 0.7315543660789253\n",
            "epoch 8 batch id 1201 loss 0.9056680202484131 train acc 0.7318640716069942\n",
            "epoch 8 batch id 1211 loss 0.5402027368545532 train acc 0.7319364161849711\n",
            "epoch 8 batch id 1221 loss 0.6436294317245483 train acc 0.7318540131040131\n",
            "epoch 8 batch id 1231 loss 0.9413321018218994 train acc 0.7317094841592201\n",
            "epoch 8 batch id 1241 loss 0.6601579785346985 train acc 0.7317561442385173\n",
            "epoch 8 batch id 1251 loss 0.5087502002716064 train acc 0.7316771582733813\n",
            "epoch 8 batch id 1261 loss 0.8584931492805481 train acc 0.7317481165741475\n",
            "epoch 8 batch id 1271 loss 0.6491608619689941 train acc 0.7314983280881195\n",
            "epoch 8 batch id 1281 loss 0.7737125158309937 train acc 0.731447599531616\n",
            "epoch 8 batch id 1291 loss 0.7713976502418518 train acc 0.7314944810224632\n",
            "epoch 8 batch id 1301 loss 0.9289799928665161 train acc 0.7314205418908531\n",
            "epoch 8 batch id 1311 loss 1.0162709951400757 train acc 0.7312047101449275\n",
            "epoch 8 batch id 1321 loss 0.7012064456939697 train acc 0.7312641937925813\n",
            "epoch 8 batch id 1331 loss 0.6675007343292236 train acc 0.7313814800901578\n",
            "epoch 8 batch id 1341 loss 0.7491095066070557 train acc 0.7313455443698732\n",
            "epoch 8 batch id 1351 loss 0.6284319758415222 train acc 0.7313795336787565\n",
            "epoch 8 batch id 1361 loss 0.8785642385482788 train acc 0.731332659808964\n",
            "epoch 8 batch id 1371 loss 0.5434140563011169 train acc 0.7314004376367614\n",
            "epoch 8 batch id 1381 loss 0.8414495587348938 train acc 0.7312748913830558\n",
            "epoch 8 batch id 1391 loss 0.4982505142688751 train acc 0.7314319734004313\n",
            "epoch 8 batch id 1401 loss 0.5759946703910828 train acc 0.7315422019985724\n",
            "epoch 8 batch id 1411 loss 0.6169493198394775 train acc 0.7316508681785967\n",
            "epoch 8 batch id 1421 loss 0.629480242729187 train acc 0.7316370513722731\n",
            "epoch 8 batch id 1431 loss 0.619091808795929 train acc 0.7316889412997903\n",
            "epoch 8 batch id 1441 loss 0.7126798033714294 train acc 0.7316425225537821\n",
            "epoch 8 batch id 1451 loss 0.701133131980896 train acc 0.731769038594073\n",
            "epoch 8 batch id 1461 loss 0.6529412865638733 train acc 0.731733401779603\n",
            "epoch 8 batch id 1471 loss 0.9404755234718323 train acc 0.7317194935418083\n",
            "epoch 8 batch id 1481 loss 0.6224991679191589 train acc 0.7317690749493585\n",
            "epoch 8 batch id 1491 loss 0.7863737344741821 train acc 0.7317236753856472\n",
            "epoch 8 batch id 1501 loss 0.7198418974876404 train acc 0.7319495336442372\n",
            "epoch 8 batch id 1511 loss 0.5496336221694946 train acc 0.7320793348775645\n",
            "epoch 8 batch id 1521 loss 0.6885648369789124 train acc 0.7322998849441157\n",
            "epoch 8 batch id 1531 loss 0.8141142725944519 train acc 0.7322828216851731\n",
            "epoch 8 batch id 1541 loss 0.8072401285171509 train acc 0.7323775146009085\n",
            "epoch 8 batch id 1551 loss 0.6201748251914978 train acc 0.7324709864603481\n",
            "epoch 8 batch id 1561 loss 0.6734391450881958 train acc 0.7325432415118514\n",
            "epoch 8 batch id 1571 loss 0.6281924843788147 train acc 0.732733927434755\n",
            "epoch 8 batch id 1581 loss 0.758955717086792 train acc 0.732764073371284\n",
            "epoch 8 batch id 1591 loss 0.6654371023178101 train acc 0.7328429446888749\n",
            "epoch 8 batch id 1601 loss 0.8944985866546631 train acc 0.73295986883198\n",
            "epoch 8 batch id 1611 loss 0.5959327816963196 train acc 0.7331529329608939\n",
            "epoch 8 batch id 1621 loss 0.6764007210731506 train acc 0.7329966070326959\n",
            "epoch 8 batch id 1631 loss 0.6791836619377136 train acc 0.7329763182096873\n",
            "epoch 8 batch id 1641 loss 0.6142870187759399 train acc 0.7330800578915295\n",
            "epoch 8 batch id 1651 loss 0.7794129848480225 train acc 0.7330216535433071\n",
            "epoch 8 batch id 1661 loss 0.8546246290206909 train acc 0.7330674292594822\n",
            "epoch 8 batch id 1671 loss 0.8326358199119568 train acc 0.73308460502693\n",
            "epoch 8 batch id 1681 loss 0.8517217040061951 train acc 0.7329900356930399\n",
            "epoch 8 batch id 1691 loss 0.7769150137901306 train acc 0.7331553075103489\n",
            "epoch 8 batch id 1701 loss 0.6326596140861511 train acc 0.7332084068195179\n",
            "epoch 8 batch id 1711 loss 0.7833545207977295 train acc 0.7332152250146113\n",
            "epoch 8 batch id 1721 loss 0.8365787267684937 train acc 0.7333581493317839\n",
            "epoch 8 batch id 1731 loss 0.45849084854125977 train acc 0.7333911034084344\n",
            "epoch 8 batch id 1741 loss 0.7168306708335876 train acc 0.7333608558299828\n",
            "epoch 8 batch id 1751 loss 0.5460634231567383 train acc 0.7333844945745288\n",
            "epoch 8 batch id 1761 loss 0.5854657292366028 train acc 0.7334522288472459\n",
            "epoch 8 batch id 1771 loss 0.683784008026123 train acc 0.7335544889892716\n",
            "epoch 8 batch id 1781 loss 0.7599716782569885 train acc 0.7336029618192027\n",
            "epoch 8 batch id 1791 loss 1.0566956996917725 train acc 0.7336159966499163\n",
            "epoch 8 batch id 1801 loss 0.6372946500778198 train acc 0.7336028595224875\n",
            "epoch 8 batch id 1811 loss 0.6012421250343323 train acc 0.7336934014356709\n",
            "epoch 8 batch id 1821 loss 0.7427738308906555 train acc 0.7338001098297638\n",
            "epoch 8 batch id 1831 loss 0.5966169238090515 train acc 0.7338885854724194\n",
            "epoch 8 batch id 1841 loss 0.5073899626731873 train acc 0.7340609722976643\n",
            "epoch 8 batch id 1851 loss 0.7168270349502563 train acc 0.7340711102106969\n",
            "epoch 8 batch id 1861 loss 0.8141247034072876 train acc 0.7341902874798495\n",
            "epoch 8 batch id 1871 loss 0.8868153095245361 train acc 0.7343833511491181\n",
            "epoch 8 batch id 1881 loss 0.6585439443588257 train acc 0.7343417729930888\n",
            "epoch 8 batch id 1891 loss 0.6345139145851135 train acc 0.7345485193019566\n",
            "epoch 8 batch id 1901 loss 0.6955097913742065 train acc 0.7345065097317202\n",
            "epoch 8 batch id 1911 loss 0.5674207210540771 train acc 0.7344894688644689\n",
            "epoch 8 batch id 1921 loss 0.7346421480178833 train acc 0.7342692608016658\n",
            "epoch 8 batch id 1931 loss 0.7035933136940002 train acc 0.734585383221129\n",
            "epoch 8 batch id 1941 loss 0.5935349464416504 train acc 0.7347452988150438\n",
            "epoch 8 batch id 1951 loss 0.8429990410804749 train acc 0.7347674269605331\n",
            "epoch 8 batch id 1961 loss 0.5831521153450012 train acc 0.7349327511473738\n",
            "epoch 8 batch id 1971 loss 0.7241828441619873 train acc 0.7349299213597159\n",
            "epoch 8 batch id 1981 loss 0.7191075086593628 train acc 0.7350059944472489\n",
            "epoch 8 batch id 1991 loss 0.807662844657898 train acc 0.7350656077348067\n",
            "epoch 8 batch id 2001 loss 0.9215919375419617 train acc 0.7350777736131934\n",
            "epoch 8 batch id 2011 loss 0.6720106601715088 train acc 0.735097588264545\n",
            "epoch 8 batch id 2021 loss 0.819824755191803 train acc 0.7351635947550718\n",
            "epoch 8 batch id 2031 loss 0.5256254076957703 train acc 0.7351981782373215\n",
            "epoch 8 batch id 2041 loss 0.5402417778968811 train acc 0.735201800587947\n",
            "epoch 8 batch id 2051 loss 0.7480496168136597 train acc 0.7352663334958557\n",
            "epoch 8 batch id 2061 loss 0.8172987103462219 train acc 0.735269590004852\n",
            "epoch 8 batch id 2071 loss 0.7240152955055237 train acc 0.7353029937228392\n",
            "epoch 8 batch id 2081 loss 0.9933825135231018 train acc 0.7351708913983662\n",
            "epoch 8 batch id 2091 loss 0.8301452398300171 train acc 0.735159612625538\n",
            "epoch 8 batch id 2101 loss 0.5713380575180054 train acc 0.7352451213707758\n",
            "epoch 8 batch id 2111 loss 0.5729677081108093 train acc 0.73531501657982\n",
            "epoch 8 batch id 2121 loss 0.37623947858810425 train acc 0.735376885902876\n",
            "epoch 8 batch id 2131 loss 0.9222033023834229 train acc 0.735452839042703\n",
            "epoch 8 batch id 2141 loss 0.8391149044036865 train acc 0.7354186127977581\n",
            "epoch 8 batch id 2151 loss 0.7124571204185486 train acc 0.7354791376104137\n",
            "epoch 8 batch id 2161 loss 0.861706018447876 train acc 0.735408954187876\n",
            "epoch 8 batch id 2171 loss 0.6901094913482666 train acc 0.735497754491018\n",
            "epoch 8 batch id 2181 loss 0.7919825315475464 train acc 0.735471114167813\n",
            "epoch 8 batch id 2191 loss 0.5316066741943359 train acc 0.7355588201734368\n",
            "epoch 8 batch id 2201 loss 0.7837202548980713 train acc 0.7356528282598819\n",
            "epoch 8 batch id 2211 loss 0.6610341668128967 train acc 0.7356329149706016\n",
            "epoch 8 batch id 2221 loss 0.8885775208473206 train acc 0.7356202161188654\n",
            "epoch 8 batch id 2231 loss 0.6694281101226807 train acc 0.7356566562079785\n",
            "epoch 8 batch id 2241 loss 0.5927559733390808 train acc 0.7357346050870147\n",
            "epoch 8 batch id 2251 loss 0.7364374995231628 train acc 0.7357702132385606\n",
            "epoch 8 batch id 2261 loss 0.6150614023208618 train acc 0.7357709531180894\n",
            "epoch 8 batch id 2271 loss 0.7649920582771301 train acc 0.7358404887714664\n",
            "epoch 8 batch id 2281 loss 0.9031452536582947 train acc 0.7360669662428759\n",
            "epoch 8 batch id 2291 loss 0.6624050736427307 train acc 0.7362710061108686\n",
            "epoch 8 batch id 2301 loss 0.7563085556030273 train acc 0.7363170903954802\n",
            "epoch 8 batch id 2311 loss 0.7994135022163391 train acc 0.7363492535698831\n",
            "epoch 8 batch id 2321 loss 0.4419810473918915 train acc 0.7362936234381732\n",
            "epoch 8 batch id 2331 loss 0.6854114532470703 train acc 0.7361714392964392\n",
            "epoch 8 batch id 2341 loss 0.8248729705810547 train acc 0.7363172789406237\n",
            "epoch 8 batch id 2351 loss 0.6756397485733032 train acc 0.736282433007231\n",
            "epoch 8 batch id 2361 loss 0.8105254769325256 train acc 0.7363140618382041\n",
            "epoch 8 batch id 2371 loss 0.5760706663131714 train acc 0.7363915541965416\n",
            "epoch 8 batch id 2381 loss 0.7977050542831421 train acc 0.73647495800084\n",
            "epoch 8 batch id 2391 loss 0.7232382297515869 train acc 0.736440035549979\n",
            "epoch 8 batch id 2401 loss 0.7732406258583069 train acc 0.7365225426905456\n",
            "epoch 8 batch id 2411 loss 0.6600204706192017 train acc 0.7366043654085441\n",
            "epoch 8 batch id 2421 loss 0.7058389186859131 train acc 0.7366532424617926\n",
            "epoch 8 batch id 2431 loss 0.766381561756134 train acc 0.7366181612505142\n",
            "epoch 8 batch id 2441 loss 0.6440433859825134 train acc 0.7367625972961901\n",
            "epoch 8 batch id 2451 loss 0.7298412322998047 train acc 0.7367719808241534\n",
            "epoch 8 batch id 2461 loss 0.8819131851196289 train acc 0.7367685900040634\n",
            "epoch 8 batch id 2471 loss 0.9244376420974731 train acc 0.7366703763658438\n",
            "epoch 8 batch id 2481 loss 0.8210591673851013 train acc 0.7366359330914953\n",
            "epoch 8 batch id 2491 loss 0.6893883943557739 train acc 0.736582948615014\n",
            "epoch 8 batch id 2501 loss 0.5846968293190002 train acc 0.7366990703718512\n",
            "epoch 8 batch id 2511 loss 0.7160927653312683 train acc 0.736857825567503\n",
            "epoch 8 batch id 2521 loss 0.6584912538528442 train acc 0.7367488099960333\n",
            "epoch 8 batch id 2531 loss 0.8611705899238586 train acc 0.7368135124456736\n",
            "epoch 8 batch id 2541 loss 0.7302973866462708 train acc 0.7368900039354584\n",
            "epoch 8 batch id 2551 loss 0.819730281829834 train acc 0.7368556448451588\n",
            "epoch 8 batch id 2561 loss 0.8125455975532532 train acc 0.7367971495509567\n",
            "epoch 8 batch id 2571 loss 0.6751556992530823 train acc 0.7368971217425127\n",
            "epoch 8 batch id 2581 loss 0.7985255718231201 train acc 0.7368812960092987\n",
            "epoch 8 batch id 2591 loss 0.6511054635047913 train acc 0.7369078058664609\n",
            "epoch 8 batch id 2601 loss 0.6698663830757141 train acc 0.7369281045751634\n",
            "epoch 8 batch id 2611 loss 0.6376670002937317 train acc 0.7369422635005745\n",
            "epoch 8 batch id 2621 loss 0.6851330399513245 train acc 0.7368966997329264\n",
            "epoch 8 batch id 2631 loss 0.7594285607337952 train acc 0.7369702584568605\n",
            "epoch 8 batch id 2641 loss 0.6206449270248413 train acc 0.7370550927678909\n",
            "epoch 8 batch id 2651 loss 0.6354240775108337 train acc 0.7371274990569596\n",
            "epoch 8 batch id 2661 loss 0.7840865254402161 train acc 0.737140642615558\n",
            "epoch 8 batch id 2671 loss 0.6852312088012695 train acc 0.737182937102209\n",
            "epoch 8 batch id 2681 loss 0.632099986076355 train acc 0.7372016038791496\n",
            "epoch 8 batch id 2691 loss 0.8851556777954102 train acc 0.7371040040876997\n",
            "epoch 8 batch id 2701 loss 0.804215133190155 train acc 0.7371112550907072\n",
            "epoch 8 batch id 2711 loss 0.7059665322303772 train acc 0.7371184526005164\n",
            "epoch 8 batch id 2721 loss 0.4439789652824402 train acc 0.7371945056964352\n",
            "epoch 8 batch id 2731 loss 0.58339524269104 train acc 0.7371612962284877\n",
            "epoch 8 batch id 2741 loss 0.7136844396591187 train acc 0.7372138361911711\n",
            "epoch 8 batch id 2751 loss 0.6119467616081238 train acc 0.7372319156670302\n",
            "epoch 8 batch id 2761 loss 0.6313436031341553 train acc 0.7372328866352771\n",
            "epoch 8 batch id 2771 loss 0.7117897272109985 train acc 0.737228211836882\n",
            "epoch 8 batch id 2781 loss 0.8625108599662781 train acc 0.737403362099964\n",
            "epoch 8 batch id 2791 loss 0.7318210601806641 train acc 0.7374317001074884\n",
            "epoch 8 batch id 2801 loss 0.7312465906143188 train acc 0.7374431006783292\n",
            "epoch 8 batch id 2811 loss 0.5963090658187866 train acc 0.7375433564567769\n",
            "epoch 8 batch id 2821 loss 0.7093750238418579 train acc 0.7375875132931584\n",
            "epoch 8 batch id 2831 loss 0.598193883895874 train acc 0.7377141469445425\n",
            "epoch 8 batch id 2841 loss 0.7131917476654053 train acc 0.7378288894755368\n",
            "epoch 8 batch id 2851 loss 0.844411313533783 train acc 0.7377071641529288\n",
            "epoch 8 batch id 2861 loss 0.4972038269042969 train acc 0.7377883607130374\n",
            "epoch 8 batch id 2871 loss 0.5052428841590881 train acc 0.7377982410309997\n",
            "epoch 8 batch id 2881 loss 0.6802107095718384 train acc 0.7379002516487331\n",
            "epoch 8 batch id 2891 loss 0.5933042764663696 train acc 0.7378826530612245\n",
            "epoch 8 batch id 2901 loss 0.689836859703064 train acc 0.7378813340227508\n",
            "epoch 8 batch id 2911 loss 0.5598576068878174 train acc 0.7380088457574716\n",
            "epoch 8 batch id 2921 loss 0.4509286880493164 train acc 0.7379803577541938\n",
            "epoch 8 batch id 2931 loss 0.5588470101356506 train acc 0.7379360713067212\n",
            "epoch 8 batch id 2941 loss 0.72175532579422 train acc 0.7379717783066984\n",
            "epoch 8 batch id 2951 loss 0.6800283789634705 train acc 0.7381131396136903\n",
            "epoch 8 batch id 2961 loss 0.6030412316322327 train acc 0.7381743920972644\n",
            "epoch 8 batch id 2971 loss 0.6277904510498047 train acc 0.738277305621003\n",
            "epoch 8 batch id 2981 loss 0.721390962600708 train acc 0.7383480795035223\n",
            "epoch 8 batch id 2991 loss 0.5721110105514526 train acc 0.7384549481778669\n",
            "epoch 8 batch id 3001 loss 0.661910891532898 train acc 0.7385506914361879\n",
            "epoch 8 batch id 3011 loss 0.6463764905929565 train acc 0.7385731484556626\n",
            "epoch 8 batch id 3021 loss 0.6298929452896118 train acc 0.7386316617014234\n",
            "epoch 8 batch id 3031 loss 0.6346673369407654 train acc 0.7386382382052128\n",
            "epoch 8 batch id 3041 loss 1.1246895790100098 train acc 0.7385522854324236\n",
            "epoch 8 batch id 3051 loss 0.7210094928741455 train acc 0.7385642002622091\n",
            "epoch 8 batch id 3061 loss 0.4978111982345581 train acc 0.7386015599477295\n",
            "epoch 8 batch id 3071 loss 0.684006929397583 train acc 0.7386234125691957\n",
            "epoch 8 batch id 3081 loss 0.9210915565490723 train acc 0.7386400519311912\n",
            "epoch 8 batch id 3091 loss 0.8413903713226318 train acc 0.7386211986412164\n",
            "epoch 8 batch id 3101 loss 0.740018367767334 train acc 0.7386780474040632\n",
            "epoch 8 batch id 3111 loss 0.7786746025085449 train acc 0.7387144406943105\n",
            "epoch 8 batch id 3121 loss 0.8211974501609802 train acc 0.7387806392181993\n",
            "epoch 8 batch id 3131 loss 0.6571632623672485 train acc 0.7388264532098371\n",
            "epoch 8 batch id 3141 loss 0.71048504114151 train acc 0.7388272047118752\n",
            "epoch 8 batch id 3151 loss 0.5788288116455078 train acc 0.7389568787686449\n",
            "epoch 8 batch id 3161 loss 0.5527995824813843 train acc 0.7388880101233787\n",
            "epoch 8 batch id 3171 loss 0.6177569031715393 train acc 0.7388984153263954\n",
            "epoch 8 batch id 3181 loss 0.6103741526603699 train acc 0.7389333149952845\n",
            "epoch 8 batch id 3191 loss 0.5191230773925781 train acc 0.7389973754308994\n",
            "epoch 8 batch id 3201 loss 0.8086979389190674 train acc 0.7389682911590129\n",
            "epoch 8 batch id 3211 loss 0.7249029874801636 train acc 0.7390269775770788\n",
            "epoch 8 batch id 3221 loss 1.022226095199585 train acc 0.7390270878609128\n",
            "epoch 8 batch id 3231 loss 0.4615345597267151 train acc 0.7391287527081399\n",
            "epoch 8 batch id 3241 loss 0.7260093092918396 train acc 0.7391237272446776\n",
            "epoch 8 batch id 3251 loss 0.5692145228385925 train acc 0.7391764072593048\n",
            "epoch 8 batch id 3261 loss 0.5801148414611816 train acc 0.7391952238577124\n",
            "epoch 8 batch id 3271 loss 0.4759219288825989 train acc 0.7391661571384898\n",
            "epoch 8 batch id 3281 loss 0.8446648120880127 train acc 0.7391563166717464\n",
            "epoch 8 batch id 3291 loss 0.6354754567146301 train acc 0.7392842221209359\n",
            "epoch 8 batch id 3301 loss 0.5030387043952942 train acc 0.7393545516510148\n",
            "epoch 8 batch id 3311 loss 0.8936317563056946 train acc 0.739381984294775\n",
            "epoch 8 batch id 3321 loss 0.7126897573471069 train acc 0.7394092517314061\n",
            "epoch 8 batch id 3331 loss 0.8955501317977905 train acc 0.739455118583008\n",
            "epoch 8 batch id 3341 loss 0.8052999973297119 train acc 0.739421206225681\n",
            "epoch 8 batch id 3351 loss 0.550588846206665 train acc 0.7395133915249179\n",
            "epoch 8 batch id 3361 loss 0.6703976988792419 train acc 0.7396096771794108\n",
            "epoch 8 batch id 3371 loss 0.8204763531684875 train acc 0.7396173242361317\n",
            "epoch 8 batch id 3381 loss 0.5694284439086914 train acc 0.7396387902987281\n",
            "epoch 8 batch id 3391 loss 0.8052573204040527 train acc 0.7396094441167798\n",
            "epoch 8 batch id 3401 loss 0.7484654784202576 train acc 0.7396767494854455\n",
            "epoch 8 batch id 3411 loss 0.6306969523429871 train acc 0.7397207563764292\n",
            "epoch 8 batch id 3421 loss 0.8740168809890747 train acc 0.7397919102601579\n",
            "epoch 8 batch id 3431 loss 0.627330482006073 train acc 0.7398125546487905\n",
            "epoch 8 batch id 3441 loss 0.6828884482383728 train acc 0.7398739465271723\n",
            "epoch 8 batch id 3451 loss 0.6667067408561707 train acc 0.7399078165749058\n",
            "epoch 8 batch id 3461 loss 0.7347133755683899 train acc 0.7400453264952326\n",
            "epoch 8 batch id 3471 loss 0.7041650414466858 train acc 0.7401370282339383\n",
            "epoch 8 batch id 3481 loss 0.6557635068893433 train acc 0.7401653619649526\n",
            "epoch 8 batch id 3491 loss 0.5060439705848694 train acc 0.7402069607562303\n",
            "epoch 8 batch id 3501 loss 0.7114791870117188 train acc 0.7402661739502999\n",
            "epoch 8 batch id 3511 loss 0.507734477519989 train acc 0.7403562019367701\n",
            "epoch 8 batch id 3521 loss 0.8810684680938721 train acc 0.7404368432263562\n",
            "epoch 8 batch id 3531 loss 1.0985710620880127 train acc 0.7403931251770037\n",
            "epoch 8 batch id 3541 loss 0.46725428104400635 train acc 0.7405482208415701\n",
            "epoch 8 batch id 3551 loss 0.8274437189102173 train acc 0.7406496409462123\n",
            "epoch 8 batch id 3561 loss 0.46548452973365784 train acc 0.7407592670598147\n",
            "epoch 8 batch id 3571 loss 0.8960392475128174 train acc 0.7407851442173061\n",
            "epoch 8 batch id 3581 loss 0.7050274610519409 train acc 0.7407628804803128\n",
            "epoch 8 batch id 3591 loss 0.7762685418128967 train acc 0.7408234126984127\n",
            "epoch 8 batch id 3601 loss 0.6523181796073914 train acc 0.7408662524298806\n",
            "epoch 8 batch id 3611 loss 0.6228479146957397 train acc 0.7409348172251454\n",
            "epoch 8 batch id 3621 loss 0.5566213130950928 train acc 0.7409857428887048\n",
            "epoch 8 batch id 3631 loss 0.573112964630127 train acc 0.7409933558248416\n",
            "epoch 8 batch id 3641 loss 0.8244719505310059 train acc 0.7410009269431475\n",
            "epoch 8 batch id 3651 loss 0.6743344068527222 train acc 0.7410598123801698\n",
            "epoch 8 batch id 3661 loss 0.6353147029876709 train acc 0.7411354479650368\n",
            "epoch 8 batch id 3671 loss 0.6587154865264893 train acc 0.7411893898120403\n",
            "epoch 8 batch id 3681 loss 0.9197145104408264 train acc 0.741285486280902\n",
            "epoch 8 batch id 3691 loss 0.6704556941986084 train acc 0.7413344960715254\n",
            "epoch 8 batch id 3701 loss 0.4739968478679657 train acc 0.7413621318562551\n",
            "epoch 8 batch id 3711 loss 0.6903943419456482 train acc 0.7414275127997845\n",
            "epoch 8 batch id 3721 loss 0.5716411471366882 train acc 0.7414925423273314\n",
            "epoch 8 batch id 3731 loss 0.4968363046646118 train acc 0.7415865384615384\n",
            "epoch 8 batch id 3741 loss 0.8135449290275574 train acc 0.7415505546645282\n",
            "epoch 8 batch id 3751 loss 0.7281768321990967 train acc 0.7416022394028259\n",
            "epoch 8 batch id 3761 loss 0.7449766397476196 train acc 0.7416578037755917\n",
            "epoch 8 batch id 3771 loss 0.5681069493293762 train acc 0.7416674953593211\n",
            "epoch 8 batch id 3781 loss 0.6206510663032532 train acc 0.7417680507802169\n",
            "epoch 8 batch id 3791 loss 0.4568699300289154 train acc 0.74181037325244\n",
            "epoch 8 batch id 3801 loss 0.6751558184623718 train acc 0.7418935806366745\n",
            "epoch 8 batch id 3811 loss 0.7319909334182739 train acc 0.741918951718709\n",
            "epoch 8 batch id 3821 loss 0.589043378829956 train acc 0.741993260926459\n",
            "epoch 8 batch id 3831 loss 0.720223605632782 train acc 0.742038632210911\n",
            "epoch 8 batch id 3841 loss 0.542527437210083 train acc 0.7421081749544389\n",
            "epoch 8 batch id 3851 loss 0.5884146690368652 train acc 0.7422219877953778\n",
            "epoch 8 batch id 3861 loss 0.5896654725074768 train acc 0.7422461797461798\n",
            "epoch 8 batch id 3871 loss 0.708565354347229 train acc 0.7423025381038492\n",
            "epoch 8 batch id 3881 loss 0.9094459414482117 train acc 0.7423545800051533\n",
            "epoch 8 batch id 3891 loss 0.7212348580360413 train acc 0.7423902916987921\n",
            "epoch 8 batch id 3901 loss 0.5674883723258972 train acc 0.742389771853371\n",
            "epoch 8 batch id 3911 loss 0.47281795740127563 train acc 0.7424132255177703\n",
            "epoch 8 batch id 3921 loss 0.6198932528495789 train acc 0.7424246046926805\n",
            "epoch 8 batch id 3931 loss 0.6902564167976379 train acc 0.7424240015263291\n",
            "epoch 8 batch id 3941 loss 0.6105592250823975 train acc 0.742411507231667\n",
            "epoch 8 batch id 3951 loss 0.6441810727119446 train acc 0.7424623513034675\n",
            "epoch 8 batch id 3961 loss 0.8358138799667358 train acc 0.742441933855087\n",
            "epoch 8 batch id 3971 loss 0.6446459293365479 train acc 0.7425554016620498\n",
            "epoch 8 batch id 3981 loss 0.6200363039970398 train acc 0.7426015762371263\n",
            "epoch 8 batch id 3991 loss 0.8324956893920898 train acc 0.7426357742420446\n",
            "epoch 8 batch id 4001 loss 0.6255092024803162 train acc 0.7426307485628593\n",
            "epoch 8 batch id 4011 loss 0.6987852454185486 train acc 0.7426140613313388\n",
            "epoch 8 batch id 4021 loss 0.6397782564163208 train acc 0.7426285438945536\n",
            "epoch 8 batch id 4031 loss 0.7141246795654297 train acc 0.742681716695609\n",
            "epoch 8 batch id 4041 loss 0.76185542345047 train acc 0.7426920935412027\n",
            "epoch 8 batch id 4051 loss 0.6477735042572021 train acc 0.7427911318193039\n",
            "epoch 8 batch id 4061 loss 0.7418550252914429 train acc 0.7428819871952721\n",
            "epoch 8 batch id 4071 loss 0.9775805473327637 train acc 0.7429186624907885\n",
            "epoch 8 batch id 4081 loss 0.46682921051979065 train acc 0.7429704729233031\n",
            "epoch 8 batch id 4091 loss 0.5342463850975037 train acc 0.7430373075042777\n",
            "epoch 8 batch id 4101 loss 0.8433297872543335 train acc 0.7430314252621312\n",
            "epoch 8 batch id 4111 loss 0.49332353472709656 train acc 0.7431015872050596\n",
            "epoch 8 batch id 4121 loss 0.6203203797340393 train acc 0.7431221184178597\n",
            "epoch 8 batch id 4131 loss 0.4718363285064697 train acc 0.7431160735899298\n",
            "epoch 8 batch id 4141 loss 0.7152822017669678 train acc 0.7431779763342188\n",
            "epoch 8 batch id 4151 loss 0.5855799317359924 train acc 0.743243344977114\n",
            "epoch 8 batch id 4161 loss 0.9227105975151062 train acc 0.7432408074981975\n",
            "epoch 8 batch id 4171 loss 0.6860893964767456 train acc 0.7433169503716135\n",
            "epoch 8 batch id 4181 loss 0.4441733658313751 train acc 0.743276877541258\n",
            "epoch 8 batch id 4191 loss 0.8159223198890686 train acc 0.7433227451682176\n",
            "epoch 8 batch id 4201 loss 0.6051649451255798 train acc 0.743379552487503\n",
            "epoch 8 batch id 4211 loss 0.7186182141304016 train acc 0.7434249584421753\n",
            "epoch 8 batch id 4221 loss 0.6395224928855896 train acc 0.7434331319592513\n",
            "epoch 8 batch id 4231 loss 0.6006736755371094 train acc 0.7434745036634366\n",
            "epoch 8 batch id 4241 loss 0.6991482377052307 train acc 0.7435598915350153\n",
            "epoch 8 batch id 4251 loss 0.6825681924819946 train acc 0.7436889849447189\n",
            "epoch 8 batch id 4261 loss 0.7246308326721191 train acc 0.7437221309551748\n",
            "epoch 8 batch id 4271 loss 0.5423631072044373 train acc 0.7438319480215406\n",
            "epoch 8 batch id 4281 loss 0.6867164373397827 train acc 0.7438792046250876\n",
            "epoch 8 batch id 4291 loss 0.6140981316566467 train acc 0.7439808611046376\n",
            "epoch 8 batch id 4301 loss 0.4271117150783539 train acc 0.7440057544757033\n",
            "epoch 8 batch id 4311 loss 0.7090798616409302 train acc 0.7440522790535838\n",
            "epoch 8 batch id 4321 loss 0.617759644985199 train acc 0.7440009546401296\n",
            "epoch 8 batch id 4331 loss 0.4873299300670624 train acc 0.7441230374047564\n",
            "epoch 8 batch id 4341 loss 0.6163676381111145 train acc 0.7441077804653305\n",
            "epoch 8 batch id 4351 loss 0.6712126731872559 train acc 0.7441464605837739\n",
            "epoch 8 batch id 4361 loss 0.9140515327453613 train acc 0.7442172093556524\n",
            "epoch 8 batch id 4371 loss 0.8139442205429077 train acc 0.7443162319835278\n",
            "epoch 8 batch id 4381 loss 0.6085599660873413 train acc 0.744372004108651\n",
            "epoch 8 batch id 4391 loss 0.4874962866306305 train acc 0.7444132885447506\n",
            "epoch 8 batch id 4401 loss 0.5992419123649597 train acc 0.7444650363553738\n",
            "epoch 8 batch id 4411 loss 0.621290922164917 train acc 0.7445059226932669\n",
            "epoch 8 batch id 4421 loss 0.6590425968170166 train acc 0.7445678296765438\n",
            "epoch 8 batch id 4431 loss 0.7663744688034058 train acc 0.7446259309410969\n",
            "epoch 8 batch id 4441 loss 0.3928786516189575 train acc 0.7446521053816708\n",
            "epoch 8 batch id 4451 loss 0.6221789121627808 train acc 0.7446886935520108\n",
            "epoch 8 batch id 4461 loss 0.5923481583595276 train acc 0.744732122842412\n",
            "epoch 8 batch id 4471 loss 0.6885772347450256 train acc 0.7447054629836726\n",
            "epoch 8 batch id 4481 loss 0.6907114386558533 train acc 0.7447382001785315\n",
            "epoch 8 batch id 4491 loss 0.5803900361061096 train acc 0.7447429581384992\n",
            "epoch 8 batch id 4501 loss 0.4523576498031616 train acc 0.7447719951121973\n",
            "epoch 8 batch id 4511 loss 0.8410348296165466 train acc 0.744800903347373\n",
            "epoch 8 batch id 4521 loss 0.6554220914840698 train acc 0.7448262276045122\n",
            "epoch 8 batch id 4531 loss 0.652614414691925 train acc 0.7447997130876186\n",
            "epoch 8 batch id 4541 loss 0.6015541553497314 train acc 0.7448283693019159\n",
            "epoch 8 batch id 4551 loss 0.7537951469421387 train acc 0.7449118325642716\n",
            "epoch 8 batch id 4561 loss 0.9173648357391357 train acc 0.744919562595922\n",
            "epoch 8 batch id 4571 loss 0.7559215426445007 train acc 0.7450742452417414\n",
            "epoch 8 batch id 4581 loss 0.7626665234565735 train acc 0.7451498035363457\n",
            "epoch 8 batch id 4591 loss 0.6942371129989624 train acc 0.7451637715094751\n",
            "epoch 8 batch id 4601 loss 0.7174672484397888 train acc 0.7452320147793958\n",
            "epoch 8 batch id 4611 loss 0.4836370348930359 train acc 0.7453609574929516\n",
            "epoch 8 batch id 4621 loss 0.5334180593490601 train acc 0.7454183347760225\n",
            "epoch 8 batch id 4631 loss 0.5543419122695923 train acc 0.7454889602677608\n",
            "epoch 8 batch id 4641 loss 0.6273409724235535 train acc 0.7455256140917905\n",
            "epoch 8 batch id 4651 loss 0.7787029147148132 train acc 0.745555391313696\n",
            "epoch 8 batch id 4661 loss 0.6388429403305054 train acc 0.7455984499034541\n",
            "epoch 8 batch id 4671 loss 0.5783026814460754 train acc 0.7456446692357097\n",
            "epoch 8 batch id 4681 loss 0.8726036548614502 train acc 0.7456873531296732\n",
            "epoch 8 batch id 4691 loss 0.4379432499408722 train acc 0.7457132008100619\n",
            "epoch 8 batch id 4701 loss 0.5953748226165771 train acc 0.7457422622846203\n",
            "epoch 8 batch id 4711 loss 0.32685965299606323 train acc 0.7458408511993208\n",
            "epoch 8 batch id 4721 loss 0.6562164425849915 train acc 0.74588606757043\n",
            "epoch 8 batch id 4731 loss 0.5061392784118652 train acc 0.7459046713168463\n",
            "epoch 8 batch id 4741 loss 0.7353549599647522 train acc 0.7459792237924489\n",
            "epoch 8 batch id 4751 loss 0.6228508949279785 train acc 0.7460435960850347\n",
            "epoch 8 batch id 4761 loss 0.5457373261451721 train acc 0.7460683154799412\n",
            "epoch 8 batch id 4771 loss 0.5688098073005676 train acc 0.7461387811779501\n",
            "epoch 8 batch id 4781 loss 0.6852987408638 train acc 0.746182806944154\n",
            "epoch 8 batch id 4791 loss 0.7500149011611938 train acc 0.7462331715716969\n",
            "epoch 8 batch id 4801 loss 0.5744252800941467 train acc 0.7462833263903353\n",
            "epoch 8 batch id 4811 loss 0.589623212814331 train acc 0.7463040428185408\n",
            "epoch 8 batch id 4821 loss 0.8010513782501221 train acc 0.7462987450736361\n",
            "epoch 8 batch id 4831 loss 0.5749419331550598 train acc 0.7463646243013868\n",
            "epoch 8 batch id 4841 loss 0.7297775149345398 train acc 0.7463559956620532\n",
            "epoch 8 batch id 4851 loss 0.7398106455802917 train acc 0.746379612451041\n",
            "epoch 8 batch id 4861 loss 0.4929925203323364 train acc 0.7464384900226291\n",
            "epoch 8 batch id 4871 loss 0.7632321119308472 train acc 0.7465484500102648\n",
            "epoch 8 batch id 4881 loss 0.5489920973777771 train acc 0.7466259475517312\n",
            "epoch 8 batch id 4891 loss 0.6781795620918274 train acc 0.7466456246166429\n",
            "epoch 8 batch id 4901 loss 0.7645825743675232 train acc 0.746690726382371\n",
            "epoch 8 batch id 4911 loss 0.756231963634491 train acc 0.7467547342700062\n",
            "epoch 8 batch id 4921 loss 0.5598289370536804 train acc 0.7467518034952245\n",
            "epoch 8 batch id 4931 loss 0.8365553021430969 train acc 0.7467647282498479\n",
            "epoch 8 batch id 4941 loss 0.6235452890396118 train acc 0.7467839253187614\n",
            "epoch 8 batch id 4951 loss 0.8697580099105835 train acc 0.7468409159765704\n",
            "epoch 8 train acc 0.7469183844700791\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "254dbf853f9545a6b6a3115aa9424a6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8 loss 0.9780598282814026 test acc 0.7058410465542522\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b03c0b95edc43a7b7d72d65da52e8a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9 batch id 1 loss 0.9052358865737915 train acc 0.6875\n",
            "epoch 9 batch id 11 loss 0.5211507081985474 train acc 0.7784090909090909\n",
            "epoch 9 batch id 21 loss 0.5046331286430359 train acc 0.7872023809523809\n",
            "epoch 9 batch id 31 loss 0.6009702682495117 train acc 0.7691532258064516\n",
            "epoch 9 batch id 41 loss 0.48853981494903564 train acc 0.7736280487804879\n",
            "epoch 9 batch id 51 loss 0.523199737071991 train acc 0.7720588235294118\n",
            "epoch 9 batch id 61 loss 0.743371844291687 train acc 0.7710040983606558\n",
            "epoch 9 batch id 71 loss 0.9110144376754761 train acc 0.7713468309859155\n",
            "epoch 9 batch id 81 loss 0.6356171369552612 train acc 0.7719907407407407\n",
            "epoch 9 batch id 91 loss 0.6677683591842651 train acc 0.7742101648351648\n",
            "epoch 9 batch id 101 loss 0.5750394463539124 train acc 0.7722772277227723\n",
            "epoch 9 batch id 111 loss 0.5760862231254578 train acc 0.7740709459459459\n",
            "epoch 9 batch id 121 loss 0.7765969038009644 train acc 0.7736311983471075\n",
            "epoch 9 batch id 131 loss 0.49409252405166626 train acc 0.7757633587786259\n",
            "epoch 9 batch id 141 loss 0.6791428923606873 train acc 0.773936170212766\n",
            "epoch 9 batch id 151 loss 0.5598753690719604 train acc 0.7751448675496688\n",
            "epoch 9 batch id 161 loss 0.8573849201202393 train acc 0.7749417701863354\n",
            "epoch 9 batch id 171 loss 0.6094259023666382 train acc 0.7745796783625731\n",
            "epoch 9 batch id 181 loss 0.41806209087371826 train acc 0.7736533149171271\n",
            "epoch 9 batch id 191 loss 0.812093198299408 train acc 0.7732329842931938\n",
            "epoch 9 batch id 201 loss 0.614926815032959 train acc 0.7737873134328358\n",
            "epoch 9 batch id 211 loss 0.7437275648117065 train acc 0.7730302132701422\n",
            "epoch 9 batch id 221 loss 0.8438403010368347 train acc 0.7730486425339367\n",
            "epoch 9 batch id 231 loss 0.5881652235984802 train acc 0.7721861471861472\n",
            "epoch 9 batch id 241 loss 0.733284592628479 train acc 0.7701633817427386\n",
            "epoch 9 batch id 251 loss 0.7004638314247131 train acc 0.7705428286852589\n",
            "epoch 9 batch id 261 loss 0.5461581349372864 train acc 0.7708333333333334\n",
            "epoch 9 batch id 271 loss 0.5236707925796509 train acc 0.7720249077490775\n",
            "epoch 9 batch id 281 loss 0.4417215585708618 train acc 0.772075177935943\n",
            "epoch 9 batch id 291 loss 0.7891589999198914 train acc 0.7723904639175257\n",
            "epoch 9 batch id 301 loss 0.6322676539421082 train acc 0.7728405315614618\n",
            "epoch 9 batch id 311 loss 0.5998051762580872 train acc 0.7733621382636656\n",
            "epoch 9 batch id 321 loss 0.8159456253051758 train acc 0.7727803738317757\n",
            "epoch 9 batch id 331 loss 0.8162146806716919 train acc 0.7721393504531722\n",
            "epoch 9 batch id 341 loss 0.7802190780639648 train acc 0.7725439882697948\n",
            "epoch 9 batch id 351 loss 0.7865017056465149 train acc 0.7726139601139601\n",
            "epoch 9 batch id 361 loss 0.6614657044410706 train acc 0.7720308171745153\n",
            "epoch 9 batch id 371 loss 0.8574444651603699 train acc 0.7716896900269542\n",
            "epoch 9 batch id 381 loss 0.4863983690738678 train acc 0.7715715223097113\n",
            "epoch 9 batch id 391 loss 0.7900792956352234 train acc 0.7709798593350383\n",
            "epoch 9 batch id 401 loss 0.35322433710098267 train acc 0.7710801122194514\n",
            "epoch 9 batch id 411 loss 0.5823458433151245 train acc 0.7709854014598541\n",
            "epoch 9 batch id 421 loss 0.799447238445282 train acc 0.7710807600950119\n",
            "epoch 9 batch id 431 loss 0.6508952975273132 train acc 0.7705916473317865\n",
            "epoch 9 batch id 441 loss 0.8500698208808899 train acc 0.7705144557823129\n",
            "epoch 9 batch id 451 loss 0.5646952986717224 train acc 0.7708564301552107\n",
            "epoch 9 batch id 461 loss 0.5797856450080872 train acc 0.7703362255965293\n",
            "epoch 9 batch id 471 loss 0.5728070735931396 train acc 0.7711319002123143\n",
            "epoch 9 batch id 481 loss 0.7789561152458191 train acc 0.7713422557172557\n",
            "epoch 9 batch id 491 loss 0.6431466341018677 train acc 0.7711621690427699\n",
            "epoch 9 batch id 501 loss 0.9945140480995178 train acc 0.7706150199600799\n",
            "epoch 9 batch id 511 loss 0.6057718992233276 train acc 0.7704562133072407\n",
            "epoch 9 batch id 521 loss 0.6543482542037964 train acc 0.7702435220729367\n",
            "epoch 9 batch id 531 loss 0.6383817195892334 train acc 0.7702448210922788\n",
            "epoch 9 batch id 541 loss 0.5801553130149841 train acc 0.7705348890942699\n",
            "epoch 9 batch id 551 loss 0.507106363773346 train acc 0.7705308529945554\n",
            "epoch 9 batch id 561 loss 0.5482335090637207 train acc 0.7704991087344029\n",
            "epoch 9 batch id 571 loss 0.8087501525878906 train acc 0.7703590192644484\n",
            "epoch 9 batch id 581 loss 0.6339552402496338 train acc 0.7703851118760757\n",
            "epoch 9 batch id 591 loss 0.5397875308990479 train acc 0.7701723773265652\n",
            "epoch 9 batch id 601 loss 0.5555247664451599 train acc 0.7703826955074875\n",
            "epoch 9 batch id 611 loss 0.6272400617599487 train acc 0.7702281096563012\n",
            "epoch 9 batch id 621 loss 0.6268827319145203 train acc 0.7704559178743962\n",
            "epoch 9 batch id 631 loss 0.6969517469406128 train acc 0.7703298335974643\n",
            "epoch 9 batch id 641 loss 0.6208561062812805 train acc 0.7704758190327613\n",
            "epoch 9 batch id 651 loss 0.48808789253234863 train acc 0.7704253072196621\n",
            "epoch 9 batch id 661 loss 0.7360032796859741 train acc 0.7705417927382754\n",
            "epoch 9 batch id 671 loss 0.8529577851295471 train acc 0.7707712369597616\n",
            "epoch 9 batch id 681 loss 0.5606692433357239 train acc 0.7709480543318649\n",
            "epoch 9 batch id 691 loss 0.5989570617675781 train acc 0.7713232633863966\n",
            "epoch 9 batch id 701 loss 0.5849084258079529 train acc 0.7714648716119828\n",
            "epoch 9 batch id 711 loss 0.6055318117141724 train acc 0.7714486638537271\n",
            "epoch 9 batch id 721 loss 0.5681909322738647 train acc 0.7714762482662968\n",
            "epoch 9 batch id 731 loss 0.46591717004776 train acc 0.7717168262653898\n",
            "epoch 9 batch id 741 loss 0.8051368594169617 train acc 0.7717611336032388\n",
            "epoch 9 batch id 751 loss 0.772275984287262 train acc 0.771512982689747\n",
            "epoch 9 batch id 761 loss 0.4828151762485504 train acc 0.7719283837056504\n",
            "epoch 9 batch id 771 loss 0.8774654865264893 train acc 0.7718060959792478\n",
            "epoch 9 batch id 781 loss 0.5889890789985657 train acc 0.7720670614596671\n",
            "epoch 9 batch id 791 loss 0.4723065197467804 train acc 0.7720053729456384\n",
            "epoch 9 batch id 801 loss 0.6494215726852417 train acc 0.7724133895131086\n",
            "epoch 9 batch id 811 loss 0.702986478805542 train acc 0.7725030826140568\n",
            "epoch 9 batch id 821 loss 0.5122995972633362 train acc 0.772476400730816\n",
            "epoch 9 batch id 831 loss 0.6673924922943115 train acc 0.7726007821901324\n",
            "epoch 9 batch id 841 loss 0.518739640712738 train acc 0.7730194708680143\n",
            "epoch 9 batch id 851 loss 0.6948848962783813 train acc 0.7728591363102233\n",
            "epoch 9 batch id 861 loss 0.7113944888114929 train acc 0.7726843786295006\n",
            "epoch 9 batch id 871 loss 0.5023341178894043 train acc 0.7729441733639495\n",
            "epoch 9 batch id 881 loss 0.5996196866035461 train acc 0.7726660045402951\n",
            "epoch 9 batch id 891 loss 0.5419716238975525 train acc 0.7726921997755332\n",
            "epoch 9 batch id 901 loss 0.9202235341072083 train acc 0.7725964206437292\n",
            "epoch 9 batch id 911 loss 1.1413640975952148 train acc 0.7723312294182217\n",
            "epoch 9 batch id 921 loss 0.8902741074562073 train acc 0.7723093105320304\n",
            "epoch 9 batch id 931 loss 0.42140263319015503 train acc 0.7722375134264232\n",
            "epoch 9 batch id 941 loss 0.45191723108291626 train acc 0.7724495217853348\n",
            "epoch 9 batch id 951 loss 0.7352122068405151 train acc 0.7723284700315457\n",
            "epoch 9 batch id 961 loss 0.5855371356010437 train acc 0.772144901144641\n",
            "epoch 9 batch id 971 loss 0.6106890439987183 train acc 0.7724639546858908\n",
            "epoch 9 batch id 981 loss 0.6972978115081787 train acc 0.7728720693170235\n",
            "epoch 9 batch id 991 loss 0.6346253752708435 train acc 0.7729723763874874\n",
            "epoch 9 batch id 1001 loss 0.5713353157043457 train acc 0.7729926323676324\n",
            "epoch 9 batch id 1011 loss 0.7501410245895386 train acc 0.7730588526211671\n",
            "epoch 9 batch id 1021 loss 0.44310468435287476 train acc 0.7732615083251714\n",
            "epoch 9 batch id 1031 loss 0.7371780276298523 train acc 0.7731268186226964\n",
            "epoch 9 batch id 1041 loss 0.576896071434021 train acc 0.7734750240153698\n",
            "epoch 9 batch id 1051 loss 0.4377962052822113 train acc 0.7734151998097051\n",
            "epoch 9 batch id 1061 loss 0.6086615324020386 train acc 0.7733565032987747\n",
            "epoch 9 batch id 1071 loss 0.3983118534088135 train acc 0.7736198646125116\n",
            "epoch 9 batch id 1081 loss 0.8639100193977356 train acc 0.7734302728954672\n",
            "epoch 9 batch id 1091 loss 0.8861902356147766 train acc 0.7733730522456462\n",
            "epoch 9 batch id 1101 loss 0.6604409217834473 train acc 0.7733736376021798\n",
            "epoch 9 batch id 1111 loss 0.6404707431793213 train acc 0.7732898289828983\n",
            "epoch 9 batch id 1121 loss 0.42875897884368896 train acc 0.7733608385370205\n",
            "epoch 9 batch id 1131 loss 0.6110824942588806 train acc 0.7734305923961097\n",
            "epoch 9 batch id 1141 loss 0.6642487049102783 train acc 0.7735402059596845\n",
            "epoch 9 batch id 1151 loss 0.7221437692642212 train acc 0.7736343397046047\n",
            "epoch 9 batch id 1161 loss 0.7175111770629883 train acc 0.7736864771748493\n",
            "epoch 9 batch id 1171 loss 0.474879652261734 train acc 0.7738711571306576\n",
            "epoch 9 batch id 1181 loss 0.7047792673110962 train acc 0.77380133361558\n",
            "epoch 9 batch id 1191 loss 0.5475098490715027 train acc 0.7740213056255247\n",
            "epoch 9 batch id 1201 loss 0.8660645484924316 train acc 0.7739253746877602\n",
            "epoch 9 batch id 1211 loss 0.4634983241558075 train acc 0.77407617671346\n",
            "epoch 9 batch id 1221 loss 0.5633288621902466 train acc 0.7742117117117117\n",
            "epoch 9 batch id 1231 loss 0.7412710785865784 train acc 0.7739642567018684\n",
            "epoch 9 batch id 1241 loss 0.5708556771278381 train acc 0.7738970588235294\n",
            "epoch 9 batch id 1251 loss 0.5163586139678955 train acc 0.7739933053557154\n",
            "epoch 9 batch id 1261 loss 0.7856131792068481 train acc 0.773976506740682\n",
            "epoch 9 batch id 1271 loss 0.6129995584487915 train acc 0.7738739181746657\n",
            "epoch 9 batch id 1281 loss 0.7260450124740601 train acc 0.7736753512880562\n",
            "epoch 9 batch id 1291 loss 0.5424560308456421 train acc 0.7737582300542215\n",
            "epoch 9 batch id 1301 loss 0.9370018243789673 train acc 0.7736716948501153\n",
            "epoch 9 batch id 1311 loss 0.7182972431182861 train acc 0.7735864797864226\n",
            "epoch 9 batch id 1321 loss 0.4599924385547638 train acc 0.7734434140802422\n",
            "epoch 9 batch id 1331 loss 0.5895551443099976 train acc 0.7733846731780616\n",
            "epoch 9 batch id 1341 loss 0.6365922689437866 train acc 0.7732452460850112\n",
            "epoch 9 batch id 1351 loss 0.5950519442558289 train acc 0.7733507586972613\n",
            "epoch 9 batch id 1361 loss 0.6562575101852417 train acc 0.7733169544452608\n",
            "epoch 9 batch id 1371 loss 0.5107033848762512 train acc 0.7733520240700219\n",
            "epoch 9 batch id 1381 loss 0.7680287957191467 train acc 0.7733639572773353\n",
            "epoch 9 batch id 1391 loss 0.3652975261211395 train acc 0.7736116103522646\n",
            "epoch 9 batch id 1401 loss 0.5480613708496094 train acc 0.7735992148465382\n",
            "epoch 9 batch id 1411 loss 0.5138450860977173 train acc 0.7736423635719348\n",
            "epoch 9 batch id 1421 loss 0.41044989228248596 train acc 0.7735859429978889\n",
            "epoch 9 batch id 1431 loss 0.441352516412735 train acc 0.7737705276030747\n",
            "epoch 9 batch id 1441 loss 0.6568984985351562 train acc 0.7737248438584317\n",
            "epoch 9 batch id 1451 loss 0.5364089608192444 train acc 0.7737659372846313\n",
            "epoch 9 batch id 1461 loss 0.510153591632843 train acc 0.7738492470910335\n",
            "epoch 9 batch id 1471 loss 0.725030779838562 train acc 0.7738358259687288\n",
            "epoch 9 batch id 1481 loss 0.4785754680633545 train acc 0.7740546927751519\n",
            "epoch 9 batch id 1491 loss 0.5750468969345093 train acc 0.7740191146881288\n",
            "epoch 9 batch id 1501 loss 0.6886510252952576 train acc 0.7739840106595602\n",
            "epoch 9 batch id 1511 loss 0.4652394950389862 train acc 0.774197551290536\n",
            "epoch 9 batch id 1521 loss 0.5284526348114014 train acc 0.7742850098619329\n",
            "epoch 9 batch id 1531 loss 0.6049241423606873 train acc 0.7743100914435009\n",
            "epoch 9 batch id 1541 loss 0.47049349546432495 train acc 0.7744261031797534\n",
            "epoch 9 batch id 1551 loss 0.7021880745887756 train acc 0.7743895067698259\n",
            "epoch 9 batch id 1561 loss 0.5904769897460938 train acc 0.7745035233824471\n",
            "epoch 9 batch id 1571 loss 0.620922863483429 train acc 0.7746857097390197\n",
            "epoch 9 batch id 1581 loss 0.6710102558135986 train acc 0.7746975806451613\n",
            "epoch 9 batch id 1591 loss 0.6744504570960999 train acc 0.7747387649277184\n",
            "epoch 9 batch id 1601 loss 0.6433964371681213 train acc 0.7748770299812617\n",
            "epoch 9 batch id 1611 loss 0.5094254612922668 train acc 0.7750329764121664\n",
            "epoch 9 batch id 1621 loss 0.5442046523094177 train acc 0.7751002467612584\n",
            "epoch 9 batch id 1631 loss 0.4895308017730713 train acc 0.7751187921520539\n",
            "epoch 9 batch id 1641 loss 0.7424069046974182 train acc 0.7753275441803779\n",
            "epoch 9 batch id 1651 loss 0.600034773349762 train acc 0.775230920654149\n",
            "epoch 9 batch id 1661 loss 0.8298869729042053 train acc 0.775210716435882\n",
            "epoch 9 batch id 1671 loss 0.7973268628120422 train acc 0.7752468581687613\n",
            "epoch 9 batch id 1681 loss 0.8584198355674744 train acc 0.775115258774539\n",
            "epoch 9 batch id 1691 loss 0.6354741454124451 train acc 0.775336339444116\n",
            "epoch 9 batch id 1701 loss 0.6274152398109436 train acc 0.775407848324515\n",
            "epoch 9 batch id 1711 loss 0.6893812417984009 train acc 0.7753141437755698\n",
            "epoch 9 batch id 1721 loss 0.6912392973899841 train acc 0.7752669233004067\n",
            "epoch 9 batch id 1731 loss 0.33001047372817993 train acc 0.775319540727903\n",
            "epoch 9 batch id 1741 loss 0.6825796365737915 train acc 0.7753356547960942\n",
            "epoch 9 batch id 1751 loss 0.42617419362068176 train acc 0.7753158909194746\n",
            "epoch 9 batch id 1761 loss 0.5262283682823181 train acc 0.775473807495741\n",
            "epoch 9 batch id 1771 loss 0.5036067962646484 train acc 0.7754711321287409\n",
            "epoch 9 batch id 1781 loss 0.6049138307571411 train acc 0.7754509404828748\n",
            "epoch 9 batch id 1791 loss 1.0046274662017822 train acc 0.7755356644332775\n",
            "epoch 9 batch id 1801 loss 0.5492876172065735 train acc 0.7757148806218768\n",
            "epoch 9 batch id 1811 loss 0.4336957633495331 train acc 0.7758403506350083\n",
            "epoch 9 batch id 1821 loss 0.589934766292572 train acc 0.7759901839648545\n",
            "epoch 9 batch id 1831 loss 0.34535419940948486 train acc 0.7761213134898962\n",
            "epoch 9 batch id 1841 loss 0.3955081105232239 train acc 0.7762679929386204\n",
            "epoch 9 batch id 1851 loss 0.8168184757232666 train acc 0.7762358184764991\n",
            "epoch 9 batch id 1861 loss 0.6500979065895081 train acc 0.7763299301450833\n",
            "epoch 9 batch id 1871 loss 0.5760969519615173 train acc 0.776464791555318\n",
            "epoch 9 batch id 1881 loss 0.746198832988739 train acc 0.7764653110047847\n",
            "epoch 9 batch id 1891 loss 0.6505597233772278 train acc 0.7767715494447383\n",
            "epoch 9 batch id 1901 loss 0.6751466989517212 train acc 0.7766635981062598\n",
            "epoch 9 batch id 1911 loss 0.5342506170272827 train acc 0.7766303636839351\n",
            "epoch 9 batch id 1921 loss 0.6749919652938843 train acc 0.7764754685059865\n",
            "epoch 9 batch id 1931 loss 0.6586270928382874 train acc 0.7766863024339721\n",
            "epoch 9 batch id 1941 loss 0.5469315648078918 train acc 0.7768627640391551\n",
            "epoch 9 batch id 1951 loss 0.6263205409049988 train acc 0.7768051640184521\n",
            "epoch 9 batch id 1961 loss 0.35927614569664 train acc 0.7769553161652218\n",
            "epoch 9 batch id 1971 loss 0.6247674226760864 train acc 0.7769612506341959\n",
            "epoch 9 batch id 1981 loss 0.6444852948188782 train acc 0.7770302246340233\n",
            "epoch 9 batch id 1991 loss 0.7054125070571899 train acc 0.7769729407332998\n",
            "epoch 9 batch id 2001 loss 0.7533944249153137 train acc 0.7769865067466267\n",
            "epoch 9 batch id 2011 loss 0.7991306185722351 train acc 0.7769843983092989\n",
            "epoch 9 batch id 2021 loss 0.9625461101531982 train acc 0.7771910564077189\n",
            "epoch 9 batch id 2031 loss 0.5134029388427734 train acc 0.7771187223042836\n",
            "epoch 9 batch id 2041 loss 0.3853916823863983 train acc 0.7772155193532582\n",
            "epoch 9 batch id 2051 loss 0.6655527949333191 train acc 0.777311372501219\n",
            "epoch 9 batch id 2061 loss 0.5798951387405396 train acc 0.7772698326055313\n",
            "epoch 9 batch id 2071 loss 0.6684226393699646 train acc 0.7773343191694834\n",
            "epoch 9 batch id 2081 loss 0.7012019157409668 train acc 0.7772930682364247\n",
            "epoch 9 batch id 2091 loss 0.6434635519981384 train acc 0.7773418818747011\n",
            "epoch 9 batch id 2101 loss 0.4246644675731659 train acc 0.7773530461684912\n",
            "epoch 9 batch id 2111 loss 0.7007752060890198 train acc 0.7772974893415443\n",
            "epoch 9 batch id 2121 loss 0.2760767340660095 train acc 0.7773603253182461\n",
            "epoch 9 batch id 2131 loss 0.8531779646873474 train acc 0.7775398873768185\n",
            "epoch 9 batch id 2141 loss 0.7145507335662842 train acc 0.7774696403549743\n",
            "epoch 9 batch id 2151 loss 0.5556612014770508 train acc 0.7775961761971176\n",
            "epoch 9 batch id 2161 loss 0.6987247467041016 train acc 0.7775841624248033\n",
            "epoch 9 batch id 2171 loss 0.5857115387916565 train acc 0.7777090050667895\n",
            "epoch 9 batch id 2181 loss 0.6367546916007996 train acc 0.7777968821641449\n",
            "epoch 9 batch id 2191 loss 0.46303117275238037 train acc 0.77786969420356\n",
            "epoch 9 batch id 2201 loss 0.6987787485122681 train acc 0.7778779532030895\n",
            "epoch 9 batch id 2211 loss 0.47824427485466003 train acc 0.7778084011759385\n",
            "epoch 9 batch id 2221 loss 1.014545202255249 train acc 0.7777042998649257\n",
            "epoch 9 batch id 2231 loss 0.4858495593070984 train acc 0.7778112393545495\n",
            "epoch 9 batch id 2241 loss 0.5379505753517151 train acc 0.7778475011155734\n",
            "epoch 9 batch id 2251 loss 0.6674482822418213 train acc 0.7778140270990671\n",
            "epoch 9 batch id 2261 loss 0.5173646807670593 train acc 0.7778084918177798\n",
            "epoch 9 batch id 2271 loss 0.8167716860771179 train acc 0.777851166886834\n",
            "epoch 9 batch id 2281 loss 0.7509192824363708 train acc 0.7781058198158702\n",
            "epoch 9 batch id 2291 loss 0.9075360298156738 train acc 0.7781877455259711\n",
            "epoch 9 batch id 2301 loss 0.629686176776886 train acc 0.7782689591481964\n",
            "epoch 9 batch id 2311 loss 0.6645690202713013 train acc 0.7782345305062743\n",
            "epoch 9 batch id 2321 loss 0.4017743468284607 train acc 0.778234058595433\n",
            "epoch 9 batch id 2331 loss 0.5883904099464417 train acc 0.7781397468897469\n",
            "epoch 9 batch id 2341 loss 0.7286062836647034 train acc 0.7782331268688595\n",
            "epoch 9 batch id 2351 loss 0.5692426562309265 train acc 0.778232666950234\n",
            "epoch 9 batch id 2361 loss 0.6837859153747559 train acc 0.778351334180432\n",
            "epoch 9 batch id 2371 loss 0.4116789698600769 train acc 0.7785414909320961\n",
            "epoch 9 batch id 2381 loss 0.6677332520484924 train acc 0.7785397417051659\n",
            "epoch 9 batch id 2391 loss 0.62740558385849 train acc 0.7784987975742367\n",
            "epoch 9 batch id 2401 loss 0.6611788272857666 train acc 0.7784581945022907\n",
            "epoch 9 batch id 2411 loss 0.6180871725082397 train acc 0.7785475425134799\n",
            "epoch 9 batch id 2421 loss 0.5904033184051514 train acc 0.7785651590251962\n",
            "epoch 9 batch id 2431 loss 0.6225091814994812 train acc 0.7785633484162896\n",
            "epoch 9 batch id 2441 loss 0.5650870203971863 train acc 0.7786639696845555\n",
            "epoch 9 batch id 2451 loss 0.6802723407745361 train acc 0.778687270501836\n",
            "epoch 9 batch id 2461 loss 0.6889128684997559 train acc 0.7786278443722064\n",
            "epoch 9 batch id 2471 loss 0.6627175211906433 train acc 0.7784993423715095\n",
            "epoch 9 batch id 2481 loss 0.6690322756767273 train acc 0.778529322853688\n",
            "epoch 9 batch id 2491 loss 0.5271204113960266 train acc 0.7785088819751104\n",
            "epoch 9 batch id 2501 loss 0.4206407368183136 train acc 0.7785885645741704\n",
            "epoch 9 batch id 2511 loss 0.6865907311439514 train acc 0.7786676125049781\n",
            "epoch 9 batch id 2521 loss 0.5974830389022827 train acc 0.7785910848869496\n",
            "epoch 9 batch id 2531 loss 0.6797301769256592 train acc 0.7786448044251284\n",
            "epoch 9 batch id 2541 loss 0.6407220959663391 train acc 0.7787718909878001\n",
            "epoch 9 batch id 2551 loss 0.827319324016571 train acc 0.7787326048608388\n",
            "epoch 9 batch id 2561 loss 0.6500951647758484 train acc 0.7786936255368997\n",
            "epoch 9 batch id 2571 loss 0.5746208429336548 train acc 0.7788068844807468\n",
            "epoch 9 batch id 2581 loss 0.7476816773414612 train acc 0.7787860809763657\n",
            "epoch 9 batch id 2591 loss 0.4570102393627167 train acc 0.7788016209957546\n",
            "epoch 9 batch id 2601 loss 0.6995438933372498 train acc 0.7787990196078431\n",
            "epoch 9 batch id 2611 loss 0.4642716646194458 train acc 0.7788084067407124\n",
            "epoch 9 batch id 2621 loss 0.7229304909706116 train acc 0.7787819534528806\n",
            "epoch 9 batch id 2631 loss 0.7297120690345764 train acc 0.7788150893196504\n",
            "epoch 9 batch id 2641 loss 0.654837965965271 train acc 0.7788420579326013\n",
            "epoch 9 batch id 2651 loss 0.5561742186546326 train acc 0.7788098830629951\n",
            "epoch 9 batch id 2661 loss 0.662421703338623 train acc 0.7788484122510334\n",
            "epoch 9 batch id 2671 loss 0.6708430051803589 train acc 0.778810604642456\n",
            "epoch 9 batch id 2681 loss 0.5926432013511658 train acc 0.7787147985826184\n",
            "epoch 9 batch id 2691 loss 0.7726069092750549 train acc 0.7787184132292828\n",
            "epoch 9 batch id 2701 loss 0.6824941039085388 train acc 0.7787740651610514\n",
            "epoch 9 batch id 2711 loss 0.49981689453125 train acc 0.7787601438583549\n",
            "epoch 9 batch id 2721 loss 0.4014921486377716 train acc 0.7787692943770672\n",
            "epoch 9 batch id 2731 loss 0.4489958882331848 train acc 0.7787669351885756\n",
            "epoch 9 batch id 2741 loss 0.6018199324607849 train acc 0.7787417913170376\n",
            "epoch 9 batch id 2751 loss 0.5300536751747131 train acc 0.7788645038167938\n",
            "epoch 9 batch id 2761 loss 0.4662679135799408 train acc 0.7788618254255705\n",
            "epoch 9 batch id 2771 loss 0.5918070673942566 train acc 0.7788760826416457\n",
            "epoch 9 batch id 2781 loss 0.8160827159881592 train acc 0.7788846188421431\n",
            "epoch 9 batch id 2791 loss 0.6327607035636902 train acc 0.7789154872805446\n",
            "epoch 9 batch id 2801 loss 0.6443942189216614 train acc 0.7789126651196001\n",
            "epoch 9 batch id 2811 loss 0.4772866666316986 train acc 0.7789598897189612\n",
            "epoch 9 batch id 2821 loss 0.5920348167419434 train acc 0.7789347749025168\n",
            "epoch 9 batch id 2831 loss 0.4516358971595764 train acc 0.7789098375132462\n",
            "epoch 9 batch id 2841 loss 0.6591098308563232 train acc 0.7789620732136572\n",
            "epoch 9 batch id 2851 loss 0.6061490774154663 train acc 0.7789481760785689\n",
            "epoch 9 batch id 2861 loss 0.3924125134944916 train acc 0.7789070692065712\n",
            "epoch 9 batch id 2871 loss 0.4579553008079529 train acc 0.7788553639846744\n",
            "epoch 9 batch id 2881 loss 0.5505985617637634 train acc 0.7789396043040611\n",
            "epoch 9 batch id 2891 loss 0.5579735636711121 train acc 0.7789800242130751\n",
            "epoch 9 batch id 2901 loss 0.6275992393493652 train acc 0.7789824629438125\n",
            "epoch 9 batch id 2911 loss 0.4072554111480713 train acc 0.77909760391618\n",
            "epoch 9 batch id 2921 loss 0.43870505690574646 train acc 0.7790193854844232\n",
            "epoch 9 batch id 2931 loss 0.2954188883304596 train acc 0.7789950102354145\n",
            "epoch 9 batch id 2941 loss 0.6867974996566772 train acc 0.7789708007480449\n",
            "epoch 9 batch id 2951 loss 0.6700961589813232 train acc 0.7790261775669265\n",
            "epoch 9 batch id 2961 loss 0.5320910811424255 train acc 0.7791339496791625\n",
            "epoch 9 batch id 2971 loss 0.6928160786628723 train acc 0.7791989229215752\n",
            "epoch 9 batch id 2981 loss 0.5702307820320129 train acc 0.779279184837303\n",
            "epoch 9 batch id 2991 loss 0.4768519103527069 train acc 0.7793275660314276\n",
            "epoch 9 batch id 3001 loss 0.44163572788238525 train acc 0.7793808313895368\n",
            "epoch 9 batch id 3011 loss 0.5084245204925537 train acc 0.7794129857190302\n",
            "epoch 9 batch id 3021 loss 0.5413590669631958 train acc 0.779434582919563\n",
            "epoch 9 batch id 3031 loss 0.4631136357784271 train acc 0.7794766578686902\n",
            "epoch 9 batch id 3041 loss 0.8145516514778137 train acc 0.779431108188096\n",
            "epoch 9 batch id 3051 loss 0.702142596244812 train acc 0.7794473123566044\n",
            "epoch 9 batch id 3061 loss 0.4115465581417084 train acc 0.7794736197321137\n",
            "epoch 9 batch id 3071 loss 0.6231368184089661 train acc 0.7795150195376099\n",
            "epoch 9 batch id 3081 loss 0.7774094343185425 train acc 0.7795105079519636\n",
            "epoch 9 batch id 3091 loss 0.6026080846786499 train acc 0.779571740537043\n",
            "epoch 9 batch id 3101 loss 0.6370739936828613 train acc 0.7794763785875524\n",
            "epoch 9 batch id 3111 loss 0.6973751187324524 train acc 0.7794871022179364\n",
            "epoch 9 batch id 3121 loss 0.7416369915008545 train acc 0.7795528276193527\n",
            "epoch 9 batch id 3131 loss 0.5165210962295532 train acc 0.7796331044394762\n",
            "epoch 9 batch id 3141 loss 0.6688817143440247 train acc 0.7796183540273798\n",
            "epoch 9 batch id 3151 loss 0.4161701202392578 train acc 0.7797475007933989\n",
            "epoch 9 batch id 3161 loss 0.48953044414520264 train acc 0.7797423679215438\n",
            "epoch 9 batch id 3171 loss 0.49256327748298645 train acc 0.779756977294229\n",
            "epoch 9 batch id 3181 loss 0.436291366815567 train acc 0.7797665828355863\n",
            "epoch 9 batch id 3191 loss 0.3658252954483032 train acc 0.7798838530241303\n",
            "epoch 9 batch id 3201 loss 0.6769726872444153 train acc 0.7798295454545454\n",
            "epoch 9 batch id 3211 loss 0.6136485934257507 train acc 0.7797999065711616\n",
            "epoch 9 batch id 3221 loss 0.695894718170166 train acc 0.779852918348339\n",
            "epoch 9 batch id 3231 loss 0.39108625054359436 train acc 0.7799346177653977\n",
            "epoch 9 batch id 3241 loss 0.6485579013824463 train acc 0.7799049290342487\n",
            "epoch 9 batch id 3251 loss 0.4280742406845093 train acc 0.7799667410027684\n",
            "epoch 9 batch id 3261 loss 0.39240360260009766 train acc 0.7800712971481141\n",
            "epoch 9 batch id 3271 loss 0.4042661190032959 train acc 0.780089231121981\n",
            "epoch 9 batch id 3281 loss 0.8062809705734253 train acc 0.7801070557756782\n",
            "epoch 9 batch id 3291 loss 0.5835959911346436 train acc 0.7801817456700091\n",
            "epoch 9 batch id 3301 loss 0.47016826272010803 train acc 0.7802323159648591\n",
            "epoch 9 batch id 3311 loss 0.7526131868362427 train acc 0.7802684234370281\n",
            "epoch 9 batch id 3321 loss 0.4276922047138214 train acc 0.7803090183679614\n",
            "epoch 9 batch id 3331 loss 0.9620767831802368 train acc 0.7803634419093365\n",
            "epoch 9 batch id 3341 loss 0.8278831243515015 train acc 0.7803099745585154\n",
            "epoch 9 batch id 3351 loss 0.45916587114334106 train acc 0.780308116980006\n",
            "epoch 9 batch id 3361 loss 0.5213155150413513 train acc 0.7803806530794406\n",
            "epoch 9 batch id 3371 loss 0.6888303160667419 train acc 0.7804295832097301\n",
            "epoch 9 batch id 3381 loss 0.35548269748687744 train acc 0.7804689810706892\n",
            "epoch 9 batch id 3391 loss 0.7054010033607483 train acc 0.7805173621350634\n",
            "epoch 9 batch id 3401 loss 0.7177996635437012 train acc 0.780602212584534\n",
            "epoch 9 batch id 3411 loss 0.4364212155342102 train acc 0.7806499193784814\n",
            "epoch 9 batch id 3421 loss 0.6923514008522034 train acc 0.780724751534639\n",
            "epoch 9 batch id 3431 loss 0.6454660892486572 train acc 0.7806488633051588\n",
            "epoch 9 batch id 3441 loss 0.6787914037704468 train acc 0.7807005594303982\n",
            "epoch 9 batch id 3451 loss 0.45873451232910156 train acc 0.7807655389742104\n",
            "epoch 9 batch id 3461 loss 0.6412088871002197 train acc 0.7808617451603582\n",
            "epoch 9 batch id 3471 loss 0.6011142134666443 train acc 0.7808853716508211\n",
            "epoch 9 batch id 3481 loss 0.5285549759864807 train acc 0.7808819304797472\n",
            "epoch 9 batch id 3491 loss 0.42600885033607483 train acc 0.7809590733314237\n",
            "epoch 9 batch id 3501 loss 0.6283097267150879 train acc 0.7809732933447586\n",
            "epoch 9 batch id 3511 loss 0.5326666235923767 train acc 0.7809963329535745\n",
            "epoch 9 batch id 3521 loss 0.800683856010437 train acc 0.7811124325475717\n",
            "epoch 9 batch id 3531 loss 0.9160283803939819 train acc 0.7811128221467006\n",
            "epoch 9 batch id 3541 loss 0.5981433391571045 train acc 0.7812102866421915\n",
            "epoch 9 batch id 3551 loss 0.5795434713363647 train acc 0.7812411996620671\n",
            "epoch 9 batch id 3561 loss 0.5186291933059692 train acc 0.7813377562482449\n",
            "epoch 9 batch id 3571 loss 0.856646716594696 train acc 0.7813418860263232\n",
            "epoch 9 batch id 3581 loss 0.5805911421775818 train acc 0.7813198129014242\n",
            "epoch 9 batch id 3591 loss 0.7782278656959534 train acc 0.7813805346700083\n",
            "epoch 9 batch id 3601 loss 0.680492103099823 train acc 0.7814756317689531\n",
            "epoch 9 batch id 3611 loss 0.4678208529949188 train acc 0.7815096233730269\n",
            "epoch 9 batch id 3621 loss 0.4082293212413788 train acc 0.781543427230047\n",
            "epoch 9 batch id 3631 loss 0.4663521647453308 train acc 0.7816114706692371\n",
            "epoch 9 batch id 3641 loss 0.6864925026893616 train acc 0.7816533919252953\n",
            "epoch 9 batch id 3651 loss 0.5180047154426575 train acc 0.7816822445905232\n",
            "epoch 9 batch id 3661 loss 0.6825474500656128 train acc 0.781740815350997\n",
            "epoch 9 batch id 3671 loss 0.48236221075057983 train acc 0.7818033233451376\n",
            "epoch 9 batch id 3681 loss 0.7400159239768982 train acc 0.7818018201575658\n",
            "epoch 9 batch id 3691 loss 0.6478675603866577 train acc 0.7818341912760769\n",
            "epoch 9 batch id 3701 loss 0.4138493835926056 train acc 0.7819297149419075\n",
            "epoch 9 batch id 3711 loss 0.5614963173866272 train acc 0.7820878806251684\n",
            "epoch 9 batch id 3721 loss 0.460409939289093 train acc 0.7821780099435636\n",
            "epoch 9 batch id 3731 loss 0.431132435798645 train acc 0.7822132136156527\n",
            "epoch 9 batch id 3741 loss 0.6562577486038208 train acc 0.7821939321037156\n",
            "epoch 9 batch id 3751 loss 0.6669967174530029 train acc 0.7821872500666489\n",
            "epoch 9 batch id 3761 loss 0.489006370306015 train acc 0.782263693166711\n",
            "epoch 9 batch id 3771 loss 0.41193142533302307 train acc 0.7822278573322726\n",
            "epoch 9 batch id 3781 loss 0.6536009907722473 train acc 0.7822665961385876\n",
            "epoch 9 batch id 3791 loss 0.4890647828578949 train acc 0.7823381034027961\n",
            "epoch 9 batch id 3801 loss 0.4778573513031006 train acc 0.7824667850565641\n",
            "epoch 9 batch id 3811 loss 0.4476693272590637 train acc 0.7824881920755707\n",
            "epoch 9 batch id 3821 loss 0.7215873003005981 train acc 0.7825422009945041\n",
            "epoch 9 batch id 3831 loss 0.5696386694908142 train acc 0.7825918493865831\n",
            "epoch 9 batch id 3841 loss 0.5482212901115417 train acc 0.7826168315542827\n",
            "epoch 9 batch id 3851 loss 0.42076244950294495 train acc 0.7826863152427941\n",
            "epoch 9 batch id 3861 loss 0.4349237382411957 train acc 0.7827473452473452\n",
            "epoch 9 batch id 3871 loss 0.6466122269630432 train acc 0.7828080599328339\n",
            "epoch 9 batch id 3881 loss 0.6256169676780701 train acc 0.7828644357124452\n",
            "epoch 9 batch id 3891 loss 0.6741827130317688 train acc 0.7828482395271138\n",
            "epoch 9 batch id 3901 loss 0.457179456949234 train acc 0.7828281209946167\n",
            "epoch 9 batch id 3911 loss 0.3923623859882355 train acc 0.782899993607773\n",
            "epoch 9 batch id 3921 loss 0.40476366877555847 train acc 0.7829117253251722\n",
            "epoch 9 batch id 3931 loss 0.609601616859436 train acc 0.7828836491986771\n",
            "epoch 9 batch id 3941 loss 0.4242919087409973 train acc 0.7829310454199442\n",
            "epoch 9 batch id 3951 loss 0.5855019688606262 train acc 0.7829702923310554\n",
            "epoch 9 batch id 3961 loss 0.7211157083511353 train acc 0.7829659492552385\n",
            "epoch 9 batch id 3971 loss 0.5991893410682678 train acc 0.7830049106018635\n",
            "epoch 9 batch id 3981 loss 0.5913721919059753 train acc 0.7830162019593067\n",
            "epoch 9 batch id 3991 loss 0.7052463889122009 train acc 0.7830078614382361\n",
            "epoch 9 batch id 4001 loss 0.45695215463638306 train acc 0.7830190889777555\n",
            "epoch 9 batch id 4011 loss 0.6529867649078369 train acc 0.7830302605335328\n",
            "epoch 9 batch id 4021 loss 0.5731645822525024 train acc 0.7830102897289232\n",
            "epoch 9 batch id 4031 loss 0.5569441318511963 train acc 0.7830756946167204\n",
            "epoch 9 batch id 4041 loss 0.7411028742790222 train acc 0.7830827765404603\n",
            "epoch 9 batch id 4051 loss 0.4482242166996002 train acc 0.7831746790915823\n",
            "epoch 9 batch id 4061 loss 0.6718409657478333 train acc 0.7832084154149225\n",
            "epoch 9 batch id 4071 loss 0.9263622760772705 train acc 0.7832535003684599\n",
            "epoch 9 batch id 4081 loss 0.4228358566761017 train acc 0.7833519664297967\n",
            "epoch 9 batch id 4091 loss 0.4550979733467102 train acc 0.7834575898313371\n",
            "epoch 9 batch id 4101 loss 0.7963597178459167 train acc 0.7834598268714947\n",
            "epoch 9 batch id 4111 loss 0.49568745493888855 train acc 0.7834620530284603\n",
            "epoch 9 batch id 4121 loss 0.42498183250427246 train acc 0.7835059754913856\n",
            "epoch 9 batch id 4131 loss 0.41711747646331787 train acc 0.7835194262890341\n",
            "epoch 9 batch id 4141 loss 0.5794115662574768 train acc 0.7835969572567013\n",
            "epoch 9 batch id 4151 loss 0.46484631299972534 train acc 0.7836327089857865\n",
            "epoch 9 batch id 4161 loss 0.5517195463180542 train acc 0.7836532684450853\n",
            "epoch 9 batch id 4171 loss 0.3821902573108673 train acc 0.7836699832174538\n",
            "epoch 9 batch id 4181 loss 0.40336617827415466 train acc 0.7836604580243961\n",
            "epoch 9 batch id 4191 loss 0.6184998750686646 train acc 0.7837218146027202\n",
            "epoch 9 batch id 4201 loss 0.7091580033302307 train acc 0.7837494049035943\n",
            "epoch 9 batch id 4211 loss 0.7131115198135376 train acc 0.7837620220850154\n",
            "epoch 9 batch id 4221 loss 0.4814178943634033 train acc 0.7837930881307748\n",
            "epoch 9 batch id 4231 loss 0.5733140110969543 train acc 0.78379446348381\n",
            "epoch 9 batch id 4241 loss 0.5450830459594727 train acc 0.7838068851685923\n",
            "epoch 9 batch id 4251 loss 0.6523340344429016 train acc 0.7838596800752764\n",
            "epoch 9 batch id 4261 loss 0.601729691028595 train acc 0.7838718904013142\n",
            "epoch 9 batch id 4271 loss 0.4412827491760254 train acc 0.7839316026691642\n",
            "epoch 9 batch id 4281 loss 0.6851337552070618 train acc 0.7839837362765709\n",
            "epoch 9 batch id 4291 loss 0.4668918251991272 train acc 0.784068398974598\n",
            "epoch 9 batch id 4301 loss 0.3942103385925293 train acc 0.7840836433387585\n",
            "epoch 9 batch id 4311 loss 0.4326705038547516 train acc 0.7841676815124101\n",
            "epoch 9 batch id 4321 loss 0.6168474555015564 train acc 0.7841464649386716\n",
            "epoch 9 batch id 4331 loss 0.44869282841682434 train acc 0.7841938928653891\n",
            "epoch 9 batch id 4341 loss 0.5402330160140991 train acc 0.7841943100668048\n",
            "epoch 9 batch id 4351 loss 0.48328328132629395 train acc 0.7842665479200184\n",
            "epoch 9 batch id 4361 loss 0.8475512862205505 train acc 0.7842811281816098\n",
            "epoch 9 batch id 4371 loss 0.5921351313591003 train acc 0.784349262182567\n",
            "epoch 9 batch id 4381 loss 0.4750543534755707 train acc 0.7843493209312943\n",
            "epoch 9 batch id 4391 loss 0.4485194683074951 train acc 0.7843529378273741\n",
            "epoch 9 batch id 4401 loss 0.5884982347488403 train acc 0.7843245853215178\n",
            "epoch 9 batch id 4411 loss 0.3961677551269531 train acc 0.7843388687372478\n",
            "epoch 9 batch id 4421 loss 0.5563730597496033 train acc 0.7843601560732866\n",
            "epoch 9 batch id 4431 loss 0.5346563458442688 train acc 0.7844448205822614\n",
            "epoch 9 batch id 4441 loss 0.3829962909221649 train acc 0.7844974386399459\n",
            "epoch 9 batch id 4451 loss 0.6595844030380249 train acc 0.7844971635587509\n",
            "epoch 9 batch id 4461 loss 0.5651482343673706 train acc 0.7845003922887245\n",
            "epoch 9 batch id 4471 loss 0.5187891125679016 train acc 0.7845036065757102\n",
            "epoch 9 batch id 4481 loss 0.7715004682540894 train acc 0.7844998326266458\n",
            "epoch 9 batch id 4491 loss 0.44790729880332947 train acc 0.784541304831886\n",
            "epoch 9 batch id 4501 loss 0.43470299243927 train acc 0.7845548211508554\n",
            "epoch 9 batch id 4511 loss 0.687092125415802 train acc 0.7845752050543117\n",
            "epoch 9 batch id 4521 loss 0.4867573380470276 train acc 0.7845678500331785\n",
            "epoch 9 batch id 4531 loss 0.5123518705368042 train acc 0.7844881096888104\n",
            "epoch 9 batch id 4541 loss 0.4967581033706665 train acc 0.7845360328121559\n",
            "epoch 9 batch id 4551 loss 0.7391640543937683 train acc 0.7845837453306965\n",
            "epoch 9 batch id 4561 loss 0.5721587538719177 train acc 0.7846141197105898\n",
            "epoch 9 batch id 4571 loss 0.6285271644592285 train acc 0.7846853806606869\n",
            "epoch 9 batch id 4581 loss 0.5403704643249512 train acc 0.7847222222222222\n",
            "epoch 9 batch id 4591 loss 0.5068772435188293 train acc 0.7847554998910913\n",
            "epoch 9 batch id 4601 loss 0.5251501798629761 train acc 0.7848022169093676\n",
            "epoch 9 batch id 4611 loss 0.5636250376701355 train acc 0.7848826176534375\n",
            "epoch 9 batch id 4621 loss 0.5318545699119568 train acc 0.7849254760874269\n",
            "epoch 9 batch id 4631 loss 0.4086512327194214 train acc 0.7850086374433167\n",
            "epoch 9 batch id 4641 loss 0.4909901022911072 train acc 0.7850375727213963\n",
            "epoch 9 batch id 4651 loss 0.6366179585456848 train acc 0.785079821543754\n",
            "epoch 9 batch id 4661 loss 0.5873773694038391 train acc 0.7850984230851749\n",
            "epoch 9 batch id 4671 loss 0.574889600276947 train acc 0.7851336705202312\n",
            "epoch 9 batch id 4681 loss 0.8028541207313538 train acc 0.785235526596881\n",
            "epoch 9 batch id 4691 loss 0.37956902384757996 train acc 0.7852570081006182\n",
            "epoch 9 batch id 4701 loss 0.4381076693534851 train acc 0.7853016645394597\n",
            "epoch 9 batch id 4711 loss 0.32476043701171875 train acc 0.7853361812778603\n",
            "epoch 9 batch id 4721 loss 0.49013233184814453 train acc 0.7853672421097225\n",
            "epoch 9 batch id 4731 loss 0.4022267162799835 train acc 0.7853816582117945\n",
            "epoch 9 batch id 4741 loss 0.6845878958702087 train acc 0.7854091963720734\n",
            "epoch 9 batch id 4751 loss 0.542035698890686 train acc 0.7854267522626815\n",
            "epoch 9 batch id 4761 loss 0.7117895483970642 train acc 0.7854409525309809\n",
            "epoch 9 batch id 4771 loss 0.4320485591888428 train acc 0.785533693146091\n",
            "epoch 9 batch id 4781 loss 0.6734359860420227 train acc 0.7854985881614724\n",
            "epoch 9 batch id 4791 loss 0.7064299583435059 train acc 0.7855516854518889\n",
            "epoch 9 batch id 4801 loss 0.5800630450248718 train acc 0.7855720162466153\n",
            "epoch 9 batch id 4811 loss 0.3488004505634308 train acc 0.7856312357098316\n",
            "epoch 9 batch id 4821 loss 0.5396243929862976 train acc 0.7856318709811243\n",
            "epoch 9 batch id 4831 loss 0.3885073661804199 train acc 0.7856616125025875\n",
            "epoch 9 batch id 4841 loss 0.5760621428489685 train acc 0.7856524994835777\n",
            "epoch 9 batch id 4851 loss 0.6186865568161011 train acc 0.7857014017728303\n",
            "epoch 9 batch id 4861 loss 0.45800909399986267 train acc 0.7857018874717137\n",
            "epoch 9 batch id 4871 loss 0.7272037267684937 train acc 0.785792188462328\n",
            "epoch 9 batch id 4881 loss 0.5053423047065735 train acc 0.7858244980536775\n",
            "epoch 9 batch id 4891 loss 0.6223068833351135 train acc 0.785827923737477\n",
            "epoch 9 batch id 4901 loss 0.5771127939224243 train acc 0.7858695929402163\n",
            "epoch 9 batch id 4911 loss 0.5635430812835693 train acc 0.7859333638770107\n",
            "epoch 9 batch id 4921 loss 0.5062335133552551 train acc 0.7859365474497053\n",
            "epoch 9 batch id 4931 loss 0.5489693880081177 train acc 0.7859397181099168\n",
            "epoch 9 batch id 4941 loss 0.5061933994293213 train acc 0.7859397136207246\n",
            "epoch 9 batch id 4951 loss 0.7849470376968384 train acc 0.7860028277115734\n",
            "epoch 9 train acc 0.7860549590111321\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70fdec110e964af6a50a8b3af88d10a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9 loss 0.9201897978782654 test acc 0.7342054618768329\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a17580a6791a45c78d1da6ca217c01df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10 batch id 1 loss 0.4895477294921875 train acc 0.828125\n",
            "epoch 10 batch id 11 loss 0.46005332469940186 train acc 0.8210227272727273\n",
            "epoch 10 batch id 21 loss 0.5363701581954956 train acc 0.8229166666666666\n",
            "epoch 10 batch id 31 loss 0.541837751865387 train acc 0.8034274193548387\n",
            "epoch 10 batch id 41 loss 0.2925683856010437 train acc 0.8029725609756098\n",
            "epoch 10 batch id 51 loss 0.5105231404304504 train acc 0.8036151960784313\n",
            "epoch 10 batch id 61 loss 0.4572632312774658 train acc 0.8030225409836066\n",
            "epoch 10 batch id 71 loss 0.6269370317459106 train acc 0.8030369718309859\n",
            "epoch 10 batch id 81 loss 0.6597670912742615 train acc 0.8026620370370371\n",
            "epoch 10 batch id 91 loss 0.6289849281311035 train acc 0.8035714285714286\n",
            "epoch 10 batch id 101 loss 0.5786073207855225 train acc 0.8043007425742574\n",
            "epoch 10 batch id 111 loss 0.5154005289077759 train acc 0.805884009009009\n",
            "epoch 10 batch id 121 loss 0.7122834324836731 train acc 0.8041064049586777\n",
            "epoch 10 batch id 131 loss 0.6697733402252197 train acc 0.804270038167939\n",
            "epoch 10 batch id 141 loss 0.5410161018371582 train acc 0.8048537234042553\n",
            "epoch 10 batch id 151 loss 0.4577172100543976 train acc 0.8054635761589404\n",
            "epoch 10 batch id 161 loss 0.8220046758651733 train acc 0.8063858695652174\n",
            "epoch 10 batch id 171 loss 0.5628005862236023 train acc 0.8057383040935673\n",
            "epoch 10 batch id 181 loss 0.5259228944778442 train acc 0.8072341160220995\n",
            "epoch 10 batch id 191 loss 0.43044885993003845 train acc 0.8080824607329843\n",
            "epoch 10 batch id 201 loss 0.5368733406066895 train acc 0.8075248756218906\n",
            "epoch 10 batch id 211 loss 0.6630507707595825 train acc 0.8055390995260664\n",
            "epoch 10 batch id 221 loss 0.7986928224563599 train acc 0.8050056561085973\n",
            "epoch 10 batch id 231 loss 0.488827109336853 train acc 0.8049918831168831\n",
            "epoch 10 batch id 241 loss 0.5819222331047058 train acc 0.8030342323651453\n",
            "epoch 10 batch id 251 loss 0.6568405032157898 train acc 0.8023530876494024\n",
            "epoch 10 batch id 261 loss 0.6194269061088562 train acc 0.8033405172413793\n",
            "epoch 10 batch id 271 loss 0.38112401962280273 train acc 0.8040244464944649\n",
            "epoch 10 batch id 281 loss 0.4702036380767822 train acc 0.8037700177935944\n",
            "epoch 10 batch id 291 loss 0.7440634369850159 train acc 0.8038552405498282\n",
            "epoch 10 batch id 301 loss 0.5985766649246216 train acc 0.8043500830564784\n",
            "epoch 10 batch id 311 loss 0.4465595483779907 train acc 0.804863344051447\n",
            "epoch 10 batch id 321 loss 0.5248578786849976 train acc 0.8046631619937694\n",
            "epoch 10 batch id 331 loss 0.7354593873023987 train acc 0.8045222809667674\n",
            "epoch 10 batch id 341 loss 0.7650571465492249 train acc 0.8049395161290323\n",
            "epoch 10 batch id 351 loss 0.8017299175262451 train acc 0.8050213675213675\n",
            "epoch 10 batch id 361 loss 0.7298609614372253 train acc 0.8051419667590027\n",
            "epoch 10 batch id 371 loss 0.7988167405128479 train acc 0.8051297169811321\n",
            "epoch 10 batch id 381 loss 0.3803524076938629 train acc 0.8049540682414699\n",
            "epoch 10 batch id 391 loss 0.6261280179023743 train acc 0.8047074808184144\n",
            "epoch 10 batch id 401 loss 0.5000824332237244 train acc 0.804122506234414\n",
            "epoch 10 batch id 411 loss 0.4771430194377899 train acc 0.8043643552311436\n",
            "epoch 10 batch id 421 loss 0.7160885334014893 train acc 0.8039266627078385\n",
            "epoch 10 batch id 431 loss 0.42877307534217834 train acc 0.8038355568445475\n",
            "epoch 10 batch id 441 loss 0.5893814563751221 train acc 0.8035714285714286\n",
            "epoch 10 batch id 451 loss 0.507483720779419 train acc 0.8041158536585366\n",
            "epoch 10 batch id 461 loss 0.4043460190296173 train acc 0.8042299349240781\n",
            "epoch 10 batch id 471 loss 0.4354592263698578 train acc 0.805234872611465\n",
            "epoch 10 batch id 481 loss 0.7042600512504578 train acc 0.8052559771309772\n",
            "epoch 10 batch id 491 loss 0.673980712890625 train acc 0.805339867617108\n",
            "epoch 10 batch id 501 loss 1.0438300371170044 train acc 0.8049837824351297\n",
            "epoch 10 batch id 511 loss 0.4284858703613281 train acc 0.8045499021526419\n",
            "epoch 10 batch id 521 loss 0.5287726521492004 train acc 0.8046125239923224\n",
            "epoch 10 batch id 531 loss 0.5664140582084656 train acc 0.8049964689265536\n",
            "epoch 10 batch id 541 loss 0.415768027305603 train acc 0.8052506931608133\n",
            "epoch 10 batch id 551 loss 0.5333769917488098 train acc 0.8054956896551724\n",
            "epoch 10 batch id 561 loss 0.37054452300071716 train acc 0.8053420231729055\n",
            "epoch 10 batch id 571 loss 0.8688934445381165 train acc 0.8050295534150613\n",
            "epoch 10 batch id 581 loss 0.40841710567474365 train acc 0.8048891996557659\n",
            "epoch 10 batch id 591 loss 0.40376970171928406 train acc 0.8052294839255499\n",
            "epoch 10 batch id 601 loss 0.5245023369789124 train acc 0.8054544509151415\n",
            "epoch 10 batch id 611 loss 0.5059093236923218 train acc 0.8055186170212766\n",
            "epoch 10 batch id 621 loss 0.6410952210426331 train acc 0.8058574879227053\n",
            "epoch 10 batch id 631 loss 0.5716677904129028 train acc 0.8057894215530903\n",
            "epoch 10 batch id 641 loss 0.6613410711288452 train acc 0.8056503510140406\n",
            "epoch 10 batch id 651 loss 0.36704421043395996 train acc 0.8056835637480799\n",
            "epoch 10 batch id 661 loss 0.6128620505332947 train acc 0.805999432677761\n",
            "epoch 10 batch id 671 loss 0.8386803269386292 train acc 0.8058401639344263\n",
            "epoch 10 batch id 681 loss 0.5389621257781982 train acc 0.8058691262848752\n",
            "epoch 10 batch id 691 loss 0.5388069152832031 train acc 0.805942474674385\n",
            "epoch 10 batch id 701 loss 0.49645867943763733 train acc 0.8059914407988588\n",
            "epoch 10 batch id 711 loss 0.5615392923355103 train acc 0.80568741209564\n",
            "epoch 10 batch id 721 loss 0.45420944690704346 train acc 0.805760228848821\n",
            "epoch 10 batch id 731 loss 0.3118394613265991 train acc 0.8061944254445964\n",
            "epoch 10 batch id 741 loss 0.6650540828704834 train acc 0.806068657219973\n",
            "epoch 10 batch id 751 loss 0.7645257711410522 train acc 0.805863015978695\n",
            "epoch 10 batch id 761 loss 0.4830334484577179 train acc 0.8060528909329829\n",
            "epoch 10 batch id 771 loss 0.739191472530365 train acc 0.8058527885862516\n",
            "epoch 10 batch id 781 loss 0.5155665278434753 train acc 0.8057778489116517\n",
            "epoch 10 batch id 791 loss 0.3200165033340454 train acc 0.8058628318584071\n",
            "epoch 10 batch id 801 loss 0.49957048892974854 train acc 0.805984706616729\n",
            "epoch 10 batch id 811 loss 0.4853227734565735 train acc 0.8061806411837238\n",
            "epoch 10 batch id 821 loss 0.4267604649066925 train acc 0.806238580998782\n",
            "epoch 10 batch id 831 loss 0.541056752204895 train acc 0.8065395607701564\n",
            "epoch 10 batch id 841 loss 0.39580249786376953 train acc 0.8066847502972652\n",
            "epoch 10 batch id 851 loss 0.7477532625198364 train acc 0.8066245593419507\n",
            "epoch 10 batch id 861 loss 0.7267931699752808 train acc 0.8064931765389083\n",
            "epoch 10 batch id 871 loss 0.323794960975647 train acc 0.8066697761194029\n",
            "epoch 10 batch id 881 loss 0.45023486018180847 train acc 0.8064167139614075\n",
            "epoch 10 batch id 891 loss 0.5265026092529297 train acc 0.8065551346801347\n",
            "epoch 10 batch id 901 loss 0.7788030505180359 train acc 0.8063609877913429\n",
            "epoch 10 batch id 911 loss 0.7065909504890442 train acc 0.8062568605927553\n",
            "epoch 10 batch id 921 loss 0.6443427205085754 train acc 0.8060871335504886\n",
            "epoch 10 batch id 931 loss 0.4514569640159607 train acc 0.8060553168635876\n",
            "epoch 10 batch id 941 loss 0.3515040874481201 train acc 0.8062566418703507\n",
            "epoch 10 batch id 951 loss 0.7299596667289734 train acc 0.8061744216614091\n",
            "epoch 10 batch id 961 loss 0.5336579084396362 train acc 0.8059963579604579\n",
            "epoch 10 batch id 971 loss 0.6593891382217407 train acc 0.8063851699279093\n",
            "epoch 10 batch id 981 loss 0.5116338133811951 train acc 0.8068297655453619\n",
            "epoch 10 batch id 991 loss 0.5165663361549377 train acc 0.8068081483350151\n",
            "epoch 10 batch id 1001 loss 0.6176751852035522 train acc 0.8067245254745254\n",
            "epoch 10 batch id 1011 loss 0.5642563104629517 train acc 0.806688921859545\n",
            "epoch 10 batch id 1021 loss 0.424116849899292 train acc 0.806669319294809\n",
            "epoch 10 batch id 1031 loss 0.5481359958648682 train acc 0.8067107177497576\n",
            "epoch 10 batch id 1041 loss 0.7076107263565063 train acc 0.8068413784822286\n",
            "epoch 10 batch id 1051 loss 0.3291223645210266 train acc 0.8066722169362512\n",
            "epoch 10 batch id 1061 loss 0.6190761923789978 train acc 0.8067124175306315\n",
            "epoch 10 batch id 1071 loss 0.35874781012535095 train acc 0.8067080999066293\n",
            "epoch 10 batch id 1081 loss 0.6243076920509338 train acc 0.806588228492137\n",
            "epoch 10 batch id 1091 loss 0.8790863752365112 train acc 0.8063273373052245\n",
            "epoch 10 batch id 1101 loss 0.4781608581542969 train acc 0.8064969346049047\n",
            "epoch 10 batch id 1111 loss 0.5749884247779846 train acc 0.8065228397839784\n",
            "epoch 10 batch id 1121 loss 0.31387272477149963 train acc 0.8066458519179304\n",
            "epoch 10 batch id 1131 loss 0.6607438325881958 train acc 0.8066699823165341\n",
            "epoch 10 batch id 1141 loss 0.5091065168380737 train acc 0.8067484662576687\n",
            "epoch 10 batch id 1151 loss 0.6114277243614197 train acc 0.8066626846220678\n",
            "epoch 10 batch id 1161 loss 0.5099422931671143 train acc 0.8064976313522825\n",
            "epoch 10 batch id 1171 loss 0.4414442777633667 train acc 0.8065622331340735\n",
            "epoch 10 batch id 1181 loss 0.5923577547073364 train acc 0.8065198983911939\n",
            "epoch 10 batch id 1191 loss 0.5450098514556885 train acc 0.8065307514693535\n",
            "epoch 10 batch id 1201 loss 0.8382883667945862 train acc 0.80676259367194\n",
            "epoch 10 batch id 1211 loss 0.42176851630210876 train acc 0.8068228736581338\n",
            "epoch 10 batch id 1221 loss 0.5209522843360901 train acc 0.8068181818181818\n",
            "epoch 10 batch id 1231 loss 0.6911704540252686 train acc 0.8066866368805848\n",
            "epoch 10 batch id 1241 loss 0.5484467148780823 train acc 0.8065320306204674\n",
            "epoch 10 batch id 1251 loss 0.6255549192428589 train acc 0.8066296962430056\n",
            "epoch 10 batch id 1261 loss 0.4238152503967285 train acc 0.8069364591593973\n",
            "epoch 10 batch id 1271 loss 0.41843655705451965 train acc 0.8069802321007081\n",
            "epoch 10 batch id 1281 loss 0.5677487254142761 train acc 0.8070111241217799\n",
            "epoch 10 batch id 1291 loss 0.46873214840888977 train acc 0.8069931254841208\n",
            "epoch 10 batch id 1301 loss 0.8883936405181885 train acc 0.8068673136049193\n",
            "epoch 10 batch id 1311 loss 0.5750845670700073 train acc 0.8068268497330282\n",
            "epoch 10 batch id 1321 loss 0.37814822793006897 train acc 0.8066568887206662\n",
            "epoch 10 batch id 1331 loss 0.5089806318283081 train acc 0.8065833959429001\n",
            "epoch 10 batch id 1341 loss 0.4427136778831482 train acc 0.8066042132736764\n",
            "epoch 10 batch id 1351 loss 0.3970383405685425 train acc 0.8065668948926721\n",
            "epoch 10 batch id 1361 loss 0.5940115451812744 train acc 0.8066449301983836\n",
            "epoch 10 batch id 1371 loss 0.4157162606716156 train acc 0.8067332239241429\n",
            "epoch 10 batch id 1381 loss 0.726667046546936 train acc 0.8067070963070239\n",
            "epoch 10 batch id 1391 loss 0.21524186432361603 train acc 0.807007099209202\n",
            "epoch 10 batch id 1401 loss 0.5953751802444458 train acc 0.8069570842255531\n",
            "epoch 10 batch id 1411 loss 0.5582228302955627 train acc 0.8071071048901488\n",
            "epoch 10 batch id 1421 loss 0.48241937160491943 train acc 0.8071560520760028\n",
            "epoch 10 batch id 1431 loss 0.47008946537971497 train acc 0.8072916666666666\n",
            "epoch 10 batch id 1441 loss 0.5231863856315613 train acc 0.8072952810548231\n",
            "epoch 10 batch id 1451 loss 0.6039963364601135 train acc 0.8074172984148863\n",
            "epoch 10 batch id 1461 loss 0.400216281414032 train acc 0.807398613963039\n",
            "epoch 10 batch id 1471 loss 0.6286792755126953 train acc 0.8075288919102651\n",
            "epoch 10 batch id 1481 loss 0.4154720604419708 train acc 0.8075624577987845\n",
            "epoch 10 batch id 1491 loss 0.4840559661388397 train acc 0.8076270120724346\n",
            "epoch 10 batch id 1501 loss 0.5242080092430115 train acc 0.8078052131912059\n",
            "epoch 10 batch id 1511 loss 0.41089653968811035 train acc 0.8077225347452018\n",
            "epoch 10 batch id 1521 loss 0.3439761996269226 train acc 0.8078772189349113\n",
            "epoch 10 batch id 1531 loss 0.5967874526977539 train acc 0.8079788536903985\n",
            "epoch 10 batch id 1541 loss 0.5984597206115723 train acc 0.8080386112913692\n",
            "epoch 10 batch id 1551 loss 0.45050856471061707 train acc 0.8080472275951\n",
            "epoch 10 batch id 1561 loss 0.6032557487487793 train acc 0.8080657431133889\n",
            "epoch 10 batch id 1571 loss 0.5723607540130615 train acc 0.8082033736473584\n",
            "epoch 10 batch id 1581 loss 0.6079268455505371 train acc 0.808191018342821\n",
            "epoch 10 batch id 1591 loss 0.4947769045829773 train acc 0.8083850565681961\n",
            "epoch 10 batch id 1601 loss 0.7502585649490356 train acc 0.8084888351030606\n",
            "epoch 10 batch id 1611 loss 0.4545850455760956 train acc 0.8084264432029795\n",
            "epoch 10 batch id 1621 loss 0.330425500869751 train acc 0.8084033775447255\n",
            "epoch 10 batch id 1631 loss 0.3543681502342224 train acc 0.8083135346413244\n",
            "epoch 10 batch id 1641 loss 0.6873728036880493 train acc 0.8084723491773309\n",
            "epoch 10 batch id 1651 loss 0.5005890727043152 train acc 0.8084399606299213\n",
            "epoch 10 batch id 1661 loss 0.7177062630653381 train acc 0.808426776038531\n",
            "epoch 10 batch id 1671 loss 0.5955377221107483 train acc 0.8083950478755236\n",
            "epoch 10 batch id 1681 loss 0.8236691951751709 train acc 0.8082056811421773\n",
            "epoch 10 batch id 1691 loss 0.5221970081329346 train acc 0.8084158781785925\n",
            "epoch 10 batch id 1701 loss 0.5481950044631958 train acc 0.8084858171663727\n",
            "epoch 10 batch id 1711 loss 0.6505617499351501 train acc 0.8083814289888954\n",
            "epoch 10 batch id 1721 loss 0.5047335624694824 train acc 0.8084689134224288\n",
            "epoch 10 batch id 1731 loss 0.24720469117164612 train acc 0.8085644136337378\n",
            "epoch 10 batch id 1741 loss 0.6553180813789368 train acc 0.8085152211372775\n",
            "epoch 10 batch id 1751 loss 0.44608479738235474 train acc 0.8085201313535123\n",
            "epoch 10 batch id 1761 loss 0.42843011021614075 train acc 0.8086492049971608\n",
            "epoch 10 batch id 1771 loss 0.47136685252189636 train acc 0.808891516092603\n",
            "epoch 10 batch id 1781 loss 0.38471803069114685 train acc 0.8090082818641213\n",
            "epoch 10 batch id 1791 loss 0.7385367155075073 train acc 0.809088847012842\n",
            "epoch 10 batch id 1801 loss 0.4510694742202759 train acc 0.8091598417545808\n",
            "epoch 10 batch id 1811 loss 0.40323054790496826 train acc 0.80930770292656\n",
            "epoch 10 batch id 1821 loss 0.46835631132125854 train acc 0.8093509747391543\n",
            "epoch 10 batch id 1831 loss 0.26625463366508484 train acc 0.8094279082468596\n",
            "epoch 10 batch id 1841 loss 0.37544921040534973 train acc 0.8095040059750136\n",
            "epoch 10 batch id 1851 loss 0.6221024990081787 train acc 0.8095117504051864\n",
            "epoch 10 batch id 1861 loss 0.4955846667289734 train acc 0.8096537479849544\n",
            "epoch 10 batch id 1871 loss 0.4925552010536194 train acc 0.8099361972207376\n",
            "epoch 10 batch id 1881 loss 0.47218331694602966 train acc 0.8100328947368421\n",
            "epoch 10 batch id 1891 loss 0.48605430126190186 train acc 0.8102607747223691\n",
            "epoch 10 batch id 1901 loss 0.5516893863677979 train acc 0.8102807732772225\n",
            "epoch 10 batch id 1911 loss 0.35582593083381653 train acc 0.810251504447933\n",
            "epoch 10 batch id 1921 loss 0.4624849855899811 train acc 0.8101656038521603\n",
            "epoch 10 batch id 1931 loss 0.6976824402809143 train acc 0.8102990678404971\n",
            "epoch 10 batch id 1941 loss 0.42661261558532715 train acc 0.8104875064399794\n",
            "epoch 10 batch id 1951 loss 0.7121268510818481 train acc 0.810545873910815\n",
            "epoch 10 batch id 1961 loss 0.2706683874130249 train acc 0.8105956782253952\n",
            "epoch 10 batch id 1971 loss 0.6700040102005005 train acc 0.8105657026889903\n",
            "epoch 10 batch id 1981 loss 0.6693800091743469 train acc 0.8105281423523473\n",
            "epoch 10 batch id 1991 loss 0.632508397102356 train acc 0.8104517202410849\n",
            "epoch 10 batch id 2001 loss 0.699927568435669 train acc 0.8103604447776112\n",
            "epoch 10 batch id 2011 loss 0.6799854040145874 train acc 0.8102856166086524\n",
            "epoch 10 batch id 2021 loss 0.8309449553489685 train acc 0.8103274987629886\n",
            "epoch 10 batch id 2031 loss 0.4230995178222656 train acc 0.8102997291974396\n",
            "epoch 10 batch id 2041 loss 0.36084380745887756 train acc 0.8103258206761391\n",
            "epoch 10 batch id 2051 loss 0.581045925617218 train acc 0.8104049853729888\n",
            "epoch 10 batch id 2061 loss 0.5255169868469238 train acc 0.8104378942261038\n",
            "epoch 10 batch id 2071 loss 0.5568979978561401 train acc 0.8104478512795751\n",
            "epoch 10 batch id 2081 loss 0.7343048453330994 train acc 0.810337578087458\n",
            "epoch 10 batch id 2091 loss 0.6319756507873535 train acc 0.8102806671449068\n",
            "epoch 10 batch id 2101 loss 0.3628363311290741 train acc 0.8103284150404569\n",
            "epoch 10 batch id 2111 loss 0.47174328565597534 train acc 0.8104053173851256\n",
            "epoch 10 batch id 2121 loss 0.2572661340236664 train acc 0.810400459688826\n",
            "epoch 10 batch id 2131 loss 0.759071409702301 train acc 0.810483634443923\n",
            "epoch 10 batch id 2141 loss 0.6090565323829651 train acc 0.8103981784212985\n",
            "epoch 10 batch id 2151 loss 0.3641083836555481 train acc 0.8104805904230591\n",
            "epoch 10 batch id 2161 loss 0.5303980112075806 train acc 0.8103887089310504\n",
            "epoch 10 batch id 2171 loss 0.4003998041152954 train acc 0.810398433901428\n",
            "epoch 10 batch id 2181 loss 0.5353690981864929 train acc 0.8104009055479138\n",
            "epoch 10 batch id 2191 loss 0.4123428165912628 train acc 0.8105174577818348\n",
            "epoch 10 batch id 2201 loss 0.5385145545005798 train acc 0.8104909700136301\n",
            "epoch 10 batch id 2211 loss 0.35417014360427856 train acc 0.8104717887833559\n",
            "epoch 10 batch id 2221 loss 0.6146812438964844 train acc 0.8103753939666817\n",
            "epoch 10 batch id 2231 loss 0.5545024871826172 train acc 0.8104619565217391\n",
            "epoch 10 batch id 2241 loss 0.4303087294101715 train acc 0.8104849955377064\n",
            "epoch 10 batch id 2251 loss 0.6042502522468567 train acc 0.8103759440248778\n",
            "epoch 10 batch id 2261 loss 0.5295259356498718 train acc 0.8103438743918621\n",
            "epoch 10 batch id 2271 loss 0.7602812051773071 train acc 0.8103740092470277\n",
            "epoch 10 batch id 2281 loss 0.6423174738883972 train acc 0.8105408811924595\n",
            "epoch 10 batch id 2291 loss 0.43196699023246765 train acc 0.8107335770405937\n",
            "epoch 10 batch id 2301 loss 0.4568370580673218 train acc 0.8107616253802694\n",
            "epoch 10 batch id 2311 loss 0.5339028239250183 train acc 0.8108164755517092\n",
            "epoch 10 batch id 2321 loss 0.4559648633003235 train acc 0.8108573890564412\n",
            "epoch 10 batch id 2331 loss 0.5361651182174683 train acc 0.8107907014157014\n",
            "epoch 10 batch id 2341 loss 0.5756019353866577 train acc 0.8108113519863306\n",
            "epoch 10 batch id 2351 loss 0.3849896490573883 train acc 0.8108717035304126\n",
            "epoch 10 batch id 2361 loss 0.837192177772522 train acc 0.8109183079203727\n",
            "epoch 10 batch id 2371 loss 0.3402077555656433 train acc 0.8110370097005483\n",
            "epoch 10 batch id 2381 loss 0.4830833971500397 train acc 0.8110890907181857\n",
            "epoch 10 batch id 2391 loss 0.48058682680130005 train acc 0.811108061480552\n",
            "epoch 10 batch id 2401 loss 0.47404783964157104 train acc 0.8111333819241983\n",
            "epoch 10 batch id 2411 loss 0.5504893064498901 train acc 0.8111714537536292\n",
            "epoch 10 batch id 2421 loss 0.5531017184257507 train acc 0.8110930400660884\n",
            "epoch 10 batch id 2431 loss 0.5215375423431396 train acc 0.8110538358700123\n",
            "epoch 10 batch id 2441 loss 0.49823516607284546 train acc 0.8111173699303564\n",
            "epoch 10 batch id 2451 loss 0.5393649935722351 train acc 0.8110847613219094\n",
            "epoch 10 batch id 2461 loss 0.5327163338661194 train acc 0.8111476533929297\n",
            "epoch 10 batch id 2471 loss 0.5722754001617432 train acc 0.8110519526507487\n",
            "epoch 10 batch id 2481 loss 0.6560103297233582 train acc 0.8111018742442564\n",
            "epoch 10 batch id 2491 loss 0.5591822266578674 train acc 0.8110573063026897\n",
            "epoch 10 batch id 2501 loss 0.33758223056793213 train acc 0.8110380847660935\n",
            "epoch 10 batch id 2511 loss 0.5528843402862549 train acc 0.8111310234966149\n",
            "epoch 10 batch id 2521 loss 0.5358267426490784 train acc 0.8110806723522411\n",
            "epoch 10 batch id 2531 loss 0.5155361294746399 train acc 0.8111109739233504\n",
            "epoch 10 batch id 2541 loss 0.5676929950714111 train acc 0.8112148268398268\n",
            "epoch 10 batch id 2551 loss 0.5498743057250977 train acc 0.811219864758918\n",
            "epoch 10 batch id 2561 loss 0.5544432997703552 train acc 0.8111638520109332\n",
            "epoch 10 batch id 2571 loss 0.6329911947250366 train acc 0.8112359004278491\n",
            "epoch 10 batch id 2581 loss 0.4966023862361908 train acc 0.8113376598217745\n",
            "epoch 10 batch id 2591 loss 0.4996693432331085 train acc 0.8113119934388268\n",
            "epoch 10 batch id 2601 loss 0.6879338026046753 train acc 0.8113526047673971\n",
            "epoch 10 batch id 2611 loss 0.5920505523681641 train acc 0.8112552661815396\n",
            "epoch 10 batch id 2621 loss 0.5677244663238525 train acc 0.8112063620755436\n",
            "epoch 10 batch id 2631 loss 0.6566348075866699 train acc 0.8111637685290763\n",
            "epoch 10 batch id 2641 loss 0.42724528908729553 train acc 0.8111924933737221\n",
            "epoch 10 batch id 2651 loss 0.5178491473197937 train acc 0.8111915314975481\n",
            "epoch 10 batch id 2661 loss 0.5067893266677856 train acc 0.811184704998121\n",
            "epoch 10 batch id 2671 loss 0.6406667232513428 train acc 0.8111837794833395\n",
            "epoch 10 batch id 2681 loss 0.4450262188911438 train acc 0.8111187523312197\n",
            "epoch 10 batch id 2691 loss 0.7323220372200012 train acc 0.8110658212560387\n",
            "epoch 10 batch id 2701 loss 0.529441773891449 train acc 0.8111752591632728\n",
            "epoch 10 batch id 2711 loss 0.37373417615890503 train acc 0.8111801457026927\n",
            "epoch 10 batch id 2721 loss 0.3883717358112335 train acc 0.8112424200661521\n",
            "epoch 10 batch id 2731 loss 0.37799975275993347 train acc 0.8111612046869279\n",
            "epoch 10 batch id 2741 loss 0.4505991041660309 train acc 0.8111546880700474\n",
            "epoch 10 batch id 2751 loss 0.48927828669548035 train acc 0.8112106961105052\n",
            "epoch 10 batch id 2761 loss 0.448623389005661 train acc 0.8113172310756972\n",
            "epoch 10 batch id 2771 loss 0.6347629427909851 train acc 0.8113553320101047\n",
            "epoch 10 batch id 2781 loss 0.7387232184410095 train acc 0.8113257371449119\n",
            "epoch 10 batch id 2791 loss 0.4177367687225342 train acc 0.8113971246864923\n",
            "epoch 10 batch id 2801 loss 0.4725510776042938 train acc 0.8114122188504106\n",
            "epoch 10 batch id 2811 loss 0.4955202639102936 train acc 0.8114827908217717\n",
            "epoch 10 batch id 2821 loss 0.525878369808197 train acc 0.811491935483871\n",
            "epoch 10 batch id 2831 loss 0.5131776332855225 train acc 0.8115506888025432\n",
            "epoch 10 batch id 2841 loss 0.5998633503913879 train acc 0.811603528687082\n",
            "epoch 10 batch id 2851 loss 0.5363415479660034 train acc 0.8115080235005261\n",
            "epoch 10 batch id 2861 loss 0.24742762744426727 train acc 0.811566104508913\n",
            "epoch 10 batch id 2871 loss 0.4179244041442871 train acc 0.8115693573667712\n",
            "epoch 10 batch id 2881 loss 0.48881152272224426 train acc 0.8116268222839292\n",
            "epoch 10 batch id 2891 loss 0.3526395261287689 train acc 0.8116460567277759\n",
            "epoch 10 batch id 2901 loss 0.5243152379989624 train acc 0.8116059117545674\n",
            "epoch 10 batch id 2911 loss 0.4215172827243805 train acc 0.8116572913088286\n",
            "epoch 10 batch id 2921 loss 0.16600842773914337 train acc 0.8116013351591921\n",
            "epoch 10 batch id 2931 loss 0.2900323271751404 train acc 0.8115670846127602\n",
            "epoch 10 batch id 2941 loss 0.5538533926010132 train acc 0.8115171285277116\n",
            "epoch 10 batch id 2951 loss 0.527529239654541 train acc 0.8115998813961369\n",
            "epoch 10 batch id 2961 loss 0.5385181903839111 train acc 0.811613475177305\n",
            "epoch 10 batch id 2971 loss 0.44918885827064514 train acc 0.8116427549646583\n",
            "epoch 10 batch id 2981 loss 0.5439288020133972 train acc 0.8116508721905401\n",
            "epoch 10 batch id 2991 loss 0.5571584701538086 train acc 0.8116432631227014\n",
            "epoch 10 batch id 3001 loss 0.5050127506256104 train acc 0.8116513245584805\n",
            "epoch 10 batch id 3011 loss 0.4504115581512451 train acc 0.811638575224178\n",
            "epoch 10 batch id 3021 loss 0.5838297009468079 train acc 0.8116672873220788\n",
            "epoch 10 batch id 3031 loss 0.3787083923816681 train acc 0.8117267403497196\n",
            "epoch 10 batch id 3041 loss 0.684119701385498 train acc 0.8116470733311411\n",
            "epoch 10 batch id 3051 loss 0.5113564729690552 train acc 0.81164474762373\n",
            "epoch 10 batch id 3061 loss 0.41674119234085083 train acc 0.8116118098660569\n",
            "epoch 10 batch id 3071 loss 0.37427690625190735 train acc 0.8116757570823836\n",
            "epoch 10 batch id 3081 loss 0.719369113445282 train acc 0.8117190035702694\n",
            "epoch 10 batch id 3091 loss 0.48997023701667786 train acc 0.8117720802329343\n",
            "epoch 10 batch id 3101 loss 0.5691047310829163 train acc 0.8117895436955821\n",
            "epoch 10 batch id 3111 loss 0.6248301863670349 train acc 0.8117616923818708\n",
            "epoch 10 batch id 3121 loss 0.569334089756012 train acc 0.8118041092598526\n",
            "epoch 10 batch id 3131 loss 0.41888436675071716 train acc 0.8119011497923986\n",
            "epoch 10 batch id 3141 loss 0.5529743432998657 train acc 0.8118732091690545\n",
            "epoch 10 batch id 3151 loss 0.4611845910549164 train acc 0.8119842907013647\n",
            "epoch 10 batch id 3161 loss 0.4398234486579895 train acc 0.8119908652325214\n",
            "epoch 10 batch id 3171 loss 0.40892770886421204 train acc 0.8120614553768527\n",
            "epoch 10 batch id 3181 loss 0.46503746509552 train acc 0.8120677459918265\n",
            "epoch 10 batch id 3191 loss 0.25653666257858276 train acc 0.8121719288624256\n",
            "epoch 10 batch id 3201 loss 0.626854658126831 train acc 0.8121778350515464\n",
            "epoch 10 batch id 3211 loss 0.5084013342857361 train acc 0.8121642401121146\n",
            "epoch 10 batch id 3221 loss 0.6743532419204712 train acc 0.8121652825209562\n",
            "epoch 10 batch id 3231 loss 0.41589128971099854 train acc 0.8122340219746209\n",
            "epoch 10 batch id 3241 loss 0.4067355990409851 train acc 0.8122734109842641\n",
            "epoch 10 batch id 3251 loss 0.46045342087745667 train acc 0.8123317825284527\n",
            "epoch 10 batch id 3261 loss 0.43052995204925537 train acc 0.812370630174793\n",
            "epoch 10 batch id 3271 loss 0.3467239737510681 train acc 0.8123805793335371\n",
            "epoch 10 batch id 3281 loss 0.7300512790679932 train acc 0.8124095169155745\n",
            "epoch 10 batch id 3291 loss 0.39486801624298096 train acc 0.8124857566089334\n",
            "epoch 10 batch id 3301 loss 0.47398877143859863 train acc 0.8125615343835202\n",
            "epoch 10 batch id 3311 loss 0.5841769576072693 train acc 0.8126698882512836\n",
            "epoch 10 batch id 3321 loss 0.40945249795913696 train acc 0.8126317374284854\n",
            "epoch 10 batch id 3331 loss 0.8630238771438599 train acc 0.8126125788051636\n",
            "epoch 10 batch id 3341 loss 0.6913303136825562 train acc 0.8125654744088596\n",
            "epoch 10 batch id 3351 loss 0.3804888427257538 train acc 0.8125419650850493\n",
            "epoch 10 batch id 3361 loss 0.5439627170562744 train acc 0.8126069250223148\n",
            "epoch 10 batch id 3371 loss 0.6159027218818665 train acc 0.8126205132008306\n",
            "epoch 10 batch id 3381 loss 0.293407142162323 train acc 0.8126340209997043\n",
            "epoch 10 batch id 3391 loss 0.5700510144233704 train acc 0.8126981347685048\n",
            "epoch 10 batch id 3401 loss 0.6378034949302673 train acc 0.8127710599823581\n",
            "epoch 10 batch id 3411 loss 0.36951982975006104 train acc 0.8128114922310173\n",
            "epoch 10 batch id 3421 loss 0.5587432980537415 train acc 0.8128562554808536\n",
            "epoch 10 batch id 3431 loss 0.5658043026924133 train acc 0.8128552171378607\n",
            "epoch 10 batch id 3441 loss 0.5168122053146362 train acc 0.8128360215053764\n",
            "epoch 10 batch id 3451 loss 0.5712500810623169 train acc 0.8128667415241959\n",
            "epoch 10 batch id 3461 loss 0.6262337565422058 train acc 0.8129379153423866\n",
            "epoch 10 batch id 3471 loss 0.5854976177215576 train acc 0.8129996758859118\n",
            "epoch 10 batch id 3481 loss 0.4256581664085388 train acc 0.813002729100833\n",
            "epoch 10 batch id 3491 loss 0.35539767146110535 train acc 0.8130684259524491\n",
            "epoch 10 batch id 3501 loss 0.6022764444351196 train acc 0.8131069694373037\n",
            "epoch 10 batch id 3511 loss 0.585541307926178 train acc 0.8131586442608943\n",
            "epoch 10 batch id 3521 loss 0.769926905632019 train acc 0.8132189008804317\n",
            "epoch 10 batch id 3531 loss 0.7961947321891785 train acc 0.8132611158312093\n",
            "epoch 10 batch id 3541 loss 0.37847989797592163 train acc 0.8133428057046033\n",
            "epoch 10 batch id 3551 loss 0.4525465965270996 train acc 0.8133668332863982\n",
            "epoch 10 batch id 3561 loss 0.33778253197669983 train acc 0.8135004212299916\n",
            "epoch 10 batch id 3571 loss 0.6733123064041138 train acc 0.8135019952394287\n",
            "epoch 10 batch id 3581 loss 0.565653920173645 train acc 0.8135384669086847\n",
            "epoch 10 batch id 3591 loss 0.6059432625770569 train acc 0.8135660331384016\n",
            "epoch 10 batch id 3601 loss 0.4223752021789551 train acc 0.8136108025548459\n",
            "epoch 10 batch id 3611 loss 0.3937705159187317 train acc 0.8136769592910551\n",
            "epoch 10 batch id 3621 loss 0.33591389656066895 train acc 0.8137341204087268\n",
            "epoch 10 batch id 3631 loss 0.42230868339538574 train acc 0.8137694505645827\n",
            "epoch 10 batch id 3641 loss 0.8155345320701599 train acc 0.8137573812139522\n",
            "epoch 10 batch id 3651 loss 0.5421457290649414 train acc 0.8137924541221583\n",
            "epoch 10 batch id 3661 loss 0.5046723484992981 train acc 0.8138358713466266\n",
            "epoch 10 batch id 3671 loss 0.44114407896995544 train acc 0.8138620266957233\n",
            "epoch 10 batch id 3681 loss 0.5966941714286804 train acc 0.8138837951643575\n",
            "epoch 10 batch id 3691 loss 0.6645736694335938 train acc 0.8139266120292603\n",
            "epoch 10 batch id 3701 loss 0.4290289282798767 train acc 0.813964975682248\n",
            "epoch 10 batch id 3711 loss 0.45945659279823303 train acc 0.8140115534896254\n",
            "epoch 10 batch id 3721 loss 0.4105437099933624 train acc 0.8140914740661113\n",
            "epoch 10 batch id 3731 loss 0.4061405062675476 train acc 0.814103960064326\n",
            "epoch 10 batch id 3741 loss 0.49044105410575867 train acc 0.8141038492381716\n",
            "epoch 10 batch id 3751 loss 0.466949999332428 train acc 0.8140370901093041\n",
            "epoch 10 batch id 3761 loss 0.3740176260471344 train acc 0.8140662390321723\n",
            "epoch 10 batch id 3771 loss 0.3409712016582489 train acc 0.8140786595067622\n",
            "epoch 10 batch id 3781 loss 0.5551635026931763 train acc 0.8141240743189633\n",
            "epoch 10 batch id 3791 loss 0.3329630196094513 train acc 0.814218708783962\n",
            "epoch 10 batch id 3801 loss 0.3611334264278412 train acc 0.8142717377006051\n",
            "epoch 10 batch id 3811 loss 0.42609018087387085 train acc 0.8143285882970349\n",
            "epoch 10 batch id 3821 loss 0.4301278293132782 train acc 0.8143646951059932\n",
            "epoch 10 batch id 3831 loss 0.49445685744285583 train acc 0.8144373205429392\n",
            "epoch 10 batch id 3841 loss 0.2914579212665558 train acc 0.8144648203592815\n",
            "epoch 10 batch id 3851 loss 0.40532535314559937 train acc 0.8145368086211374\n",
            "epoch 10 batch id 3861 loss 0.30248773097991943 train acc 0.8145436739186739\n",
            "epoch 10 batch id 3871 loss 0.5153653621673584 train acc 0.8145706858692844\n",
            "epoch 10 batch id 3881 loss 0.5301262140274048 train acc 0.8146056106673538\n",
            "epoch 10 batch id 3891 loss 0.642294704914093 train acc 0.8146443716268311\n",
            "epoch 10 batch id 3901 loss 0.34682872891426086 train acc 0.8146068315816457\n",
            "epoch 10 batch id 3911 loss 0.4155353307723999 train acc 0.8146453912042956\n",
            "epoch 10 batch id 3921 loss 0.40939661860466003 train acc 0.8146160099464422\n",
            "epoch 10 batch id 3931 loss 0.4685153663158417 train acc 0.814638450775884\n",
            "epoch 10 batch id 3941 loss 0.391011118888855 train acc 0.8146449188023345\n",
            "epoch 10 batch id 3951 loss 0.41456326842308044 train acc 0.8146948557327259\n",
            "epoch 10 batch id 3961 loss 0.5550575852394104 train acc 0.8147208722544812\n",
            "epoch 10 batch id 3971 loss 0.4892628490924835 train acc 0.8148215185091916\n",
            "epoch 10 batch id 3981 loss 0.4332817494869232 train acc 0.8148706355187139\n",
            "epoch 10 batch id 3991 loss 0.6526883244514465 train acc 0.814942996742671\n",
            "epoch 10 batch id 4001 loss 0.39324086904525757 train acc 0.8149134591352162\n",
            "epoch 10 batch id 4011 loss 0.6078765988349915 train acc 0.8149308152580403\n",
            "epoch 10 batch id 4021 loss 0.42288708686828613 train acc 0.8149403133548868\n",
            "epoch 10 batch id 4031 loss 0.5499250888824463 train acc 0.8150040312577524\n",
            "epoch 10 batch id 4041 loss 0.6536889672279358 train acc 0.8150403674832962\n",
            "epoch 10 batch id 4051 loss 0.5776798725128174 train acc 0.8150880955319674\n",
            "epoch 10 batch id 4061 loss 0.6625745296478271 train acc 0.8150855700566363\n",
            "epoch 10 batch id 4071 loss 0.817449688911438 train acc 0.8150600282485876\n",
            "epoch 10 batch id 4081 loss 0.28189602494239807 train acc 0.8151647880421465\n",
            "epoch 10 batch id 4091 loss 0.27225151658058167 train acc 0.8151888291371303\n",
            "epoch 10 batch id 4101 loss 0.5493909120559692 train acc 0.8151670324311143\n",
            "epoch 10 batch id 4111 loss 0.3869268596172333 train acc 0.815175747993189\n",
            "epoch 10 batch id 4121 loss 0.42730796337127686 train acc 0.8151920043678719\n",
            "epoch 10 batch id 4131 loss 0.3483896255493164 train acc 0.8152460058097313\n",
            "epoch 10 batch id 4141 loss 0.35452425479888916 train acc 0.8153186126539483\n",
            "epoch 10 batch id 4151 loss 0.48047152161598206 train acc 0.8153946338231751\n",
            "epoch 10 batch id 4161 loss 0.547635018825531 train acc 0.8153764119202115\n",
            "epoch 10 batch id 4171 loss 0.48456010222435 train acc 0.8153919923279789\n",
            "epoch 10 batch id 4181 loss 0.2676162123680115 train acc 0.8153776010523798\n",
            "epoch 10 batch id 4191 loss 0.604000449180603 train acc 0.8154415712240516\n",
            "epoch 10 batch id 4201 loss 0.5622116923332214 train acc 0.8154457272078076\n",
            "epoch 10 batch id 4211 loss 0.7122628092765808 train acc 0.8154350213725956\n",
            "epoch 10 batch id 4221 loss 0.4359377920627594 train acc 0.8154206645344705\n",
            "epoch 10 batch id 4231 loss 0.4187399744987488 train acc 0.8154506913259276\n",
            "epoch 10 batch id 4241 loss 0.5297703146934509 train acc 0.8154916293327046\n",
            "epoch 10 batch id 4251 loss 0.5566045641899109 train acc 0.8155654551870148\n",
            "epoch 10 batch id 4261 loss 0.5581480264663696 train acc 0.8155435930532738\n",
            "epoch 10 batch id 4271 loss 0.3027825951576233 train acc 0.815562075626317\n",
            "epoch 10 batch id 4281 loss 0.3508833944797516 train acc 0.8155914213968699\n",
            "epoch 10 batch id 4291 loss 0.4334617853164673 train acc 0.8156060650198089\n",
            "epoch 10 batch id 4301 loss 0.27893033623695374 train acc 0.8155952104161823\n",
            "epoch 10 batch id 4311 loss 0.3708483576774597 train acc 0.815642397355602\n",
            "epoch 10 batch id 4321 loss 0.43402570486068726 train acc 0.8156459731543624\n",
            "epoch 10 batch id 4331 loss 0.3230288028717041 train acc 0.8157108635419071\n",
            "epoch 10 batch id 4341 loss 0.4391957223415375 train acc 0.8157034669431007\n",
            "epoch 10 batch id 4351 loss 0.5152758359909058 train acc 0.8157679269133532\n",
            "epoch 10 batch id 4361 loss 0.714966893196106 train acc 0.8158249254758083\n",
            "epoch 10 batch id 4371 loss 0.4910415709018707 train acc 0.815878088538092\n",
            "epoch 10 batch id 4381 loss 0.3648231029510498 train acc 0.8158917769915545\n",
            "epoch 10 batch id 4391 loss 0.42836058139801025 train acc 0.8158876110225461\n",
            "epoch 10 batch id 4401 loss 0.6050286889076233 train acc 0.8158870143149284\n",
            "epoch 10 batch id 4411 loss 0.3725035488605499 train acc 0.8158687089095443\n",
            "epoch 10 batch id 4421 loss 0.5626766681671143 train acc 0.8159247059488803\n",
            "epoch 10 batch id 4431 loss 0.5714185833930969 train acc 0.8159698713608666\n",
            "epoch 10 batch id 4441 loss 0.3136613368988037 train acc 0.8160113150191398\n",
            "epoch 10 batch id 4451 loss 0.7007408738136292 train acc 0.8159893844079982\n",
            "epoch 10 batch id 4461 loss 0.35399067401885986 train acc 0.8160936449226631\n",
            "epoch 10 batch id 4471 loss 0.7068843841552734 train acc 0.8160716282710803\n",
            "epoch 10 batch id 4481 loss 0.45317840576171875 train acc 0.8160810923900915\n",
            "epoch 10 batch id 4491 loss 0.31209591031074524 train acc 0.8160800768203073\n",
            "epoch 10 batch id 4501 loss 0.30960288643836975 train acc 0.816124194623417\n",
            "epoch 10 batch id 4511 loss 0.663465142250061 train acc 0.8161092329860341\n",
            "epoch 10 batch id 4521 loss 0.4110648036003113 train acc 0.816073600973236\n",
            "epoch 10 batch id 4531 loss 0.35881346464157104 train acc 0.8160243323769587\n",
            "epoch 10 batch id 4541 loss 0.4371145963668823 train acc 0.8160544208324158\n",
            "epoch 10 batch id 4551 loss 0.6012001037597656 train acc 0.8160706438145463\n",
            "epoch 10 batch id 4561 loss 0.6784209609031677 train acc 0.816059389388292\n",
            "epoch 10 batch id 4571 loss 0.5903963446617126 train acc 0.8161575694596368\n",
            "epoch 10 batch id 4581 loss 0.6121336221694946 train acc 0.8161700502073783\n",
            "epoch 10 batch id 4591 loss 0.468770295381546 train acc 0.8161790731866696\n",
            "epoch 10 batch id 4601 loss 0.5106679201126099 train acc 0.8162152249510976\n",
            "epoch 10 batch id 4611 loss 0.47393888235092163 train acc 0.8163291585339406\n",
            "epoch 10 batch id 4621 loss 0.42191869020462036 train acc 0.8164020233715646\n",
            "epoch 10 batch id 4631 loss 0.35025662183761597 train acc 0.816467825523645\n",
            "epoch 10 batch id 4641 loss 0.4649690091609955 train acc 0.8164727429433312\n",
            "epoch 10 batch id 4651 loss 0.6463721990585327 train acc 0.8164642012470437\n",
            "epoch 10 batch id 4661 loss 0.5552025437355042 train acc 0.816475809912036\n",
            "epoch 10 batch id 4671 loss 0.4014448821544647 train acc 0.8165040944123314\n",
            "epoch 10 batch id 4681 loss 0.6486121416091919 train acc 0.8165589617603076\n",
            "epoch 10 batch id 4691 loss 0.3473179042339325 train acc 0.8165669633340439\n",
            "epoch 10 batch id 4701 loss 0.49021950364112854 train acc 0.8165981971920868\n",
            "epoch 10 batch id 4711 loss 0.28139546513557434 train acc 0.8166458819783485\n",
            "epoch 10 batch id 4721 loss 0.4019429087638855 train acc 0.8166404098707901\n",
            "epoch 10 batch id 4731 loss 0.28372856974601746 train acc 0.8166547770027478\n",
            "epoch 10 batch id 4741 loss 0.5020281076431274 train acc 0.8166954492723054\n",
            "epoch 10 batch id 4751 loss 0.44409453868865967 train acc 0.816732661544938\n",
            "epoch 10 batch id 4761 loss 0.4462707042694092 train acc 0.8167073618987608\n",
            "epoch 10 batch id 4771 loss 0.40855884552001953 train acc 0.816803343114651\n",
            "epoch 10 batch id 4781 loss 0.4299336373806 train acc 0.816784537753608\n",
            "epoch 10 batch id 4791 loss 0.6246417760848999 train acc 0.816772333542058\n",
            "epoch 10 batch id 4801 loss 0.5712781548500061 train acc 0.8167764528223287\n",
            "epoch 10 batch id 4811 loss 0.3080962300300598 train acc 0.8168162803990854\n",
            "epoch 10 batch id 4821 loss 0.5627211332321167 train acc 0.8168138093756482\n",
            "epoch 10 batch id 4831 loss 0.45752444863319397 train acc 0.816885737942455\n",
            "epoch 10 batch id 4841 loss 0.5651925802230835 train acc 0.8168573125387316\n",
            "epoch 10 batch id 4851 loss 0.581207275390625 train acc 0.8168579931972789\n",
            "epoch 10 batch id 4861 loss 0.40898242592811584 train acc 0.8168297418226702\n",
            "epoch 10 batch id 4871 loss 0.6043301820755005 train acc 0.8168818004516526\n",
            "epoch 10 batch id 4881 loss 0.41899558901786804 train acc 0.8169528528989961\n",
            "epoch 10 batch id 4891 loss 0.5191769599914551 train acc 0.8169788897975874\n",
            "epoch 10 batch id 4901 loss 0.4612925350666046 train acc 0.8170462660681493\n",
            "epoch 10 batch id 4911 loss 0.5121768116950989 train acc 0.817056098554266\n",
            "epoch 10 batch id 4921 loss 0.31069740653038025 train acc 0.8170563655761024\n",
            "epoch 10 batch id 4931 loss 0.5380397439002991 train acc 0.8170249442303792\n",
            "epoch 10 batch id 4941 loss 0.4206339716911316 train acc 0.817006299332119\n",
            "epoch 10 batch id 4951 loss 0.6554008722305298 train acc 0.817069783882044\n",
            "epoch 10 train acc 0.8171281544005722\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fec40de8e9d245dd8a23b2c481de1101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10 loss 0.9318071603775024 test acc 0.7521868126832845\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90b946c6c4b244a6897f48571c22efbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 11 batch id 1 loss 0.5475379228591919 train acc 0.828125\n",
            "epoch 11 batch id 11 loss 0.3334488868713379 train acc 0.859375\n",
            "epoch 11 batch id 21 loss 0.33128592371940613 train acc 0.8601190476190477\n",
            "epoch 11 batch id 31 loss 0.5336233973503113 train acc 0.8366935483870968\n",
            "epoch 11 batch id 41 loss 0.39029282331466675 train acc 0.833079268292683\n",
            "epoch 11 batch id 51 loss 0.5005935430526733 train acc 0.8342524509803921\n",
            "epoch 11 batch id 61 loss 0.5249817371368408 train acc 0.8283811475409836\n",
            "epoch 11 batch id 71 loss 0.5679974555969238 train acc 0.8283450704225352\n",
            "epoch 11 batch id 81 loss 0.6654171943664551 train acc 0.8275462962962963\n",
            "epoch 11 batch id 91 loss 0.5396551489830017 train acc 0.8257211538461539\n",
            "epoch 11 batch id 101 loss 0.48451390862464905 train acc 0.8259591584158416\n",
            "epoch 11 batch id 111 loss 0.40316036343574524 train acc 0.8272804054054054\n",
            "epoch 11 batch id 121 loss 0.5868729948997498 train acc 0.8261880165289256\n",
            "epoch 11 batch id 131 loss 0.5904536247253418 train acc 0.8263358778625954\n",
            "epoch 11 batch id 141 loss 0.49221566319465637 train acc 0.8261303191489362\n",
            "epoch 11 batch id 151 loss 0.37225210666656494 train acc 0.8272971854304636\n",
            "epoch 11 batch id 161 loss 0.687924861907959 train acc 0.828513198757764\n",
            "epoch 11 batch id 171 loss 0.4295945465564728 train acc 0.8285818713450293\n",
            "epoch 11 batch id 181 loss 0.4064584970474243 train acc 0.8297651933701657\n",
            "epoch 11 batch id 191 loss 0.484322726726532 train acc 0.8293520942408377\n",
            "epoch 11 batch id 201 loss 0.38108375668525696 train acc 0.8302238805970149\n",
            "epoch 11 batch id 211 loss 0.5014922618865967 train acc 0.830050355450237\n",
            "epoch 11 batch id 221 loss 0.49492165446281433 train acc 0.8293269230769231\n",
            "epoch 11 batch id 231 loss 0.3450283408164978 train acc 0.8295454545454546\n",
            "epoch 11 batch id 241 loss 0.4147757589817047 train acc 0.8292920124481328\n",
            "epoch 11 batch id 251 loss 0.5877867341041565 train acc 0.8288720119521913\n",
            "epoch 11 batch id 261 loss 0.5475016832351685 train acc 0.8293223180076629\n",
            "epoch 11 batch id 271 loss 0.3395000994205475 train acc 0.8303159594095941\n",
            "epoch 11 batch id 281 loss 0.28945428133010864 train acc 0.8306272241992882\n",
            "epoch 11 batch id 291 loss 0.6446913480758667 train acc 0.8315614261168385\n",
            "epoch 11 batch id 301 loss 0.5503162145614624 train acc 0.8314472591362126\n",
            "epoch 11 batch id 311 loss 0.5264285802841187 train acc 0.831290192926045\n",
            "epoch 11 batch id 321 loss 0.5306063890457153 train acc 0.8310455607476636\n",
            "epoch 11 batch id 331 loss 0.5422613024711609 train acc 0.8310045317220544\n",
            "epoch 11 batch id 341 loss 0.6695085763931274 train acc 0.8311950146627566\n",
            "epoch 11 batch id 351 loss 0.6600367426872253 train acc 0.8311520655270656\n",
            "epoch 11 batch id 361 loss 0.5550404191017151 train acc 0.8304189750692521\n",
            "epoch 11 batch id 371 loss 0.5795902609825134 train acc 0.8303150269541779\n",
            "epoch 11 batch id 381 loss 0.2723007798194885 train acc 0.8302575459317585\n",
            "epoch 11 batch id 391 loss 0.6580632925033569 train acc 0.8301630434782609\n",
            "epoch 11 batch id 401 loss 0.3566523790359497 train acc 0.8303460099750624\n",
            "epoch 11 batch id 411 loss 0.41887304186820984 train acc 0.8305961070559611\n",
            "epoch 11 batch id 421 loss 0.5531495213508606 train acc 0.830500296912114\n",
            "epoch 11 batch id 431 loss 0.4480515718460083 train acc 0.8305176914153132\n",
            "epoch 11 batch id 441 loss 0.6771492958068848 train acc 0.8307468820861678\n",
            "epoch 11 batch id 451 loss 0.5225235819816589 train acc 0.8312777161862528\n",
            "epoch 11 batch id 461 loss 0.3529064357280731 train acc 0.8309381778741866\n",
            "epoch 11 batch id 471 loss 0.3492022156715393 train acc 0.831906847133758\n",
            "epoch 11 batch id 481 loss 0.7044559717178345 train acc 0.831795738045738\n",
            "epoch 11 batch id 491 loss 0.5933026075363159 train acc 0.8319119144602851\n",
            "epoch 11 batch id 501 loss 0.8441946506500244 train acc 0.8321482035928144\n",
            "epoch 11 batch id 511 loss 0.41503503918647766 train acc 0.8320694716242661\n",
            "epoch 11 batch id 521 loss 0.4619934856891632 train acc 0.8319637715930902\n",
            "epoch 11 batch id 531 loss 0.4356100857257843 train acc 0.8320386064030132\n",
            "epoch 11 batch id 541 loss 0.4287758469581604 train acc 0.8322550831792976\n",
            "epoch 11 batch id 551 loss 0.5031710863113403 train acc 0.8319816243194192\n",
            "epoch 11 batch id 561 loss 0.4322447180747986 train acc 0.8318293226381461\n",
            "epoch 11 batch id 571 loss 0.771640419960022 train acc 0.8320380910683012\n",
            "epoch 11 batch id 581 loss 0.2527908682823181 train acc 0.8322665662650602\n",
            "epoch 11 batch id 591 loss 0.32570570707321167 train acc 0.8326195008460237\n",
            "epoch 11 batch id 601 loss 0.32319575548171997 train acc 0.8329866888519135\n",
            "epoch 11 batch id 611 loss 0.5127390027046204 train acc 0.8332139934533551\n",
            "epoch 11 batch id 621 loss 0.5408836603164673 train acc 0.8334088164251208\n",
            "epoch 11 batch id 631 loss 0.6216213703155518 train acc 0.8333498415213946\n",
            "epoch 11 batch id 641 loss 0.4586986303329468 train acc 0.8330245709828393\n",
            "epoch 11 batch id 651 loss 0.24085146188735962 train acc 0.8332613287250384\n",
            "epoch 11 batch id 661 loss 0.42442166805267334 train acc 0.8335381996974282\n",
            "epoch 11 batch id 671 loss 0.4835304617881775 train acc 0.8335273845007451\n",
            "epoch 11 batch id 681 loss 0.4831036925315857 train acc 0.8336316079295154\n",
            "epoch 11 batch id 691 loss 0.48576053977012634 train acc 0.8337780390738061\n",
            "epoch 11 batch id 701 loss 0.4903128445148468 train acc 0.8336082382310984\n",
            "epoch 11 batch id 711 loss 0.47429826855659485 train acc 0.8337069268635724\n",
            "epoch 11 batch id 721 loss 0.531545102596283 train acc 0.8335428224687933\n",
            "epoch 11 batch id 731 loss 0.24170392751693726 train acc 0.8337679548563611\n",
            "epoch 11 batch id 741 loss 0.7230218648910522 train acc 0.8338815789473685\n",
            "epoch 11 batch id 751 loss 0.5619755983352661 train acc 0.8335968708388815\n",
            "epoch 11 batch id 761 loss 0.3043573498725891 train acc 0.8336481603153745\n",
            "epoch 11 batch id 771 loss 0.5668142437934875 train acc 0.8334549286640727\n",
            "epoch 11 batch id 781 loss 0.4477674067020416 train acc 0.8334266965428937\n",
            "epoch 11 batch id 791 loss 0.2846604585647583 train acc 0.833557206068268\n",
            "epoch 11 batch id 801 loss 0.38168007135391235 train acc 0.8338600187265918\n",
            "epoch 11 batch id 811 loss 0.5825979113578796 train acc 0.8340397657213316\n",
            "epoch 11 batch id 821 loss 0.3671870231628418 train acc 0.8341390073081608\n",
            "epoch 11 batch id 831 loss 0.4705199599266052 train acc 0.8344426895306859\n",
            "epoch 11 batch id 841 loss 0.3355652987957001 train acc 0.8347205707491082\n",
            "epoch 11 batch id 851 loss 0.5188217163085938 train acc 0.8344043772032902\n",
            "epoch 11 batch id 861 loss 0.6166743040084839 train acc 0.8342044134727061\n",
            "epoch 11 batch id 871 loss 0.32511231303215027 train acc 0.8342781285878301\n",
            "epoch 11 batch id 881 loss 0.3919762074947357 train acc 0.834119608399546\n",
            "epoch 11 batch id 891 loss 0.469563364982605 train acc 0.8342978395061729\n",
            "epoch 11 batch id 901 loss 0.5270351767539978 train acc 0.8341079356270811\n",
            "epoch 11 batch id 911 loss 0.6789069175720215 train acc 0.8340765642151482\n",
            "epoch 11 batch id 921 loss 0.5565976500511169 train acc 0.8341646308360477\n",
            "epoch 11 batch id 931 loss 0.28844451904296875 train acc 0.8341333243823845\n",
            "epoch 11 batch id 941 loss 0.24254930019378662 train acc 0.8345510095642933\n",
            "epoch 11 batch id 951 loss 0.44497987627983093 train acc 0.834565588853838\n",
            "epoch 11 batch id 961 loss 0.4200119078159332 train acc 0.8345148283038502\n",
            "epoch 11 batch id 971 loss 0.5934251546859741 train acc 0.8346260298661174\n",
            "epoch 11 batch id 981 loss 0.5660322308540344 train acc 0.834894240570846\n",
            "epoch 11 batch id 991 loss 0.4775976240634918 train acc 0.8348732341069627\n",
            "epoch 11 batch id 1001 loss 0.5016568303108215 train acc 0.8349150849150849\n",
            "epoch 11 batch id 1011 loss 0.40153345465660095 train acc 0.8350642927794263\n",
            "epoch 11 batch id 1021 loss 0.3728230893611908 train acc 0.835164666993144\n",
            "epoch 11 batch id 1031 loss 0.5696467757225037 train acc 0.8352024733268671\n",
            "epoch 11 batch id 1041 loss 0.40555453300476074 train acc 0.8353446205571565\n",
            "epoch 11 batch id 1051 loss 0.23666344583034515 train acc 0.8352759276879163\n",
            "epoch 11 batch id 1061 loss 0.5865872502326965 train acc 0.8351790763430725\n",
            "epoch 11 batch id 1071 loss 0.24574537575244904 train acc 0.835404995331466\n",
            "epoch 11 batch id 1081 loss 0.47599607706069946 train acc 0.8351931082331174\n",
            "epoch 11 batch id 1091 loss 0.8108287453651428 train acc 0.8350710357470211\n",
            "epoch 11 batch id 1101 loss 0.3456496298313141 train acc 0.8352208219800181\n",
            "epoch 11 batch id 1111 loss 0.5392493009567261 train acc 0.8353116561656165\n",
            "epoch 11 batch id 1121 loss 0.28887873888015747 train acc 0.8354008697591436\n",
            "epoch 11 batch id 1131 loss 0.5085263848304749 train acc 0.8352950928381963\n",
            "epoch 11 batch id 1141 loss 0.41461634635925293 train acc 0.8354239702015775\n",
            "epoch 11 batch id 1151 loss 0.664821982383728 train acc 0.8354555821025196\n",
            "epoch 11 batch id 1161 loss 0.3647884130477905 train acc 0.8354866494401378\n",
            "epoch 11 batch id 1171 loss 0.4543776512145996 train acc 0.8357973953885568\n",
            "epoch 11 batch id 1181 loss 0.5231964588165283 train acc 0.8358382726502963\n",
            "epoch 11 batch id 1191 loss 0.2792099416255951 train acc 0.8360358942065491\n",
            "epoch 11 batch id 1201 loss 0.7221112847328186 train acc 0.8362432348043297\n",
            "epoch 11 batch id 1211 loss 0.32091549038887024 train acc 0.8361632947976878\n",
            "epoch 11 batch id 1221 loss 0.4313752353191376 train acc 0.8361742424242424\n",
            "epoch 11 batch id 1231 loss 0.5619279146194458 train acc 0.8361342404549147\n",
            "epoch 11 batch id 1241 loss 0.38056516647338867 train acc 0.8359312046736502\n",
            "epoch 11 batch id 1251 loss 0.41642844676971436 train acc 0.8359312549960032\n",
            "epoch 11 batch id 1261 loss 0.401539146900177 train acc 0.8361543417922284\n",
            "epoch 11 batch id 1271 loss 0.35470208525657654 train acc 0.8361403422501967\n",
            "epoch 11 batch id 1281 loss 0.4363217055797577 train acc 0.8361753512880562\n",
            "epoch 11 batch id 1291 loss 0.469971239566803 train acc 0.8362098179705655\n",
            "epoch 11 batch id 1301 loss 0.6427586078643799 train acc 0.8360636049192929\n",
            "epoch 11 batch id 1311 loss 0.4024564027786255 train acc 0.8360864797864226\n",
            "epoch 11 batch id 1321 loss 0.26991385221481323 train acc 0.8360143830431491\n",
            "epoch 11 batch id 1331 loss 0.43564221262931824 train acc 0.8359903268219384\n",
            "epoch 11 batch id 1341 loss 0.4248753786087036 train acc 0.8358967188665175\n",
            "epoch 11 batch id 1351 loss 0.41326630115509033 train acc 0.8358854552183568\n",
            "epoch 11 batch id 1361 loss 0.6921245455741882 train acc 0.8358743570903747\n",
            "epoch 11 batch id 1371 loss 0.36499330401420593 train acc 0.8359887855579868\n",
            "epoch 11 batch id 1381 loss 0.5315713286399841 train acc 0.8359997284576394\n",
            "epoch 11 batch id 1391 loss 0.2642362713813782 train acc 0.836100377426312\n",
            "epoch 11 batch id 1401 loss 0.4908198416233063 train acc 0.836076909350464\n",
            "epoch 11 batch id 1411 loss 0.2836597263813019 train acc 0.8360869950389794\n",
            "epoch 11 batch id 1421 loss 0.3391721248626709 train acc 0.8359539936664321\n",
            "epoch 11 batch id 1431 loss 0.3827347755432129 train acc 0.8360739867225716\n",
            "epoch 11 batch id 1441 loss 0.39986729621887207 train acc 0.8359754510756419\n",
            "epoch 11 batch id 1451 loss 0.41947227716445923 train acc 0.8362228635423845\n",
            "epoch 11 batch id 1461 loss 0.3734363317489624 train acc 0.836188826146475\n",
            "epoch 11 batch id 1471 loss 0.6269162893295288 train acc 0.8363570700203943\n",
            "epoch 11 batch id 1481 loss 0.45073702931404114 train acc 0.8363964382174207\n",
            "epoch 11 batch id 1491 loss 0.5622670650482178 train acc 0.8364038397048961\n",
            "epoch 11 batch id 1501 loss 0.41917774081230164 train acc 0.8366089273817455\n",
            "epoch 11 batch id 1511 loss 0.28418827056884766 train acc 0.836573461283918\n",
            "epoch 11 batch id 1521 loss 0.3468301296234131 train acc 0.8367747370151216\n",
            "epoch 11 batch id 1531 loss 0.4053557813167572 train acc 0.8369121489222731\n",
            "epoch 11 batch id 1541 loss 0.36601731181144714 train acc 0.8370680564568462\n",
            "epoch 11 batch id 1551 loss 0.5735921263694763 train acc 0.8369197292069632\n",
            "epoch 11 batch id 1561 loss 0.4886326789855957 train acc 0.8370035233824471\n",
            "epoch 11 batch id 1571 loss 0.3953810930252075 train acc 0.8370862507956716\n",
            "epoch 11 batch id 1581 loss 0.4731309711933136 train acc 0.8370394528779254\n",
            "epoch 11 batch id 1591 loss 0.44137850403785706 train acc 0.8371209145191704\n",
            "epoch 11 batch id 1601 loss 0.6849337220191956 train acc 0.8371818394753279\n",
            "epoch 11 batch id 1611 loss 0.47430282831192017 train acc 0.8372129112352577\n",
            "epoch 11 batch id 1621 loss 0.43065646290779114 train acc 0.8372532387415176\n",
            "epoch 11 batch id 1631 loss 0.3608032464981079 train acc 0.8373026517473943\n",
            "epoch 11 batch id 1641 loss 0.544461727142334 train acc 0.8373990706886045\n",
            "epoch 11 batch id 1651 loss 0.39649778604507446 train acc 0.83736182616596\n",
            "epoch 11 batch id 1661 loss 0.633832573890686 train acc 0.8373720650210716\n",
            "epoch 11 batch id 1671 loss 0.5224272012710571 train acc 0.8375224416517055\n",
            "epoch 11 batch id 1681 loss 0.5645550489425659 train acc 0.8373178167757287\n",
            "epoch 11 batch id 1691 loss 0.4982451796531677 train acc 0.8374574955647546\n",
            "epoch 11 batch id 1701 loss 0.3554041385650635 train acc 0.8375496031746031\n",
            "epoch 11 batch id 1711 loss 0.5569045543670654 train acc 0.8375036528345996\n",
            "epoch 11 batch id 1721 loss 0.5344973802566528 train acc 0.8374219203951191\n",
            "epoch 11 batch id 1731 loss 0.2656300961971283 train acc 0.837440424610052\n",
            "epoch 11 batch id 1741 loss 0.659072995185852 train acc 0.8374856404365307\n",
            "epoch 11 batch id 1751 loss 0.3598170876502991 train acc 0.8375035693889206\n",
            "epoch 11 batch id 1761 loss 0.4025154709815979 train acc 0.8375301675184554\n",
            "epoch 11 batch id 1771 loss 0.4501464068889618 train acc 0.8377329192546584\n",
            "epoch 11 batch id 1781 loss 0.4033231735229492 train acc 0.8379070746771476\n",
            "epoch 11 batch id 1791 loss 0.7848996520042419 train acc 0.837913525963149\n",
            "epoch 11 batch id 1801 loss 0.3865596354007721 train acc 0.8380500416435314\n",
            "epoch 11 batch id 1811 loss 0.2805158197879791 train acc 0.8381160270568746\n",
            "epoch 11 batch id 1821 loss 0.34170398116111755 train acc 0.8381727073036793\n",
            "epoch 11 batch id 1831 loss 0.2668456435203552 train acc 0.8382373020207536\n",
            "epoch 11 batch id 1841 loss 0.270078182220459 train acc 0.8382757332971211\n",
            "epoch 11 batch id 1851 loss 0.5423851013183594 train acc 0.8383643976229065\n",
            "epoch 11 batch id 1861 loss 0.37735438346862793 train acc 0.8383933369156368\n",
            "epoch 11 batch id 1871 loss 0.47009411454200745 train acc 0.8385138295029396\n",
            "epoch 11 batch id 1881 loss 0.3728366494178772 train acc 0.8386662679425837\n",
            "epoch 11 batch id 1891 loss 0.5038990378379822 train acc 0.8388088313061872\n",
            "epoch 11 batch id 1901 loss 0.35816600918769836 train acc 0.8389909915833772\n",
            "epoch 11 batch id 1911 loss 0.3861050009727478 train acc 0.8389341313448456\n",
            "epoch 11 batch id 1921 loss 0.5385145545005798 train acc 0.8386826522644456\n",
            "epoch 11 batch id 1931 loss 0.541388213634491 train acc 0.8388869109269809\n",
            "epoch 11 batch id 1941 loss 0.4400850832462311 train acc 0.8390005151983514\n",
            "epoch 11 batch id 1951 loss 0.5128071904182434 train acc 0.8390649026140441\n",
            "epoch 11 batch id 1961 loss 0.2781203091144562 train acc 0.8390409867414584\n",
            "epoch 11 batch id 1971 loss 0.4934827983379364 train acc 0.8390648782343988\n",
            "epoch 11 batch id 1981 loss 0.41933709383010864 train acc 0.8390885285209491\n",
            "epoch 11 batch id 1991 loss 0.5993137955665588 train acc 0.8389785283776996\n",
            "epoch 11 batch id 2001 loss 0.5446734428405762 train acc 0.8389711394302849\n",
            "epoch 11 batch id 2011 loss 0.5062108039855957 train acc 0.8389793635007459\n",
            "epoch 11 batch id 2021 loss 0.690240740776062 train acc 0.8389875061850569\n",
            "epoch 11 batch id 2031 loss 0.2873704135417938 train acc 0.8390186484490398\n",
            "epoch 11 batch id 2041 loss 0.27769920229911804 train acc 0.8389423076923077\n",
            "epoch 11 batch id 2051 loss 0.5449318289756775 train acc 0.8389124207703559\n",
            "epoch 11 batch id 2061 loss 0.4233359694480896 train acc 0.8389813803978651\n",
            "epoch 11 batch id 2071 loss 0.42912396788597107 train acc 0.8388987807822308\n",
            "epoch 11 batch id 2081 loss 0.5568406581878662 train acc 0.8387418909178279\n",
            "epoch 11 batch id 2091 loss 0.4468780755996704 train acc 0.838795731707317\n",
            "epoch 11 batch id 2101 loss 0.23573827743530273 train acc 0.8388416230366492\n",
            "epoch 11 batch id 2111 loss 0.4037739634513855 train acc 0.8388944812884889\n",
            "epoch 11 batch id 2121 loss 0.2043382227420807 train acc 0.8388363389910419\n",
            "epoch 11 batch id 2131 loss 0.5641188621520996 train acc 0.8389547160957297\n",
            "epoch 11 batch id 2141 loss 0.6060980558395386 train acc 0.8389260275572162\n",
            "epoch 11 batch id 2151 loss 0.5138817429542542 train acc 0.8389193979544398\n",
            "epoch 11 batch id 2161 loss 0.5576984882354736 train acc 0.8388911383618695\n",
            "epoch 11 batch id 2171 loss 0.39599698781967163 train acc 0.8390430677107323\n",
            "epoch 11 batch id 2181 loss 0.5503661036491394 train acc 0.8390574850985786\n",
            "epoch 11 batch id 2191 loss 0.3843221664428711 train acc 0.8390860337745322\n",
            "epoch 11 batch id 2201 loss 0.4234137535095215 train acc 0.8391072239890959\n",
            "epoch 11 batch id 2211 loss 0.4614209532737732 train acc 0.8391140886476708\n",
            "epoch 11 batch id 2221 loss 0.6972514390945435 train acc 0.8390505402971634\n",
            "epoch 11 batch id 2231 loss 0.26016199588775635 train acc 0.8391276333482743\n",
            "epoch 11 batch id 2241 loss 0.3545035123825073 train acc 0.839141287371709\n",
            "epoch 11 batch id 2251 loss 0.34035590291023254 train acc 0.8390368169702355\n",
            "epoch 11 batch id 2261 loss 0.24877481162548065 train acc 0.8390645731977001\n",
            "epoch 11 batch id 2271 loss 0.5094569325447083 train acc 0.8391264861294584\n",
            "epoch 11 batch id 2281 loss 0.7074377536773682 train acc 0.8392221065322227\n",
            "epoch 11 batch id 2291 loss 0.4043799638748169 train acc 0.8393578131820166\n",
            "epoch 11 batch id 2301 loss 0.30333423614501953 train acc 0.8394312255541069\n",
            "epoch 11 batch id 2311 loss 0.42068469524383545 train acc 0.8394499134573777\n",
            "epoch 11 batch id 2321 loss 0.3296239376068115 train acc 0.8394617083153812\n",
            "epoch 11 batch id 2331 loss 0.41369444131851196 train acc 0.839372854997855\n",
            "epoch 11 batch id 2341 loss 0.5836069583892822 train acc 0.8393982272533106\n",
            "epoch 11 batch id 2351 loss 0.4053272008895874 train acc 0.8393635686941727\n",
            "epoch 11 batch id 2361 loss 0.49078473448753357 train acc 0.8393689114781873\n",
            "epoch 11 batch id 2371 loss 0.3038557767868042 train acc 0.839440109658372\n",
            "epoch 11 batch id 2381 loss 0.23983491957187653 train acc 0.8394910226795465\n",
            "epoch 11 batch id 2391 loss 0.3685985505580902 train acc 0.8395023002927645\n",
            "epoch 11 batch id 2401 loss 0.3275212347507477 train acc 0.8395199916701375\n",
            "epoch 11 batch id 2411 loss 0.45776626467704773 train acc 0.8395440170053919\n",
            "epoch 11 batch id 2421 loss 0.5536308884620667 train acc 0.8395742978108219\n",
            "epoch 11 batch id 2431 loss 0.47093671560287476 train acc 0.8395079185520362\n",
            "epoch 11 batch id 2441 loss 0.40349480509757996 train acc 0.839595708725932\n",
            "epoch 11 batch id 2451 loss 0.38394609093666077 train acc 0.8395680334557324\n",
            "epoch 11 batch id 2461 loss 0.6291567087173462 train acc 0.8395532811865095\n",
            "epoch 11 batch id 2471 loss 0.4324885606765747 train acc 0.8395070315661676\n",
            "epoch 11 batch id 2481 loss 0.5523365139961243 train acc 0.8395619205965337\n",
            "epoch 11 batch id 2491 loss 0.3904101252555847 train acc 0.8395160076274588\n",
            "epoch 11 batch id 2501 loss 0.2702407240867615 train acc 0.8394642143142743\n",
            "epoch 11 batch id 2511 loss 0.47231459617614746 train acc 0.839618180007965\n",
            "epoch 11 batch id 2521 loss 0.4761408269405365 train acc 0.8395973819912733\n",
            "epoch 11 batch id 2531 loss 0.5255737900733948 train acc 0.8394903200316081\n",
            "epoch 11 batch id 2541 loss 0.36335909366607666 train acc 0.8394947855175128\n",
            "epoch 11 batch id 2551 loss 0.5021876096725464 train acc 0.8394808408467268\n",
            "epoch 11 batch id 2561 loss 0.5189169645309448 train acc 0.839497510737993\n",
            "epoch 11 batch id 2571 loss 0.4971890151500702 train acc 0.8395565927654609\n",
            "epoch 11 batch id 2581 loss 0.47222477197647095 train acc 0.8395728399845022\n",
            "epoch 11 batch id 2591 loss 0.2685987949371338 train acc 0.8395527788498649\n",
            "epoch 11 batch id 2601 loss 0.5111498832702637 train acc 0.8396830545943867\n",
            "epoch 11 batch id 2611 loss 0.33989760279655457 train acc 0.8397165836844122\n",
            "epoch 11 batch id 2621 loss 0.5041601061820984 train acc 0.8396842808088516\n",
            "epoch 11 batch id 2631 loss 0.5400747060775757 train acc 0.8396878563283923\n",
            "epoch 11 batch id 2641 loss 0.3995586931705475 train acc 0.8396854884513442\n",
            "epoch 11 batch id 2651 loss 0.3746626675128937 train acc 0.8397302904564315\n",
            "epoch 11 batch id 2661 loss 0.467470645904541 train acc 0.8397982431416761\n",
            "epoch 11 batch id 2671 loss 0.38279256224632263 train acc 0.8397896387120929\n",
            "epoch 11 batch id 2681 loss 0.38895493745803833 train acc 0.8397111618798956\n",
            "epoch 11 batch id 2691 loss 0.6200594305992126 train acc 0.8396332683017466\n",
            "epoch 11 batch id 2701 loss 0.4896119236946106 train acc 0.8397410681229175\n",
            "epoch 11 batch id 2711 loss 0.3041822016239166 train acc 0.8398019642198451\n",
            "epoch 11 batch id 2721 loss 0.5512011051177979 train acc 0.8398222160970231\n",
            "epoch 11 batch id 2731 loss 0.38349199295043945 train acc 0.8397564994507506\n",
            "epoch 11 batch id 2741 loss 0.43751823902130127 train acc 0.8397710689529368\n",
            "epoch 11 batch id 2751 loss 0.5333664417266846 train acc 0.8398252908033442\n",
            "epoch 11 batch id 2761 loss 0.2967960834503174 train acc 0.839918734154292\n",
            "epoch 11 batch id 2771 loss 0.46806246042251587 train acc 0.8398987278960665\n",
            "epoch 11 batch id 2781 loss 0.6928263306617737 train acc 0.839940668824164\n",
            "epoch 11 batch id 2791 loss 0.6672135591506958 train acc 0.83987594052311\n",
            "epoch 11 batch id 2801 loss 0.5583871603012085 train acc 0.8399734469832203\n",
            "epoch 11 batch id 2811 loss 0.2626499831676483 train acc 0.8400535841337602\n",
            "epoch 11 batch id 2821 loss 0.5783289074897766 train acc 0.8400888426090038\n",
            "epoch 11 batch id 2831 loss 0.314532607793808 train acc 0.8402176792652772\n",
            "epoch 11 batch id 2841 loss 0.5596904754638672 train acc 0.8402301126363957\n",
            "epoch 11 batch id 2851 loss 0.40543290972709656 train acc 0.8402040950543669\n",
            "epoch 11 batch id 2861 loss 0.3628997802734375 train acc 0.8401946434813002\n",
            "epoch 11 batch id 2871 loss 0.4213162660598755 train acc 0.8401417189132706\n",
            "epoch 11 batch id 2881 loss 0.31609591841697693 train acc 0.8401976310308921\n",
            "epoch 11 batch id 2891 loss 0.429788738489151 train acc 0.8401342528536838\n",
            "epoch 11 batch id 2901 loss 0.43917906284332275 train acc 0.8400766976904516\n",
            "epoch 11 batch id 2911 loss 0.37144944071769714 train acc 0.8401054190999656\n",
            "epoch 11 batch id 2921 loss 0.2651083171367645 train acc 0.8400376583361863\n",
            "epoch 11 batch id 2931 loss 0.24700307846069336 train acc 0.8400236693961105\n",
            "epoch 11 batch id 2941 loss 0.4522799551486969 train acc 0.8399885243114587\n",
            "epoch 11 batch id 2951 loss 0.5745840072631836 train acc 0.8400436292782107\n",
            "epoch 11 batch id 2961 loss 0.3928857147693634 train acc 0.8401036389733199\n",
            "epoch 11 batch id 2971 loss 0.3641999363899231 train acc 0.8401790222147425\n",
            "epoch 11 batch id 2981 loss 0.4868742525577545 train acc 0.8401438275746393\n",
            "epoch 11 batch id 2991 loss 0.3383326828479767 train acc 0.8402342443998663\n",
            "epoch 11 batch id 3001 loss 0.373304158449173 train acc 0.8402251332889037\n",
            "epoch 11 batch id 3011 loss 0.44794154167175293 train acc 0.8401901361673862\n",
            "epoch 11 batch id 3021 loss 0.41146790981292725 train acc 0.8401760592519033\n",
            "epoch 11 batch id 3031 loss 0.4606068730354309 train acc 0.8402548663807324\n",
            "epoch 11 batch id 3041 loss 0.5814242959022522 train acc 0.8403126027622493\n",
            "epoch 11 batch id 3051 loss 0.39900243282318115 train acc 0.8403085054080629\n",
            "epoch 11 batch id 3061 loss 0.3210006058216095 train acc 0.8403299575302189\n",
            "epoch 11 batch id 3071 loss 0.43814578652381897 train acc 0.8403410941061543\n",
            "epoch 11 batch id 3081 loss 0.601490318775177 train acc 0.8403876582278481\n",
            "epoch 11 batch id 3091 loss 0.5519309043884277 train acc 0.8403884260757036\n",
            "epoch 11 batch id 3101 loss 0.4108479619026184 train acc 0.840404305062883\n",
            "epoch 11 batch id 3111 loss 0.6532512307167053 train acc 0.8403899469623916\n",
            "epoch 11 batch id 3121 loss 0.40836477279663086 train acc 0.840420738545338\n",
            "epoch 11 batch id 3131 loss 0.3478660583496094 train acc 0.8404812759501756\n",
            "epoch 11 batch id 3141 loss 0.606462836265564 train acc 0.8404220391595033\n",
            "epoch 11 batch id 3151 loss 0.30590975284576416 train acc 0.8405168993970168\n",
            "epoch 11 batch id 3161 loss 0.48349177837371826 train acc 0.8404826399873457\n",
            "epoch 11 batch id 3171 loss 0.6467204689979553 train acc 0.8403993219804478\n",
            "epoch 11 batch id 3181 loss 0.33440402150154114 train acc 0.8404196793461176\n",
            "epoch 11 batch id 3191 loss 0.3122089207172394 train acc 0.8404741852083987\n",
            "epoch 11 batch id 3201 loss 0.49012690782546997 train acc 0.8404453686348016\n",
            "epoch 11 batch id 3211 loss 0.5531049370765686 train acc 0.8404361958891311\n",
            "epoch 11 batch id 3221 loss 0.6176595091819763 train acc 0.8403979742316051\n",
            "epoch 11 batch id 3231 loss 0.3339170515537262 train acc 0.8404131847725163\n",
            "epoch 11 batch id 3241 loss 0.3413487374782562 train acc 0.8404138383215057\n",
            "epoch 11 batch id 3251 loss 0.4627385139465332 train acc 0.8404192940633651\n",
            "epoch 11 batch id 3261 loss 0.32818108797073364 train acc 0.8404103419196566\n",
            "epoch 11 batch id 3271 loss 0.346272349357605 train acc 0.8403250152858454\n",
            "epoch 11 batch id 3281 loss 0.4373507499694824 train acc 0.8403830768058519\n",
            "epoch 11 batch id 3291 loss 0.31438058614730835 train acc 0.8404360376785172\n",
            "epoch 11 batch id 3301 loss 0.38968825340270996 train acc 0.8404744774310815\n",
            "epoch 11 batch id 3311 loss 0.6151443719863892 train acc 0.8405032467532467\n",
            "epoch 11 batch id 3321 loss 0.5402944087982178 train acc 0.8405412526347485\n",
            "epoch 11 batch id 3331 loss 0.8237596154212952 train acc 0.8404992870009006\n",
            "epoch 11 batch id 3341 loss 0.6030697226524353 train acc 0.8404295121221191\n",
            "epoch 11 batch id 3351 loss 0.37487390637397766 train acc 0.8404767233661593\n",
            "epoch 11 batch id 3361 loss 0.3280428647994995 train acc 0.8405608449866111\n",
            "epoch 11 batch id 3371 loss 0.4699682593345642 train acc 0.8405564001779887\n",
            "epoch 11 batch id 3381 loss 0.1745179146528244 train acc 0.8405381174208814\n",
            "epoch 11 batch id 3391 loss 0.6424532532691956 train acc 0.8405660203479799\n",
            "epoch 11 batch id 3401 loss 0.5834776759147644 train acc 0.8405891649514848\n",
            "epoch 11 batch id 3411 loss 0.3576318025588989 train acc 0.8405984315450015\n",
            "epoch 11 batch id 3421 loss 0.48791536688804626 train acc 0.8406715872551885\n",
            "epoch 11 batch id 3431 loss 0.5690022706985474 train acc 0.840635018944914\n",
            "epoch 11 batch id 3441 loss 0.4813116490840912 train acc 0.8405986631793083\n",
            "epoch 11 batch id 3451 loss 0.4163524806499481 train acc 0.8406213778614894\n",
            "epoch 11 batch id 3461 loss 0.45839089155197144 train acc 0.8406620196475008\n",
            "epoch 11 batch id 3471 loss 0.47472190856933594 train acc 0.8406574114088159\n",
            "epoch 11 batch id 3481 loss 0.3955323100090027 train acc 0.8405989658144212\n",
            "epoch 11 batch id 3491 loss 0.3007693886756897 train acc 0.8406035161844744\n",
            "epoch 11 batch id 3501 loss 0.4417717158794403 train acc 0.840683911739503\n",
            "epoch 11 batch id 3511 loss 0.5283331871032715 train acc 0.8406659427513529\n",
            "epoch 11 batch id 3521 loss 0.6004089713096619 train acc 0.8407057654075547\n",
            "epoch 11 batch id 3531 loss 0.727138876914978 train acc 0.8406878363069952\n",
            "epoch 11 batch id 3541 loss 0.23045530915260315 train acc 0.8407317848065519\n",
            "epoch 11 batch id 3551 loss 0.45510146021842957 train acc 0.8407798859476204\n",
            "epoch 11 batch id 3561 loss 0.2793731093406677 train acc 0.8408496559955069\n",
            "epoch 11 batch id 3571 loss 0.571237325668335 train acc 0.8408402758331\n",
            "epoch 11 batch id 3581 loss 0.4260638356208801 train acc 0.8408353113655403\n",
            "epoch 11 batch id 3591 loss 0.6145038604736328 train acc 0.8408477791701476\n",
            "epoch 11 batch id 3601 loss 0.41185086965560913 train acc 0.8408601777284088\n",
            "epoch 11 batch id 3611 loss 0.3180837631225586 train acc 0.8408681805594018\n",
            "epoch 11 batch id 3621 loss 0.29424434900283813 train acc 0.8409279204639603\n",
            "epoch 11 batch id 3631 loss 0.48409444093704224 train acc 0.8409184797576426\n",
            "epoch 11 batch id 3641 loss 0.7284987568855286 train acc 0.8409477135402362\n",
            "epoch 11 batch id 3651 loss 0.36385929584503174 train acc 0.8410195836757053\n",
            "epoch 11 batch id 3661 loss 0.4673113524913788 train acc 0.8410441136301557\n",
            "epoch 11 batch id 3671 loss 0.32203561067581177 train acc 0.8410812789430673\n",
            "epoch 11 batch id 3681 loss 0.4790945053100586 train acc 0.8411309766367835\n",
            "epoch 11 batch id 3691 loss 0.41500309109687805 train acc 0.8411804050392847\n",
            "epoch 11 batch id 3701 loss 0.3326340615749359 train acc 0.8411409078627398\n",
            "epoch 11 batch id 3711 loss 0.4362373650074005 train acc 0.8411816222042576\n",
            "epoch 11 batch id 3721 loss 0.34609758853912354 train acc 0.8412515116904058\n",
            "epoch 11 batch id 3731 loss 0.31692230701446533 train acc 0.8413126507638703\n",
            "epoch 11 batch id 3741 loss 0.5348038077354431 train acc 0.841285752472601\n",
            "epoch 11 batch id 3751 loss 0.45252490043640137 train acc 0.8412798253798986\n",
            "epoch 11 batch id 3761 loss 0.3377770185470581 train acc 0.8413320925285829\n",
            "epoch 11 batch id 3771 loss 0.3921680450439453 train acc 0.8412929262795015\n",
            "epoch 11 batch id 3781 loss 0.45318588614463806 train acc 0.8413572798201534\n",
            "epoch 11 batch id 3791 loss 0.3142859935760498 train acc 0.8413347401740966\n",
            "epoch 11 batch id 3801 loss 0.3014746308326721 train acc 0.841406866614049\n",
            "epoch 11 batch id 3811 loss 0.3280712068080902 train acc 0.8415155143007085\n",
            "epoch 11 batch id 3821 loss 0.4776129424571991 train acc 0.8415458976707668\n",
            "epoch 11 batch id 3831 loss 0.45328471064567566 train acc 0.84157204385278\n",
            "epoch 11 batch id 3841 loss 0.28026431798934937 train acc 0.8416143256964332\n",
            "epoch 11 batch id 3851 loss 0.23051783442497253 train acc 0.8416563879511815\n",
            "epoch 11 batch id 3861 loss 0.2556919455528259 train acc 0.841673951048951\n",
            "epoch 11 batch id 3871 loss 0.4854469299316406 train acc 0.8417277512270731\n",
            "epoch 11 batch id 3881 loss 0.5210736989974976 train acc 0.8417611440350425\n",
            "epoch 11 batch id 3891 loss 0.4920441806316376 train acc 0.8418144435877667\n",
            "epoch 11 batch id 3901 loss 0.2088630646467209 train acc 0.8417873622148168\n",
            "epoch 11 batch id 3911 loss 0.3553852140903473 train acc 0.8418083610329838\n",
            "epoch 11 batch id 3921 loss 0.31727510690689087 train acc 0.8418571474113746\n",
            "epoch 11 batch id 3931 loss 0.4268282651901245 train acc 0.8418579877893666\n",
            "epoch 11 batch id 3941 loss 0.3377586305141449 train acc 0.8418905417406749\n",
            "epoch 11 batch id 3951 loss 0.41309854388237 train acc 0.8418833839534295\n",
            "epoch 11 batch id 3961 loss 0.5631208419799805 train acc 0.8419196541277455\n",
            "epoch 11 batch id 3971 loss 0.3863529860973358 train acc 0.8419675459581969\n",
            "epoch 11 batch id 3981 loss 0.39130184054374695 train acc 0.8419798731474504\n",
            "epoch 11 batch id 3991 loss 0.5235922932624817 train acc 0.8420195439739414\n",
            "epoch 11 batch id 4001 loss 0.33088651299476624 train acc 0.8420238690327418\n",
            "epoch 11 batch id 4011 loss 0.4489463269710541 train acc 0.8420164859137372\n",
            "epoch 11 batch id 4021 loss 0.35779640078544617 train acc 0.8420207970654067\n",
            "epoch 11 batch id 4031 loss 0.5490300059318542 train acc 0.8420638489208633\n",
            "epoch 11 batch id 4041 loss 0.6369354128837585 train acc 0.8420293553575847\n",
            "epoch 11 batch id 4051 loss 0.3926846981048584 train acc 0.8420606023204147\n",
            "epoch 11 batch id 4061 loss 0.4761921465396881 train acc 0.8421186284166462\n",
            "epoch 11 batch id 4071 loss 0.8649706840515137 train acc 0.8421264738393515\n",
            "epoch 11 batch id 4081 loss 0.2681289613246918 train acc 0.8422414849301642\n",
            "epoch 11 batch id 4091 loss 0.37993577122688293 train acc 0.8422757272060621\n",
            "epoch 11 batch id 4101 loss 0.563807487487793 train acc 0.8422678919775665\n",
            "epoch 11 batch id 4111 loss 0.3528892993927002 train acc 0.842279098759426\n",
            "epoch 11 batch id 4121 loss 0.4084317982196808 train acc 0.8422447524872604\n",
            "epoch 11 batch id 4131 loss 0.2515001893043518 train acc 0.8422181372549019\n",
            "epoch 11 batch id 4141 loss 0.347060889005661 train acc 0.8422708886742333\n",
            "epoch 11 batch id 4151 loss 0.36371105909347534 train acc 0.8423120934714526\n",
            "epoch 11 batch id 4161 loss 0.3415379226207733 train acc 0.8423005287190579\n",
            "epoch 11 batch id 4171 loss 0.48707178235054016 train acc 0.8423639415008392\n",
            "epoch 11 batch id 4181 loss 0.23559384047985077 train acc 0.8423822052140636\n",
            "epoch 11 batch id 4191 loss 0.43825438618659973 train acc 0.8424152946790742\n",
            "epoch 11 batch id 4201 loss 0.35112324357032776 train acc 0.842440787907641\n",
            "epoch 11 batch id 4211 loss 0.5741796493530273 train acc 0.8424253443362621\n",
            "epoch 11 batch id 4221 loss 0.34695732593536377 train acc 0.8423951670220327\n",
            "epoch 11 batch id 4231 loss 0.4025441110134125 train acc 0.8424057551406287\n",
            "epoch 11 batch id 4241 loss 0.38650205731391907 train acc 0.8424199775996227\n",
            "epoch 11 batch id 4251 loss 0.5140122175216675 train acc 0.8424635379910609\n",
            "epoch 11 batch id 4261 loss 0.4757421016693115 train acc 0.8425178948603614\n",
            "epoch 11 batch id 4271 loss 0.2934187352657318 train acc 0.8426085811285413\n",
            "epoch 11 batch id 4281 loss 0.338771790266037 train acc 0.8426440960056062\n",
            "epoch 11 batch id 4291 loss 0.37381574511528015 train acc 0.8426940107201119\n",
            "epoch 11 batch id 4301 loss 0.3141340911388397 train acc 0.8426892001860032\n",
            "epoch 11 batch id 4311 loss 0.4198850095272064 train acc 0.8427242809093017\n",
            "epoch 11 batch id 4321 loss 0.3776915669441223 train acc 0.842719422587364\n",
            "epoch 11 batch id 4331 loss 0.291485995054245 train acc 0.8427470561071346\n",
            "epoch 11 batch id 4341 loss 0.24452222883701324 train acc 0.8427529659064732\n",
            "epoch 11 batch id 4351 loss 0.3232499361038208 train acc 0.84280194208228\n",
            "epoch 11 batch id 4361 loss 0.6859946250915527 train acc 0.8428614423297409\n",
            "epoch 11 batch id 4371 loss 0.46891820430755615 train acc 0.8429421185083505\n",
            "epoch 11 batch id 4381 loss 0.4534774720668793 train acc 0.8429617952522255\n",
            "epoch 11 batch id 4391 loss 0.3927757740020752 train acc 0.8430169665224323\n",
            "epoch 11 batch id 4401 loss 0.3772701919078827 train acc 0.8429831288343558\n",
            "epoch 11 batch id 4411 loss 0.3413529098033905 train acc 0.8430025787803219\n",
            "epoch 11 batch id 4421 loss 0.444517582654953 train acc 0.8430290092739199\n",
            "epoch 11 batch id 4431 loss 0.5410491228103638 train acc 0.8430764782216204\n",
            "epoch 11 batch id 4441 loss 0.1955013871192932 train acc 0.8431342884485477\n",
            "epoch 11 batch id 4451 loss 0.4099627733230591 train acc 0.8431462031004269\n",
            "epoch 11 batch id 4461 loss 0.40722188353538513 train acc 0.8432246133154001\n",
            "epoch 11 batch id 4471 loss 0.442282497882843 train acc 0.8432362726459405\n",
            "epoch 11 batch id 4481 loss 0.29685109853744507 train acc 0.8432304452131221\n",
            "epoch 11 batch id 4491 loss 0.28699469566345215 train acc 0.8432246437319083\n",
            "epoch 11 batch id 4501 loss 0.32895299792289734 train acc 0.8432015107753833\n",
            "epoch 11 batch id 4511 loss 0.6397809982299805 train acc 0.8431750166260252\n",
            "epoch 11 batch id 4521 loss 0.27912405133247375 train acc 0.8432177615571776\n",
            "epoch 11 batch id 4531 loss 0.39203235507011414 train acc 0.8431775546237034\n",
            "epoch 11 batch id 4541 loss 0.3911249041557312 train acc 0.8431994604712618\n",
            "epoch 11 batch id 4551 loss 0.5555210113525391 train acc 0.843210970116458\n",
            "epoch 11 batch id 4561 loss 0.5830703377723694 train acc 0.8432121519403639\n",
            "epoch 11 batch id 4571 loss 0.4896899461746216 train acc 0.8432509297746664\n",
            "epoch 11 batch id 4581 loss 0.35367345809936523 train acc 0.843282716655752\n",
            "epoch 11 batch id 4591 loss 0.5062058568000793 train acc 0.8432973480723154\n",
            "epoch 11 batch id 4601 loss 0.33183541893959045 train acc 0.8433628559008911\n",
            "epoch 11 batch id 4611 loss 0.39429739117622375 train acc 0.8434348568640209\n",
            "epoch 11 batch id 4621 loss 0.2937553822994232 train acc 0.8434997835966241\n",
            "epoch 11 batch id 4631 loss 0.3893282115459442 train acc 0.8435374379183762\n",
            "epoch 11 batch id 4641 loss 0.6326189041137695 train acc 0.8435143288084465\n",
            "epoch 11 batch id 4651 loss 0.5156256556510925 train acc 0.8435249139969899\n",
            "epoch 11 batch id 4661 loss 0.4488295614719391 train acc 0.8435890903239648\n",
            "epoch 11 batch id 4671 loss 0.33762648701667786 train acc 0.8436362663241276\n",
            "epoch 11 batch id 4681 loss 0.5402700304985046 train acc 0.8436899166844691\n",
            "epoch 11 batch id 4691 loss 0.29810866713523865 train acc 0.8436700596887657\n",
            "epoch 11 batch id 4701 loss 0.33945614099502563 train acc 0.843696819825569\n",
            "epoch 11 batch id 4711 loss 0.2573844790458679 train acc 0.8436936160050944\n",
            "epoch 11 batch id 4721 loss 0.329490065574646 train acc 0.8436540192755773\n",
            "epoch 11 batch id 4731 loss 0.23317471146583557 train acc 0.8436344060452335\n",
            "epoch 11 batch id 4741 loss 0.4045482277870178 train acc 0.8436610156085214\n",
            "epoch 11 batch id 4751 loss 0.3601774573326111 train acc 0.8436809355925068\n",
            "epoch 11 batch id 4761 loss 0.41166621446609497 train acc 0.8436384162990969\n",
            "epoch 11 batch id 4771 loss 0.41999825835227966 train acc 0.8436681251309998\n",
            "epoch 11 batch id 4781 loss 0.5841225385665894 train acc 0.8436911733946874\n",
            "epoch 11 batch id 4791 loss 0.5583398342132568 train acc 0.8437043414735963\n",
            "epoch 11 batch id 4801 loss 0.38950344920158386 train acc 0.8436849093938763\n",
            "epoch 11 batch id 4811 loss 0.3221663534641266 train acc 0.8436753013926419\n",
            "epoch 11 batch id 4821 loss 0.4224158227443695 train acc 0.8437143486828459\n",
            "epoch 11 batch id 4831 loss 0.2194819152355194 train acc 0.8437823432001657\n",
            "epoch 11 batch id 4841 loss 0.5021571516990662 train acc 0.8437661381945879\n",
            "epoch 11 batch id 4851 loss 0.5084924101829529 train acc 0.8437757678829108\n",
            "epoch 11 batch id 4861 loss 0.29367631673812866 train acc 0.8437821435918536\n",
            "epoch 11 batch id 4871 loss 0.27607306838035583 train acc 0.8438301940053378\n",
            "epoch 11 batch id 4881 loss 0.40493646264076233 train acc 0.8438812487195246\n",
            "epoch 11 batch id 4891 loss 0.39269185066223145 train acc 0.8438969535882233\n",
            "epoch 11 batch id 4901 loss 0.5193853974342346 train acc 0.8439062181187513\n",
            "epoch 11 batch id 4911 loss 0.5510146617889404 train acc 0.8438709020566076\n",
            "epoch 11 batch id 4921 loss 0.2970311939716339 train acc 0.8438547805324121\n",
            "epoch 11 batch id 4931 loss 0.5944371819496155 train acc 0.843867242952748\n",
            "epoch 11 batch id 4941 loss 0.4427909851074219 train acc 0.8438290578830197\n",
            "epoch 11 batch id 4951 loss 0.423962265253067 train acc 0.843863613411432\n",
            "epoch 11 train acc 0.8438720725512132\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b917609902f5463c9b759ad9b4622518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 11 loss 0.8838638663291931 test acc 0.7704579820381232\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc74752b40194b769fd0cd92cde892e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 12 batch id 1 loss 0.3649761378765106 train acc 0.90625\n",
            "epoch 12 batch id 11 loss 0.42660558223724365 train acc 0.8792613636363636\n",
            "epoch 12 batch id 21 loss 0.516410231590271 train acc 0.8742559523809523\n",
            "epoch 12 batch id 31 loss 0.4822925329208374 train acc 0.8639112903225806\n",
            "epoch 12 batch id 41 loss 0.2097901701927185 train acc 0.8628048780487805\n",
            "epoch 12 batch id 51 loss 0.27404627203941345 train acc 0.8618259803921569\n",
            "epoch 12 batch id 61 loss 0.4473665654659271 train acc 0.8547643442622951\n",
            "epoch 12 batch id 71 loss 0.5441187620162964 train acc 0.8565140845070423\n",
            "epoch 12 batch id 81 loss 0.4782377779483795 train acc 0.8564814814814815\n",
            "epoch 12 batch id 91 loss 0.514228343963623 train acc 0.8567994505494505\n",
            "epoch 12 batch id 101 loss 0.3655995726585388 train acc 0.8558168316831684\n",
            "epoch 12 batch id 111 loss 0.3719703257083893 train acc 0.8569819819819819\n",
            "epoch 12 batch id 121 loss 0.609891414642334 train acc 0.8570506198347108\n",
            "epoch 12 batch id 131 loss 0.45289403200149536 train acc 0.857824427480916\n",
            "epoch 12 batch id 141 loss 0.3655695915222168 train acc 0.8563829787234043\n",
            "epoch 12 batch id 151 loss 0.32780686020851135 train acc 0.8567880794701986\n",
            "epoch 12 batch id 161 loss 0.4996373951435089 train acc 0.8581133540372671\n",
            "epoch 12 batch id 171 loss 0.40925756096839905 train acc 0.8584612573099415\n",
            "epoch 12 batch id 181 loss 0.3617361783981323 train acc 0.8585980662983426\n",
            "epoch 12 batch id 191 loss 0.408057302236557 train acc 0.8589659685863874\n",
            "epoch 12 batch id 201 loss 0.3444834351539612 train acc 0.8606187810945274\n",
            "epoch 12 batch id 211 loss 0.5849572420120239 train acc 0.8596712085308057\n",
            "epoch 12 batch id 221 loss 0.6169646978378296 train acc 0.8579609728506787\n",
            "epoch 12 batch id 231 loss 0.4366236925125122 train acc 0.8585633116883117\n",
            "epoch 12 batch id 241 loss 0.5740649700164795 train acc 0.8574948132780082\n",
            "epoch 12 batch id 251 loss 0.4612870514392853 train acc 0.8573207171314741\n",
            "epoch 12 batch id 261 loss 0.2561887204647064 train acc 0.857698754789272\n",
            "epoch 12 batch id 271 loss 0.3765571713447571 train acc 0.8579335793357934\n",
            "epoch 12 batch id 281 loss 0.29802411794662476 train acc 0.858818950177936\n",
            "epoch 12 batch id 291 loss 0.5177721977233887 train acc 0.8584621993127147\n",
            "epoch 12 batch id 301 loss 0.4078823924064636 train acc 0.8592711794019934\n",
            "epoch 12 batch id 311 loss 0.30293673276901245 train acc 0.8591237942122186\n",
            "epoch 12 batch id 321 loss 0.4669859707355499 train acc 0.8582067757009346\n",
            "epoch 12 batch id 331 loss 0.4787236154079437 train acc 0.8583836858006042\n",
            "epoch 12 batch id 341 loss 0.5152480602264404 train acc 0.8584127565982405\n",
            "epoch 12 batch id 351 loss 0.6164647936820984 train acc 0.8583511396011396\n",
            "epoch 12 batch id 361 loss 0.4753422141075134 train acc 0.8580765235457064\n",
            "epoch 12 batch id 371 loss 0.6327752470970154 train acc 0.8579851752021563\n",
            "epoch 12 batch id 381 loss 0.16106678545475006 train acc 0.8580216535433071\n",
            "epoch 12 batch id 391 loss 0.5736044049263 train acc 0.8576166879795396\n",
            "epoch 12 batch id 401 loss 0.3273985981941223 train acc 0.8573877805486284\n",
            "epoch 12 batch id 411 loss 0.27935945987701416 train acc 0.8575121654501217\n",
            "epoch 12 batch id 421 loss 0.4551220238208771 train acc 0.8575935273159145\n",
            "epoch 12 batch id 431 loss 0.3926558792591095 train acc 0.8575623549883991\n",
            "epoch 12 batch id 441 loss 0.6469566822052002 train acc 0.8571428571428571\n",
            "epoch 12 batch id 451 loss 0.4698420763015747 train acc 0.857434866962306\n",
            "epoch 12 batch id 461 loss 0.23344188928604126 train acc 0.8573752711496746\n",
            "epoch 12 batch id 471 loss 0.39795610308647156 train acc 0.85828025477707\n",
            "epoch 12 batch id 481 loss 0.44005975127220154 train acc 0.8583030145530145\n",
            "epoch 12 batch id 491 loss 0.5093640685081482 train acc 0.8578475050916496\n",
            "epoch 12 batch id 501 loss 0.6658931970596313 train acc 0.8577844311377245\n",
            "epoch 12 batch id 511 loss 0.3516978621482849 train acc 0.8577544031311155\n",
            "epoch 12 batch id 521 loss 0.4702322483062744 train acc 0.857665547024952\n",
            "epoch 12 batch id 531 loss 0.38578394055366516 train acc 0.8578742937853108\n",
            "epoch 12 batch id 541 loss 0.37179824709892273 train acc 0.8579886783733827\n",
            "epoch 12 batch id 551 loss 0.3659075200557709 train acc 0.8579571234119783\n",
            "epoch 12 batch id 561 loss 0.4373477101325989 train acc 0.8575646167557932\n",
            "epoch 12 batch id 571 loss 0.6341884136199951 train acc 0.8574321366024519\n",
            "epoch 12 batch id 581 loss 0.32666489481925964 train acc 0.8573848967297762\n",
            "epoch 12 batch id 591 loss 0.24399742484092712 train acc 0.8574978849407784\n",
            "epoch 12 batch id 601 loss 0.45309823751449585 train acc 0.8575031198003328\n",
            "epoch 12 batch id 611 loss 0.5660430192947388 train acc 0.8575337561374795\n",
            "epoch 12 batch id 621 loss 0.3941898047924042 train acc 0.857613727858293\n",
            "epoch 12 batch id 631 loss 0.41771891713142395 train acc 0.8577159270998416\n",
            "epoch 12 batch id 641 loss 0.38430365920066833 train acc 0.8578880655226209\n",
            "epoch 12 batch id 651 loss 0.25436824560165405 train acc 0.8578629032258065\n",
            "epoch 12 batch id 661 loss 0.43494972586631775 train acc 0.8579803328290468\n",
            "epoch 12 batch id 671 loss 0.4373244643211365 train acc 0.8580476900149031\n",
            "epoch 12 batch id 681 loss 0.25872698426246643 train acc 0.8579983480176211\n",
            "epoch 12 batch id 691 loss 0.4688262641429901 train acc 0.8583348408104197\n",
            "epoch 12 batch id 701 loss 0.24618473649024963 train acc 0.8581044935805991\n",
            "epoch 12 batch id 711 loss 0.3928506374359131 train acc 0.8579685302390999\n",
            "epoch 12 batch id 721 loss 0.3555169403553009 train acc 0.8576412968099861\n",
            "epoch 12 batch id 731 loss 0.2536047697067261 train acc 0.8578787619699042\n",
            "epoch 12 batch id 741 loss 0.5290511846542358 train acc 0.8581098178137652\n",
            "epoch 12 batch id 751 loss 0.5562326312065125 train acc 0.8576897470039947\n",
            "epoch 12 batch id 761 loss 0.27782580256462097 train acc 0.85785561760841\n",
            "epoch 12 batch id 771 loss 0.5260956287384033 train acc 0.8578347924773022\n",
            "epoch 12 batch id 781 loss 0.5105847120285034 train acc 0.8574943982074263\n",
            "epoch 12 batch id 791 loss 0.3572881519794464 train acc 0.8575181731984829\n",
            "epoch 12 batch id 801 loss 0.5266910791397095 train acc 0.8577169163545568\n",
            "epoch 12 batch id 811 loss 0.5299213528633118 train acc 0.857756627620222\n",
            "epoch 12 batch id 821 loss 0.17888393998146057 train acc 0.857795371498173\n",
            "epoch 12 batch id 831 loss 0.5939229130744934 train acc 0.8579836040914561\n",
            "epoch 12 batch id 841 loss 0.46251675486564636 train acc 0.8582230975029727\n",
            "epoch 12 batch id 851 loss 0.44814056158065796 train acc 0.8581264688601645\n",
            "epoch 12 batch id 861 loss 0.4490396976470947 train acc 0.8582680023228804\n",
            "epoch 12 batch id 871 loss 0.20757736265659332 train acc 0.8584959816303099\n",
            "epoch 12 batch id 881 loss 0.4498790502548218 train acc 0.8582044551645857\n",
            "epoch 12 batch id 891 loss 0.3001989722251892 train acc 0.8581649831649831\n",
            "epoch 12 batch id 901 loss 0.6085875630378723 train acc 0.8581784128745837\n",
            "epoch 12 batch id 911 loss 0.6543208360671997 train acc 0.8582258507135017\n",
            "epoch 12 batch id 921 loss 0.41741228103637695 train acc 0.8581535016286646\n",
            "epoch 12 batch id 931 loss 0.30193793773651123 train acc 0.8581162728249194\n",
            "epoch 12 batch id 941 loss 0.33117637038230896 train acc 0.85846174282678\n",
            "epoch 12 batch id 951 loss 0.3387858271598816 train acc 0.8583070452155626\n",
            "epoch 12 batch id 961 loss 0.2998284697532654 train acc 0.8580580124869928\n",
            "epoch 12 batch id 971 loss 0.5659103393554688 train acc 0.8581842173017508\n",
            "epoch 12 batch id 981 loss 0.3782104253768921 train acc 0.8586423292558614\n",
            "epoch 12 batch id 991 loss 0.4314914643764496 train acc 0.8585866548940464\n",
            "epoch 12 batch id 1001 loss 0.5680217742919922 train acc 0.8586413586413586\n",
            "epoch 12 batch id 1011 loss 0.361511766910553 train acc 0.8586640702274976\n",
            "epoch 12 batch id 1021 loss 0.2584720551967621 train acc 0.8589158912830558\n",
            "epoch 12 batch id 1031 loss 0.4429102838039398 train acc 0.8589658098933075\n",
            "epoch 12 batch id 1041 loss 0.4175751805305481 train acc 0.8592249039385207\n",
            "epoch 12 batch id 1051 loss 0.2494174987077713 train acc 0.8590181969552807\n",
            "epoch 12 batch id 1061 loss 0.4433373808860779 train acc 0.8591099198868991\n",
            "epoch 12 batch id 1071 loss 0.21179261803627014 train acc 0.8591707516339869\n",
            "epoch 12 batch id 1081 loss 0.5129125714302063 train acc 0.8589124653098983\n",
            "epoch 12 batch id 1091 loss 0.6093626022338867 train acc 0.8587448441796517\n",
            "epoch 12 batch id 1101 loss 0.27150586247444153 train acc 0.8587931425976385\n",
            "epoch 12 batch id 1111 loss 0.521729052066803 train acc 0.8587561881188119\n",
            "epoch 12 batch id 1121 loss 0.1587202250957489 train acc 0.8589847234611954\n",
            "epoch 12 batch id 1131 loss 0.4778880774974823 train acc 0.8587947612732095\n",
            "epoch 12 batch id 1141 loss 0.4151739180088043 train acc 0.8588135407537248\n",
            "epoch 12 batch id 1151 loss 0.36067870259284973 train acc 0.8589405951346655\n",
            "epoch 12 batch id 1161 loss 0.2897493839263916 train acc 0.8588770456503014\n",
            "epoch 12 batch id 1171 loss 0.30597642064094543 train acc 0.859028074295474\n",
            "epoch 12 batch id 1181 loss 0.49028149247169495 train acc 0.8590442421676545\n",
            "epoch 12 batch id 1191 loss 0.378414124250412 train acc 0.8589420654911839\n",
            "epoch 12 batch id 1201 loss 0.4954283535480499 train acc 0.8592318900915903\n",
            "epoch 12 batch id 1211 loss 0.2017170786857605 train acc 0.8592717795210569\n",
            "epoch 12 batch id 1221 loss 0.39180874824523926 train acc 0.8594517813267813\n",
            "epoch 12 batch id 1231 loss 0.3674723207950592 train acc 0.8594384646628757\n",
            "epoch 12 batch id 1241 loss 0.3243756592273712 train acc 0.8593624093473006\n",
            "epoch 12 batch id 1251 loss 0.35059916973114014 train acc 0.8593625099920064\n",
            "epoch 12 batch id 1261 loss 0.28222736716270447 train acc 0.8593502180808882\n",
            "epoch 12 batch id 1271 loss 0.3417879045009613 train acc 0.8593627065302911\n",
            "epoch 12 batch id 1281 loss 0.3061927556991577 train acc 0.8594481850117096\n",
            "epoch 12 batch id 1291 loss 0.4386778771877289 train acc 0.8595323392718822\n",
            "epoch 12 batch id 1301 loss 0.547710657119751 train acc 0.8593629900076863\n",
            "epoch 12 batch id 1311 loss 0.4693739116191864 train acc 0.8594226735316552\n",
            "epoch 12 batch id 1321 loss 0.23566938936710358 train acc 0.859375\n",
            "epoch 12 batch id 1331 loss 0.42256075143814087 train acc 0.859375\n",
            "epoch 12 batch id 1341 loss 0.4116849899291992 train acc 0.8592817859806114\n",
            "epoch 12 batch id 1351 loss 0.2546978294849396 train acc 0.8593518689859363\n",
            "epoch 12 batch id 1361 loss 0.5163116455078125 train acc 0.8591798310066128\n",
            "epoch 12 batch id 1371 loss 0.36804431676864624 train acc 0.8592040481400438\n",
            "epoch 12 batch id 1381 loss 0.4318807125091553 train acc 0.8592731716147719\n",
            "epoch 12 batch id 1391 loss 0.14337389171123505 train acc 0.8593188353702372\n",
            "epoch 12 batch id 1401 loss 0.302887886762619 train acc 0.8593303890078515\n",
            "epoch 12 batch id 1411 loss 0.21309395134449005 train acc 0.8594525159461375\n",
            "epoch 12 batch id 1421 loss 0.30502888560295105 train acc 0.8593859957776214\n",
            "epoch 12 batch id 1431 loss 0.4550560712814331 train acc 0.8594077568134172\n",
            "epoch 12 batch id 1441 loss 0.3653988838195801 train acc 0.8593641568355309\n",
            "epoch 12 batch id 1451 loss 0.2961852550506592 train acc 0.8594611474844934\n",
            "epoch 12 batch id 1461 loss 0.3693138659000397 train acc 0.8593856947296372\n",
            "epoch 12 batch id 1471 loss 0.4872772991657257 train acc 0.8594599762066621\n",
            "epoch 12 batch id 1481 loss 0.2922292649745941 train acc 0.8595332545577312\n",
            "epoch 12 batch id 1491 loss 0.36140766739845276 train acc 0.8596789067739772\n",
            "epoch 12 batch id 1501 loss 0.40553808212280273 train acc 0.859780979347102\n",
            "epoch 12 batch id 1511 loss 0.3498811721801758 train acc 0.859840337524818\n",
            "epoch 12 batch id 1521 loss 0.29200780391693115 train acc 0.8598989151873767\n",
            "epoch 12 batch id 1531 loss 0.31686314940452576 train acc 0.859977139124755\n",
            "epoch 12 batch id 1541 loss 0.37913432717323303 train acc 0.8600036502271252\n",
            "epoch 12 batch id 1551 loss 0.3231949210166931 train acc 0.8599593004513217\n",
            "epoch 12 batch id 1561 loss 0.4209422767162323 train acc 0.8599455477258168\n",
            "epoch 12 batch id 1571 loss 0.4430826008319855 train acc 0.8600314290260981\n",
            "epoch 12 batch id 1581 loss 0.3674416244029999 train acc 0.8600075110689437\n",
            "epoch 12 batch id 1591 loss 0.43924301862716675 train acc 0.8601017441860465\n",
            "epoch 12 batch id 1601 loss 0.6255816221237183 train acc 0.8601167239225485\n",
            "epoch 12 batch id 1611 loss 0.4096176028251648 train acc 0.8600539261328367\n",
            "epoch 12 batch id 1621 loss 0.23537296056747437 train acc 0.8601268507094386\n",
            "epoch 12 batch id 1631 loss 0.22636133432388306 train acc 0.8602180410790926\n",
            "epoch 12 batch id 1641 loss 0.4256996512413025 train acc 0.8603271633150518\n",
            "epoch 12 batch id 1651 loss 0.3024439513683319 train acc 0.8603403240460327\n",
            "epoch 12 batch id 1661 loss 0.4219367802143097 train acc 0.8603439193257074\n",
            "epoch 12 batch id 1671 loss 0.42024168372154236 train acc 0.8603381208856972\n",
            "epoch 12 batch id 1681 loss 0.46202462911605835 train acc 0.8601093099345628\n",
            "epoch 12 batch id 1691 loss 0.6048741340637207 train acc 0.8601419278533412\n",
            "epoch 12 batch id 1701 loss 0.5129004120826721 train acc 0.8601557907113463\n",
            "epoch 12 batch id 1711 loss 0.3652816712856293 train acc 0.8601238310929281\n",
            "epoch 12 batch id 1721 loss 0.38273710012435913 train acc 0.8600831638582219\n",
            "epoch 12 batch id 1731 loss 0.3121546804904938 train acc 0.8602234979780474\n",
            "epoch 12 batch id 1741 loss 0.4184039831161499 train acc 0.8602186243538197\n",
            "epoch 12 batch id 1751 loss 0.26803871989250183 train acc 0.8601781125071388\n",
            "epoch 12 batch id 1761 loss 0.5504497289657593 train acc 0.8601824247586598\n",
            "epoch 12 batch id 1771 loss 0.48104262351989746 train acc 0.8602131564088086\n",
            "epoch 12 batch id 1781 loss 0.3403284251689911 train acc 0.860243542953397\n",
            "epoch 12 batch id 1791 loss 0.6019112467765808 train acc 0.8601863484087102\n",
            "epoch 12 batch id 1801 loss 0.2760777175426483 train acc 0.8603640338700722\n",
            "epoch 12 batch id 1811 loss 0.24405838549137115 train acc 0.8604707344008835\n",
            "epoch 12 batch id 1821 loss 0.27935218811035156 train acc 0.8605076194398682\n",
            "epoch 12 batch id 1831 loss 0.24847912788391113 train acc 0.8605611687602404\n",
            "epoch 12 batch id 1841 loss 0.19712412357330322 train acc 0.8606226235741445\n",
            "epoch 12 batch id 1851 loss 0.40493205189704895 train acc 0.8606580902215019\n",
            "epoch 12 batch id 1861 loss 0.40501654148101807 train acc 0.8606511955937668\n",
            "epoch 12 batch id 1871 loss 0.5513516664505005 train acc 0.8607529396044896\n",
            "epoch 12 batch id 1881 loss 0.403273344039917 train acc 0.8608286815523658\n",
            "epoch 12 batch id 1891 loss 0.28102993965148926 train acc 0.8610440904283448\n",
            "epoch 12 batch id 1901 loss 0.4599965214729309 train acc 0.8610599684376644\n",
            "epoch 12 batch id 1911 loss 0.3112707734107971 train acc 0.861100209314495\n",
            "epoch 12 batch id 1921 loss 0.45609036087989807 train acc 0.8609204190525768\n",
            "epoch 12 batch id 1931 loss 0.41113975644111633 train acc 0.8609447824961161\n",
            "epoch 12 batch id 1941 loss 0.3490726947784424 train acc 0.8609286450283359\n",
            "epoch 12 batch id 1951 loss 0.4495619833469391 train acc 0.8609286904151717\n",
            "epoch 12 batch id 1961 loss 0.19614921510219574 train acc 0.8609685747067822\n",
            "epoch 12 batch id 1971 loss 0.35657474398612976 train acc 0.8610239091831557\n",
            "epoch 12 batch id 1981 loss 0.41525980830192566 train acc 0.8610550227158001\n",
            "epoch 12 batch id 1991 loss 0.4751957058906555 train acc 0.860960258663988\n",
            "epoch 12 batch id 2001 loss 0.540835976600647 train acc 0.8608664417791104\n",
            "epoch 12 batch id 2011 loss 0.4272870719432831 train acc 0.8608279462953754\n",
            "epoch 12 batch id 2021 loss 0.5974591374397278 train acc 0.8608052944087086\n",
            "epoch 12 batch id 2031 loss 0.23745685815811157 train acc 0.8608597981290005\n",
            "epoch 12 batch id 2041 loss 0.2812904417514801 train acc 0.8608525232729054\n",
            "epoch 12 batch id 2051 loss 0.4331494867801666 train acc 0.8608453193564115\n",
            "epoch 12 batch id 2061 loss 0.43213513493537903 train acc 0.8608306040756915\n",
            "epoch 12 batch id 2071 loss 0.3954492211341858 train acc 0.8607858522452921\n",
            "epoch 12 batch id 2081 loss 0.4754471778869629 train acc 0.8607865809706872\n",
            "epoch 12 batch id 2091 loss 0.34360915422439575 train acc 0.8607051052128168\n",
            "epoch 12 batch id 2101 loss 0.20780642330646515 train acc 0.8608028914802475\n",
            "epoch 12 batch id 2111 loss 0.48775193095207214 train acc 0.8607665206063477\n",
            "epoch 12 batch id 2121 loss 0.16783882677555084 train acc 0.860723125884017\n",
            "epoch 12 batch id 2131 loss 0.4924529194831848 train acc 0.8607314641013608\n",
            "epoch 12 batch id 2141 loss 0.45762795209884644 train acc 0.8607908103689864\n",
            "epoch 12 batch id 2151 loss 0.40851789712905884 train acc 0.8607842282659228\n",
            "epoch 12 batch id 2161 loss 0.5266858339309692 train acc 0.8606909416936603\n",
            "epoch 12 batch id 2171 loss 0.22322703897953033 train acc 0.8607784431137725\n",
            "epoch 12 batch id 2181 loss 0.41826289892196655 train acc 0.8607218592388812\n",
            "epoch 12 batch id 2191 loss 0.20987485349178314 train acc 0.8607941579187586\n",
            "epoch 12 batch id 2201 loss 0.31062051653862 train acc 0.8607380168105406\n",
            "epoch 12 batch id 2211 loss 0.33493903279304504 train acc 0.8607035843509724\n",
            "epoch 12 batch id 2221 loss 0.7111424207687378 train acc 0.8606905673120216\n",
            "epoch 12 batch id 2231 loss 0.211488738656044 train acc 0.8607477028238458\n",
            "epoch 12 batch id 2241 loss 0.295780748128891 train acc 0.8607555220883534\n",
            "epoch 12 batch id 2251 loss 0.28433284163475037 train acc 0.8607702132385606\n",
            "epoch 12 batch id 2261 loss 0.32610857486724854 train acc 0.8607778637770898\n",
            "epoch 12 batch id 2271 loss 0.5150544047355652 train acc 0.8607716864817261\n",
            "epoch 12 batch id 2281 loss 0.5459666848182678 train acc 0.8608957145988602\n",
            "epoch 12 batch id 2291 loss 0.4294303357601166 train acc 0.8609163574858141\n",
            "epoch 12 batch id 2301 loss 0.364921897649765 train acc 0.8610047262059974\n",
            "epoch 12 batch id 2311 loss 0.4323190152645111 train acc 0.8609638684552142\n",
            "epoch 12 batch id 2321 loss 0.334686279296875 train acc 0.8608560426540285\n",
            "epoch 12 batch id 2331 loss 0.3538452982902527 train acc 0.860802767052767\n",
            "epoch 12 batch id 2341 loss 0.4397084414958954 train acc 0.8608500640751815\n",
            "epoch 12 batch id 2351 loss 0.3943488597869873 train acc 0.8609168970650787\n",
            "epoch 12 batch id 2361 loss 0.4608863592147827 train acc 0.8609765459551038\n",
            "epoch 12 batch id 2371 loss 0.35790717601776123 train acc 0.8610488717840573\n",
            "epoch 12 batch id 2381 loss 0.2763150930404663 train acc 0.8610877782444352\n",
            "epoch 12 batch id 2391 loss 0.27418452501296997 train acc 0.8611067544960268\n",
            "epoch 12 batch id 2401 loss 0.2215079516172409 train acc 0.8610149416909622\n",
            "epoch 12 batch id 2411 loss 0.3479645252227783 train acc 0.8611377540439652\n",
            "epoch 12 batch id 2421 loss 0.5077580213546753 train acc 0.861136926889715\n",
            "epoch 12 batch id 2431 loss 0.41343435645103455 train acc 0.8611168243521184\n",
            "epoch 12 batch id 2441 loss 0.4080035984516144 train acc 0.8610840843916427\n",
            "epoch 12 batch id 2451 loss 0.3112260699272156 train acc 0.8610962362301101\n",
            "epoch 12 batch id 2461 loss 0.38519659638404846 train acc 0.861178128809427\n",
            "epoch 12 batch id 2471 loss 0.4811244606971741 train acc 0.8611265681910157\n",
            "epoch 12 batch id 2481 loss 0.5180480480194092 train acc 0.8612013804917372\n",
            "epoch 12 batch id 2491 loss 0.5603182911872864 train acc 0.8611375953432356\n",
            "epoch 12 batch id 2501 loss 0.21869292855262756 train acc 0.8611680327868853\n",
            "epoch 12 batch id 2511 loss 0.2693096101284027 train acc 0.8611795599362804\n",
            "epoch 12 batch id 2521 loss 0.3789594769477844 train acc 0.8610794327647758\n",
            "epoch 12 batch id 2531 loss 0.3767075538635254 train acc 0.8610418312919794\n",
            "epoch 12 batch id 2541 loss 0.439206063747406 train acc 0.8610045257772531\n",
            "epoch 12 batch id 2551 loss 0.31996339559555054 train acc 0.8609613876911015\n",
            "epoch 12 batch id 2561 loss 0.4525296092033386 train acc 0.8609124853572823\n",
            "epoch 12 batch id 2571 loss 0.3612426221370697 train acc 0.8609490470633995\n",
            "epoch 12 batch id 2581 loss 0.34881895780563354 train acc 0.8609126791941109\n",
            "epoch 12 batch id 2591 loss 0.2922452986240387 train acc 0.8609368969509842\n",
            "epoch 12 batch id 2601 loss 0.5363835096359253 train acc 0.860996972318339\n",
            "epoch 12 batch id 2611 loss 0.26455676555633545 train acc 0.8609847759479127\n",
            "epoch 12 batch id 2621 loss 0.4305744767189026 train acc 0.8609786341091187\n",
            "epoch 12 batch id 2631 loss 0.41660839319229126 train acc 0.8610081717977955\n",
            "epoch 12 batch id 2641 loss 0.3000783622264862 train acc 0.8610138205225294\n",
            "epoch 12 batch id 2651 loss 0.26723185181617737 train acc 0.861043002640513\n",
            "epoch 12 batch id 2661 loss 0.3924584984779358 train acc 0.8611835306275836\n",
            "epoch 12 batch id 2671 loss 0.46244680881500244 train acc 0.8611358105578435\n",
            "epoch 12 batch id 2681 loss 0.27650102972984314 train acc 0.8611001025736665\n",
            "epoch 12 batch id 2691 loss 0.49701741337776184 train acc 0.861122723894463\n",
            "epoch 12 batch id 2701 loss 0.5181339979171753 train acc 0.8611914568678267\n",
            "epoch 12 batch id 2711 loss 0.30070438981056213 train acc 0.8611732294356326\n",
            "epoch 12 batch id 2721 loss 0.19571547210216522 train acc 0.8612010749724366\n",
            "epoch 12 batch id 2731 loss 0.4520065486431122 train acc 0.8611200109849871\n",
            "epoch 12 batch id 2741 loss 0.41205883026123047 train acc 0.8611079441809558\n",
            "epoch 12 batch id 2751 loss 0.41272085905075073 train acc 0.8610675663395129\n",
            "epoch 12 batch id 2761 loss 0.4231904149055481 train acc 0.8611180278884463\n",
            "epoch 12 batch id 2771 loss 0.5135602951049805 train acc 0.8610835438469866\n",
            "epoch 12 batch id 2781 loss 0.5551852583885193 train acc 0.8611111111111112\n",
            "epoch 12 batch id 2791 loss 0.3358573913574219 train acc 0.8612000627015407\n",
            "epoch 12 batch id 2801 loss 0.4393281936645508 train acc 0.8612270171367369\n",
            "epoch 12 batch id 2811 loss 0.310455322265625 train acc 0.8612815723941658\n",
            "epoch 12 batch id 2821 loss 0.42258042097091675 train acc 0.861302507975895\n",
            "epoch 12 batch id 2831 loss 0.3160763680934906 train acc 0.8613619304132816\n",
            "epoch 12 batch id 2841 loss 0.3756999373435974 train acc 0.8613549366420274\n",
            "epoch 12 batch id 2851 loss 0.3353690505027771 train acc 0.8613425113995089\n",
            "epoch 12 batch id 2861 loss 0.22921785712242126 train acc 0.8613574799021321\n",
            "epoch 12 batch id 2871 loss 0.45320263504981995 train acc 0.8613669017763845\n",
            "epoch 12 batch id 2881 loss 0.28826797008514404 train acc 0.8613762582436654\n",
            "epoch 12 batch id 2891 loss 0.42253628373146057 train acc 0.8614125735039778\n",
            "epoch 12 batch id 2901 loss 0.36431747674942017 train acc 0.8613893915891072\n",
            "epoch 12 batch id 2911 loss 0.17638973891735077 train acc 0.861387839230505\n",
            "epoch 12 batch id 2921 loss 0.12223227322101593 train acc 0.8613221071550838\n",
            "epoch 12 batch id 2931 loss 0.19918815791606903 train acc 0.8613421187308086\n",
            "epoch 12 batch id 2941 loss 0.3189318776130676 train acc 0.8613673070384223\n",
            "epoch 12 batch id 2951 loss 0.34231141209602356 train acc 0.8614135038969841\n",
            "epoch 12 batch id 2961 loss 0.33100980520248413 train acc 0.861496327254306\n",
            "epoch 12 batch id 2971 loss 0.26125064492225647 train acc 0.8614891871423763\n",
            "epoch 12 batch id 2981 loss 0.383193701505661 train acc 0.8615030610533378\n",
            "epoch 12 batch id 2991 loss 0.2234792411327362 train acc 0.8615952022734872\n",
            "epoch 12 batch id 3001 loss 0.3628213107585907 train acc 0.8615825974675109\n",
            "epoch 12 batch id 3011 loss 0.4162468910217285 train acc 0.861575265692461\n",
            "epoch 12 batch id 3021 loss 0.34856605529785156 train acc 0.8615679824561403\n",
            "epoch 12 batch id 3031 loss 0.3498685657978058 train acc 0.8615813675354669\n",
            "epoch 12 batch id 3041 loss 0.5453447699546814 train acc 0.8615741121341663\n",
            "epoch 12 batch id 3051 loss 0.5398443341255188 train acc 0.86156178302196\n",
            "epoch 12 batch id 3061 loss 0.3621828854084015 train acc 0.8615546390068605\n",
            "epoch 12 batch id 3071 loss 0.3344961106777191 train acc 0.8615475415174211\n",
            "epoch 12 batch id 3081 loss 0.54768967628479 train acc 0.8615607757221682\n",
            "epoch 12 batch id 3091 loss 0.285667359828949 train acc 0.8616194192817859\n",
            "epoch 12 batch id 3101 loss 0.2885549068450928 train acc 0.8616373750403096\n",
            "epoch 12 batch id 3111 loss 0.4391110837459564 train acc 0.8616401478624237\n",
            "epoch 12 batch id 3121 loss 0.3867272436618805 train acc 0.8617129926305671\n",
            "epoch 12 batch id 3131 loss 0.3297545313835144 train acc 0.8618702091983392\n",
            "epoch 12 batch id 3141 loss 0.36760950088500977 train acc 0.8618423670805476\n",
            "epoch 12 batch id 3151 loss 0.25900954008102417 train acc 0.8619337115201523\n",
            "epoch 12 batch id 3161 loss 0.3041698634624481 train acc 0.8619503321733628\n",
            "epoch 12 batch id 3171 loss 0.3382396697998047 train acc 0.8618978634500157\n",
            "epoch 12 batch id 3181 loss 0.2688901424407959 train acc 0.8618801084564602\n",
            "epoch 12 batch id 3191 loss 0.21929675340652466 train acc 0.861926120338452\n",
            "epoch 12 batch id 3201 loss 0.40806928277015686 train acc 0.8618937441424555\n",
            "epoch 12 batch id 3211 loss 0.3840101659297943 train acc 0.8618956322018063\n",
            "epoch 12 batch id 3221 loss 0.4822815954685211 train acc 0.8619023595156784\n",
            "epoch 12 batch id 3231 loss 0.2210746854543686 train acc 0.861899373259053\n",
            "epoch 12 batch id 3241 loss 0.3550170063972473 train acc 0.8619494369021907\n",
            "epoch 12 batch id 3251 loss 0.44471585750579834 train acc 0.8619270993540449\n",
            "epoch 12 batch id 3261 loss 0.2980300188064575 train acc 0.8619576050291322\n",
            "epoch 12 batch id 3271 loss 0.3131963014602661 train acc 0.8619449327422807\n",
            "epoch 12 batch id 3281 loss 0.49452653527259827 train acc 0.8619466245047241\n",
            "epoch 12 batch id 3291 loss 0.35658740997314453 train acc 0.8620100273473108\n",
            "epoch 12 batch id 3301 loss 0.21961484849452972 train acc 0.8620635792184187\n",
            "epoch 12 batch id 3311 loss 0.36120590567588806 train acc 0.8620696164300815\n",
            "epoch 12 batch id 3321 loss 0.2976265549659729 train acc 0.8621556007226739\n",
            "epoch 12 batch id 3331 loss 0.5535960793495178 train acc 0.8621237991594116\n",
            "epoch 12 batch id 3341 loss 0.6659071445465088 train acc 0.8621296019155942\n",
            "epoch 12 batch id 3351 loss 0.23876023292541504 train acc 0.8621819979110713\n",
            "epoch 12 batch id 3361 loss 0.3686186373233795 train acc 0.8622759223445403\n",
            "epoch 12 batch id 3371 loss 0.4557204246520996 train acc 0.862258046573717\n",
            "epoch 12 batch id 3381 loss 0.1201828196644783 train acc 0.8622726264418811\n",
            "epoch 12 batch id 3391 loss 0.3075520694255829 train acc 0.862342413742259\n",
            "epoch 12 batch id 3401 loss 0.4246487319469452 train acc 0.8624209791237871\n",
            "epoch 12 batch id 3411 loss 0.3227706849575043 train acc 0.8624120492524187\n",
            "epoch 12 batch id 3421 loss 0.5009331107139587 train acc 0.8624397106109325\n",
            "epoch 12 batch id 3431 loss 0.4014902412891388 train acc 0.8624216700670359\n",
            "epoch 12 batch id 3441 loss 0.24887727200984955 train acc 0.8624446018599244\n",
            "epoch 12 batch id 3451 loss 0.33184996247291565 train acc 0.8624674007534048\n",
            "epoch 12 batch id 3461 loss 0.4647977948188782 train acc 0.8625713305403063\n",
            "epoch 12 batch id 3471 loss 0.48122546076774597 train acc 0.8626071377124748\n",
            "epoch 12 batch id 3481 loss 0.3011156916618347 train acc 0.8625619434070669\n",
            "epoch 12 batch id 3491 loss 0.24086147546768188 train acc 0.8626020481237467\n",
            "epoch 12 batch id 3501 loss 0.558254063129425 train acc 0.8626196086832334\n",
            "epoch 12 batch id 3511 loss 0.35265329480171204 train acc 0.8625569638279692\n",
            "epoch 12 batch id 3521 loss 0.3510505259037018 train acc 0.8625923033229196\n",
            "epoch 12 batch id 3531 loss 0.5974321961402893 train acc 0.8626584182951006\n",
            "epoch 12 batch id 3541 loss 0.24027946591377258 train acc 0.8627153346512285\n",
            "epoch 12 batch id 3551 loss 0.31041762232780457 train acc 0.8627411292593635\n",
            "epoch 12 batch id 3561 loss 0.28679847717285156 train acc 0.8628633108677338\n",
            "epoch 12 batch id 3571 loss 0.4723780155181885 train acc 0.8628010361243349\n",
            "epoch 12 batch id 3581 loss 0.41738277673721313 train acc 0.8628394652331751\n",
            "epoch 12 batch id 3591 loss 0.5849336385726929 train acc 0.8628950849345586\n",
            "epoch 12 batch id 3601 loss 0.36501842737197876 train acc 0.86295473479589\n",
            "epoch 12 batch id 3611 loss 0.2768428921699524 train acc 0.8630313625034617\n",
            "epoch 12 batch id 3621 loss 0.32763540744781494 train acc 0.8630298950566142\n",
            "epoch 12 batch id 3631 loss 0.30917131900787354 train acc 0.8630585582484164\n",
            "epoch 12 batch id 3641 loss 0.6415764689445496 train acc 0.8630613155726449\n",
            "epoch 12 batch id 3651 loss 0.4566597044467926 train acc 0.8630512188441523\n",
            "epoch 12 batch id 3661 loss 0.3168504536151886 train acc 0.863113732586725\n",
            "epoch 12 batch id 3671 loss 0.3375180661678314 train acc 0.8632014437482974\n",
            "epoch 12 batch id 3681 loss 0.4040829539299011 train acc 0.8631740695463189\n",
            "epoch 12 batch id 3691 loss 0.4114307463169098 train acc 0.8631807098347332\n",
            "epoch 12 batch id 3701 loss 0.2586078941822052 train acc 0.8632379762226425\n",
            "epoch 12 batch id 3711 loss 0.4124666154384613 train acc 0.8632696712476421\n",
            "epoch 12 batch id 3721 loss 0.3805554211139679 train acc 0.8633389881752217\n",
            "epoch 12 batch id 3731 loss 0.29032155871391296 train acc 0.8634163093004557\n",
            "epoch 12 batch id 3741 loss 0.5120954513549805 train acc 0.8633762697139802\n",
            "epoch 12 batch id 3751 loss 0.5442270040512085 train acc 0.8633281125033324\n",
            "epoch 12 batch id 3761 loss 0.2233981192111969 train acc 0.863392382345121\n",
            "epoch 12 batch id 3771 loss 0.2738640308380127 train acc 0.8633941593741713\n",
            "epoch 12 batch id 3781 loss 0.42424657940864563 train acc 0.8634165895265803\n",
            "epoch 12 batch id 3791 loss 0.10176202654838562 train acc 0.8634924821946716\n",
            "epoch 12 batch id 3801 loss 0.2898561954498291 train acc 0.8635679755327546\n",
            "epoch 12 batch id 3811 loss 0.30599984526634216 train acc 0.8635938729992128\n",
            "epoch 12 batch id 3821 loss 0.4715713858604431 train acc 0.8636032779377126\n",
            "epoch 12 batch id 3831 loss 0.2737632691860199 train acc 0.8636126337770818\n",
            "epoch 12 batch id 3841 loss 0.26531800627708435 train acc 0.8636626204113512\n",
            "epoch 12 batch id 3851 loss 0.17236725986003876 train acc 0.8637164048299143\n",
            "epoch 12 batch id 3861 loss 0.3283746838569641 train acc 0.86369301994302\n",
            "epoch 12 batch id 3871 loss 0.5477498769760132 train acc 0.8637303022474813\n",
            "epoch 12 batch id 3881 loss 0.36460188031196594 train acc 0.8637673924246329\n",
            "epoch 12 batch id 3891 loss 0.38439807295799255 train acc 0.8637842135697764\n",
            "epoch 12 batch id 3901 loss 0.24885106086730957 train acc 0.8637488784926942\n",
            "epoch 12 batch id 3911 loss 0.16808252036571503 train acc 0.8638016172334442\n",
            "epoch 12 batch id 3921 loss 0.2575644254684448 train acc 0.8638022825809742\n",
            "epoch 12 batch id 3931 loss 0.29468846321105957 train acc 0.8638784660391757\n",
            "epoch 12 batch id 3941 loss 0.3722245693206787 train acc 0.8638630740928698\n",
            "epoch 12 batch id 3951 loss 0.30544841289520264 train acc 0.8638714882308276\n",
            "epoch 12 batch id 3961 loss 0.41751083731651306 train acc 0.8638759151729362\n",
            "epoch 12 batch id 3971 loss 0.3417750597000122 train acc 0.8639196675900277\n",
            "epoch 12 batch id 3981 loss 0.29748353362083435 train acc 0.8639278761617684\n",
            "epoch 12 batch id 3991 loss 0.6386542916297913 train acc 0.8639125532448008\n",
            "epoch 12 batch id 4001 loss 0.252834290266037 train acc 0.8638660647338166\n",
            "epoch 12 batch id 4011 loss 0.47892630100250244 train acc 0.8638431812515582\n",
            "epoch 12 batch id 4021 loss 0.2316499948501587 train acc 0.8638631559313603\n",
            "epoch 12 batch id 4031 loss 0.5397326946258545 train acc 0.8639295460183577\n",
            "epoch 12 batch id 4041 loss 0.44829288125038147 train acc 0.8639376082652809\n",
            "epoch 12 batch id 4051 loss 0.281737744808197 train acc 0.8639494877807948\n",
            "epoch 12 batch id 4061 loss 0.4575965404510498 train acc 0.8639997845358286\n",
            "epoch 12 batch id 4071 loss 0.45660674571990967 train acc 0.8639961004667158\n",
            "epoch 12 batch id 4081 loss 0.2645127475261688 train acc 0.8640422077922078\n",
            "epoch 12 batch id 4091 loss 0.297669380903244 train acc 0.8641033671473968\n",
            "epoch 12 batch id 4101 loss 0.48142778873443604 train acc 0.8640765971714216\n",
            "epoch 12 batch id 4111 loss 0.25204986333847046 train acc 0.8640575589880808\n",
            "epoch 12 batch id 4121 loss 0.4080306589603424 train acc 0.8640424047561271\n",
            "epoch 12 batch id 4131 loss 0.30738747119903564 train acc 0.8640613652868555\n",
            "epoch 12 batch id 4141 loss 0.42237773537635803 train acc 0.8641028737020043\n",
            "epoch 12 batch id 4151 loss 0.23135660588741302 train acc 0.8641667670440858\n",
            "epoch 12 batch id 4161 loss 0.5034275054931641 train acc 0.8641928022110069\n",
            "epoch 12 batch id 4171 loss 0.4076673090457916 train acc 0.8642074742268041\n",
            "epoch 12 batch id 4181 loss 0.09455133974552155 train acc 0.8642332874910309\n",
            "epoch 12 batch id 4191 loss 0.3332507312297821 train acc 0.864281346933906\n",
            "epoch 12 batch id 4201 loss 0.3359086215496063 train acc 0.8643328969293026\n",
            "epoch 12 batch id 4211 loss 0.5610590577125549 train acc 0.864317412728568\n",
            "epoch 12 batch id 4221 loss 0.24788415431976318 train acc 0.8643057036247335\n",
            "epoch 12 batch id 4231 loss 0.26679715514183044 train acc 0.8643457515953675\n",
            "epoch 12 batch id 4241 loss 0.414947509765625 train acc 0.8643745578872908\n",
            "epoch 12 batch id 4251 loss 0.48785313963890076 train acc 0.8644289578922606\n",
            "epoch 12 batch id 4261 loss 0.30012431740760803 train acc 0.8644574337010091\n",
            "epoch 12 batch id 4271 loss 0.21830925345420837 train acc 0.8645040681339264\n",
            "epoch 12 batch id 4281 loss 0.35753199458122253 train acc 0.8645468348516702\n",
            "epoch 12 batch id 4291 loss 0.1616351455450058 train acc 0.8646039676066185\n",
            "epoch 12 batch id 4301 loss 0.24578748643398285 train acc 0.8645918100441757\n",
            "epoch 12 batch id 4311 loss 0.24786321818828583 train acc 0.8646268267223383\n",
            "epoch 12 batch id 4321 loss 0.3245733678340912 train acc 0.8646508331404767\n",
            "epoch 12 batch id 4331 loss 0.22182875871658325 train acc 0.8646891595474486\n",
            "epoch 12 batch id 4341 loss 0.2347741276025772 train acc 0.8647021135683023\n",
            "epoch 12 batch id 4351 loss 0.22700899839401245 train acc 0.8647114169156516\n",
            "epoch 12 batch id 4361 loss 0.5001277327537537 train acc 0.8647170947030498\n",
            "epoch 12 batch id 4371 loss 0.37970206141471863 train acc 0.8647370452985587\n",
            "epoch 12 batch id 4381 loss 0.3221842348575592 train acc 0.8647176729057293\n",
            "epoch 12 batch id 4391 loss 0.21080030500888824 train acc 0.8647446481439308\n",
            "epoch 12 batch id 4401 loss 0.3790057301521301 train acc 0.8647821517836856\n",
            "epoch 12 batch id 4411 loss 0.23538297414779663 train acc 0.8648017739741555\n",
            "epoch 12 batch id 4421 loss 0.5676597356796265 train acc 0.864870787152228\n",
            "epoch 12 batch id 4431 loss 0.4143483340740204 train acc 0.8649112784924397\n",
            "epoch 12 batch id 4441 loss 0.2131173312664032 train acc 0.8649515874802972\n",
            "epoch 12 batch id 4451 loss 0.3558468222618103 train acc 0.8649741631094137\n",
            "epoch 12 batch id 4461 loss 0.3935427963733673 train acc 0.8649861297915266\n",
            "epoch 12 batch id 4471 loss 0.40028145909309387 train acc 0.8649770744799821\n",
            "epoch 12 batch id 4481 loss 0.4304311275482178 train acc 0.8649854943093059\n",
            "epoch 12 batch id 4491 loss 0.22126564383506775 train acc 0.8649869182810065\n",
            "epoch 12 batch id 4501 loss 0.2071509212255478 train acc 0.8650126360808709\n",
            "epoch 12 batch id 4511 loss 0.37685781717300415 train acc 0.8650036023054755\n",
            "epoch 12 batch id 4521 loss 0.44672641158103943 train acc 0.8649946084936961\n",
            "epoch 12 batch id 4531 loss 0.3087804913520813 train acc 0.8649477212535864\n",
            "epoch 12 batch id 4541 loss 0.32067206501960754 train acc 0.8649595353446378\n",
            "epoch 12 batch id 4551 loss 0.5947703123092651 train acc 0.864964430894309\n",
            "epoch 12 batch id 4561 loss 0.46255162358283997 train acc 0.8649624534093401\n",
            "epoch 12 batch id 4571 loss 0.2646333575248718 train acc 0.8650527783854737\n",
            "epoch 12 batch id 4581 loss 0.6341103911399841 train acc 0.8650813141235538\n",
            "epoch 12 batch id 4591 loss 0.5632216930389404 train acc 0.8650348507950337\n",
            "epoch 12 batch id 4601 loss 0.3313885033130646 train acc 0.8650734894588133\n",
            "epoch 12 batch id 4611 loss 0.3903737962245941 train acc 0.8651594014313598\n",
            "epoch 12 batch id 4621 loss 0.36058667302131653 train acc 0.865217891149102\n",
            "epoch 12 batch id 4631 loss 0.5384570360183716 train acc 0.8652356402504858\n",
            "epoch 12 batch id 4641 loss 0.6275678277015686 train acc 0.8652701465201466\n",
            "epoch 12 batch id 4651 loss 0.48917821049690247 train acc 0.8652944259299076\n",
            "epoch 12 batch id 4661 loss 0.37052834033966064 train acc 0.8653253057283845\n",
            "epoch 12 batch id 4671 loss 0.28429287672042847 train acc 0.8653627435238707\n",
            "epoch 12 batch id 4681 loss 0.5480222702026367 train acc 0.8653866695150609\n",
            "epoch 12 batch id 4691 loss 0.24806922674179077 train acc 0.8653838467277766\n",
            "epoch 12 batch id 4701 loss 0.40246352553367615 train acc 0.865377712188896\n",
            "epoch 12 batch id 4711 loss 0.16369681060314178 train acc 0.8654180375716408\n",
            "epoch 12 batch id 4721 loss 0.24223697185516357 train acc 0.865468121160771\n",
            "epoch 12 batch id 4731 loss 0.24639450013637543 train acc 0.8654321232297612\n",
            "epoch 12 batch id 4741 loss 0.34323611855506897 train acc 0.8654523043661675\n",
            "epoch 12 batch id 4751 loss 0.36294516921043396 train acc 0.8654460902967797\n",
            "epoch 12 batch id 4761 loss 0.31770578026771545 train acc 0.8654464660785549\n",
            "epoch 12 batch id 4771 loss 0.42608970403671265 train acc 0.8655123401802557\n",
            "epoch 12 batch id 4781 loss 0.36444395780563354 train acc 0.8655158439656976\n",
            "epoch 12 batch id 4791 loss 0.6292404532432556 train acc 0.8654899812147777\n",
            "epoch 12 batch id 4801 loss 0.26728853583335876 train acc 0.8655228077483857\n",
            "epoch 12 batch id 4811 loss 0.2775185704231262 train acc 0.8655197723965912\n",
            "epoch 12 batch id 4821 loss 0.3833503723144531 train acc 0.8654940624351795\n",
            "epoch 12 batch id 4831 loss 0.4328257143497467 train acc 0.8655202080314635\n",
            "epoch 12 batch id 4841 loss 0.3612072467803955 train acc 0.865484920470977\n",
            "epoch 12 batch id 4851 loss 0.48599445819854736 train acc 0.8654980931766646\n",
            "epoch 12 batch id 4861 loss 0.32853832840919495 train acc 0.865479068092985\n",
            "epoch 12 batch id 4871 loss 0.5044718980789185 train acc 0.8655435228905769\n",
            "epoch 12 batch id 4881 loss 0.29176804423332214 train acc 0.865569299323909\n",
            "epoch 12 batch id 4891 loss 0.3468945622444153 train acc 0.8655917757104886\n",
            "epoch 12 batch id 4901 loss 0.4114859700202942 train acc 0.8656109722505612\n",
            "epoch 12 batch id 4911 loss 0.37492454051971436 train acc 0.8656078191814295\n",
            "epoch 12 batch id 4921 loss 0.32509395480155945 train acc 0.8656015037593985\n",
            "epoch 12 batch id 4931 loss 0.3670472204685211 train acc 0.865560357939566\n",
            "epoch 12 batch id 4941 loss 0.3123750388622284 train acc 0.8655193786682858\n",
            "epoch 12 batch id 4951 loss 0.3526567220687866 train acc 0.8655227479297112\n",
            "epoch 12 train acc 0.8655740504704091\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd2aef49a31842af87d7ae3054329fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 12 loss 0.8982793688774109 test acc 0.7787493126832845\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e62ac63de71419395f3a2563a98df8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13 batch id 1 loss 0.3017770051956177 train acc 0.890625\n",
            "epoch 13 batch id 11 loss 0.3353574872016907 train acc 0.8806818181818182\n",
            "epoch 13 batch id 21 loss 0.2893747091293335 train acc 0.8898809523809523\n",
            "epoch 13 batch id 31 loss 0.3485477864742279 train acc 0.8785282258064516\n",
            "epoch 13 batch id 41 loss 0.31417983770370483 train acc 0.879954268292683\n",
            "epoch 13 batch id 51 loss 0.3479132056236267 train acc 0.8789828431372549\n",
            "epoch 13 batch id 61 loss 0.2913241982460022 train acc 0.875\n",
            "epoch 13 batch id 71 loss 0.4085952639579773 train acc 0.876100352112676\n",
            "epoch 13 batch id 81 loss 0.561043918132782 train acc 0.876929012345679\n",
            "epoch 13 batch id 91 loss 0.3109918236732483 train acc 0.8792925824175825\n",
            "epoch 13 batch id 101 loss 0.29426804184913635 train acc 0.8780940594059405\n",
            "epoch 13 batch id 111 loss 0.352766215801239 train acc 0.8785191441441441\n",
            "epoch 13 batch id 121 loss 0.5314720869064331 train acc 0.8771952479338843\n",
            "epoch 13 batch id 131 loss 0.38546279072761536 train acc 0.8772662213740458\n",
            "epoch 13 batch id 141 loss 0.33587631583213806 train acc 0.8762189716312057\n",
            "epoch 13 batch id 151 loss 0.31099623441696167 train acc 0.8761382450331126\n",
            "epoch 13 batch id 161 loss 0.5179517865180969 train acc 0.8773291925465838\n",
            "epoch 13 batch id 171 loss 0.3451676666736603 train acc 0.8770102339181286\n",
            "epoch 13 batch id 181 loss 0.21560943126678467 train acc 0.8778487569060773\n",
            "epoch 13 batch id 191 loss 0.37217018008232117 train acc 0.8776178010471204\n",
            "epoch 13 batch id 201 loss 0.41807839274406433 train acc 0.8777985074626866\n",
            "epoch 13 batch id 211 loss 0.5111274719238281 train acc 0.8771475118483413\n",
            "epoch 13 batch id 221 loss 0.5156188607215881 train acc 0.8766968325791855\n",
            "epoch 13 batch id 231 loss 0.21521282196044922 train acc 0.8775027056277056\n",
            "epoch 13 batch id 241 loss 0.36967799067497253 train acc 0.8766856846473029\n",
            "epoch 13 batch id 251 loss 0.4544741213321686 train acc 0.875933764940239\n",
            "epoch 13 batch id 261 loss 0.33656707406044006 train acc 0.8763769157088123\n",
            "epoch 13 batch id 271 loss 0.3822680115699768 train acc 0.8764990774907749\n",
            "epoch 13 batch id 281 loss 0.33907926082611084 train acc 0.8766125444839857\n",
            "epoch 13 batch id 291 loss 0.5393753051757812 train acc 0.877147766323024\n",
            "epoch 13 batch id 301 loss 0.346633642911911 train acc 0.8775955149501661\n",
            "epoch 13 batch id 311 loss 0.32354652881622314 train acc 0.8779642282958199\n",
            "epoch 13 batch id 321 loss 0.374426007270813 train acc 0.8781152647975078\n",
            "epoch 13 batch id 331 loss 0.36886096000671387 train acc 0.8784932024169184\n",
            "epoch 13 batch id 341 loss 0.38472113013267517 train acc 0.8786656891495601\n",
            "epoch 13 batch id 351 loss 0.49066540598869324 train acc 0.8781160968660968\n",
            "epoch 13 batch id 361 loss 0.3667776584625244 train acc 0.8779432132963989\n",
            "epoch 13 batch id 371 loss 0.3763206899166107 train acc 0.8782008086253369\n",
            "epoch 13 batch id 381 loss 0.21849042177200317 train acc 0.8781988188976378\n",
            "epoch 13 batch id 391 loss 0.4826059937477112 train acc 0.8776774296675192\n",
            "epoch 13 batch id 401 loss 0.37906414270401 train acc 0.8774158354114713\n",
            "epoch 13 batch id 411 loss 0.3534650206565857 train acc 0.8770529197080292\n",
            "epoch 13 batch id 421 loss 0.36036112904548645 train acc 0.8767072446555819\n",
            "epoch 13 batch id 431 loss 0.365710586309433 train acc 0.876957656612529\n",
            "epoch 13 batch id 441 loss 0.42945045232772827 train acc 0.8772321428571429\n",
            "epoch 13 batch id 451 loss 0.3987996578216553 train acc 0.8773905210643016\n",
            "epoch 13 batch id 461 loss 0.26858311891555786 train acc 0.8770675162689805\n",
            "epoch 13 batch id 471 loss 0.2707374393939972 train acc 0.877687101910828\n",
            "epoch 13 batch id 481 loss 0.4483531415462494 train acc 0.8779885654885655\n",
            "epoch 13 batch id 491 loss 0.2575368881225586 train acc 0.8781186354378818\n",
            "epoch 13 batch id 501 loss 0.666427493095398 train acc 0.8776821357285429\n",
            "epoch 13 batch id 511 loss 0.2694011926651001 train acc 0.8777825342465754\n",
            "epoch 13 batch id 521 loss 0.3374690115451813 train acc 0.8776391554702495\n",
            "epoch 13 batch id 531 loss 0.2667293846607208 train acc 0.8773540489642184\n",
            "epoch 13 batch id 541 loss 0.36051666736602783 train acc 0.8773971811460258\n",
            "epoch 13 batch id 551 loss 0.34884074330329895 train acc 0.8773536751361162\n",
            "epoch 13 batch id 561 loss 0.22122342884540558 train acc 0.8773395721925134\n",
            "epoch 13 batch id 571 loss 0.4874555766582489 train acc 0.8773533274956217\n",
            "epoch 13 batch id 581 loss 0.4474070072174072 train acc 0.8773666092943201\n",
            "epoch 13 batch id 591 loss 0.2130974531173706 train acc 0.877617385786802\n",
            "epoch 13 batch id 601 loss 0.27888116240501404 train acc 0.8777038269550749\n",
            "epoch 13 batch id 611 loss 0.40765976905822754 train acc 0.8777107201309329\n",
            "epoch 13 batch id 621 loss 0.27052152156829834 train acc 0.878094806763285\n",
            "epoch 13 batch id 631 loss 0.2330431044101715 train acc 0.8778229001584786\n",
            "epoch 13 batch id 641 loss 0.32547011971473694 train acc 0.8779251170046802\n",
            "epoch 13 batch id 651 loss 0.17217014729976654 train acc 0.8778801843317973\n",
            "epoch 13 batch id 661 loss 0.23530703783035278 train acc 0.8780966338880484\n",
            "epoch 13 batch id 671 loss 0.3807222247123718 train acc 0.8780970566318927\n",
            "epoch 13 batch id 681 loss 0.3334352374076843 train acc 0.8779598017621145\n",
            "epoch 13 batch id 691 loss 0.3120771646499634 train acc 0.877826519536903\n",
            "epoch 13 batch id 701 loss 0.28127995133399963 train acc 0.8776524607703281\n",
            "epoch 13 batch id 711 loss 0.49303585290908813 train acc 0.8774173699015471\n",
            "epoch 13 batch id 721 loss 0.35365888476371765 train acc 0.8773838418862691\n",
            "epoch 13 batch id 731 loss 0.25231999158859253 train acc 0.8773512311901505\n",
            "epoch 13 batch id 741 loss 0.3425084352493286 train acc 0.8775936234817814\n",
            "epoch 13 batch id 751 loss 0.4645759165287018 train acc 0.8773302263648469\n",
            "epoch 13 batch id 761 loss 0.21553181111812592 train acc 0.8776281208935611\n",
            "epoch 13 batch id 771 loss 0.5140737295150757 train acc 0.8773103112840467\n",
            "epoch 13 batch id 781 loss 0.337891161441803 train acc 0.8772407170294494\n",
            "epoch 13 batch id 791 loss 0.3632219433784485 train acc 0.8774494310998736\n",
            "epoch 13 batch id 801 loss 0.24964137375354767 train acc 0.8773798377028714\n",
            "epoch 13 batch id 811 loss 0.5744978785514832 train acc 0.8774082922318126\n",
            "epoch 13 batch id 821 loss 0.4211684763431549 train acc 0.877436053593179\n",
            "epoch 13 batch id 831 loss 0.4132801294326782 train acc 0.8776323706377858\n",
            "epoch 13 batch id 841 loss 0.3161467909812927 train acc 0.8777682818073722\n",
            "epoch 13 batch id 851 loss 0.4188186228275299 train acc 0.8776072267920094\n",
            "epoch 13 batch id 861 loss 0.2875984013080597 train acc 0.8776495354239257\n",
            "epoch 13 batch id 871 loss 0.3658082187175751 train acc 0.8775114810562572\n",
            "epoch 13 batch id 881 loss 0.2667945921421051 train acc 0.8775893870601589\n",
            "epoch 13 batch id 891 loss 0.2748104929924011 train acc 0.8778233726150393\n",
            "epoch 13 batch id 901 loss 0.4609898328781128 train acc 0.8777226692563818\n",
            "epoch 13 batch id 911 loss 0.5559215545654297 train acc 0.8776756311745335\n",
            "epoch 13 batch id 921 loss 0.456549733877182 train acc 0.8774769272529859\n",
            "epoch 13 batch id 931 loss 0.2046905905008316 train acc 0.8773831901181526\n",
            "epoch 13 batch id 941 loss 0.1827678382396698 train acc 0.8777563761955367\n",
            "epoch 13 batch id 951 loss 0.426174134016037 train acc 0.8778752628811777\n",
            "epoch 13 batch id 961 loss 0.32632458209991455 train acc 0.8778616024973985\n",
            "epoch 13 batch id 971 loss 0.32490500807762146 train acc 0.8779286817713697\n",
            "epoch 13 batch id 981 loss 0.2861221134662628 train acc 0.8782014525993884\n",
            "epoch 13 batch id 991 loss 0.42412951588630676 train acc 0.8780587790110999\n",
            "epoch 13 batch id 1001 loss 0.5002259612083435 train acc 0.8777628621378621\n",
            "epoch 13 batch id 1011 loss 0.2930406332015991 train acc 0.8776428041543026\n",
            "epoch 13 batch id 1021 loss 0.3927445411682129 train acc 0.8776628305582762\n",
            "epoch 13 batch id 1031 loss 0.2637428343296051 train acc 0.8777582444228904\n",
            "epoch 13 batch id 1041 loss 0.3843948543071747 train acc 0.8777467579250721\n",
            "epoch 13 batch id 1051 loss 0.31985950469970703 train acc 0.877631422454805\n",
            "epoch 13 batch id 1061 loss 0.31374576687812805 train acc 0.8776066211121584\n",
            "epoch 13 batch id 1071 loss 0.3046545088291168 train acc 0.8776698179271709\n",
            "epoch 13 batch id 1081 loss 0.407598078250885 train acc 0.8774861239592969\n",
            "epoch 13 batch id 1091 loss 0.6666281223297119 train acc 0.8774633363886343\n",
            "epoch 13 batch id 1101 loss 0.22410307824611664 train acc 0.8775970708446866\n",
            "epoch 13 batch id 1111 loss 0.5761857628822327 train acc 0.877573694869487\n",
            "epoch 13 batch id 1121 loss 0.1789291650056839 train acc 0.8776204281891169\n",
            "epoch 13 batch id 1131 loss 0.37364718317985535 train acc 0.8775558134394341\n",
            "epoch 13 batch id 1141 loss 0.2934443950653076 train acc 0.8775881901840491\n",
            "epoch 13 batch id 1151 loss 0.41666677594184875 train acc 0.8775521285838401\n",
            "epoch 13 batch id 1161 loss 0.2296360433101654 train acc 0.8775570628768303\n",
            "epoch 13 batch id 1171 loss 0.35180366039276123 train acc 0.8774551665243382\n",
            "epoch 13 batch id 1181 loss 0.3365488350391388 train acc 0.8774079170194751\n",
            "epoch 13 batch id 1191 loss 0.30410343408584595 train acc 0.8774270570948782\n",
            "epoch 13 batch id 1201 loss 0.3840095102787018 train acc 0.8777320982514571\n",
            "epoch 13 batch id 1211 loss 0.26100704073905945 train acc 0.8777611478117259\n",
            "epoch 13 batch id 1221 loss 0.301399290561676 train acc 0.8779304873054873\n",
            "epoch 13 batch id 1231 loss 0.3346998691558838 train acc 0.8777543663688059\n",
            "epoch 13 batch id 1241 loss 0.3785606324672699 train acc 0.8777573529411765\n",
            "epoch 13 batch id 1251 loss 0.3892463445663452 train acc 0.8777478017585931\n",
            "epoch 13 batch id 1261 loss 0.3670843243598938 train acc 0.8779242664551943\n",
            "epoch 13 batch id 1271 loss 0.293479859828949 train acc 0.8779750196695515\n",
            "epoch 13 batch id 1281 loss 0.31455421447753906 train acc 0.8779152029664324\n",
            "epoch 13 batch id 1291 loss 0.39882412552833557 train acc 0.877989446165763\n",
            "epoch 13 batch id 1301 loss 0.7242761850357056 train acc 0.8777863182167563\n",
            "epoch 13 batch id 1311 loss 0.3820924758911133 train acc 0.8778484935163997\n",
            "epoch 13 batch id 1321 loss 0.1576942354440689 train acc 0.8777914458743377\n",
            "epoch 13 batch id 1331 loss 0.2501007914543152 train acc 0.8779113448534936\n",
            "epoch 13 batch id 1341 loss 0.4662895202636719 train acc 0.8778546793437733\n",
            "epoch 13 batch id 1351 loss 0.37555205821990967 train acc 0.8778451147298297\n",
            "epoch 13 batch id 1361 loss 0.3437756299972534 train acc 0.8778012490815577\n",
            "epoch 13 batch id 1371 loss 0.1771000474691391 train acc 0.8777580233406272\n",
            "epoch 13 batch id 1381 loss 0.4614149034023285 train acc 0.8777719949312093\n",
            "epoch 13 batch id 1391 loss 0.14635460078716278 train acc 0.8778643961179008\n",
            "epoch 13 batch id 1401 loss 0.24641980230808258 train acc 0.8778439507494646\n",
            "epoch 13 batch id 1411 loss 0.2776557207107544 train acc 0.8779234585400425\n",
            "epoch 13 batch id 1421 loss 0.2388778179883957 train acc 0.877990851513019\n",
            "epoch 13 batch id 1431 loss 0.3232264816761017 train acc 0.8779590321453529\n",
            "epoch 13 batch id 1441 loss 0.38177770376205444 train acc 0.8778734385843164\n",
            "epoch 13 batch id 1451 loss 0.3845462203025818 train acc 0.8780043935217091\n",
            "epoch 13 batch id 1461 loss 0.2285689115524292 train acc 0.8780800821355236\n",
            "epoch 13 batch id 1471 loss 0.6048325896263123 train acc 0.8781334976206662\n",
            "epoch 13 batch id 1481 loss 0.40736445784568787 train acc 0.8781123396353815\n",
            "epoch 13 batch id 1491 loss 0.34690576791763306 train acc 0.8782381790744467\n",
            "epoch 13 batch id 1501 loss 0.2550033926963806 train acc 0.8783935709526982\n",
            "epoch 13 batch id 1511 loss 0.3367207944393158 train acc 0.8783814526803442\n",
            "epoch 13 batch id 1521 loss 0.22438576817512512 train acc 0.878564677843524\n",
            "epoch 13 batch id 1531 loss 0.36515316367149353 train acc 0.8786842749836709\n",
            "epoch 13 batch id 1541 loss 0.4199094772338867 train acc 0.878731343283582\n",
            "epoch 13 batch id 1551 loss 0.19980882108211517 train acc 0.8787778046421664\n",
            "epoch 13 batch id 1561 loss 0.41305848956108093 train acc 0.8787335842408712\n",
            "epoch 13 batch id 1571 loss 0.2900479733943939 train acc 0.8788092775302355\n",
            "epoch 13 batch id 1581 loss 0.3041784167289734 train acc 0.8788938962681847\n",
            "epoch 13 batch id 1591 loss 0.3069639801979065 train acc 0.8788988843494657\n",
            "epoch 13 batch id 1601 loss 0.5447214245796204 train acc 0.8789428482198626\n",
            "epoch 13 batch id 1611 loss 0.24275924265384674 train acc 0.8789183736809435\n",
            "epoch 13 batch id 1621 loss 0.18888401985168457 train acc 0.8790291486736582\n",
            "epoch 13 batch id 1631 loss 0.25928986072540283 train acc 0.8790236051502146\n",
            "epoch 13 batch id 1641 loss 0.272612988948822 train acc 0.8791038238878732\n",
            "epoch 13 batch id 1651 loss 0.23754842579364777 train acc 0.8790316474863719\n",
            "epoch 13 batch id 1661 loss 0.5246990323066711 train acc 0.879073223961469\n",
            "epoch 13 batch id 1671 loss 0.3000239431858063 train acc 0.879114302812687\n",
            "epoch 13 batch id 1681 loss 0.5557355284690857 train acc 0.8788574509220702\n",
            "epoch 13 batch id 1691 loss 0.414646178483963 train acc 0.8789824807806031\n",
            "epoch 13 batch id 1701 loss 0.29404640197753906 train acc 0.8790049970605526\n",
            "epoch 13 batch id 1711 loss 0.3131350874900818 train acc 0.8789450613676213\n",
            "epoch 13 batch id 1721 loss 0.2690359055995941 train acc 0.8790220075537478\n",
            "epoch 13 batch id 1731 loss 0.2144445776939392 train acc 0.8791702772963604\n",
            "epoch 13 batch id 1741 loss 0.38004547357559204 train acc 0.8791732481332567\n",
            "epoch 13 batch id 1751 loss 0.2348504215478897 train acc 0.8791940319817247\n",
            "epoch 13 batch id 1761 loss 0.2927331030368805 train acc 0.8792411981828506\n",
            "epoch 13 batch id 1771 loss 0.2609662413597107 train acc 0.8793584133258047\n",
            "epoch 13 batch id 1781 loss 0.18111278116703033 train acc 0.8793339416058394\n",
            "epoch 13 batch id 1791 loss 0.5062425136566162 train acc 0.8792050530429928\n",
            "epoch 13 batch id 1801 loss 0.3432933986186981 train acc 0.8792597862298723\n",
            "epoch 13 batch id 1811 loss 0.25170013308525085 train acc 0.8792190088348979\n",
            "epoch 13 batch id 1821 loss 0.264940083026886 train acc 0.8791271965952773\n",
            "epoch 13 batch id 1831 loss 0.21478044986724854 train acc 0.8791387902785364\n",
            "epoch 13 batch id 1841 loss 0.25603315234184265 train acc 0.8790399239543726\n",
            "epoch 13 batch id 1851 loss 0.333509236574173 train acc 0.8790180983252296\n",
            "epoch 13 batch id 1861 loss 0.3422839343547821 train acc 0.8790132993014508\n",
            "epoch 13 batch id 1871 loss 0.3902142643928528 train acc 0.8791922768572956\n",
            "epoch 13 batch id 1881 loss 0.266208291053772 train acc 0.8793112041467305\n",
            "epoch 13 batch id 1891 loss 0.23857995867729187 train acc 0.8794536620835537\n",
            "epoch 13 batch id 1901 loss 0.3394247889518738 train acc 0.8795535244608101\n",
            "epoch 13 batch id 1911 loss 0.30880746245384216 train acc 0.8795869309262166\n",
            "epoch 13 batch id 1921 loss 0.38292163610458374 train acc 0.8795061166059344\n",
            "epoch 13 batch id 1931 loss 0.37885215878486633 train acc 0.8796041558777835\n",
            "epoch 13 batch id 1941 loss 0.37340062856674194 train acc 0.8796287351880474\n",
            "epoch 13 batch id 1951 loss 0.44731074571609497 train acc 0.8796931060994362\n",
            "epoch 13 batch id 1961 loss 0.18485410511493683 train acc 0.8797807241203468\n",
            "epoch 13 batch id 1971 loss 0.40621426701545715 train acc 0.87984367072552\n",
            "epoch 13 batch id 1981 loss 0.44310465455055237 train acc 0.879882319535588\n",
            "epoch 13 batch id 1991 loss 0.4734009802341461 train acc 0.8797793194374686\n",
            "epoch 13 batch id 2001 loss 0.3730439245700836 train acc 0.8797632433783108\n",
            "epoch 13 batch id 2011 loss 0.39643946290016174 train acc 0.8797784062655395\n",
            "epoch 13 batch id 2021 loss 0.42222028970718384 train acc 0.8798011504205838\n",
            "epoch 13 batch id 2031 loss 0.2581947147846222 train acc 0.879746738060069\n",
            "epoch 13 batch id 2041 loss 0.29556378722190857 train acc 0.8797234811366977\n",
            "epoch 13 batch id 2051 loss 0.37253502011299133 train acc 0.8798756704046806\n",
            "epoch 13 batch id 2061 loss 0.3469278812408447 train acc 0.8799278262979137\n",
            "epoch 13 batch id 2071 loss 0.33892494440078735 train acc 0.8799266658619025\n",
            "epoch 13 batch id 2081 loss 0.4440594017505646 train acc 0.8799330249879865\n",
            "epoch 13 batch id 2091 loss 0.4109627604484558 train acc 0.8799094332855093\n",
            "epoch 13 batch id 2101 loss 0.26069173216819763 train acc 0.8799529985721085\n",
            "epoch 13 batch id 2111 loss 0.33778515458106995 train acc 0.8798999289436286\n",
            "epoch 13 batch id 2121 loss 0.14110887050628662 train acc 0.8799725954738331\n",
            "epoch 13 batch id 2131 loss 0.42820221185684204 train acc 0.8800372477709996\n",
            "epoch 13 batch id 2141 loss 0.3802339434623718 train acc 0.8800064222326016\n",
            "epoch 13 batch id 2151 loss 0.39607948064804077 train acc 0.8799904114365411\n",
            "epoch 13 batch id 2161 loss 0.42712658643722534 train acc 0.879873322535863\n",
            "epoch 13 batch id 2171 loss 0.23690222203731537 train acc 0.8799516351911562\n",
            "epoch 13 batch id 2181 loss 0.2663171589374542 train acc 0.880043558000917\n",
            "epoch 13 batch id 2191 loss 0.38343194127082825 train acc 0.8800490643541762\n",
            "epoch 13 batch id 2201 loss 0.2562154531478882 train acc 0.8801184120854157\n",
            "epoch 13 batch id 2211 loss 0.3811465799808502 train acc 0.8800387268204433\n",
            "epoch 13 batch id 2221 loss 0.4995804727077484 train acc 0.8800019698334084\n",
            "epoch 13 batch id 2231 loss 0.2686194181442261 train acc 0.8799305244285074\n",
            "epoch 13 batch id 2241 loss 0.23498806357383728 train acc 0.8799782463186078\n",
            "epoch 13 batch id 2251 loss 0.26781806349754333 train acc 0.879935306530431\n",
            "epoch 13 batch id 2261 loss 0.31285417079925537 train acc 0.8799342105263158\n",
            "epoch 13 batch id 2271 loss 0.7099773287773132 train acc 0.8798712021136064\n",
            "epoch 13 batch id 2281 loss 0.5538139939308167 train acc 0.879979997807979\n",
            "epoch 13 batch id 2291 loss 0.4386238157749176 train acc 0.8801287647315583\n",
            "epoch 13 batch id 2301 loss 0.21614891290664673 train acc 0.8801472186006084\n",
            "epoch 13 batch id 2311 loss 0.3229745924472809 train acc 0.8801857961921247\n",
            "epoch 13 batch id 2321 loss 0.24001039564609528 train acc 0.8802375053856096\n",
            "epoch 13 batch id 2331 loss 0.30501434206962585 train acc 0.8801278957528957\n",
            "epoch 13 batch id 2341 loss 0.3917537331581116 train acc 0.8801193400256301\n",
            "epoch 13 batch id 2351 loss 0.21033138036727905 train acc 0.8801042109740536\n",
            "epoch 13 batch id 2361 loss 0.3705941140651703 train acc 0.8801355357899195\n",
            "epoch 13 batch id 2371 loss 0.14028777182102203 train acc 0.8802259067903838\n",
            "epoch 13 batch id 2381 loss 0.18244129419326782 train acc 0.8802695821083578\n",
            "epoch 13 batch id 2391 loss 0.27616772055625916 train acc 0.880293287327478\n",
            "epoch 13 batch id 2401 loss 0.17099781334400177 train acc 0.880271241149521\n",
            "epoch 13 batch id 2411 loss 0.3387064039707184 train acc 0.8802817814184986\n",
            "epoch 13 batch id 2421 loss 0.5805777907371521 train acc 0.8803180503923999\n",
            "epoch 13 batch id 2431 loss 0.3223188519477844 train acc 0.880309029206088\n",
            "epoch 13 batch id 2441 loss 0.3336876332759857 train acc 0.8803896968455551\n",
            "epoch 13 batch id 2451 loss 0.34131738543510437 train acc 0.880405956752346\n",
            "epoch 13 batch id 2461 loss 0.46185076236724854 train acc 0.8804220845184885\n",
            "epoch 13 batch id 2471 loss 0.46716490387916565 train acc 0.8803432314852286\n",
            "epoch 13 batch id 2481 loss 0.357918381690979 train acc 0.8804350564288593\n",
            "epoch 13 batch id 2491 loss 0.4054091274738312 train acc 0.8803567844239262\n",
            "epoch 13 batch id 2501 loss 0.26800209283828735 train acc 0.8804103358656538\n",
            "epoch 13 batch id 2511 loss 0.4068715572357178 train acc 0.8804510155316607\n",
            "epoch 13 batch id 2521 loss 0.3322794437408447 train acc 0.8803612157873859\n",
            "epoch 13 batch id 2531 loss 0.4361409544944763 train acc 0.8803770742789412\n",
            "epoch 13 batch id 2541 loss 0.35534220933914185 train acc 0.8803866587957497\n",
            "epoch 13 batch id 2551 loss 0.45427921414375305 train acc 0.880359417875343\n",
            "epoch 13 batch id 2561 loss 0.5059874057769775 train acc 0.8802896817649356\n",
            "epoch 13 batch id 2571 loss 0.32448235154151917 train acc 0.8803177265655387\n",
            "epoch 13 batch id 2581 loss 0.3179163336753845 train acc 0.8803273924835335\n",
            "epoch 13 batch id 2591 loss 0.24701057374477386 train acc 0.880300800849093\n",
            "epoch 13 batch id 2601 loss 0.4255669116973877 train acc 0.8803044502114571\n",
            "epoch 13 batch id 2611 loss 0.4452550709247589 train acc 0.88026618153964\n",
            "epoch 13 batch id 2621 loss 0.3171576261520386 train acc 0.8802580122090805\n",
            "epoch 13 batch id 2631 loss 0.3485799729824066 train acc 0.880243966172558\n",
            "epoch 13 batch id 2641 loss 0.2814168930053711 train acc 0.8802004449072321\n",
            "epoch 13 batch id 2651 loss 0.31513190269470215 train acc 0.8802456620143342\n",
            "epoch 13 batch id 2661 loss 0.3691036105155945 train acc 0.8802846674182638\n",
            "epoch 13 batch id 2671 loss 0.3516415059566498 train acc 0.8802941314114564\n",
            "epoch 13 batch id 2681 loss 0.2669566571712494 train acc 0.8803093528534129\n",
            "epoch 13 batch id 2691 loss 0.3850419819355011 train acc 0.8803302675585284\n",
            "epoch 13 batch id 2701 loss 0.459674209356308 train acc 0.8803683820807109\n",
            "epoch 13 batch id 2711 loss 0.3790602385997772 train acc 0.8803889247510144\n",
            "epoch 13 batch id 2721 loss 0.2493775635957718 train acc 0.8804265435501654\n",
            "epoch 13 batch id 2731 loss 0.3678065538406372 train acc 0.8803609025997803\n",
            "epoch 13 batch id 2741 loss 0.4541226029396057 train acc 0.8803698467712514\n",
            "epoch 13 batch id 2751 loss 0.26450443267822266 train acc 0.8803957651762996\n",
            "epoch 13 batch id 2761 loss 0.15009073913097382 train acc 0.8804780876494024\n",
            "epoch 13 batch id 2771 loss 0.31281301379203796 train acc 0.8804639570552147\n",
            "epoch 13 batch id 2781 loss 0.40995919704437256 train acc 0.8805398238043869\n",
            "epoch 13 batch id 2791 loss 0.2780846357345581 train acc 0.8805983518452167\n",
            "epoch 13 batch id 2801 loss 0.4025987684726715 train acc 0.8806118350589075\n",
            "epoch 13 batch id 2811 loss 0.3398967981338501 train acc 0.8806641319815013\n",
            "epoch 13 batch id 2821 loss 0.3730772137641907 train acc 0.88062743707905\n",
            "epoch 13 batch id 2831 loss 0.23031218349933624 train acc 0.8806903479335924\n",
            "epoch 13 batch id 2841 loss 0.3614542484283447 train acc 0.8806978176698346\n",
            "epoch 13 batch id 2851 loss 0.21642939746379852 train acc 0.8807381182041389\n",
            "epoch 13 batch id 2861 loss 0.19927068054676056 train acc 0.880728984620762\n",
            "epoch 13 batch id 2871 loss 0.2975219488143921 train acc 0.8807144723092999\n",
            "epoch 13 batch id 2881 loss 0.1833381950855255 train acc 0.8807814127039223\n",
            "epoch 13 batch id 2891 loss 0.2992309033870697 train acc 0.8807992476651678\n",
            "epoch 13 batch id 2901 loss 0.29428762197494507 train acc 0.8807577128576353\n",
            "epoch 13 batch id 2911 loss 0.1582983434200287 train acc 0.8807969769838544\n",
            "epoch 13 batch id 2921 loss 0.1583147495985031 train acc 0.8807396867511126\n",
            "epoch 13 batch id 2931 loss 0.16050781309604645 train acc 0.8808373848515865\n",
            "epoch 13 batch id 2941 loss 0.3794921934604645 train acc 0.8807750340020402\n",
            "epoch 13 batch id 2951 loss 0.3581015467643738 train acc 0.8808031175872586\n",
            "epoch 13 batch id 2961 loss 0.41932210326194763 train acc 0.8807835190813914\n",
            "epoch 13 batch id 2971 loss 0.2828032672405243 train acc 0.880790348367553\n",
            "epoch 13 batch id 2981 loss 0.40920794010162354 train acc 0.8807814072458906\n",
            "epoch 13 batch id 2991 loss 0.23960067331790924 train acc 0.8808143179538616\n",
            "epoch 13 batch id 3001 loss 0.206241637468338 train acc 0.880794943352216\n",
            "epoch 13 batch id 3011 loss 0.29306289553642273 train acc 0.8807756974427101\n",
            "epoch 13 batch id 3021 loss 0.3034836947917938 train acc 0.8808186444885799\n",
            "epoch 13 batch id 3031 loss 0.3640310764312744 train acc 0.8808355328274496\n",
            "epoch 13 batch id 3041 loss 0.4540126323699951 train acc 0.8807855146333443\n",
            "epoch 13 batch id 3051 loss 0.27399584650993347 train acc 0.8808433710258932\n",
            "epoch 13 batch id 3061 loss 0.2860462963581085 train acc 0.8807987585756288\n",
            "epoch 13 batch id 3071 loss 0.35240304470062256 train acc 0.8808256675350049\n",
            "epoch 13 batch id 3081 loss 0.5375205874443054 train acc 0.8808118305744888\n",
            "epoch 13 batch id 3091 loss 0.28945624828338623 train acc 0.8808334681332902\n",
            "epoch 13 batch id 3101 loss 0.3917388319969177 train acc 0.8808499274427604\n",
            "epoch 13 batch id 3111 loss 0.26237940788269043 train acc 0.8808763259402121\n",
            "epoch 13 batch id 3121 loss 0.380466490983963 train acc 0.880877523229734\n",
            "epoch 13 batch id 3131 loss 0.3298645615577698 train acc 0.8809086553816672\n",
            "epoch 13 batch id 3141 loss 0.35982513427734375 train acc 0.8809097421203438\n",
            "epoch 13 batch id 3151 loss 0.3098530173301697 train acc 0.8809306569343066\n",
            "epoch 13 batch id 3161 loss 0.3457619845867157 train acc 0.8808970658019614\n",
            "epoch 13 batch id 3171 loss 0.26733964681625366 train acc 0.8809375985493535\n",
            "epoch 13 batch id 3181 loss 0.20405228435993195 train acc 0.8809385806350204\n",
            "epoch 13 batch id 3191 loss 0.14680396020412445 train acc 0.880993418990912\n",
            "epoch 13 batch id 3201 loss 0.3362795114517212 train acc 0.8809893392689785\n",
            "epoch 13 batch id 3211 loss 0.4321257174015045 train acc 0.8809463562753036\n",
            "epoch 13 batch id 3221 loss 0.4302562475204468 train acc 0.8809375970195591\n",
            "epoch 13 batch id 3231 loss 0.23206479847431183 train acc 0.8809192200557103\n",
            "epoch 13 batch id 3241 loss 0.40071403980255127 train acc 0.8809588090095649\n",
            "epoch 13 batch id 3251 loss 0.24949592351913452 train acc 0.8809789295601353\n",
            "epoch 13 batch id 3261 loss 0.3298113942146301 train acc 0.8810180926096289\n",
            "epoch 13 batch id 3271 loss 0.2829675078392029 train acc 0.8809471491898502\n",
            "epoch 13 batch id 3281 loss 0.36763614416122437 train acc 0.880976645839683\n",
            "epoch 13 batch id 3291 loss 0.35134127736091614 train acc 0.8809727286539046\n",
            "epoch 13 batch id 3301 loss 0.20680785179138184 train acc 0.8810256361708573\n",
            "epoch 13 batch id 3311 loss 0.5342788696289062 train acc 0.8810168755662942\n",
            "epoch 13 batch id 3321 loss 0.3055620789527893 train acc 0.8810175775368865\n",
            "epoch 13 batch id 3331 loss 0.44057729840278625 train acc 0.8809666766736716\n",
            "epoch 13 batch id 3341 loss 0.6740201115608215 train acc 0.8809347874887759\n",
            "epoch 13 batch id 3351 loss 0.2784157991409302 train acc 0.8808657863324381\n",
            "epoch 13 batch id 3361 loss 0.4529654085636139 train acc 0.8809087697113954\n",
            "epoch 13 batch id 3371 loss 0.427642822265625 train acc 0.8809375927024622\n",
            "epoch 13 batch id 3381 loss 0.2048850804567337 train acc 0.8809154096421177\n",
            "epoch 13 batch id 3391 loss 0.37079739570617676 train acc 0.8809762975523444\n",
            "epoch 13 batch id 3401 loss 0.41659700870513916 train acc 0.8809816965598354\n",
            "epoch 13 batch id 3411 loss 0.31737130880355835 train acc 0.8809916446789797\n",
            "epoch 13 batch id 3421 loss 0.34020712971687317 train acc 0.8810243715287928\n",
            "epoch 13 batch id 3431 loss 0.36846819519996643 train acc 0.8810113669484115\n",
            "epoch 13 batch id 3441 loss 0.3413664996623993 train acc 0.8810438462656205\n",
            "epoch 13 batch id 3451 loss 0.25687897205352783 train acc 0.8810761373514924\n",
            "epoch 13 batch id 3461 loss 0.2655913233757019 train acc 0.8811217856110951\n",
            "epoch 13 batch id 3471 loss 0.31836825609207153 train acc 0.8811671708441371\n",
            "epoch 13 batch id 3481 loss 0.37111976742744446 train acc 0.8812078066647515\n",
            "epoch 13 batch id 3491 loss 0.2499701827764511 train acc 0.8812392580922371\n",
            "epoch 13 batch id 3501 loss 0.5508222579956055 train acc 0.8812571408169094\n",
            "epoch 13 batch id 3511 loss 0.22555717825889587 train acc 0.8812304186841355\n",
            "epoch 13 batch id 3521 loss 0.45031797885894775 train acc 0.8812526625958534\n",
            "epoch 13 batch id 3531 loss 0.5555757880210876 train acc 0.8812880557915604\n",
            "epoch 13 batch id 3541 loss 0.2697463631629944 train acc 0.8813408994634284\n",
            "epoch 13 batch id 3551 loss 0.26666259765625 train acc 0.8813802450014081\n",
            "epoch 13 batch id 3561 loss 0.3221896290779114 train acc 0.8814456964335861\n",
            "epoch 13 batch id 3571 loss 0.5231905579566956 train acc 0.8814495239428731\n",
            "epoch 13 batch id 3581 loss 0.4488263428211212 train acc 0.881457693381737\n",
            "epoch 13 batch id 3591 loss 0.4278831481933594 train acc 0.8814614661654135\n",
            "epoch 13 batch id 3601 loss 0.3424621522426605 train acc 0.8814652179950014\n",
            "epoch 13 batch id 3611 loss 0.26088911294937134 train acc 0.8814516408197175\n",
            "epoch 13 batch id 3621 loss 0.31350043416023254 train acc 0.8814856048053024\n",
            "epoch 13 batch id 3631 loss 0.4513449966907501 train acc 0.8814462269347287\n",
            "epoch 13 batch id 3641 loss 0.45461323857307434 train acc 0.8814800192254875\n",
            "epoch 13 batch id 3651 loss 0.38350731134414673 train acc 0.8814451520131471\n",
            "epoch 13 batch id 3661 loss 0.3480311632156372 train acc 0.8814787626331604\n",
            "epoch 13 batch id 3671 loss 0.2884836792945862 train acc 0.8815334718060474\n",
            "epoch 13 batch id 3681 loss 0.3260222375392914 train acc 0.8815454360228199\n",
            "epoch 13 batch id 3691 loss 0.25180670619010925 train acc 0.8815742684909239\n",
            "epoch 13 batch id 3701 loss 0.29381319880485535 train acc 0.8815607268305863\n",
            "epoch 13 batch id 3711 loss 0.3636939525604248 train acc 0.8816104149824845\n",
            "epoch 13 batch id 3721 loss 0.23830252885818481 train acc 0.8816976283257189\n",
            "epoch 13 batch id 3731 loss 0.21698611974716187 train acc 0.8817341195389976\n",
            "epoch 13 batch id 3741 loss 0.37045979499816895 train acc 0.8817495322106389\n",
            "epoch 13 batch id 3751 loss 0.2775788903236389 train acc 0.8817273727006132\n",
            "epoch 13 batch id 3761 loss 0.3326249420642853 train acc 0.881784266152619\n",
            "epoch 13 batch id 3771 loss 0.20309200882911682 train acc 0.8817579885971891\n",
            "epoch 13 batch id 3781 loss 0.511809229850769 train acc 0.8817483800581857\n",
            "epoch 13 batch id 3791 loss 0.28350865840911865 train acc 0.8817841598522818\n",
            "epoch 13 batch id 3801 loss 0.2724780738353729 train acc 0.8818526374638254\n",
            "epoch 13 batch id 3811 loss 0.2499934434890747 train acc 0.8818674560482813\n",
            "epoch 13 batch id 3821 loss 0.39060696959495544 train acc 0.8819067325307511\n",
            "epoch 13 batch id 3831 loss 0.34560880064964294 train acc 0.8819090968415557\n",
            "epoch 13 batch id 3841 loss 0.1563912183046341 train acc 0.8819480604009372\n",
            "epoch 13 batch id 3851 loss 0.1277799755334854 train acc 0.8819462477278629\n",
            "epoch 13 batch id 3861 loss 0.16880856454372406 train acc 0.881956585081585\n",
            "epoch 13 batch id 3871 loss 0.38726747035980225 train acc 0.8819910875742702\n",
            "epoch 13 batch id 3881 loss 0.4777773916721344 train acc 0.8819891780468951\n",
            "epoch 13 batch id 3891 loss 0.2469945102930069 train acc 0.8820113723978412\n",
            "epoch 13 batch id 3901 loss 0.21707840263843536 train acc 0.8819693668290182\n",
            "epoch 13 batch id 3911 loss 0.15669995546340942 train acc 0.8819715226284838\n",
            "epoch 13 batch id 3921 loss 0.27449238300323486 train acc 0.8819736674317776\n",
            "epoch 13 batch id 3931 loss 0.3983627259731293 train acc 0.8819837509539558\n",
            "epoch 13 batch id 3941 loss 0.3127807676792145 train acc 0.8819739596549099\n",
            "epoch 13 batch id 3951 loss 0.19999676942825317 train acc 0.8819879460895975\n",
            "epoch 13 batch id 3961 loss 0.4153132736682892 train acc 0.8819821383489018\n",
            "epoch 13 batch id 3971 loss 0.3554707467556 train acc 0.8819763598589776\n",
            "epoch 13 batch id 3981 loss 0.2895434498786926 train acc 0.881935286360211\n",
            "epoch 13 batch id 3991 loss 0.38139986991882324 train acc 0.8819453144575294\n",
            "epoch 13 batch id 4001 loss 0.26810982823371887 train acc 0.8819084291427143\n",
            "epoch 13 batch id 4011 loss 0.36126720905303955 train acc 0.8818834143605087\n",
            "epoch 13 batch id 4021 loss 0.2620255649089813 train acc 0.8818313230539667\n",
            "epoch 13 batch id 4031 loss 0.3669944405555725 train acc 0.8818919002728851\n",
            "epoch 13 batch id 4041 loss 0.32131049036979675 train acc 0.8819444444444444\n",
            "epoch 13 batch id 4051 loss 0.25603020191192627 train acc 0.8819851579856826\n",
            "epoch 13 batch id 4061 loss 0.2121247500181198 train acc 0.8820025855700566\n",
            "epoch 13 batch id 4071 loss 0.49229714274406433 train acc 0.8820160894129206\n",
            "epoch 13 batch id 4081 loss 0.32648053765296936 train acc 0.8821214163195296\n",
            "epoch 13 batch id 4091 loss 0.22034457325935364 train acc 0.8821498411146419\n",
            "epoch 13 batch id 4101 loss 0.3929891288280487 train acc 0.8821400268227262\n",
            "epoch 13 batch id 4111 loss 0.15461468696594238 train acc 0.882141662612503\n",
            "epoch 13 batch id 4121 loss 0.290540874004364 train acc 0.8820977917981072\n",
            "epoch 13 batch id 4131 loss 0.2829378843307495 train acc 0.8821184337932704\n",
            "epoch 13 batch id 4141 loss 0.2780173718929291 train acc 0.8821389760927312\n",
            "epoch 13 batch id 4151 loss 0.1932578682899475 train acc 0.882163183570224\n",
            "epoch 13 batch id 4161 loss 0.4135822355747223 train acc 0.8821459685171834\n",
            "epoch 13 batch id 4171 loss 0.24428370594978333 train acc 0.8821925197794294\n",
            "epoch 13 batch id 4181 loss 0.14671550691127777 train acc 0.8822313740731882\n",
            "epoch 13 batch id 4191 loss 0.5053702592849731 train acc 0.8822402171319494\n",
            "epoch 13 batch id 4201 loss 0.25365814566612244 train acc 0.8822713342061413\n",
            "epoch 13 batch id 4211 loss 0.429317444562912 train acc 0.8822540667299928\n",
            "epoch 13 batch id 4221 loss 0.33862462639808655 train acc 0.8822294776119403\n",
            "epoch 13 batch id 4231 loss 0.42844823002815247 train acc 0.8822493204916095\n",
            "epoch 13 batch id 4241 loss 0.39419788122177124 train acc 0.882258016977128\n",
            "epoch 13 batch id 4251 loss 0.3989224433898926 train acc 0.8822960773935544\n",
            "epoch 13 batch id 4261 loss 0.13776938617229462 train acc 0.8822899554095283\n",
            "epoch 13 batch id 4271 loss 0.2074311524629593 train acc 0.8823533715757433\n",
            "epoch 13 batch id 4281 loss 0.33111515641212463 train acc 0.8823690434477925\n",
            "epoch 13 batch id 4291 loss 0.24084700644016266 train acc 0.8823336634817059\n",
            "epoch 13 batch id 4301 loss 0.28333306312561035 train acc 0.8823129795396419\n",
            "epoch 13 batch id 4311 loss 0.194741889834404 train acc 0.882368504987242\n",
            "epoch 13 batch id 4321 loss 0.278194397687912 train acc 0.8823948449433001\n",
            "epoch 13 batch id 4331 loss 0.22502341866493225 train acc 0.8824499249595936\n",
            "epoch 13 batch id 4341 loss 0.2084999531507492 train acc 0.8824651577977425\n",
            "epoch 13 batch id 4351 loss 0.268830269575119 train acc 0.8824480004596644\n",
            "epoch 13 batch id 4361 loss 0.3774416446685791 train acc 0.8824918310020637\n",
            "epoch 13 batch id 4371 loss 0.4770091772079468 train acc 0.8825068634179821\n",
            "epoch 13 batch id 4381 loss 0.4407719373703003 train acc 0.8824861618351975\n",
            "epoch 13 batch id 4391 loss 0.19115868210792542 train acc 0.8825224891824186\n",
            "epoch 13 batch id 4401 loss 0.6263282299041748 train acc 0.8824308395819132\n",
            "epoch 13 batch id 4411 loss 0.252933531999588 train acc 0.8824210779868511\n",
            "epoch 13 batch id 4421 loss 0.38821423053741455 train acc 0.8824254976249717\n",
            "epoch 13 batch id 4431 loss 0.38144415616989136 train acc 0.8824334236064094\n",
            "epoch 13 batch id 4441 loss 0.10843899101018906 train acc 0.882469460707048\n",
            "epoch 13 batch id 4451 loss 0.38225430250167847 train acc 0.8824702314086722\n",
            "epoch 13 batch id 4461 loss 0.16002486646175385 train acc 0.8824604909213181\n",
            "epoch 13 batch id 4471 loss 0.5082499384880066 train acc 0.882443804518005\n",
            "epoch 13 batch id 4481 loss 0.3314291536808014 train acc 0.8824376534255747\n",
            "epoch 13 batch id 4491 loss 0.2274177223443985 train acc 0.8824524048096193\n",
            "epoch 13 batch id 4501 loss 0.14961141347885132 train acc 0.8824844479004665\n",
            "epoch 13 batch id 4511 loss 0.4943750202655792 train acc 0.8824817113722013\n",
            "epoch 13 batch id 4521 loss 0.32716646790504456 train acc 0.8824478821057288\n",
            "epoch 13 batch id 4531 loss 0.2623012065887451 train acc 0.8824245475612448\n",
            "epoch 13 batch id 4541 loss 0.26203060150146484 train acc 0.8824391653820745\n",
            "epoch 13 batch id 4551 loss 0.5120271444320679 train acc 0.8824502856515052\n",
            "epoch 13 batch id 4561 loss 0.40001967549324036 train acc 0.8824442282394211\n",
            "epoch 13 batch id 4571 loss 0.29430368542671204 train acc 0.8825065631152921\n",
            "epoch 13 batch id 4581 loss 0.3679295480251312 train acc 0.8825208742632613\n",
            "epoch 13 batch id 4591 loss 0.4315122663974762 train acc 0.8825249128730124\n",
            "epoch 13 batch id 4601 loss 0.36625221371650696 train acc 0.8825255379265378\n",
            "epoch 13 batch id 4611 loss 0.3923386335372925 train acc 0.8825905443504662\n",
            "epoch 13 batch id 4621 loss 0.2965847849845886 train acc 0.8826518881194546\n",
            "epoch 13 batch id 4631 loss 0.25646623969078064 train acc 0.8827028449578924\n",
            "epoch 13 batch id 4641 loss 0.4027462899684906 train acc 0.882736748545572\n",
            "epoch 13 batch id 4651 loss 0.526414692401886 train acc 0.8827369114168996\n",
            "epoch 13 batch id 4661 loss 0.3648018538951874 train acc 0.8827773010083673\n",
            "epoch 13 batch id 4671 loss 0.31138381361961365 train acc 0.8827907567972597\n",
            "epoch 13 batch id 4681 loss 0.4186133146286011 train acc 0.8828241828669088\n",
            "epoch 13 batch id 4691 loss 0.1666509062051773 train acc 0.8828607972713707\n",
            "epoch 13 batch id 4701 loss 0.3244824707508087 train acc 0.8828839608593916\n",
            "epoch 13 batch id 4711 loss 0.06997053325176239 train acc 0.8829401931649331\n",
            "epoch 13 batch id 4721 loss 0.3377252519130707 train acc 0.8829564710866342\n",
            "epoch 13 batch id 4731 loss 0.13287094235420227 train acc 0.8829132318748679\n",
            "epoch 13 batch id 4741 loss 0.27258893847465515 train acc 0.8829327937144062\n",
            "epoch 13 batch id 4751 loss 0.3042691946029663 train acc 0.8829391180804042\n",
            "epoch 13 batch id 4761 loss 0.19249479472637177 train acc 0.8829060333963453\n",
            "epoch 13 batch id 4771 loss 0.1971224546432495 train acc 0.8829877122196604\n",
            "epoch 13 batch id 4781 loss 0.40542173385620117 train acc 0.8829873457435683\n",
            "epoch 13 batch id 4791 loss 0.3686768114566803 train acc 0.8829641515341264\n",
            "epoch 13 batch id 4801 loss 0.3864159882068634 train acc 0.8829410539470943\n",
            "epoch 13 batch id 4811 loss 0.27801889181137085 train acc 0.882953777800873\n",
            "epoch 13 batch id 4821 loss 0.2579280436038971 train acc 0.8829632078406969\n",
            "epoch 13 batch id 4831 loss 0.29103049635887146 train acc 0.8829887704409025\n",
            "epoch 13 batch id 4841 loss 0.3181936740875244 train acc 0.883027137988019\n",
            "epoch 13 batch id 4851 loss 0.2735014855861664 train acc 0.8830105905998763\n",
            "epoch 13 batch id 4861 loss 0.3030764162540436 train acc 0.883026254885826\n",
            "epoch 13 batch id 4871 loss 0.43858519196510315 train acc 0.883073932457401\n",
            "epoch 13 batch id 4881 loss 0.4334625005722046 train acc 0.8831054087277197\n",
            "epoch 13 batch id 4891 loss 0.33308646082878113 train acc 0.8831335616438356\n",
            "epoch 13 batch id 4901 loss 0.4088595509529114 train acc 0.8831520352989186\n",
            "epoch 13 batch id 4911 loss 0.40480515360832214 train acc 0.8831545255548768\n",
            "epoch 13 batch id 4921 loss 0.4893343150615692 train acc 0.8831220788457631\n",
            "epoch 13 batch id 4931 loss 0.37130722403526306 train acc 0.8831119448387751\n",
            "epoch 13 batch id 4941 loss 0.2538224160671234 train acc 0.8831145011131349\n",
            "epoch 13 batch id 4951 loss 0.39974886178970337 train acc 0.8831265148454858\n",
            "epoch 13 train acc 0.8831705508280301\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2abe65e03131455998d0606ac8bb2e74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13 loss 0.7949453592300415 test acc 0.7907429893695015\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "831022525eef4d1db8a14d34d5a92385",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 14 batch id 1 loss 0.41919153928756714 train acc 0.859375\n",
            "epoch 14 batch id 11 loss 0.2512204647064209 train acc 0.8863636363636364\n",
            "epoch 14 batch id 21 loss 0.2293761819601059 train acc 0.8958333333333334\n",
            "epoch 14 batch id 31 loss 0.31877008080482483 train acc 0.8865927419354839\n",
            "epoch 14 batch id 41 loss 0.16521219909191132 train acc 0.8910060975609756\n",
            "epoch 14 batch id 51 loss 0.4014045000076294 train acc 0.8890931372549019\n",
            "epoch 14 batch id 61 loss 0.17588870227336884 train acc 0.8855020491803278\n",
            "epoch 14 batch id 71 loss 0.40943869948387146 train acc 0.8877640845070423\n",
            "epoch 14 batch id 81 loss 0.5164784789085388 train acc 0.8888888888888888\n",
            "epoch 14 batch id 91 loss 0.2711893320083618 train acc 0.8890796703296703\n",
            "epoch 14 batch id 101 loss 0.3705734312534332 train acc 0.8858292079207921\n",
            "epoch 14 batch id 111 loss 0.28220829367637634 train acc 0.8875281531531531\n",
            "epoch 14 batch id 121 loss 0.425281286239624 train acc 0.8877840909090909\n",
            "epoch 14 batch id 131 loss 0.274448961019516 train acc 0.8887166030534351\n",
            "epoch 14 batch id 141 loss 0.3805484175682068 train acc 0.8896276595744681\n",
            "epoch 14 batch id 151 loss 0.29989516735076904 train acc 0.8892798013245033\n",
            "epoch 14 batch id 161 loss 0.3521105647087097 train acc 0.8914013975155279\n",
            "epoch 14 batch id 171 loss 0.36290785670280457 train acc 0.8914473684210527\n",
            "epoch 14 batch id 181 loss 0.2954290807247162 train acc 0.8923515193370166\n",
            "epoch 14 batch id 191 loss 0.3145242929458618 train acc 0.8925065445026178\n",
            "epoch 14 batch id 201 loss 0.25583338737487793 train acc 0.8933457711442786\n",
            "epoch 14 batch id 211 loss 0.3640744984149933 train acc 0.8931427725118484\n",
            "epoch 14 batch id 221 loss 0.6929813027381897 train acc 0.8916148190045249\n",
            "epoch 14 batch id 231 loss 0.2153203934431076 train acc 0.8916396103896104\n",
            "epoch 14 batch id 241 loss 0.4409825801849365 train acc 0.8908195020746889\n",
            "epoch 14 batch id 251 loss 0.4205347001552582 train acc 0.8908740039840638\n",
            "epoch 14 batch id 261 loss 0.24117404222488403 train acc 0.8919420498084292\n",
            "epoch 14 batch id 271 loss 0.21071036159992218 train acc 0.8921817343173432\n",
            "epoch 14 batch id 281 loss 0.19643720984458923 train acc 0.8919039145907474\n",
            "epoch 14 batch id 291 loss 0.579447865486145 train acc 0.8921284364261168\n",
            "epoch 14 batch id 301 loss 0.40370985865592957 train acc 0.8924937707641196\n",
            "epoch 14 batch id 311 loss 0.337301641702652 train acc 0.8925844051446945\n",
            "epoch 14 batch id 321 loss 0.4807916283607483 train acc 0.8921339563862928\n",
            "epoch 14 batch id 331 loss 0.2415473610162735 train acc 0.8921827794561934\n",
            "epoch 14 batch id 341 loss 0.49941208958625793 train acc 0.8921829178885631\n",
            "epoch 14 batch id 351 loss 0.6880520582199097 train acc 0.8916488603988604\n",
            "epoch 14 batch id 361 loss 0.24428759515285492 train acc 0.8916204986149584\n",
            "epoch 14 batch id 371 loss 0.2623600661754608 train acc 0.8922675202156334\n",
            "epoch 14 batch id 381 loss 0.15974535048007965 train acc 0.8920193569553806\n",
            "epoch 14 batch id 391 loss 0.45996609330177307 train acc 0.892303388746803\n",
            "epoch 14 batch id 401 loss 0.22889521718025208 train acc 0.892339463840399\n",
            "epoch 14 batch id 411 loss 0.24604715406894684 train acc 0.8921076642335767\n",
            "epoch 14 batch id 421 loss 0.4518977105617523 train acc 0.8919239904988123\n",
            "epoch 14 batch id 431 loss 0.18684370815753937 train acc 0.8917488399071926\n",
            "epoch 14 batch id 441 loss 0.4295716881752014 train acc 0.891829648526077\n",
            "epoch 14 batch id 451 loss 0.2682373523712158 train acc 0.8923226164079823\n",
            "epoch 14 batch id 461 loss 0.24313049018383026 train acc 0.891675704989154\n",
            "epoch 14 batch id 471 loss 0.2526591420173645 train acc 0.892151008492569\n",
            "epoch 14 batch id 481 loss 0.3801038861274719 train acc 0.892379158004158\n",
            "epoch 14 batch id 491 loss 0.24360966682434082 train acc 0.8923434317718941\n",
            "epoch 14 batch id 501 loss 0.5534377694129944 train acc 0.8922155688622755\n",
            "epoch 14 batch id 511 loss 0.40998682379722595 train acc 0.8921232876712328\n",
            "epoch 14 batch id 521 loss 0.3805626928806305 train acc 0.8917646353166987\n",
            "epoch 14 batch id 531 loss 0.35778534412384033 train acc 0.8916843220338984\n",
            "epoch 14 batch id 541 loss 0.19976967573165894 train acc 0.8916647412199631\n",
            "epoch 14 batch id 551 loss 0.317813903093338 train acc 0.8915607985480943\n",
            "epoch 14 batch id 561 loss 0.25703081488609314 train acc 0.8915998217468806\n",
            "epoch 14 batch id 571 loss 0.4285833239555359 train acc 0.891746935201401\n",
            "epoch 14 batch id 581 loss 0.22534123063087463 train acc 0.8921041308089501\n",
            "epoch 14 batch id 591 loss 0.18305660784244537 train acc 0.8923963620981388\n",
            "epoch 14 batch id 601 loss 0.2806446850299835 train acc 0.8923928868552413\n",
            "epoch 14 batch id 611 loss 0.3216974139213562 train acc 0.8924150981996727\n",
            "epoch 14 batch id 621 loss 0.4262809157371521 train acc 0.8922604669887279\n",
            "epoch 14 batch id 631 loss 0.22641393542289734 train acc 0.8924821711568938\n",
            "epoch 14 batch id 641 loss 0.38588079810142517 train acc 0.8922825663026521\n",
            "epoch 14 batch id 651 loss 0.1604873538017273 train acc 0.8922091013824884\n",
            "epoch 14 batch id 661 loss 0.19096486270427704 train acc 0.8925869894099848\n",
            "epoch 14 batch id 671 loss 0.3645539879798889 train acc 0.8926741803278688\n",
            "epoch 14 batch id 681 loss 0.22130748629570007 train acc 0.8926211453744494\n",
            "epoch 14 batch id 691 loss 0.3820868730545044 train acc 0.8929314399421129\n",
            "epoch 14 batch id 701 loss 0.27923983335494995 train acc 0.8929208273894437\n",
            "epoch 14 batch id 711 loss 0.39599114656448364 train acc 0.8928445850914205\n",
            "epoch 14 batch id 721 loss 0.3869721293449402 train acc 0.8924670596393898\n",
            "epoch 14 batch id 731 loss 0.1509101837873459 train acc 0.8927624829001368\n",
            "epoch 14 batch id 741 loss 0.3083006739616394 train acc 0.8930921052631579\n",
            "epoch 14 batch id 751 loss 0.48597726225852966 train acc 0.8926431424766977\n",
            "epoch 14 batch id 761 loss 0.22265306115150452 train acc 0.8930067345597897\n",
            "epoch 14 batch id 771 loss 0.5691982507705688 train acc 0.892833981841764\n",
            "epoch 14 batch id 781 loss 0.30614614486694336 train acc 0.8926456466069143\n",
            "epoch 14 batch id 791 loss 0.2557978332042694 train acc 0.8927781289506953\n",
            "epoch 14 batch id 801 loss 0.20467528700828552 train acc 0.8930048377028714\n",
            "epoch 14 batch id 811 loss 0.41104841232299805 train acc 0.8931103575832305\n",
            "epoch 14 batch id 821 loss 0.22442714869976044 train acc 0.8932133069427527\n",
            "epoch 14 batch id 831 loss 0.30518338084220886 train acc 0.8934453971119134\n",
            "epoch 14 batch id 841 loss 0.1378578394651413 train acc 0.8935233353151011\n",
            "epoch 14 batch id 851 loss 0.2998923063278198 train acc 0.8933791128084606\n",
            "epoch 14 batch id 861 loss 0.3413458466529846 train acc 0.893528600464576\n",
            "epoch 14 batch id 871 loss 0.1352800577878952 train acc 0.8936925947187141\n",
            "epoch 14 batch id 881 loss 0.3432847857475281 train acc 0.8936400397275823\n",
            "epoch 14 batch id 891 loss 0.18761718273162842 train acc 0.8937114197530864\n",
            "epoch 14 batch id 901 loss 0.4006217122077942 train acc 0.8935557713651499\n",
            "epoch 14 batch id 911 loss 0.622654378414154 train acc 0.8934721459934138\n",
            "epoch 14 batch id 921 loss 0.3642677366733551 train acc 0.8934921281216069\n",
            "epoch 14 batch id 931 loss 0.2437664121389389 train acc 0.8934109828141783\n",
            "epoch 14 batch id 941 loss 0.30209651589393616 train acc 0.8936138416578109\n",
            "epoch 14 batch id 951 loss 0.33170363306999207 train acc 0.8936809936908517\n",
            "epoch 14 batch id 961 loss 0.2640242278575897 train acc 0.8934703433922997\n",
            "epoch 14 batch id 971 loss 0.3223355710506439 train acc 0.8932479402677652\n",
            "epoch 14 batch id 981 loss 0.3323943018913269 train acc 0.8935716106014271\n",
            "epoch 14 batch id 991 loss 0.391756147146225 train acc 0.8936207114026236\n",
            "epoch 14 batch id 1001 loss 0.39644187688827515 train acc 0.8936688311688312\n",
            "epoch 14 batch id 1011 loss 0.2910988926887512 train acc 0.8936232690405539\n",
            "epoch 14 batch id 1021 loss 0.2202848494052887 train acc 0.8937928501469148\n",
            "epoch 14 batch id 1031 loss 0.30719777941703796 train acc 0.8939591416100873\n",
            "epoch 14 batch id 1041 loss 0.318800687789917 train acc 0.8940171709894332\n",
            "epoch 14 batch id 1051 loss 0.23826941847801208 train acc 0.8938362274024738\n",
            "epoch 14 batch id 1061 loss 0.27833291888237 train acc 0.8937912346842601\n",
            "epoch 14 batch id 1071 loss 0.20404325425624847 train acc 0.8937616713352008\n",
            "epoch 14 batch id 1081 loss 0.2728941738605499 train acc 0.89360256706753\n",
            "epoch 14 batch id 1091 loss 0.6398338079452515 train acc 0.8933461274060495\n",
            "epoch 14 batch id 1101 loss 0.22003744542598724 train acc 0.8933923705722071\n",
            "epoch 14 batch id 1111 loss 0.4643268585205078 train acc 0.8933393339333934\n",
            "epoch 14 batch id 1121 loss 0.16490933299064636 train acc 0.8933987511150758\n",
            "epoch 14 batch id 1131 loss 0.41997575759887695 train acc 0.8931946286472149\n",
            "epoch 14 batch id 1141 loss 0.27981439232826233 train acc 0.8931173312883436\n",
            "epoch 14 batch id 1151 loss 0.28254881501197815 train acc 0.8931092528236316\n",
            "epoch 14 batch id 1161 loss 0.2737009525299072 train acc 0.8931686046511628\n",
            "epoch 14 batch id 1171 loss 0.3553628921508789 train acc 0.8932002561912895\n",
            "epoch 14 batch id 1181 loss 0.3607432246208191 train acc 0.8932313717188823\n",
            "epoch 14 batch id 1191 loss 0.16359096765518188 train acc 0.8931701301427372\n",
            "epoch 14 batch id 1201 loss 0.5877658724784851 train acc 0.8934481681931724\n",
            "epoch 14 batch id 1211 loss 0.057266056537628174 train acc 0.8935151734104047\n",
            "epoch 14 batch id 1221 loss 0.2363782525062561 train acc 0.8936578624078624\n",
            "epoch 14 batch id 1231 loss 0.24888095259666443 train acc 0.8934555239642566\n",
            "epoch 14 batch id 1241 loss 0.14511190354824066 train acc 0.8932942183722804\n",
            "epoch 14 batch id 1251 loss 0.3114045560359955 train acc 0.8933728017585931\n",
            "epoch 14 batch id 1261 loss 0.29652872681617737 train acc 0.8934377478191912\n",
            "epoch 14 batch id 1271 loss 0.23966415226459503 train acc 0.8934770849724626\n",
            "epoch 14 batch id 1281 loss 0.3202056586742401 train acc 0.8935036104605777\n",
            "epoch 14 batch id 1291 loss 0.21317337453365326 train acc 0.893602343144849\n",
            "epoch 14 batch id 1301 loss 0.5539865493774414 train acc 0.8934593581860107\n",
            "epoch 14 batch id 1311 loss 0.3578706681728363 train acc 0.893521167048055\n",
            "epoch 14 batch id 1321 loss 0.11874324083328247 train acc 0.8935110711582135\n",
            "epoch 14 batch id 1331 loss 0.26168450713157654 train acc 0.8934893876784372\n",
            "epoch 14 batch id 1341 loss 0.364413321018219 train acc 0.8934680275913497\n",
            "epoch 14 batch id 1351 loss 0.22634996473789215 train acc 0.8934585492227979\n",
            "epoch 14 batch id 1361 loss 0.34996068477630615 train acc 0.8934721711976488\n",
            "epoch 14 batch id 1371 loss 0.18125423789024353 train acc 0.8935197848285923\n",
            "epoch 14 batch id 1381 loss 0.32981157302856445 train acc 0.8936345944967415\n",
            "epoch 14 batch id 1391 loss 0.2233567088842392 train acc 0.8936578900071891\n",
            "epoch 14 batch id 1401 loss 0.12468705326318741 train acc 0.8937589221984297\n",
            "epoch 14 batch id 1411 loss 0.20940068364143372 train acc 0.8937367115520907\n",
            "epoch 14 batch id 1421 loss 0.20424824953079224 train acc 0.8937478008444757\n",
            "epoch 14 batch id 1431 loss 0.35835936665534973 train acc 0.8938679245283019\n",
            "epoch 14 batch id 1441 loss 0.35038959980010986 train acc 0.893910478834143\n",
            "epoch 14 batch id 1451 loss 0.2953018248081207 train acc 0.8940493625086148\n",
            "epoch 14 batch id 1461 loss 0.2117241621017456 train acc 0.8941221765913757\n",
            "epoch 14 batch id 1471 loss 0.3179277777671814 train acc 0.8942577328348063\n",
            "epoch 14 batch id 1481 loss 0.20144642889499664 train acc 0.8943387069547603\n",
            "epoch 14 batch id 1491 loss 0.39782169461250305 train acc 0.8944500335345406\n",
            "epoch 14 batch id 1501 loss 0.2588289976119995 train acc 0.8945494670219853\n",
            "epoch 14 batch id 1511 loss 0.23683322966098785 train acc 0.89461656187955\n",
            "epoch 14 batch id 1521 loss 0.20245559513568878 train acc 0.8947135930309007\n",
            "epoch 14 batch id 1531 loss 0.25114819407463074 train acc 0.8947379163945134\n",
            "epoch 14 batch id 1541 loss 0.3308902978897095 train acc 0.8947720635950681\n",
            "epoch 14 batch id 1551 loss 0.25752028822898865 train acc 0.8947453255963894\n",
            "epoch 14 batch id 1561 loss 0.38283392786979675 train acc 0.8948290358744395\n",
            "epoch 14 batch id 1571 loss 0.22655129432678223 train acc 0.8948917886696371\n",
            "epoch 14 batch id 1581 loss 0.19651104509830475 train acc 0.8948746837444655\n",
            "epoch 14 batch id 1591 loss 0.28659337759017944 train acc 0.8950051068510371\n",
            "epoch 14 batch id 1601 loss 0.4317874610424042 train acc 0.8950948625858838\n",
            "epoch 14 batch id 1611 loss 0.3417510986328125 train acc 0.8950671166977033\n",
            "epoch 14 batch id 1621 loss 0.19325262308120728 train acc 0.8950493522516965\n",
            "epoch 14 batch id 1631 loss 0.18616430461406708 train acc 0.8951371857755978\n",
            "epoch 14 batch id 1641 loss 0.3047243058681488 train acc 0.8952620353443023\n",
            "epoch 14 batch id 1651 loss 0.1934603452682495 train acc 0.8952907328891581\n",
            "epoch 14 batch id 1661 loss 0.34764549136161804 train acc 0.8953002709211318\n",
            "epoch 14 batch id 1671 loss 0.34568867087364197 train acc 0.8953096947935368\n",
            "epoch 14 batch id 1681 loss 0.3146713972091675 train acc 0.8952446460440214\n",
            "epoch 14 batch id 1691 loss 0.3439857065677643 train acc 0.8954298492016558\n",
            "epoch 14 batch id 1701 loss 0.2600262761116028 train acc 0.8954199735449735\n",
            "epoch 14 batch id 1711 loss 0.3592357039451599 train acc 0.8954010812390415\n",
            "epoch 14 batch id 1721 loss 0.27449652552604675 train acc 0.8955004357931435\n",
            "epoch 14 batch id 1731 loss 0.2524292469024658 train acc 0.8955444829578278\n",
            "epoch 14 batch id 1741 loss 0.3176000118255615 train acc 0.8955880241240666\n",
            "epoch 14 batch id 1751 loss 0.19460812211036682 train acc 0.8955418332381496\n",
            "epoch 14 batch id 1761 loss 0.2838621735572815 train acc 0.895629258943782\n",
            "epoch 14 batch id 1771 loss 0.29473644495010376 train acc 0.8957068746470921\n",
            "epoch 14 batch id 1781 loss 0.18444830179214478 train acc 0.895757299270073\n",
            "epoch 14 batch id 1791 loss 0.7118162512779236 train acc 0.8956065047459519\n",
            "epoch 14 batch id 1801 loss 0.23423586785793304 train acc 0.8956742781787895\n",
            "epoch 14 batch id 1811 loss 0.21460027992725372 train acc 0.8956636526780785\n",
            "epoch 14 batch id 1821 loss 0.29279419779777527 train acc 0.8955673393739704\n",
            "epoch 14 batch id 1831 loss 0.20449122786521912 train acc 0.895557413981431\n",
            "epoch 14 batch id 1841 loss 0.1595679223537445 train acc 0.8956070070613796\n",
            "epoch 14 batch id 1851 loss 0.6108303070068359 train acc 0.8955378849270664\n",
            "epoch 14 batch id 1861 loss 0.3113354444503784 train acc 0.8955030897367007\n",
            "epoch 14 batch id 1871 loss 0.3898738622665405 train acc 0.8956356894708712\n",
            "epoch 14 batch id 1881 loss 0.2743305563926697 train acc 0.8957585725677831\n",
            "epoch 14 batch id 1891 loss 0.3359339237213135 train acc 0.8958057905869911\n",
            "epoch 14 batch id 1901 loss 0.23564307391643524 train acc 0.8958525118358759\n",
            "epoch 14 batch id 1911 loss 0.19264964759349823 train acc 0.8958088042909471\n",
            "epoch 14 batch id 1921 loss 0.3212208151817322 train acc 0.8957736855804269\n",
            "epoch 14 batch id 1931 loss 0.22171054780483246 train acc 0.8958522138788193\n",
            "epoch 14 batch id 1941 loss 0.33128872513771057 train acc 0.8958735832045337\n",
            "epoch 14 batch id 1951 loss 0.43885934352874756 train acc 0.8958707073295745\n",
            "epoch 14 batch id 1961 loss 0.19568245112895966 train acc 0.8958997322794493\n",
            "epoch 14 batch id 1971 loss 0.2656244933605194 train acc 0.8959760273972602\n",
            "epoch 14 batch id 1981 loss 0.37457120418548584 train acc 0.8959174659262998\n",
            "epoch 14 batch id 1991 loss 0.39778804779052734 train acc 0.895812405826218\n",
            "epoch 14 batch id 2001 loss 0.46669313311576843 train acc 0.8958489505247377\n",
            "epoch 14 batch id 2011 loss 0.377868115901947 train acc 0.8958773620089507\n",
            "epoch 14 batch id 2021 loss 0.37688496708869934 train acc 0.8958745670460169\n",
            "epoch 14 batch id 2031 loss 0.26658394932746887 train acc 0.8958333333333334\n",
            "epoch 14 batch id 2041 loss 0.24382950365543365 train acc 0.8957465703086722\n",
            "epoch 14 batch id 2051 loss 0.4460005462169647 train acc 0.8957215992198927\n",
            "epoch 14 batch id 2061 loss 0.3308897018432617 train acc 0.8957575206210577\n",
            "epoch 14 batch id 2071 loss 0.24215464293956757 train acc 0.8957478271366489\n",
            "epoch 14 batch id 2081 loss 0.35773152112960815 train acc 0.8957232099951946\n",
            "epoch 14 batch id 2091 loss 0.20741741359233856 train acc 0.8956390483022477\n",
            "epoch 14 batch id 2101 loss 0.23412679135799408 train acc 0.895674678724417\n",
            "epoch 14 batch id 2111 loss 0.16521447896957397 train acc 0.8956951681667457\n",
            "epoch 14 batch id 2121 loss 0.18235839903354645 train acc 0.8956786303630363\n",
            "epoch 14 batch id 2131 loss 0.43522343039512634 train acc 0.8957209056780854\n",
            "epoch 14 batch id 2141 loss 0.25255271792411804 train acc 0.8957189981317142\n",
            "epoch 14 batch id 2151 loss 0.37248602509498596 train acc 0.8958115411436541\n",
            "epoch 14 batch id 2161 loss 0.4629506468772888 train acc 0.8957007751041185\n",
            "epoch 14 batch id 2171 loss 0.3545009195804596 train acc 0.8957997466605251\n",
            "epoch 14 batch id 2181 loss 0.20957587659358978 train acc 0.8958261691884457\n",
            "epoch 14 batch id 2191 loss 0.22175513207912445 train acc 0.8958380876312186\n",
            "epoch 14 batch id 2201 loss 0.25569674372673035 train acc 0.895842798727851\n",
            "epoch 14 batch id 2211 loss 0.2133074700832367 train acc 0.8957626639529624\n",
            "epoch 14 batch id 2221 loss 0.46703359484672546 train acc 0.8957324966231427\n",
            "epoch 14 batch id 2231 loss 0.2498350292444229 train acc 0.8956955961452263\n",
            "epoch 14 batch id 2241 loss 0.2330920249223709 train acc 0.8957008589915216\n",
            "epoch 14 batch id 2251 loss 0.3397088944911957 train acc 0.8956158374055975\n",
            "epoch 14 batch id 2261 loss 0.25718095898628235 train acc 0.8956490490933215\n",
            "epoch 14 batch id 2271 loss 0.3893602788448334 train acc 0.8956819682959049\n",
            "epoch 14 batch id 2281 loss 0.4940914511680603 train acc 0.8956803485313459\n",
            "epoch 14 batch id 2291 loss 0.25778552889823914 train acc 0.8958015058926233\n",
            "epoch 14 batch id 2301 loss 0.25300607085227966 train acc 0.8959080291177749\n",
            "epoch 14 batch id 2311 loss 0.29861772060394287 train acc 0.8959324967546517\n",
            "epoch 14 batch id 2321 loss 0.34319624304771423 train acc 0.8958490413614821\n",
            "epoch 14 batch id 2331 loss 0.25356894731521606 train acc 0.895766302016302\n",
            "epoch 14 batch id 2341 loss 0.4291577637195587 train acc 0.8956909440410081\n",
            "epoch 14 batch id 2351 loss 0.28257226943969727 train acc 0.895749149298171\n",
            "epoch 14 batch id 2361 loss 0.2895452082157135 train acc 0.8958267153748412\n",
            "epoch 14 batch id 2371 loss 0.1972644329071045 train acc 0.8958574968367777\n",
            "epoch 14 batch id 2381 loss 0.1637120544910431 train acc 0.8958617702645947\n",
            "epoch 14 batch id 2391 loss 0.21519693732261658 train acc 0.8959248222501046\n",
            "epoch 14 batch id 2401 loss 0.16749754548072815 train acc 0.8959157642648896\n",
            "epoch 14 batch id 2411 loss 0.3858709931373596 train acc 0.8959197428452924\n",
            "epoch 14 batch id 2421 loss 0.3471585512161255 train acc 0.8959430503923999\n",
            "epoch 14 batch id 2431 loss 0.3753868341445923 train acc 0.8959597387906212\n",
            "epoch 14 batch id 2441 loss 0.3702373206615448 train acc 0.8960018947152806\n",
            "epoch 14 batch id 2451 loss 0.2312670797109604 train acc 0.8959927070583436\n",
            "epoch 14 batch id 2461 loss 0.49103641510009766 train acc 0.8959962921576595\n",
            "epoch 14 batch id 2471 loss 0.44876283407211304 train acc 0.8958923512747875\n",
            "epoch 14 batch id 2481 loss 0.2751023471355438 train acc 0.8959592906086256\n",
            "epoch 14 batch id 2491 loss 0.5162426233291626 train acc 0.895875150541951\n",
            "epoch 14 batch id 2501 loss 0.19692060351371765 train acc 0.8959228808476609\n",
            "epoch 14 batch id 2511 loss 0.2782820761203766 train acc 0.8959702309836718\n",
            "epoch 14 batch id 2521 loss 0.39307865500450134 train acc 0.8959366322887743\n",
            "epoch 14 batch id 2531 loss 0.31080180406570435 train acc 0.8959526866851047\n",
            "epoch 14 batch id 2541 loss 0.31904056668281555 train acc 0.895987062180244\n",
            "epoch 14 batch id 2551 loss 0.34631073474884033 train acc 0.895984417875343\n",
            "epoch 14 batch id 2561 loss 0.34044957160949707 train acc 0.8959756930886372\n",
            "epoch 14 batch id 2571 loss 0.29587772488594055 train acc 0.8960399649941657\n",
            "epoch 14 batch id 2581 loss 0.26185303926467896 train acc 0.8960674157303371\n",
            "epoch 14 batch id 2591 loss 0.24095740914344788 train acc 0.8960464106522578\n",
            "epoch 14 batch id 2601 loss 0.41038668155670166 train acc 0.8960375816993464\n",
            "epoch 14 batch id 2611 loss 0.27477842569351196 train acc 0.896070710455764\n",
            "epoch 14 batch id 2621 loss 0.25805869698524475 train acc 0.8960737790919496\n",
            "epoch 14 batch id 2631 loss 0.41907018423080444 train acc 0.8960708855948308\n",
            "epoch 14 batch id 2641 loss 0.3101266324520111 train acc 0.8960680140098447\n",
            "epoch 14 batch id 2651 loss 0.1634630411863327 train acc 0.8961005281026028\n",
            "epoch 14 batch id 2661 loss 0.2733787000179291 train acc 0.8961210541149943\n",
            "epoch 14 batch id 2671 loss 0.4028424620628357 train acc 0.8960712280044927\n",
            "epoch 14 batch id 2681 loss 0.15754100680351257 train acc 0.8960101174934726\n",
            "epoch 14 batch id 2691 loss 0.2542709708213806 train acc 0.8959959123002601\n",
            "epoch 14 batch id 2701 loss 0.3876764476299286 train acc 0.8960396612365791\n",
            "epoch 14 batch id 2711 loss 0.2532217502593994 train acc 0.8960773238657322\n",
            "epoch 14 batch id 2721 loss 0.22614973783493042 train acc 0.8961376791620728\n",
            "epoch 14 batch id 2731 loss 0.126900777220726 train acc 0.8961289362870744\n",
            "epoch 14 batch id 2741 loss 0.3165148198604584 train acc 0.8961145567311201\n",
            "epoch 14 batch id 2751 loss 0.40290188789367676 train acc 0.8961116412213741\n",
            "epoch 14 batch id 2761 loss 0.25700870156288147 train acc 0.8961313835566823\n",
            "epoch 14 batch id 2771 loss 0.32689186930656433 train acc 0.8961002345723565\n",
            "epoch 14 batch id 2781 loss 0.6336385011672974 train acc 0.8961254944264653\n",
            "epoch 14 batch id 2791 loss 0.24735118448734283 train acc 0.8961841633823002\n",
            "epoch 14 batch id 2801 loss 0.30321377515792847 train acc 0.8961698946804713\n",
            "epoch 14 batch id 2811 loss 0.3575565814971924 train acc 0.8962168712202063\n",
            "epoch 14 batch id 2821 loss 0.376554012298584 train acc 0.896235820630982\n",
            "epoch 14 batch id 2831 loss 0.2669130265712738 train acc 0.8962601554221123\n",
            "epoch 14 batch id 2841 loss 0.32531654834747314 train acc 0.8962953185498064\n",
            "epoch 14 batch id 2851 loss 0.20795027911663055 train acc 0.8962809102069449\n",
            "epoch 14 batch id 2861 loss 0.14853931963443756 train acc 0.8963376004893394\n",
            "epoch 14 batch id 2871 loss 0.26884087920188904 train acc 0.896399338209683\n",
            "epoch 14 batch id 2881 loss 0.4039681553840637 train acc 0.8964226830961471\n",
            "epoch 14 batch id 2891 loss 0.2728498876094818 train acc 0.8963810100311311\n",
            "epoch 14 batch id 2901 loss 0.2959021031856537 train acc 0.8963773267838676\n",
            "epoch 14 batch id 2911 loss 0.2083907127380371 train acc 0.8963951391274476\n",
            "epoch 14 batch id 2921 loss 0.11503945291042328 train acc 0.8963753851420746\n",
            "epoch 14 batch id 2931 loss 0.12703485786914825 train acc 0.896419737291027\n",
            "epoch 14 batch id 2941 loss 0.32891321182250977 train acc 0.8963575314518871\n",
            "epoch 14 batch id 2951 loss 0.22793716192245483 train acc 0.896412233141308\n",
            "epoch 14 batch id 2961 loss 0.27816081047058105 train acc 0.8964929500168862\n",
            "epoch 14 batch id 2971 loss 0.2354895919561386 train acc 0.8964994951194883\n",
            "epoch 14 batch id 2981 loss 0.5908607244491577 train acc 0.8964745471318349\n",
            "epoch 14 batch id 2991 loss 0.3579619824886322 train acc 0.8964758859913072\n",
            "epoch 14 batch id 3001 loss 0.30744051933288574 train acc 0.8964668027324225\n",
            "epoch 14 batch id 3011 loss 0.4332495629787445 train acc 0.8964525905014945\n",
            "epoch 14 batch id 3021 loss 0.23037414252758026 train acc 0.8964074395895398\n",
            "epoch 14 batch id 3031 loss 0.2954210937023163 train acc 0.8963574315407457\n",
            "epoch 14 batch id 3041 loss 0.392360121011734 train acc 0.8963180286090102\n",
            "epoch 14 batch id 3051 loss 0.2762027084827423 train acc 0.8963249754178958\n",
            "epoch 14 batch id 3061 loss 0.29301831126213074 train acc 0.8963012495916367\n",
            "epoch 14 batch id 3071 loss 0.17259563505649567 train acc 0.8963438212308694\n",
            "epoch 14 batch id 3081 loss 0.3186853229999542 train acc 0.8964165449529373\n",
            "epoch 14 batch id 3091 loss 0.15615691244602203 train acc 0.896443303138143\n",
            "epoch 14 batch id 3101 loss 0.30327311158180237 train acc 0.8963943082876491\n",
            "epoch 14 batch id 3111 loss 0.4246072769165039 train acc 0.8963958534233365\n",
            "epoch 14 batch id 3121 loss 0.21263429522514343 train acc 0.8963923822492791\n",
            "epoch 14 batch id 3131 loss 0.22935418784618378 train acc 0.896473770360907\n",
            "epoch 14 batch id 3141 loss 0.3258146643638611 train acc 0.8964849968163006\n",
            "epoch 14 batch id 3151 loss 0.2777234613895416 train acc 0.8965358219612821\n",
            "epoch 14 batch id 3161 loss 0.29367485642433167 train acc 0.8965022935779816\n",
            "epoch 14 batch id 3171 loss 0.15159130096435547 train acc 0.8965330337432986\n",
            "epoch 14 batch id 3181 loss 0.17002007365226746 train acc 0.8964899009745363\n",
            "epoch 14 batch id 3191 loss 0.13803964853286743 train acc 0.8965400736446255\n",
            "epoch 14 batch id 3201 loss 0.28641682863235474 train acc 0.8965606451109028\n",
            "epoch 14 batch id 3211 loss 0.43015891313552856 train acc 0.8965178293366552\n",
            "epoch 14 batch id 3221 loss 0.48464781045913696 train acc 0.8964849813722446\n",
            "epoch 14 batch id 3231 loss 0.17891022562980652 train acc 0.8965055323429278\n",
            "epoch 14 batch id 3241 loss 0.3045891523361206 train acc 0.896506672323357\n",
            "epoch 14 batch id 3251 loss 0.3100990653038025 train acc 0.8965222239310982\n",
            "epoch 14 batch id 3261 loss 0.22127267718315125 train acc 0.8965472631094756\n",
            "epoch 14 batch id 3271 loss 0.34070250391960144 train acc 0.8964766126566799\n",
            "epoch 14 batch id 3281 loss 0.5262154936790466 train acc 0.8964683023468455\n",
            "epoch 14 batch id 3291 loss 0.27188146114349365 train acc 0.8965170161045275\n",
            "epoch 14 batch id 3301 loss 0.2573576271533966 train acc 0.8965417676461678\n",
            "epoch 14 batch id 3311 loss 0.2830367386341095 train acc 0.896571088794926\n",
            "epoch 14 batch id 3321 loss 0.32246875762939453 train acc 0.8965720039144836\n",
            "epoch 14 batch id 3331 loss 0.3508247137069702 train acc 0.8965260057039928\n",
            "epoch 14 batch id 3341 loss 0.6272540092468262 train acc 0.8964615758754864\n",
            "epoch 14 batch id 3351 loss 0.21866004168987274 train acc 0.8964534840346166\n",
            "epoch 14 batch id 3361 loss 0.3230990171432495 train acc 0.8964640360011901\n",
            "epoch 14 batch id 3371 loss 0.36778631806373596 train acc 0.8964652551171759\n",
            "epoch 14 batch id 3381 loss 0.16976553201675415 train acc 0.8964895740905058\n",
            "epoch 14 batch id 3391 loss 0.46855542063713074 train acc 0.8965137496313772\n",
            "epoch 14 batch id 3401 loss 0.386347234249115 train acc 0.8965423772419876\n",
            "epoch 14 batch id 3411 loss 0.236205592751503 train acc 0.8965616754617414\n",
            "epoch 14 batch id 3421 loss 0.34862521290779114 train acc 0.8965671587255188\n",
            "epoch 14 batch id 3431 loss 0.34591129422187805 train acc 0.8965316234334013\n",
            "epoch 14 batch id 3441 loss 0.33361852169036865 train acc 0.8966143562917757\n",
            "epoch 14 batch id 3451 loss 0.2372196912765503 train acc 0.8965562518110692\n",
            "epoch 14 batch id 3461 loss 0.3990076184272766 train acc 0.8965887749205432\n",
            "epoch 14 batch id 3471 loss 0.339241087436676 train acc 0.8966076058772688\n",
            "epoch 14 batch id 3481 loss 0.21783490478992462 train acc 0.896603885377765\n",
            "epoch 14 batch id 3491 loss 0.132679745554924 train acc 0.8966225651675738\n",
            "epoch 14 batch id 3501 loss 0.28808557987213135 train acc 0.8966366752356469\n",
            "epoch 14 batch id 3511 loss 0.2182534635066986 train acc 0.8966106522358303\n",
            "epoch 14 batch id 3521 loss 0.36233261227607727 train acc 0.8966335913092871\n",
            "epoch 14 batch id 3531 loss 0.635077953338623 train acc 0.8966077244406684\n",
            "epoch 14 batch id 3541 loss 0.1862807422876358 train acc 0.8965908288619034\n",
            "epoch 14 batch id 3551 loss 0.23868761956691742 train acc 0.8966136299633906\n",
            "epoch 14 batch id 3561 loss 0.20726047456264496 train acc 0.8966494664420107\n",
            "epoch 14 batch id 3571 loss 0.43446075916290283 train acc 0.8966588490618874\n",
            "epoch 14 batch id 3581 loss 0.31642016768455505 train acc 0.8966899958112259\n",
            "epoch 14 batch id 3591 loss 0.5394924283027649 train acc 0.8967253202450571\n",
            "epoch 14 batch id 3601 loss 0.32122737169265747 train acc 0.8967430921966121\n",
            "epoch 14 batch id 3611 loss 0.10561943799257278 train acc 0.8967477845472168\n",
            "epoch 14 batch id 3621 loss 0.30183446407318115 train acc 0.8967308754487711\n",
            "epoch 14 batch id 3631 loss 0.41809651255607605 train acc 0.8966839369319747\n",
            "epoch 14 batch id 3641 loss 0.3918255567550659 train acc 0.896714501510574\n",
            "epoch 14 batch id 3651 loss 0.3985932767391205 train acc 0.8967363393590797\n",
            "epoch 14 batch id 3661 loss 0.2822819948196411 train acc 0.8967751297459711\n",
            "epoch 14 batch id 3671 loss 0.19477440416812897 train acc 0.8968222214655407\n",
            "epoch 14 batch id 3681 loss 0.26068249344825745 train acc 0.8968817916327085\n",
            "epoch 14 batch id 3691 loss 0.29345595836639404 train acc 0.8969283392034679\n",
            "epoch 14 batch id 3701 loss 0.21005915105342865 train acc 0.8969197514185355\n",
            "epoch 14 batch id 3711 loss 0.4816382825374603 train acc 0.896898578550256\n",
            "epoch 14 batch id 3721 loss 0.26638397574424744 train acc 0.8969279091642032\n",
            "epoch 14 batch id 3731 loss 0.20853689312934875 train acc 0.8969110158134549\n",
            "epoch 14 batch id 3741 loss 0.3039872646331787 train acc 0.8969025661587811\n",
            "epoch 14 batch id 3751 loss 0.35852116346359253 train acc 0.8968858304452146\n",
            "epoch 14 batch id 3761 loss 0.34508031606674194 train acc 0.896935655410795\n",
            "epoch 14 batch id 3771 loss 0.20373700559139252 train acc 0.8969272076372315\n",
            "epoch 14 batch id 3781 loss 0.2483397275209427 train acc 0.8969270695583179\n",
            "epoch 14 batch id 3791 loss 0.11846909672021866 train acc 0.8969887562648378\n",
            "epoch 14 batch id 3801 loss 0.21820011734962463 train acc 0.8970336753485925\n",
            "epoch 14 batch id 3811 loss 0.18747490644454956 train acc 0.8970578588297035\n",
            "epoch 14 batch id 3821 loss 0.31288424134254456 train acc 0.8970614695105993\n",
            "epoch 14 batch id 3831 loss 0.19800445437431335 train acc 0.8970895327590708\n",
            "epoch 14 batch id 3841 loss 0.2692923843860626 train acc 0.8970727024212445\n",
            "epoch 14 batch id 3851 loss 0.10417377203702927 train acc 0.897108705531031\n",
            "epoch 14 batch id 3861 loss 0.18405571579933167 train acc 0.8971121471121472\n",
            "epoch 14 batch id 3871 loss 0.39416560530662537 train acc 0.8971236437613019\n",
            "epoch 14 batch id 3881 loss 0.40913355350494385 train acc 0.8971632633341923\n",
            "epoch 14 batch id 3891 loss 0.17857661843299866 train acc 0.8971544911333847\n",
            "epoch 14 batch id 3901 loss 0.14551474153995514 train acc 0.8971137208408101\n",
            "epoch 14 batch id 3911 loss 0.10888894647359848 train acc 0.8971530618767579\n",
            "epoch 14 batch id 3921 loss 0.19914937019348145 train acc 0.8971164881407804\n",
            "epoch 14 batch id 3931 loss 0.2014470249414444 train acc 0.8970721508522005\n",
            "epoch 14 batch id 3941 loss 0.27178430557250977 train acc 0.8970914742451155\n",
            "epoch 14 batch id 3951 loss 0.20322273671627045 train acc 0.8970790622627183\n",
            "epoch 14 batch id 3961 loss 0.39576786756515503 train acc 0.8970706576622065\n",
            "epoch 14 batch id 3971 loss 0.32398712635040283 train acc 0.8970859040543944\n",
            "epoch 14 batch id 3981 loss 0.24685165286064148 train acc 0.8971010738507913\n",
            "epoch 14 batch id 3991 loss 0.3189765214920044 train acc 0.8971005073916312\n",
            "epoch 14 batch id 4001 loss 0.3141974210739136 train acc 0.8970882279430142\n",
            "epoch 14 batch id 4011 loss 0.2755306363105774 train acc 0.897091591872351\n",
            "epoch 14 batch id 4021 loss 0.2096310257911682 train acc 0.8970793956727182\n",
            "epoch 14 batch id 4031 loss 0.2943051755428314 train acc 0.8971176507070205\n",
            "epoch 14 batch id 4041 loss 0.37815991044044495 train acc 0.8971247834694382\n",
            "epoch 14 batch id 4051 loss 0.27820155024528503 train acc 0.897128023944705\n",
            "epoch 14 batch id 4061 loss 0.42916232347488403 train acc 0.8971851145038168\n",
            "epoch 14 batch id 4071 loss 0.432259202003479 train acc 0.8971843527388847\n",
            "epoch 14 batch id 4081 loss 0.21187721192836761 train acc 0.8972257106101446\n",
            "epoch 14 batch id 4091 loss 0.12604767084121704 train acc 0.8973012405279883\n",
            "epoch 14 batch id 4101 loss 0.45659559965133667 train acc 0.8972925810777859\n",
            "epoch 14 batch id 4111 loss 0.2771171033382416 train acc 0.8972573583069813\n",
            "epoch 14 batch id 4121 loss 0.28735873103141785 train acc 0.8972791798107256\n",
            "epoch 14 batch id 4131 loss 0.2339283972978592 train acc 0.8972782014040184\n",
            "epoch 14 batch id 4141 loss 0.20977094769477844 train acc 0.897311186911374\n",
            "epoch 14 batch id 4151 loss 0.2358466237783432 train acc 0.8973477776439412\n",
            "epoch 14 batch id 4161 loss 0.34191977977752686 train acc 0.8973166005767844\n",
            "epoch 14 batch id 4171 loss 0.38411638140678406 train acc 0.8973305262526972\n",
            "epoch 14 batch id 4181 loss 0.10772473365068436 train acc 0.8973219624491748\n",
            "epoch 14 batch id 4191 loss 0.3464222550392151 train acc 0.8973395371033166\n",
            "epoch 14 batch id 4201 loss 0.34809738397598267 train acc 0.8973607474410854\n",
            "epoch 14 batch id 4211 loss 0.3872358798980713 train acc 0.8973558834006174\n",
            "epoch 14 batch id 4221 loss 0.36703190207481384 train acc 0.8973436389481165\n",
            "epoch 14 batch id 4231 loss 0.25878292322158813 train acc 0.8973240664145592\n",
            "epoch 14 batch id 4241 loss 0.24897955358028412 train acc 0.8973930087243575\n",
            "epoch 14 batch id 4251 loss 0.2730370759963989 train acc 0.8974322218301576\n",
            "epoch 14 batch id 4261 loss 0.4608147144317627 train acc 0.8974492490025816\n",
            "epoch 14 batch id 4271 loss 0.14714564383029938 train acc 0.8974808300163896\n",
            "epoch 14 batch id 4281 loss 0.3643781840801239 train acc 0.8975305127306704\n",
            "epoch 14 batch id 4291 loss 0.10402252525091171 train acc 0.8975581158238173\n",
            "epoch 14 batch id 4301 loss 0.13103416562080383 train acc 0.8975347302952802\n",
            "epoch 14 batch id 4311 loss 0.19952362775802612 train acc 0.8975803177916957\n",
            "epoch 14 batch id 4321 loss 0.16115500032901764 train acc 0.897593149733858\n",
            "epoch 14 batch id 4331 loss 0.1652718484401703 train acc 0.8976095301316094\n",
            "epoch 14 batch id 4341 loss 0.12485252320766449 train acc 0.8976114374568072\n",
            "epoch 14 batch id 4351 loss 0.20625878870487213 train acc 0.8975558779590899\n",
            "epoch 14 batch id 4361 loss 0.40764448046684265 train acc 0.8975614824581518\n",
            "epoch 14 batch id 4371 loss 0.3559281826019287 train acc 0.8975920841912606\n",
            "epoch 14 batch id 4381 loss 0.3164280951023102 train acc 0.8975690481625199\n",
            "epoch 14 batch id 4391 loss 0.17191551625728607 train acc 0.8975994932817126\n",
            "epoch 14 batch id 4401 loss 0.3078339695930481 train acc 0.8976013974096796\n",
            "epoch 14 batch id 4411 loss 0.169647678732872 train acc 0.8976032929041033\n",
            "epoch 14 batch id 4421 loss 0.28400954604148865 train acc 0.8976228511648948\n",
            "epoch 14 batch id 4431 loss 0.3914574086666107 train acc 0.8976282159783344\n",
            "epoch 14 batch id 4441 loss 0.15552666783332825 train acc 0.8976652217968926\n",
            "epoch 14 batch id 4451 loss 0.20061348378658295 train acc 0.8977090822287126\n",
            "epoch 14 batch id 4461 loss 0.1793927252292633 train acc 0.8977352331315849\n",
            "epoch 14 batch id 4471 loss 0.3551504611968994 train acc 0.897736803847014\n",
            "epoch 14 batch id 4481 loss 0.294640451669693 train acc 0.8977383675518857\n",
            "epoch 14 batch id 4491 loss 0.17240409553050995 train acc 0.8977051324871966\n",
            "epoch 14 batch id 4501 loss 0.06497213244438171 train acc 0.8977310597644967\n",
            "epoch 14 batch id 4511 loss 0.3422470986843109 train acc 0.897743017069386\n",
            "epoch 14 batch id 4521 loss 0.23381437361240387 train acc 0.8977618336651183\n",
            "epoch 14 batch id 4531 loss 0.2625054121017456 train acc 0.8977460825424851\n",
            "epoch 14 batch id 4541 loss 0.3758937418460846 train acc 0.8977372825368861\n",
            "epoch 14 batch id 4551 loss 0.41576460003852844 train acc 0.8977628543177324\n",
            "epoch 14 batch id 4561 loss 0.302436500787735 train acc 0.8977574819118614\n",
            "epoch 14 batch id 4571 loss 0.35354459285736084 train acc 0.8978068256399038\n",
            "epoch 14 batch id 4581 loss 0.3426414728164673 train acc 0.8978423106308666\n",
            "epoch 14 batch id 4591 loss 0.24453113973140717 train acc 0.897857220649096\n",
            "epoch 14 batch id 4601 loss 0.22339236736297607 train acc 0.8978788578569876\n",
            "epoch 14 batch id 4611 loss 0.3814898133277893 train acc 0.8978970125786163\n",
            "epoch 14 batch id 4621 loss 0.356338232755661 train acc 0.8979252326336291\n",
            "epoch 14 batch id 4631 loss 0.1970645636320114 train acc 0.8979499568127834\n",
            "epoch 14 batch id 4641 loss 0.22034987807273865 train acc 0.8979712077138547\n",
            "epoch 14 batch id 4651 loss 0.5242850184440613 train acc 0.8979789292625242\n",
            "epoch 14 batch id 4661 loss 0.28293493390083313 train acc 0.8979933222484445\n",
            "epoch 14 batch id 4671 loss 0.31282222270965576 train acc 0.8980076536073646\n",
            "epoch 14 batch id 4681 loss 0.3508800268173218 train acc 0.8980519653920103\n",
            "epoch 14 batch id 4691 loss 0.33874592185020447 train acc 0.8980594489447878\n",
            "epoch 14 batch id 4701 loss 0.4131599962711334 train acc 0.8980702244203361\n",
            "epoch 14 batch id 4711 loss 0.15758246183395386 train acc 0.8980942209721927\n",
            "epoch 14 batch id 4721 loss 0.2446942925453186 train acc 0.8980519222622326\n",
            "epoch 14 batch id 4731 loss 0.29334381222724915 train acc 0.8980758560558022\n",
            "epoch 14 batch id 4741 loss 0.2915506958961487 train acc 0.8980996888842017\n",
            "epoch 14 batch id 4751 loss 0.31625550985336304 train acc 0.8980839560092612\n",
            "epoch 14 batch id 4761 loss 0.23636265099048615 train acc 0.8980945442134005\n",
            "epoch 14 batch id 4771 loss 0.2364894449710846 train acc 0.8981149130161392\n",
            "epoch 14 batch id 4781 loss 0.48440176248550415 train acc 0.8980763700062748\n",
            "epoch 14 batch id 4791 loss 0.4518297016620636 train acc 0.8981097370068879\n",
            "epoch 14 batch id 4801 loss 0.25987154245376587 train acc 0.8981201832951469\n",
            "epoch 14 batch id 4811 loss 0.20846907794475555 train acc 0.8981273383911869\n",
            "epoch 14 batch id 4821 loss 0.44414815306663513 train acc 0.8981247407176934\n",
            "epoch 14 batch id 4831 loss 0.1550760269165039 train acc 0.8981447940385013\n",
            "epoch 14 batch id 4841 loss 0.3357273042201996 train acc 0.8981324881222887\n",
            "epoch 14 batch id 4851 loss 0.40587103366851807 train acc 0.8981137909709338\n",
            "epoch 14 batch id 4861 loss 0.2264401763677597 train acc 0.8981080281835013\n",
            "epoch 14 batch id 4871 loss 0.28923115134239197 train acc 0.8981536132211045\n",
            "epoch 14 batch id 4881 loss 0.3031769394874573 train acc 0.8981766031550912\n",
            "epoch 14 batch id 4891 loss 0.15514972805976868 train acc 0.898164358004498\n",
            "epoch 14 batch id 4901 loss 0.26205238699913025 train acc 0.8981808559477658\n",
            "epoch 14 batch id 4911 loss 0.423029363155365 train acc 0.8981845601710446\n",
            "epoch 14 batch id 4921 loss 0.1999937891960144 train acc 0.898191424507214\n",
            "epoch 14 batch id 4931 loss 0.2928045690059662 train acc 0.8981887548164672\n",
            "epoch 14 batch id 4941 loss 0.4672609567642212 train acc 0.8981481481481481\n",
            "epoch 14 batch id 4951 loss 0.29397618770599365 train acc 0.8981361088668955\n",
            "epoch 14 train acc 0.8981571056540797\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dae77b7c5964e12ba15cd25551f15af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 14 loss 0.40672796964645386 test acc 0.7984168804985338\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd0823834eca456b86ba5e795a3d1edf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 15 batch id 1 loss 0.19792170822620392 train acc 0.90625\n",
            "epoch 15 batch id 11 loss 0.2609630823135376 train acc 0.9332386363636364\n",
            "epoch 15 batch id 21 loss 0.16993550956249237 train acc 0.9226190476190477\n",
            "epoch 15 batch id 31 loss 0.39802342653274536 train acc 0.9102822580645161\n",
            "epoch 15 batch id 41 loss 0.22804774343967438 train acc 0.9108231707317073\n",
            "epoch 15 batch id 51 loss 0.37199991941452026 train acc 0.9087009803921569\n",
            "epoch 15 batch id 61 loss 0.27346736192703247 train acc 0.9039446721311475\n",
            "epoch 15 batch id 71 loss 0.4521504342556 train acc 0.9042693661971831\n",
            "epoch 15 batch id 81 loss 0.4267125129699707 train acc 0.9033564814814815\n",
            "epoch 15 batch id 91 loss 0.3419893980026245 train acc 0.9035027472527473\n",
            "epoch 15 batch id 101 loss 0.23755145072937012 train acc 0.9033106435643564\n",
            "epoch 15 batch id 111 loss 0.1013762354850769 train acc 0.9051238738738738\n",
            "epoch 15 batch id 121 loss 0.3212295174598694 train acc 0.9054752066115702\n",
            "epoch 15 batch id 131 loss 0.3495364189147949 train acc 0.9064885496183206\n",
            "epoch 15 batch id 141 loss 0.17578016221523285 train acc 0.9052526595744681\n",
            "epoch 15 batch id 151 loss 0.28936856985092163 train acc 0.9053187086092715\n",
            "epoch 15 batch id 161 loss 0.4629092216491699 train acc 0.906055900621118\n",
            "epoch 15 batch id 171 loss 0.46363410353660583 train acc 0.9061586257309941\n",
            "epoch 15 batch id 181 loss 0.22194482386112213 train acc 0.9068542817679558\n",
            "epoch 15 batch id 191 loss 0.16130539774894714 train acc 0.9069862565445026\n",
            "epoch 15 batch id 201 loss 0.2396664023399353 train acc 0.9071828358208955\n",
            "epoch 15 batch id 211 loss 0.24269413948059082 train acc 0.9065462085308057\n",
            "epoch 15 batch id 221 loss 0.5069562792778015 train acc 0.9059671945701357\n",
            "epoch 15 batch id 231 loss 0.19226956367492676 train acc 0.906452922077922\n",
            "epoch 15 batch id 241 loss 0.30100372433662415 train acc 0.9051478215767634\n",
            "epoch 15 batch id 251 loss 0.28037554025650024 train acc 0.9050672310756972\n",
            "epoch 15 batch id 261 loss 0.20339147746562958 train acc 0.9057112068965517\n",
            "epoch 15 batch id 271 loss 0.42435020208358765 train acc 0.9054428044280443\n",
            "epoch 15 batch id 281 loss 0.21141375601291656 train acc 0.905415925266904\n",
            "epoch 15 batch id 291 loss 0.3929542899131775 train acc 0.9058204467353952\n",
            "epoch 15 batch id 301 loss 0.23822440207004547 train acc 0.9060423588039868\n",
            "epoch 15 batch id 311 loss 0.3177145719528198 train acc 0.9065514469453376\n",
            "epoch 15 batch id 321 loss 0.33937308192253113 train acc 0.9066394080996885\n",
            "epoch 15 batch id 331 loss 0.3324471712112427 train acc 0.9069108761329305\n",
            "epoch 15 batch id 341 loss 0.5912572741508484 train acc 0.9067540322580645\n",
            "epoch 15 batch id 351 loss 0.42296430468559265 train acc 0.9066951566951567\n",
            "epoch 15 batch id 361 loss 0.34461694955825806 train acc 0.9067693905817175\n",
            "epoch 15 batch id 371 loss 0.3786991536617279 train acc 0.9070080862533693\n",
            "epoch 15 batch id 381 loss 0.1465633064508438 train acc 0.9070291994750657\n",
            "epoch 15 batch id 391 loss 0.5084677338600159 train acc 0.9068893861892583\n",
            "epoch 15 batch id 401 loss 0.11993525177240372 train acc 0.9067565461346634\n",
            "epoch 15 batch id 411 loss 0.32242220640182495 train acc 0.9070103406326034\n",
            "epoch 15 batch id 421 loss 0.3918268084526062 train acc 0.9071407363420427\n",
            "epoch 15 batch id 431 loss 0.2693396806716919 train acc 0.9069388051044084\n",
            "epoch 15 batch id 441 loss 0.376527339220047 train acc 0.9068523242630385\n",
            "epoch 15 batch id 451 loss 0.2910788059234619 train acc 0.9068043237250555\n",
            "epoch 15 batch id 461 loss 0.28087082505226135 train acc 0.9060127440347071\n",
            "epoch 15 batch id 471 loss 0.25161731243133545 train acc 0.9061504777070064\n",
            "epoch 15 batch id 481 loss 0.3316566050052643 train acc 0.9065748440748441\n",
            "epoch 15 batch id 491 loss 0.2559700012207031 train acc 0.9064091140529531\n",
            "epoch 15 batch id 501 loss 0.6647711992263794 train acc 0.9064059381237525\n",
            "epoch 15 batch id 511 loss 0.3081479072570801 train acc 0.9061582681017613\n",
            "epoch 15 batch id 521 loss 0.24337320029735565 train acc 0.9059201055662188\n",
            "epoch 15 batch id 531 loss 0.26537954807281494 train acc 0.9057497645951036\n",
            "epoch 15 batch id 541 loss 0.17592446506023407 train acc 0.905643484288355\n",
            "epoch 15 batch id 551 loss 0.36447975039482117 train acc 0.9057112068965517\n",
            "epoch 15 batch id 561 loss 0.24006666243076324 train acc 0.9054979946524064\n",
            "epoch 15 batch id 571 loss 0.34853288531303406 train acc 0.9055932574430823\n",
            "epoch 15 batch id 581 loss 0.28979068994522095 train acc 0.9057390275387264\n",
            "epoch 15 batch id 591 loss 0.23357030749320984 train acc 0.9060913705583756\n",
            "epoch 15 batch id 601 loss 0.3263145685195923 train acc 0.9062759983361065\n",
            "epoch 15 batch id 611 loss 0.35281243920326233 train acc 0.9059431260229133\n",
            "epoch 15 batch id 621 loss 0.3301621079444885 train acc 0.9060487117552335\n",
            "epoch 15 batch id 631 loss 0.323038250207901 train acc 0.9059033280507132\n",
            "epoch 15 batch id 641 loss 0.20078863203525543 train acc 0.906030616224649\n",
            "epoch 15 batch id 651 loss 0.15790963172912598 train acc 0.9059139784946236\n",
            "epoch 15 batch id 661 loss 0.2371576577425003 train acc 0.9061318078668684\n",
            "epoch 15 batch id 671 loss 0.24489803612232208 train acc 0.9061335692995529\n",
            "epoch 15 batch id 681 loss 0.30568116903305054 train acc 0.9059976138032305\n",
            "epoch 15 batch id 691 loss 0.41099435091018677 train acc 0.9062726121562952\n",
            "epoch 15 batch id 701 loss 0.2651359438896179 train acc 0.9063614479315264\n",
            "epoch 15 batch id 711 loss 0.30073824524879456 train acc 0.9062939521800282\n",
            "epoch 15 batch id 721 loss 0.20903882384300232 train acc 0.9061849861303745\n",
            "epoch 15 batch id 731 loss 0.1773824244737625 train acc 0.9061431258549931\n",
            "epoch 15 batch id 741 loss 0.22116151452064514 train acc 0.9061656545209177\n",
            "epoch 15 batch id 751 loss 0.4117140471935272 train acc 0.9060003328894807\n",
            "epoch 15 batch id 761 loss 0.15211616456508636 train acc 0.9061473390275953\n",
            "epoch 15 batch id 771 loss 0.2717774212360382 train acc 0.9059460116731517\n",
            "epoch 15 batch id 781 loss 0.4190942943096161 train acc 0.905729833546735\n",
            "epoch 15 batch id 791 loss 0.18471837043762207 train acc 0.906072218710493\n",
            "epoch 15 batch id 801 loss 0.17009617388248444 train acc 0.9063085205992509\n",
            "epoch 15 batch id 811 loss 0.28642576932907104 train acc 0.9063077990135635\n",
            "epoch 15 batch id 821 loss 0.14059948921203613 train acc 0.9064212850182704\n",
            "epoch 15 batch id 831 loss 0.3998267948627472 train acc 0.9064004211793021\n",
            "epoch 15 batch id 841 loss 0.2665068805217743 train acc 0.9064915279429251\n",
            "epoch 15 batch id 851 loss 0.408041775226593 train acc 0.9063785252643948\n",
            "epoch 15 batch id 861 loss 0.34786301851272583 train acc 0.9063588850174216\n",
            "epoch 15 batch id 871 loss 0.3133913278579712 train acc 0.9063755740528129\n",
            "epoch 15 batch id 881 loss 0.1947118043899536 train acc 0.9063209421112373\n",
            "epoch 15 batch id 891 loss 0.14857804775238037 train acc 0.9065130471380471\n",
            "epoch 15 batch id 901 loss 0.336396723985672 train acc 0.9064927857935627\n",
            "epoch 15 batch id 911 loss 0.5053723454475403 train acc 0.9066959385290889\n",
            "epoch 15 batch id 921 loss 0.1876026839017868 train acc 0.9066741313789359\n",
            "epoch 15 batch id 931 loss 0.11118653416633606 train acc 0.906703141783029\n",
            "epoch 15 batch id 941 loss 0.19554857909679413 train acc 0.9068643730074389\n",
            "epoch 15 batch id 951 loss 0.2262631505727768 train acc 0.9068414826498423\n",
            "epoch 15 batch id 961 loss 0.18589258193969727 train acc 0.9069328824141519\n",
            "epoch 15 batch id 971 loss 0.3026561141014099 train acc 0.9069097579814624\n",
            "epoch 15 batch id 981 loss 0.19291387498378754 train acc 0.9070941641182467\n",
            "epoch 15 batch id 991 loss 0.3880634605884552 train acc 0.907132946518668\n",
            "epoch 15 batch id 1001 loss 0.3304782509803772 train acc 0.9071085164835165\n",
            "epoch 15 batch id 1011 loss 0.32799574732780457 train acc 0.9070691147378833\n",
            "epoch 15 batch id 1021 loss 0.21004638075828552 train acc 0.9073212536728698\n",
            "epoch 15 batch id 1031 loss 0.1950593888759613 train acc 0.9074472599418041\n",
            "epoch 15 batch id 1041 loss 0.25029808282852173 train acc 0.9075408261287223\n",
            "epoch 15 batch id 1051 loss 0.1776643544435501 train acc 0.9075434110371076\n",
            "epoch 15 batch id 1061 loss 0.2018299549818039 train acc 0.9075754005655042\n",
            "epoch 15 batch id 1071 loss 0.07624261826276779 train acc 0.9076797385620915\n",
            "epoch 15 batch id 1081 loss 0.40724876523017883 train acc 0.9074930619796485\n",
            "epoch 15 batch id 1091 loss 0.6639357209205627 train acc 0.9072668423464711\n",
            "epoch 15 batch id 1101 loss 0.22817540168762207 train acc 0.9073569482288828\n",
            "epoch 15 batch id 1111 loss 0.38296404480934143 train acc 0.9071219621962197\n",
            "epoch 15 batch id 1121 loss 0.21417135000228882 train acc 0.9072396297948261\n",
            "epoch 15 batch id 1131 loss 0.3127686083316803 train acc 0.9070650972590628\n",
            "epoch 15 batch id 1141 loss 0.34534916281700134 train acc 0.9070031770376863\n",
            "epoch 15 batch id 1151 loss 0.4322755038738251 train acc 0.9067930060816681\n",
            "epoch 15 batch id 1161 loss 0.21993155777454376 train acc 0.9068152454780362\n",
            "epoch 15 batch id 1171 loss 0.26048803329467773 train acc 0.9067570452604612\n",
            "epoch 15 batch id 1181 loss 0.2672109007835388 train acc 0.906832133784928\n",
            "epoch 15 batch id 1191 loss 0.19127166271209717 train acc 0.9068797229219143\n",
            "epoch 15 batch id 1201 loss 0.3493037521839142 train acc 0.90704360949209\n",
            "epoch 15 batch id 1211 loss 0.08106426894664764 train acc 0.9070628612716763\n",
            "epoch 15 batch id 1221 loss 0.24226121604442596 train acc 0.907158579033579\n",
            "epoch 15 batch id 1231 loss 0.16114309430122375 train acc 0.906960804224208\n",
            "epoch 15 batch id 1241 loss 0.1923554390668869 train acc 0.9069676672038679\n",
            "epoch 15 batch id 1251 loss 0.23953834176063538 train acc 0.906849520383693\n",
            "epoch 15 batch id 1261 loss 0.2543211579322815 train acc 0.9069067208564632\n",
            "epoch 15 batch id 1271 loss 0.17949897050857544 train acc 0.9069384343036979\n",
            "epoch 15 batch id 1281 loss 0.34735211730003357 train acc 0.9069086651053864\n",
            "epoch 15 batch id 1291 loss 0.14204484224319458 train acc 0.9070487993803253\n",
            "epoch 15 batch id 1301 loss 0.2921529710292816 train acc 0.9069105495772483\n",
            "epoch 15 batch id 1311 loss 0.37325239181518555 train acc 0.9069174294431731\n",
            "epoch 15 batch id 1321 loss 0.09792565554380417 train acc 0.9069596896290689\n",
            "epoch 15 batch id 1331 loss 0.28211334347724915 train acc 0.9068721825694966\n",
            "epoch 15 batch id 1341 loss 0.31425029039382935 train acc 0.9068325876211782\n",
            "epoch 15 batch id 1351 loss 0.11971791088581085 train acc 0.9069092339008142\n",
            "epoch 15 batch id 1361 loss 0.23750437796115875 train acc 0.9068469875091845\n",
            "epoch 15 batch id 1371 loss 0.1777055859565735 train acc 0.9068198395331875\n",
            "epoch 15 batch id 1381 loss 0.4533061981201172 train acc 0.9067251991310644\n",
            "epoch 15 batch id 1391 loss 0.15842536091804504 train acc 0.9068341121495327\n",
            "epoch 15 batch id 1401 loss 0.14395226538181305 train acc 0.906818790149893\n",
            "epoch 15 batch id 1411 loss 0.13002793490886688 train acc 0.9069476435152374\n",
            "epoch 15 batch id 1421 loss 0.15255725383758545 train acc 0.9069867171006334\n",
            "epoch 15 batch id 1431 loss 0.34823790192604065 train acc 0.9070143256464012\n",
            "epoch 15 batch id 1441 loss 0.24407976865768433 train acc 0.9069222761970853\n",
            "epoch 15 batch id 1451 loss 0.3498428463935852 train acc 0.9071222432804962\n",
            "epoch 15 batch id 1461 loss 0.25928860902786255 train acc 0.9071269678302533\n",
            "epoch 15 batch id 1471 loss 0.43580976128578186 train acc 0.9072378484024474\n",
            "epoch 15 batch id 1481 loss 0.20363542437553406 train acc 0.9072944800810263\n",
            "epoch 15 batch id 1491 loss 0.5904942154884338 train acc 0.9073293930248155\n",
            "epoch 15 batch id 1501 loss 0.3202414810657501 train acc 0.9073950699533644\n",
            "epoch 15 batch id 1511 loss 0.14207832515239716 train acc 0.9074805592322965\n",
            "epoch 15 batch id 1521 loss 0.22694429755210876 train acc 0.9076573800131492\n",
            "epoch 15 batch id 1531 loss 0.21931520104408264 train acc 0.9077196276943175\n",
            "epoch 15 batch id 1541 loss 0.1933203786611557 train acc 0.907669532770928\n",
            "epoch 15 batch id 1551 loss 0.19871674478054047 train acc 0.9076100096711799\n",
            "epoch 15 batch id 1561 loss 0.3066219389438629 train acc 0.9076813741191544\n",
            "epoch 15 batch id 1571 loss 0.3120909333229065 train acc 0.9076921546785487\n",
            "epoch 15 batch id 1581 loss 0.17306199669837952 train acc 0.9077225648323846\n",
            "epoch 15 batch id 1591 loss 0.4050810635089874 train acc 0.907723130106851\n",
            "epoch 15 batch id 1601 loss 0.41686493158340454 train acc 0.9076846502186133\n",
            "epoch 15 batch id 1611 loss 0.23989300429821014 train acc 0.9075981533209186\n",
            "epoch 15 batch id 1621 loss 0.11623793095350266 train acc 0.9074934454040715\n",
            "epoch 15 batch id 1631 loss 0.17245912551879883 train acc 0.9075145616186389\n",
            "epoch 15 batch id 1641 loss 0.27867811918258667 train acc 0.9076020719073735\n",
            "epoch 15 batch id 1651 loss 0.12028425186872482 train acc 0.9075938824954572\n",
            "epoch 15 batch id 1661 loss 0.2811110019683838 train acc 0.9076704545454546\n",
            "epoch 15 batch id 1671 loss 0.3378673791885376 train acc 0.9077367594254937\n",
            "epoch 15 batch id 1681 loss 0.3565623164176941 train acc 0.9076070791195717\n",
            "epoch 15 batch id 1691 loss 0.41711241006851196 train acc 0.9076360141927853\n",
            "epoch 15 batch id 1701 loss 0.24748094379901886 train acc 0.9077105379188712\n",
            "epoch 15 batch id 1711 loss 0.20929646492004395 train acc 0.9077202659263589\n",
            "epoch 15 batch id 1721 loss 0.33108824491500854 train acc 0.9077661969785009\n",
            "epoch 15 batch id 1731 loss 0.09644171595573425 train acc 0.9078567302137492\n",
            "epoch 15 batch id 1741 loss 0.35342857241630554 train acc 0.9078205772544514\n",
            "epoch 15 batch id 1751 loss 0.18788929283618927 train acc 0.9078383780696745\n",
            "epoch 15 batch id 1761 loss 0.17740127444267273 train acc 0.9078914679159569\n",
            "epoch 15 batch id 1771 loss 0.1841491162776947 train acc 0.9079263128176172\n",
            "epoch 15 batch id 1781 loss 0.2108956128358841 train acc 0.9079783127456486\n",
            "epoch 15 batch id 1791 loss 0.4296356439590454 train acc 0.9078726968174204\n",
            "epoch 15 batch id 1801 loss 0.16760176420211792 train acc 0.9078897140477512\n",
            "epoch 15 batch id 1811 loss 0.2659485340118408 train acc 0.9079324268360022\n",
            "epoch 15 batch id 1821 loss 0.22693754732608795 train acc 0.9079489291598023\n",
            "epoch 15 batch id 1831 loss 0.17835445702075958 train acc 0.9078969825232114\n",
            "epoch 15 batch id 1841 loss 0.26681506633758545 train acc 0.9078540874524715\n",
            "epoch 15 batch id 1851 loss 0.31064650416374207 train acc 0.9078791869259859\n",
            "epoch 15 batch id 1861 loss 0.1531069129705429 train acc 0.9079376007522837\n",
            "epoch 15 batch id 1871 loss 0.3314346671104431 train acc 0.9081123062533405\n",
            "epoch 15 batch id 1881 loss 0.17105641961097717 train acc 0.9082103934077618\n",
            "epoch 15 batch id 1891 loss 0.19913461804389954 train acc 0.9083570200951877\n",
            "epoch 15 batch id 1901 loss 0.2586067318916321 train acc 0.9083048395581274\n",
            "epoch 15 batch id 1911 loss 0.17502020299434662 train acc 0.9083349686028257\n",
            "epoch 15 batch id 1921 loss 0.2437041699886322 train acc 0.9081858407079646\n",
            "epoch 15 batch id 1931 loss 0.36429962515830994 train acc 0.9082081822889695\n",
            "epoch 15 batch id 1941 loss 0.14828267693519592 train acc 0.9082624935600206\n",
            "epoch 15 batch id 1951 loss 0.29170462489128113 train acc 0.9082842132239877\n",
            "epoch 15 batch id 1961 loss 0.1701790988445282 train acc 0.9082977434982152\n",
            "epoch 15 batch id 1971 loss 0.253473162651062 train acc 0.9083666286149162\n",
            "epoch 15 batch id 1981 loss 0.4656212329864502 train acc 0.9083480565371025\n",
            "epoch 15 batch id 1991 loss 0.4119011461734772 train acc 0.908235497237569\n",
            "epoch 15 batch id 2001 loss 0.3498692214488983 train acc 0.9081552973513244\n",
            "epoch 15 batch id 2011 loss 0.34657496213912964 train acc 0.9081225136747887\n",
            "epoch 15 batch id 2021 loss 0.46633800864219666 train acc 0.908105517070757\n",
            "epoch 15 batch id 2031 loss 0.16594593226909637 train acc 0.9080886878385032\n",
            "epoch 15 batch id 2041 loss 0.11293900012969971 train acc 0.9080873346398824\n",
            "epoch 15 batch id 2051 loss 0.28106099367141724 train acc 0.9080250487567041\n",
            "epoch 15 batch id 2061 loss 0.3063506782054901 train acc 0.9080467612809315\n",
            "epoch 15 batch id 2071 loss 0.24615290760993958 train acc 0.9081361661033317\n",
            "epoch 15 batch id 2081 loss 0.4283595085144043 train acc 0.908097068716963\n",
            "epoch 15 batch id 2091 loss 0.26452934741973877 train acc 0.908080762792922\n",
            "epoch 15 batch id 2101 loss 0.09518035501241684 train acc 0.9080423012851023\n",
            "epoch 15 batch id 2111 loss 0.24131804704666138 train acc 0.908041212695405\n",
            "epoch 15 batch id 2121 loss 0.13282068073749542 train acc 0.9080548679867987\n",
            "epoch 15 batch id 2131 loss 0.3068358302116394 train acc 0.9081490497419052\n",
            "epoch 15 batch id 2141 loss 0.3156738579273224 train acc 0.9081474778141055\n",
            "epoch 15 batch id 2151 loss 0.22911430895328522 train acc 0.9082839377033938\n",
            "epoch 15 batch id 2161 loss 0.31586864590644836 train acc 0.9081805298472929\n",
            "epoch 15 batch id 2171 loss 0.11335653066635132 train acc 0.9082795946568402\n",
            "epoch 15 batch id 2181 loss 0.23870737850666046 train acc 0.9082702888583218\n",
            "epoch 15 batch id 2191 loss 0.16534119844436646 train acc 0.9082681994523049\n",
            "epoch 15 batch id 2201 loss 0.3456595242023468 train acc 0.9083300204452521\n",
            "epoch 15 batch id 2211 loss 0.2018078714609146 train acc 0.9082923450927183\n",
            "epoch 15 batch id 2221 loss 0.44940388202667236 train acc 0.908247973885637\n",
            "epoch 15 batch id 2231 loss 0.22464562952518463 train acc 0.908267032720753\n",
            "epoch 15 batch id 2241 loss 0.25677621364593506 train acc 0.9083277554663097\n",
            "epoch 15 batch id 2251 loss 0.2349781095981598 train acc 0.9082768769435806\n",
            "epoch 15 batch id 2261 loss 0.22727985680103302 train acc 0.9083439296771341\n",
            "epoch 15 batch id 2271 loss 0.39232441782951355 train acc 0.9083209489211801\n",
            "epoch 15 batch id 2281 loss 0.4454200863838196 train acc 0.9084214708461201\n",
            "epoch 15 batch id 2291 loss 0.18248209357261658 train acc 0.908514295067656\n",
            "epoch 15 batch id 2301 loss 0.13533839583396912 train acc 0.9085723598435462\n",
            "epoch 15 batch id 2311 loss 0.3024882376194 train acc 0.908596116399827\n",
            "epoch 15 batch id 2321 loss 0.24235884845256805 train acc 0.9084984920292977\n",
            "epoch 15 batch id 2331 loss 0.20196448266506195 train acc 0.9084687365937366\n",
            "epoch 15 batch id 2341 loss 0.4739978611469269 train acc 0.9084592588637335\n",
            "epoch 15 batch id 2351 loss 0.26621025800704956 train acc 0.9084764461931093\n",
            "epoch 15 batch id 2361 loss 0.42703452706336975 train acc 0.9084471622193986\n",
            "epoch 15 batch id 2371 loss 0.1029515415430069 train acc 0.9085103859131168\n",
            "epoch 15 batch id 2381 loss 0.15976324677467346 train acc 0.9085468290634188\n",
            "epoch 15 batch id 2391 loss 0.1379001885652542 train acc 0.9085437578419071\n",
            "epoch 15 batch id 2401 loss 0.16362015902996063 train acc 0.9085276967930029\n",
            "epoch 15 batch id 2411 loss 0.20868085324764252 train acc 0.9085182496889258\n",
            "epoch 15 batch id 2421 loss 0.30083927512168884 train acc 0.9085540582403965\n",
            "epoch 15 batch id 2431 loss 0.2528560757637024 train acc 0.9085445804195804\n",
            "epoch 15 batch id 2441 loss 0.14041028916835785 train acc 0.9085927898402294\n",
            "epoch 15 batch id 2451 loss 0.19352920353412628 train acc 0.9085896062831498\n",
            "epoch 15 batch id 2461 loss 0.47929880023002625 train acc 0.9085166091019911\n",
            "epoch 15 batch id 2471 loss 0.37147632241249084 train acc 0.9084758195062728\n",
            "epoch 15 batch id 2481 loss 0.3552824854850769 train acc 0.9085550181378477\n",
            "epoch 15 batch id 2491 loss 0.23773270845413208 train acc 0.908457948615014\n",
            "epoch 15 batch id 2501 loss 0.18995322287082672 train acc 0.9085053478608557\n",
            "epoch 15 batch id 2511 loss 0.30231064558029175 train acc 0.9085212564715253\n",
            "epoch 15 batch id 2521 loss 0.30082160234451294 train acc 0.9084440698135661\n",
            "epoch 15 batch id 2531 loss 0.18626688420772552 train acc 0.9083613196365073\n",
            "epoch 15 batch id 2541 loss 0.2501245439052582 train acc 0.9082792207792207\n",
            "epoch 15 batch id 2551 loss 0.2684381306171417 train acc 0.9082467659741278\n",
            "epoch 15 batch id 2561 loss 0.38790538907051086 train acc 0.9081718566966029\n",
            "epoch 15 batch id 2571 loss 0.3412557542324066 train acc 0.9081947685725399\n",
            "epoch 15 batch id 2581 loss 0.19169382750988007 train acc 0.9082296106160402\n",
            "epoch 15 batch id 2591 loss 0.16752536594867706 train acc 0.9081737263604786\n",
            "epoch 15 batch id 2601 loss 0.3318862020969391 train acc 0.9082444252210689\n",
            "epoch 15 batch id 2611 loss 0.22327013313770294 train acc 0.9082906453466105\n",
            "epoch 15 batch id 2621 loss 0.2683505117893219 train acc 0.9082590137352156\n",
            "epoch 15 batch id 2631 loss 0.31327253580093384 train acc 0.9082513778031167\n",
            "epoch 15 batch id 2641 loss 0.16419269144535065 train acc 0.9082970465732677\n",
            "epoch 15 batch id 2651 loss 0.24704477190971375 train acc 0.9082716427763108\n",
            "epoch 15 batch id 2661 loss 0.3770155608654022 train acc 0.9082699173243142\n",
            "epoch 15 batch id 2671 loss 0.24513573944568634 train acc 0.9082857543991014\n",
            "epoch 15 batch id 2681 loss 0.2045717090368271 train acc 0.9082140525923162\n",
            "epoch 15 batch id 2691 loss 0.2511001229286194 train acc 0.9081719156447418\n",
            "epoch 15 batch id 2701 loss 0.29244303703308105 train acc 0.9081879396519807\n",
            "epoch 15 batch id 2711 loss 0.1898120641708374 train acc 0.9082211361121357\n",
            "epoch 15 batch id 2721 loss 0.23656141757965088 train acc 0.9082598309445057\n",
            "epoch 15 batch id 2731 loss 0.20397330820560455 train acc 0.9082581929696082\n",
            "epoch 15 batch id 2741 loss 0.318061500787735 train acc 0.9082679678949288\n",
            "epoch 15 batch id 2751 loss 0.23929864168167114 train acc 0.9082947110141767\n",
            "epoch 15 batch id 2761 loss 0.15182340145111084 train acc 0.9084004889532779\n",
            "epoch 15 batch id 2771 loss 0.20880678296089172 train acc 0.9083983670155179\n",
            "epoch 15 batch id 2781 loss 0.33086642622947693 train acc 0.9084524451636102\n",
            "epoch 15 batch id 2791 loss 0.191435307264328 train acc 0.9084837423862415\n",
            "epoch 15 batch id 2801 loss 0.28860390186309814 train acc 0.9084980810424849\n",
            "epoch 15 batch id 2811 loss 0.20886313915252686 train acc 0.9085512273212379\n",
            "epoch 15 batch id 2821 loss 0.20119549334049225 train acc 0.9085541474654378\n",
            "epoch 15 batch id 2831 loss 0.11771057546138763 train acc 0.9086398357470858\n",
            "epoch 15 batch id 2841 loss 0.34724992513656616 train acc 0.9086644227384724\n",
            "epoch 15 batch id 2851 loss 0.22534270584583282 train acc 0.9086449929849175\n",
            "epoch 15 batch id 2861 loss 0.12038252502679825 train acc 0.908625699056274\n",
            "epoch 15 batch id 2871 loss 0.29429998993873596 train acc 0.9086664054336469\n",
            "epoch 15 batch id 2881 loss 0.20992626249790192 train acc 0.9086742884415133\n",
            "epoch 15 batch id 2891 loss 0.2507166564464569 train acc 0.9086983310273262\n",
            "epoch 15 batch id 2901 loss 0.1942218840122223 train acc 0.9086898914167528\n",
            "epoch 15 batch id 2911 loss 0.09260392189025879 train acc 0.9087351855032635\n",
            "epoch 15 batch id 2921 loss 0.11676096171140671 train acc 0.9086892331393358\n",
            "epoch 15 batch id 2931 loss 0.08097213506698608 train acc 0.9087715370180826\n",
            "epoch 15 batch id 2941 loss 0.21102111041545868 train acc 0.9086938966337981\n",
            "epoch 15 batch id 2951 loss 0.2172178030014038 train acc 0.9088073957980346\n",
            "epoch 15 batch id 2961 loss 0.10519202798604965 train acc 0.908920128335022\n",
            "epoch 15 batch id 2971 loss 0.2770928144454956 train acc 0.9089216593739482\n",
            "epoch 15 batch id 2981 loss 0.2653139531612396 train acc 0.9089598708487084\n",
            "epoch 15 batch id 2991 loss 0.28154993057250977 train acc 0.9089926028084253\n",
            "epoch 15 batch id 3001 loss 0.22206558287143707 train acc 0.9089938770409863\n",
            "epoch 15 batch id 3011 loss 0.2548278272151947 train acc 0.9089795748920625\n",
            "epoch 15 batch id 3021 loss 0.228770449757576 train acc 0.9089808838133069\n",
            "epoch 15 batch id 3031 loss 0.3205769658088684 train acc 0.9090028043549984\n",
            "epoch 15 batch id 3041 loss 0.26528695225715637 train acc 0.908978337717856\n",
            "epoch 15 batch id 3051 loss 0.3465694487094879 train acc 0.9090001229105211\n",
            "epoch 15 batch id 3061 loss 0.17310157418251038 train acc 0.9089809294348252\n",
            "epoch 15 batch id 3071 loss 0.27556461095809937 train acc 0.9089720367958319\n",
            "epoch 15 batch id 3081 loss 0.3501593768596649 train acc 0.9090189873417721\n",
            "epoch 15 batch id 3091 loss 0.25042277574539185 train acc 0.9090049741184083\n",
            "epoch 15 batch id 3101 loss 0.32073333859443665 train acc 0.909016244759755\n",
            "epoch 15 batch id 3111 loss 0.3345491886138916 train acc 0.9090073529411765\n",
            "epoch 15 batch id 3121 loss 0.35392242670059204 train acc 0.909008530919577\n",
            "epoch 15 batch id 3131 loss 0.19656457006931305 train acc 0.9091045193229\n",
            "epoch 15 batch id 3141 loss 0.4162329435348511 train acc 0.9090506606176377\n",
            "epoch 15 batch id 3151 loss 0.3003924489021301 train acc 0.9090864011424944\n",
            "epoch 15 batch id 3161 loss 0.1508631557226181 train acc 0.9090972002530845\n",
            "epoch 15 batch id 3171 loss 0.11384588479995728 train acc 0.909107931251971\n",
            "epoch 15 batch id 3181 loss 0.1611987054347992 train acc 0.9090940348946872\n",
            "epoch 15 batch id 3191 loss 0.08712242543697357 train acc 0.9091732607333124\n",
            "epoch 15 batch id 3201 loss 0.3287864029407501 train acc 0.9091006716651047\n",
            "epoch 15 batch id 3211 loss 0.4048222601413727 train acc 0.90904313298038\n",
            "epoch 15 batch id 3221 loss 0.3521353304386139 train acc 0.9089762496119218\n",
            "epoch 15 batch id 3231 loss 0.21787129342556 train acc 0.9090258433921387\n",
            "epoch 15 batch id 3241 loss 0.4195517897605896 train acc 0.9090220996605985\n",
            "epoch 15 batch id 3251 loss 0.29014965891838074 train acc 0.9090568286681021\n",
            "epoch 15 batch id 3261 loss 0.18098874390125275 train acc 0.9090625958295001\n",
            "epoch 15 batch id 3271 loss 0.3251790702342987 train acc 0.9089775680220116\n",
            "epoch 15 batch id 3281 loss 0.4014793038368225 train acc 0.9089835416031697\n",
            "epoch 15 batch id 3291 loss 0.35837477445602417 train acc 0.9089467487085993\n",
            "epoch 15 batch id 3301 loss 0.0859409049153328 train acc 0.9089811799454711\n",
            "epoch 15 batch id 3311 loss 0.3371048867702484 train acc 0.9090106840833585\n",
            "epoch 15 batch id 3321 loss 0.3179355263710022 train acc 0.9090211909063535\n",
            "epoch 15 batch id 3331 loss 0.31462979316711426 train acc 0.9089800360252176\n",
            "epoch 15 batch id 3341 loss 0.47258543968200684 train acc 0.9089391275067346\n",
            "epoch 15 batch id 3351 loss 0.2619832158088684 train acc 0.908940428230379\n",
            "epoch 15 batch id 3361 loss 0.18457826972007751 train acc 0.9089835614400475\n",
            "epoch 15 batch id 3371 loss 0.21536387503147125 train acc 0.9089708172649066\n",
            "epoch 15 batch id 3381 loss 0.11589182913303375 train acc 0.9089627698905649\n",
            "epoch 15 batch id 3391 loss 0.3520461320877075 train acc 0.9089732011206134\n",
            "epoch 15 batch id 3401 loss 0.366824209690094 train acc 0.9089973537194943\n",
            "epoch 15 batch id 3411 loss 0.24848543107509613 train acc 0.9090076223981237\n",
            "epoch 15 batch id 3421 loss 0.2775382697582245 train acc 0.9089858593978369\n",
            "epoch 15 batch id 3431 loss 0.36764124035835266 train acc 0.9089687773243952\n",
            "epoch 15 batch id 3441 loss 0.24254941940307617 train acc 0.9089790395233943\n",
            "epoch 15 batch id 3451 loss 0.20442797243595123 train acc 0.9089666038829325\n",
            "epoch 15 batch id 3461 loss 0.21618323028087616 train acc 0.9090580757006645\n",
            "epoch 15 batch id 3471 loss 0.3406747281551361 train acc 0.9090139729184673\n",
            "epoch 15 batch id 3481 loss 0.249209463596344 train acc 0.9089925667911519\n",
            "epoch 15 batch id 3491 loss 0.24002619087696075 train acc 0.9089668075050129\n",
            "epoch 15 batch id 3501 loss 0.34083643555641174 train acc 0.9089858254784348\n",
            "epoch 15 batch id 3511 loss 0.15775153040885925 train acc 0.9089913842210197\n",
            "epoch 15 batch id 3521 loss 0.2849254608154297 train acc 0.9090501633058791\n",
            "epoch 15 batch id 3531 loss 0.5422918796539307 train acc 0.9090776338147833\n",
            "epoch 15 batch id 3541 loss 0.19442155957221985 train acc 0.909109361762214\n",
            "epoch 15 batch id 3551 loss 0.2689555287361145 train acc 0.9091233103351168\n",
            "epoch 15 batch id 3561 loss 0.08439347892999649 train acc 0.9091854465037911\n",
            "epoch 15 batch id 3571 loss 0.3778853118419647 train acc 0.9092078549425932\n",
            "epoch 15 batch id 3581 loss 0.41428065299987793 train acc 0.9092170483105277\n",
            "epoch 15 batch id 3591 loss 0.4606289565563202 train acc 0.9092261904761905\n",
            "epoch 15 batch id 3601 loss 0.21108205616474152 train acc 0.9092743335184671\n",
            "epoch 15 batch id 3611 loss 0.06923864036798477 train acc 0.909313555801717\n",
            "epoch 15 batch id 3621 loss 0.1787458062171936 train acc 0.9093266708091687\n",
            "epoch 15 batch id 3631 loss 0.23384198546409607 train acc 0.9093268039107684\n",
            "epoch 15 batch id 3641 loss 0.3952585756778717 train acc 0.9093355190881626\n",
            "epoch 15 batch id 3651 loss 0.23339645564556122 train acc 0.9093570254724733\n",
            "epoch 15 batch id 3661 loss 0.2280566543340683 train acc 0.9093784143676591\n",
            "epoch 15 batch id 3671 loss 0.3196277320384979 train acc 0.9094252247344048\n",
            "epoch 15 batch id 3681 loss 0.24049724638462067 train acc 0.9094845150774246\n",
            "epoch 15 batch id 3691 loss 0.20442762970924377 train acc 0.9095350176104037\n",
            "epoch 15 batch id 3701 loss 0.1737285703420639 train acc 0.9095599162388543\n",
            "epoch 15 batch id 3711 loss 0.3162820339202881 train acc 0.9095762597682565\n",
            "epoch 15 batch id 3721 loss 0.19302822649478912 train acc 0.9096219094329482\n",
            "epoch 15 batch id 3731 loss 0.25277552008628845 train acc 0.9096296234253551\n",
            "epoch 15 batch id 3741 loss 0.2007935792207718 train acc 0.9096414728682171\n",
            "epoch 15 batch id 3751 loss 0.31833237409591675 train acc 0.9095824446814182\n",
            "epoch 15 batch id 3761 loss 0.20295429229736328 train acc 0.9095735841531507\n",
            "epoch 15 batch id 3771 loss 0.18139001727104187 train acc 0.9095399098382392\n",
            "epoch 15 batch id 3781 loss 0.43210676312446594 train acc 0.9095353411795821\n",
            "epoch 15 batch id 3791 loss 0.15113209187984467 train acc 0.9095761342653653\n",
            "epoch 15 batch id 3801 loss 0.26823246479034424 train acc 0.9096084911865299\n",
            "epoch 15 batch id 3811 loss 0.17508599162101746 train acc 0.9096078785095776\n",
            "epoch 15 batch id 3821 loss 0.21874327957630157 train acc 0.9096236260141324\n",
            "epoch 15 batch id 3831 loss 0.13522233068943024 train acc 0.9096311341686244\n",
            "epoch 15 batch id 3841 loss 0.12084812670946121 train acc 0.9096467391304348\n",
            "epoch 15 batch id 3851 loss 0.13886268436908722 train acc 0.9096825499870164\n",
            "epoch 15 batch id 3861 loss 0.1220594197511673 train acc 0.9097222222222222\n",
            "epoch 15 batch id 3871 loss 0.3821733593940735 train acc 0.909701143115474\n",
            "epoch 15 batch id 3881 loss 0.28757786750793457 train acc 0.9097003027570214\n",
            "epoch 15 batch id 3891 loss 0.1799725741147995 train acc 0.9096914353636597\n",
            "epoch 15 batch id 3901 loss 0.12570835649967194 train acc 0.9096906241989233\n",
            "epoch 15 batch id 3911 loss 0.1159222275018692 train acc 0.9097337637432882\n",
            "epoch 15 batch id 3921 loss 0.11381758004426956 train acc 0.9097089390461617\n",
            "epoch 15 batch id 3931 loss 0.35143744945526123 train acc 0.9096564169422539\n",
            "epoch 15 batch id 3941 loss 0.21046610176563263 train acc 0.9096675970565846\n",
            "epoch 15 batch id 3951 loss 0.21537363529205322 train acc 0.9096431283219438\n",
            "epoch 15 batch id 3961 loss 0.2875952422618866 train acc 0.909626672557435\n",
            "epoch 15 batch id 3971 loss 0.19874706864356995 train acc 0.9096771908839083\n",
            "epoch 15 batch id 3981 loss 0.24792411923408508 train acc 0.9096725069078121\n",
            "epoch 15 batch id 3991 loss 0.4492507576942444 train acc 0.9097030819343523\n",
            "epoch 15 batch id 4001 loss 0.1924109160900116 train acc 0.90968664083979\n",
            "epoch 15 batch id 4011 loss 0.31576037406921387 train acc 0.909627430815258\n",
            "epoch 15 batch id 4021 loss 0.2205989509820938 train acc 0.9095879445411589\n",
            "epoch 15 batch id 4031 loss 0.18720769882202148 train acc 0.9096494356239146\n",
            "epoch 15 batch id 4041 loss 0.3510657846927643 train acc 0.9096526231130908\n",
            "epoch 15 batch id 4051 loss 0.1990364044904709 train acc 0.9096789372994323\n",
            "epoch 15 batch id 4061 loss 0.2654973864555359 train acc 0.9096551034228023\n",
            "epoch 15 batch id 4071 loss 0.5941070318222046 train acc 0.9096544153770573\n",
            "epoch 15 batch id 4081 loss 0.21328862011432648 train acc 0.9097226476353835\n",
            "epoch 15 batch id 4091 loss 0.22091427445411682 train acc 0.9097599914446346\n",
            "epoch 15 batch id 4101 loss 0.3510373830795288 train acc 0.9097285722994392\n",
            "epoch 15 batch id 4111 loss 0.15347953140735626 train acc 0.9096973060082705\n",
            "epoch 15 batch id 4121 loss 0.24241459369659424 train acc 0.9096851492356224\n",
            "epoch 15 batch id 4131 loss 0.12439565360546112 train acc 0.9097070927136287\n",
            "epoch 15 batch id 4141 loss 0.18032965064048767 train acc 0.9097364766964501\n",
            "epoch 15 batch id 4151 loss 0.2698777914047241 train acc 0.9097393700313178\n",
            "epoch 15 batch id 4161 loss 0.28700849413871765 train acc 0.9097309841384282\n",
            "epoch 15 batch id 4171 loss 0.31748270988464355 train acc 0.9097600994965236\n",
            "epoch 15 batch id 4181 loss 0.058890409767627716 train acc 0.9097629155704376\n",
            "epoch 15 batch id 4191 loss 0.4875144362449646 train acc 0.9097880875685994\n",
            "epoch 15 batch id 4201 loss 0.31952959299087524 train acc 0.9098019816710307\n",
            "epoch 15 batch id 4211 loss 0.4143823981285095 train acc 0.9097490204227024\n",
            "epoch 15 batch id 4221 loss 0.19741874933242798 train acc 0.9097111170338782\n",
            "epoch 15 batch id 4231 loss 0.31215789914131165 train acc 0.9097029366580005\n",
            "epoch 15 batch id 4241 loss 0.25718072056770325 train acc 0.9097242690403207\n",
            "epoch 15 batch id 4251 loss 0.2813003659248352 train acc 0.909734474241355\n",
            "epoch 15 batch id 4261 loss 0.30536162853240967 train acc 0.9097519655010561\n",
            "epoch 15 batch id 4271 loss 0.19729073345661163 train acc 0.9097876668227581\n",
            "epoch 15 batch id 4281 loss 0.22640840709209442 train acc 0.9098122518103247\n",
            "epoch 15 batch id 4291 loss 0.13253194093704224 train acc 0.909851287578653\n",
            "epoch 15 batch id 4301 loss 0.11969151347875595 train acc 0.9098392815624273\n",
            "epoch 15 batch id 4311 loss 0.10647036880254745 train acc 0.909878073532823\n",
            "epoch 15 batch id 4321 loss 0.1077384427189827 train acc 0.9098841414024531\n",
            "epoch 15 batch id 4331 loss 0.16990593075752258 train acc 0.909868534980374\n",
            "epoch 15 batch id 4341 loss 0.0830727219581604 train acc 0.9098925938723796\n",
            "epoch 15 batch id 4351 loss 0.2984825372695923 train acc 0.9098411284762123\n",
            "epoch 15 batch id 4361 loss 0.14696979522705078 train acc 0.9098723056638386\n",
            "epoch 15 batch id 4371 loss 0.2392176240682602 train acc 0.909881892015557\n",
            "epoch 15 batch id 4381 loss 0.3007470965385437 train acc 0.909909267290573\n",
            "epoch 15 batch id 4391 loss 0.13952137529850006 train acc 0.9099080505579594\n",
            "epoch 15 batch id 4401 loss 0.28961798548698425 train acc 0.9099068393546921\n",
            "epoch 15 batch id 4411 loss 0.13222676515579224 train acc 0.9099091759238268\n",
            "epoch 15 batch id 4421 loss 0.34533947706222534 train acc 0.9099150361909071\n",
            "epoch 15 batch id 4431 loss 0.3011242747306824 train acc 0.9099138174227037\n",
            "epoch 15 batch id 4441 loss 0.04285711422562599 train acc 0.909937232605269\n",
            "epoch 15 batch id 4451 loss 0.3067903518676758 train acc 0.9099394798921591\n",
            "epoch 15 batch id 4461 loss 0.10673648864030838 train acc 0.9099627325711723\n",
            "epoch 15 batch id 4471 loss 0.40042945742607117 train acc 0.909940449563856\n",
            "epoch 15 batch id 4481 loss 0.25459712743759155 train acc 0.9099112921222942\n",
            "epoch 15 batch id 4491 loss 0.22918078303337097 train acc 0.9099240146960588\n",
            "epoch 15 batch id 4501 loss 0.09498704969882965 train acc 0.9099401521884026\n",
            "epoch 15 batch id 4511 loss 0.3758664131164551 train acc 0.9099181168255376\n",
            "epoch 15 batch id 4521 loss 0.29556742310523987 train acc 0.9099065472240655\n",
            "epoch 15 batch id 4531 loss 0.2794428765773773 train acc 0.9098570955638932\n",
            "epoch 15 batch id 4541 loss 0.1717585176229477 train acc 0.9098973243778903\n",
            "epoch 15 batch id 4551 loss 0.4064639210700989 train acc 0.9099167765326301\n",
            "epoch 15 batch id 4561 loss 0.3066573441028595 train acc 0.9099395691734269\n",
            "epoch 15 batch id 4571 loss 0.2203163355588913 train acc 0.9099725169547145\n",
            "epoch 15 batch id 4581 loss 0.19672030210494995 train acc 0.9099950884086444\n",
            "epoch 15 batch id 4591 loss 0.2790524959564209 train acc 0.9099835275539099\n",
            "epoch 15 batch id 4601 loss 0.18990148603916168 train acc 0.910022956965877\n",
            "epoch 15 batch id 4611 loss 0.22135625779628754 train acc 0.9100622153545869\n",
            "epoch 15 batch id 4621 loss 0.1195896565914154 train acc 0.9101013038303397\n",
            "epoch 15 batch id 4631 loss 0.26201239228248596 train acc 0.9101064834808896\n",
            "epoch 15 batch id 4641 loss 0.2689993679523468 train acc 0.9101150075414781\n",
            "epoch 15 batch id 4651 loss 0.3878519535064697 train acc 0.9101201354547409\n",
            "epoch 15 batch id 4661 loss 0.22511237859725952 train acc 0.910125241364514\n",
            "epoch 15 batch id 4671 loss 0.2053215354681015 train acc 0.9101838471419397\n",
            "epoch 15 batch id 4681 loss 0.5015139579772949 train acc 0.9101954710531938\n",
            "epoch 15 batch id 4691 loss 0.16635344922542572 train acc 0.9101970528671925\n",
            "epoch 15 batch id 4701 loss 0.3927971124649048 train acc 0.9102085992342055\n",
            "epoch 15 batch id 4711 loss 0.0846281349658966 train acc 0.9102598970494588\n",
            "epoch 15 batch id 4721 loss 0.1865471452474594 train acc 0.9102580226646897\n",
            "epoch 15 batch id 4731 loss 0.07401716709136963 train acc 0.9102660642570282\n",
            "epoch 15 batch id 4741 loss 0.2203628271818161 train acc 0.9103070291077832\n",
            "epoch 15 batch id 4751 loss 0.3210102617740631 train acc 0.9103116449168596\n",
            "epoch 15 batch id 4761 loss 0.18507950007915497 train acc 0.9103162413358538\n",
            "epoch 15 batch id 4771 loss 0.1726321429014206 train acc 0.9103208184866904\n",
            "epoch 15 batch id 4781 loss 0.3553794324398041 train acc 0.9102894268981384\n",
            "epoch 15 batch id 4791 loss 0.3727027475833893 train acc 0.9102679503235233\n",
            "epoch 15 batch id 4801 loss 0.21454855799674988 train acc 0.910233545094772\n",
            "epoch 15 batch id 4811 loss 0.12397526949644089 train acc 0.9102772292662649\n",
            "epoch 15 batch id 4821 loss 0.295854389667511 train acc 0.910275357809583\n",
            "epoch 15 batch id 4831 loss 0.24537953734397888 train acc 0.9102961343407162\n",
            "epoch 15 batch id 4841 loss 0.33859533071517944 train acc 0.910297459202644\n",
            "epoch 15 batch id 4851 loss 0.22400997579097748 train acc 0.9103052205730777\n",
            "epoch 15 batch id 4861 loss 0.2850259244441986 train acc 0.9102968782143592\n",
            "epoch 15 batch id 4871 loss 0.31902435421943665 train acc 0.9103078166700883\n",
            "epoch 15 batch id 4881 loss 0.21183687448501587 train acc 0.9103123079287031\n",
            "epoch 15 batch id 4891 loss 0.3070710003376007 train acc 0.9102976129625844\n",
            "epoch 15 batch id 4901 loss 0.31938791275024414 train acc 0.9103339879616404\n",
            "epoch 15 batch id 4911 loss 0.26188918948173523 train acc 0.9103574882915904\n",
            "epoch 15 batch id 4921 loss 0.25330567359924316 train acc 0.9103554917699654\n",
            "epoch 15 batch id 4931 loss 0.25751248002052307 train acc 0.9103661782599878\n",
            "epoch 15 batch id 4941 loss 0.22220642864704132 train acc 0.9103388737097754\n",
            "epoch 15 batch id 4951 loss 0.2590014636516571 train acc 0.9103527065239345\n",
            "epoch 15 train acc 0.9103890045298658\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f3d38496f9e4bce8deeb72772cb6c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 15 loss 0.8640744686126709 test acc 0.8055386271994135\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5294be913d2e4cf4a9a6e0f752e7d99d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 16 batch id 1 loss 0.2890709936618805 train acc 0.921875\n",
            "epoch 16 batch id 11 loss 0.11567381024360657 train acc 0.9303977272727273\n",
            "epoch 16 batch id 21 loss 0.19090847671031952 train acc 0.9263392857142857\n",
            "epoch 16 batch id 31 loss 0.36172550916671753 train acc 0.9193548387096774\n",
            "epoch 16 batch id 41 loss 0.16679808497428894 train acc 0.9203506097560976\n",
            "epoch 16 batch id 51 loss 0.2347041666507721 train acc 0.9191176470588235\n",
            "epoch 16 batch id 61 loss 0.24896657466888428 train acc 0.9152151639344263\n",
            "epoch 16 batch id 71 loss 0.3583942651748657 train acc 0.9170334507042254\n",
            "epoch 16 batch id 81 loss 0.3157888352870941 train acc 0.9164737654320988\n",
            "epoch 16 batch id 91 loss 0.20362381637096405 train acc 0.9160370879120879\n",
            "epoch 16 batch id 101 loss 0.28721287846565247 train acc 0.9142945544554455\n",
            "epoch 16 batch id 111 loss 0.2010195255279541 train acc 0.9138513513513513\n",
            "epoch 16 batch id 121 loss 0.20218715071678162 train acc 0.9134814049586777\n",
            "epoch 16 batch id 131 loss 0.15869462490081787 train acc 0.9141221374045801\n",
            "epoch 16 batch id 141 loss 0.19394166767597198 train acc 0.9131205673758865\n",
            "epoch 16 batch id 151 loss 0.22132882475852966 train acc 0.9137003311258278\n",
            "epoch 16 batch id 161 loss 0.41730204224586487 train acc 0.9148874223602484\n",
            "epoch 16 batch id 171 loss 0.3511286973953247 train acc 0.914656432748538\n",
            "epoch 16 batch id 181 loss 0.18952400982379913 train acc 0.9150552486187845\n",
            "epoch 16 batch id 191 loss 0.1970641165971756 train acc 0.9145942408376964\n",
            "epoch 16 batch id 201 loss 0.3075471818447113 train acc 0.9151119402985075\n",
            "epoch 16 batch id 211 loss 0.272757351398468 train acc 0.9152103080568721\n",
            "epoch 16 batch id 221 loss 0.33170631527900696 train acc 0.9150876696832579\n",
            "epoch 16 batch id 231 loss 0.16706672310829163 train acc 0.9154491341991342\n",
            "epoch 16 batch id 241 loss 0.22302788496017456 train acc 0.9146135892116183\n",
            "epoch 16 batch id 251 loss 0.19619545340538025 train acc 0.9145293824701195\n",
            "epoch 16 batch id 261 loss 0.24908806383609772 train acc 0.9147509578544061\n",
            "epoch 16 batch id 271 loss 0.13411901891231537 train acc 0.914494926199262\n",
            "epoch 16 batch id 281 loss 0.08929160237312317 train acc 0.9143683274021353\n",
            "epoch 16 batch id 291 loss 0.5500037670135498 train acc 0.915002147766323\n",
            "epoch 16 batch id 301 loss 0.2838718593120575 train acc 0.9150228405315615\n",
            "epoch 16 batch id 311 loss 0.19333253800868988 train acc 0.9151929260450161\n",
            "epoch 16 batch id 321 loss 0.25155171751976013 train acc 0.9149143302180686\n",
            "epoch 16 batch id 331 loss 0.2601110339164734 train acc 0.9150302114803626\n",
            "epoch 16 batch id 341 loss 0.4047512412071228 train acc 0.9150476539589443\n",
            "epoch 16 batch id 351 loss 0.32166242599487305 train acc 0.9148415242165242\n",
            "epoch 16 batch id 361 loss 0.2724933922290802 train acc 0.914733379501385\n",
            "epoch 16 batch id 371 loss 0.16992640495300293 train acc 0.9150522237196765\n",
            "epoch 16 batch id 381 loss 0.07372579723596573 train acc 0.9152312992125984\n",
            "epoch 16 batch id 391 loss 0.33194276690483093 train acc 0.9152014066496164\n",
            "epoch 16 batch id 401 loss 0.17191356420516968 train acc 0.9148612842892768\n",
            "epoch 16 batch id 411 loss 0.14796829223632812 train acc 0.9154501216545012\n",
            "epoch 16 batch id 421 loss 0.3004648983478546 train acc 0.9151202494061758\n",
            "epoch 16 batch id 431 loss 0.24309003353118896 train acc 0.9152407192575406\n",
            "epoch 16 batch id 441 loss 0.2426268607378006 train acc 0.9156037414965986\n",
            "epoch 16 batch id 451 loss 0.2876608967781067 train acc 0.9158120842572062\n",
            "epoch 16 batch id 461 loss 0.1628221720457077 train acc 0.9155029826464208\n",
            "epoch 16 batch id 471 loss 0.21184560656547546 train acc 0.916135881104034\n",
            "epoch 16 batch id 481 loss 0.49308785796165466 train acc 0.9163526507276507\n",
            "epoch 16 batch id 491 loss 0.37872374057769775 train acc 0.9164332993890021\n",
            "epoch 16 batch id 501 loss 0.39459311962127686 train acc 0.9162924151696606\n",
            "epoch 16 batch id 511 loss 0.27121421694755554 train acc 0.9163405088062623\n",
            "epoch 16 batch id 521 loss 0.3403055667877197 train acc 0.9161168426103646\n",
            "epoch 16 batch id 531 loss 0.25389283895492554 train acc 0.9159016007532956\n",
            "epoch 16 batch id 541 loss 0.27085068821907043 train acc 0.9158964879852126\n",
            "epoch 16 batch id 551 loss 0.1594790518283844 train acc 0.9158064882032668\n",
            "epoch 16 batch id 561 loss 0.06119683012366295 train acc 0.915719696969697\n",
            "epoch 16 batch id 571 loss 0.4973321855068207 train acc 0.9158001313485113\n",
            "epoch 16 batch id 581 loss 0.28590264916419983 train acc 0.9157702237521514\n",
            "epoch 16 batch id 591 loss 0.12472882121801376 train acc 0.9160057106598984\n",
            "epoch 16 batch id 601 loss 0.17948658764362335 train acc 0.9162073627287853\n",
            "epoch 16 batch id 611 loss 0.25860512256622314 train acc 0.9159676759410802\n",
            "epoch 16 batch id 621 loss 0.23137764632701874 train acc 0.9161131239935588\n",
            "epoch 16 batch id 631 loss 0.2661823332309723 train acc 0.9158082408874801\n",
            "epoch 16 batch id 641 loss 0.2777549624443054 train acc 0.9157810062402496\n",
            "epoch 16 batch id 651 loss 0.2907024621963501 train acc 0.9157066052227343\n",
            "epoch 16 batch id 661 loss 0.09622689336538315 train acc 0.916036308623298\n",
            "epoch 16 batch id 671 loss 0.2334870994091034 train acc 0.9161233233979136\n",
            "epoch 16 batch id 681 loss 0.259581595659256 train acc 0.915886563876652\n",
            "epoch 16 batch id 691 loss 0.32056158781051636 train acc 0.9159506150506512\n",
            "epoch 16 batch id 701 loss 0.19727541506290436 train acc 0.9160351283880172\n",
            "epoch 16 batch id 711 loss 0.28866612911224365 train acc 0.9162491209563994\n",
            "epoch 16 batch id 721 loss 0.11569642275571823 train acc 0.9163271497919556\n",
            "epoch 16 batch id 731 loss 0.19584296643733978 train acc 0.9164457934336525\n",
            "epoch 16 batch id 741 loss 0.27071982622146606 train acc 0.9164558029689609\n",
            "epoch 16 batch id 751 loss 0.32738015055656433 train acc 0.9163407123834887\n",
            "epoch 16 batch id 761 loss 0.061138689517974854 train acc 0.9165776938239159\n",
            "epoch 16 batch id 771 loss 0.4052111506462097 train acc 0.9162410830090791\n",
            "epoch 16 batch id 781 loss 0.23137742280960083 train acc 0.9163532330345711\n",
            "epoch 16 batch id 791 loss 0.14945024251937866 train acc 0.916442793931732\n",
            "epoch 16 batch id 801 loss 0.1703103482723236 train acc 0.9165496254681648\n",
            "epoch 16 batch id 811 loss 0.44166621565818787 train acc 0.9165189580764488\n",
            "epoch 16 batch id 821 loss 0.08209749311208725 train acc 0.916603227771011\n",
            "epoch 16 batch id 831 loss 0.40782248973846436 train acc 0.9166478640192539\n",
            "epoch 16 batch id 841 loss 0.150186225771904 train acc 0.9168400713436385\n",
            "epoch 16 batch id 851 loss 0.21074584126472473 train acc 0.9169175969447708\n",
            "epoch 16 batch id 861 loss 0.22048768401145935 train acc 0.9169933217189314\n",
            "epoch 16 batch id 871 loss 0.1724727749824524 train acc 0.916923794489093\n",
            "epoch 16 batch id 881 loss 0.19240468740463257 train acc 0.9169799943246311\n",
            "epoch 16 batch id 891 loss 0.14050361514091492 train acc 0.917368125701459\n",
            "epoch 16 batch id 901 loss 0.3747088313102722 train acc 0.9171233351831298\n",
            "epoch 16 batch id 911 loss 0.4151758551597595 train acc 0.9170554335894622\n",
            "epoch 16 batch id 921 loss 0.27510279417037964 train acc 0.9170738327904452\n",
            "epoch 16 batch id 931 loss 0.16561740636825562 train acc 0.9170750537056928\n",
            "epoch 16 batch id 941 loss 0.13343806564807892 train acc 0.9172921094580234\n",
            "epoch 16 batch id 951 loss 0.15884597599506378 train acc 0.9172910094637224\n",
            "epoch 16 batch id 961 loss 0.19098630547523499 train acc 0.9172736732570239\n",
            "epoch 16 batch id 971 loss 0.22997692227363586 train acc 0.9174015190525232\n",
            "epoch 16 batch id 981 loss 0.34996166825294495 train acc 0.9176223241590215\n",
            "epoch 16 batch id 991 loss 0.33457833528518677 train acc 0.9177598385469223\n",
            "epoch 16 batch id 1001 loss 0.4686783254146576 train acc 0.9178321678321678\n",
            "epoch 16 batch id 1011 loss 0.2402791529893875 train acc 0.9177639713155292\n",
            "epoch 16 batch id 1021 loss 0.1711856722831726 train acc 0.9179419686581782\n",
            "epoch 16 batch id 1031 loss 0.2187713235616684 train acc 0.9180710475266731\n",
            "epoch 16 batch id 1041 loss 0.20965874195098877 train acc 0.9181075888568684\n",
            "epoch 16 batch id 1051 loss 0.1615162491798401 train acc 0.9179947668886774\n",
            "epoch 16 batch id 1061 loss 0.3087499439716339 train acc 0.9179135249764373\n",
            "epoch 16 batch id 1071 loss 0.041580408811569214 train acc 0.9179651027077498\n",
            "epoch 16 batch id 1081 loss 0.297968327999115 train acc 0.9175965541165587\n",
            "epoch 16 batch id 1091 loss 0.4592200815677643 train acc 0.9174352658111824\n",
            "epoch 16 batch id 1101 loss 0.22890818119049072 train acc 0.9175323569482289\n",
            "epoch 16 batch id 1111 loss 0.40458354353904724 train acc 0.9173886138613861\n",
            "epoch 16 batch id 1121 loss 0.15515534579753876 train acc 0.9174425735950045\n",
            "epoch 16 batch id 1131 loss 0.30008357763290405 train acc 0.9173850574712644\n",
            "epoch 16 batch id 1141 loss 0.26427385210990906 train acc 0.9175339614373357\n",
            "epoch 16 batch id 1151 loss 0.23318688571453094 train acc 0.9174766507384883\n",
            "epoch 16 batch id 1161 loss 0.13603904843330383 train acc 0.9174068690783808\n",
            "epoch 16 batch id 1171 loss 0.20504555106163025 train acc 0.9172715627668659\n",
            "epoch 16 batch id 1181 loss 0.23576198518276215 train acc 0.9172576206604572\n",
            "epoch 16 batch id 1191 loss 0.1630934625864029 train acc 0.91741446263644\n",
            "epoch 16 batch id 1201 loss 0.37099936604499817 train acc 0.9175556827643631\n",
            "epoch 16 batch id 1211 loss 0.09890395402908325 train acc 0.9175913501238646\n",
            "epoch 16 batch id 1221 loss 0.2342887669801712 train acc 0.9176904176904177\n",
            "epoch 16 batch id 1231 loss 0.21334649622440338 train acc 0.9175720958570268\n",
            "epoch 16 batch id 1241 loss 0.024065721780061722 train acc 0.9175815874294924\n",
            "epoch 16 batch id 1251 loss 0.211762472987175 train acc 0.9176533772981614\n",
            "epoch 16 batch id 1261 loss 0.3132907450199127 train acc 0.917699246629659\n",
            "epoch 16 batch id 1271 loss 0.21112966537475586 train acc 0.9177566876475216\n",
            "epoch 16 batch id 1281 loss 0.27634117007255554 train acc 0.9178498243559718\n",
            "epoch 16 batch id 1291 loss 0.23629263043403625 train acc 0.9179173121611154\n",
            "epoch 16 batch id 1301 loss 0.3240370750427246 train acc 0.9178756725595696\n",
            "epoch 16 batch id 1311 loss 0.3206615149974823 train acc 0.9178585049580473\n",
            "epoch 16 batch id 1321 loss 0.13372249901294708 train acc 0.9178652535957608\n",
            "epoch 16 batch id 1331 loss 0.23149250447750092 train acc 0.9178953794139745\n",
            "epoch 16 batch id 1341 loss 0.3333294093608856 train acc 0.9178434936614467\n",
            "epoch 16 batch id 1351 loss 0.08286980539560318 train acc 0.9178270725388601\n",
            "epoch 16 batch id 1361 loss 0.18363668024539948 train acc 0.9178682953710507\n",
            "epoch 16 batch id 1371 loss 0.13552221655845642 train acc 0.9178063457330415\n",
            "epoch 16 batch id 1381 loss 0.19602739810943604 train acc 0.9178471216509776\n",
            "epoch 16 batch id 1391 loss 0.10698600113391876 train acc 0.9178536125089863\n",
            "epoch 16 batch id 1401 loss 0.276553750038147 train acc 0.917848857958601\n",
            "epoch 16 batch id 1411 loss 0.12716293334960938 train acc 0.9179438341601701\n",
            "epoch 16 batch id 1421 loss 0.12651583552360535 train acc 0.9179275158339197\n",
            "epoch 16 batch id 1431 loss 0.12911850214004517 train acc 0.91791142557652\n",
            "epoch 16 batch id 1441 loss 0.288098007440567 train acc 0.9179497744621791\n",
            "epoch 16 batch id 1451 loss 0.20109307765960693 train acc 0.9180629738111648\n",
            "epoch 16 batch id 1461 loss 0.14139337837696075 train acc 0.9179821184120466\n",
            "epoch 16 batch id 1471 loss 0.29425233602523804 train acc 0.9180723147518695\n",
            "epoch 16 batch id 1481 loss 0.12682439386844635 train acc 0.9180346893990546\n",
            "epoch 16 batch id 1491 loss 0.32268866896629333 train acc 0.9180394869215291\n",
            "epoch 16 batch id 1501 loss 0.20921234786510468 train acc 0.9182107761492339\n",
            "epoch 16 batch id 1511 loss 0.165964275598526 train acc 0.9182557081403044\n",
            "epoch 16 batch id 1521 loss 0.16232222318649292 train acc 0.9183308678500987\n",
            "epoch 16 batch id 1531 loss 0.13899990916252136 train acc 0.9184458687132593\n",
            "epoch 16 batch id 1541 loss 0.23009249567985535 train acc 0.9183464471122648\n",
            "epoch 16 batch id 1551 loss 0.06208959221839905 train acc 0.9183188265635074\n",
            "epoch 16 batch id 1561 loss 0.26410624384880066 train acc 0.9183716367713004\n",
            "epoch 16 batch id 1571 loss 0.20064547657966614 train acc 0.9184536123488224\n",
            "epoch 16 batch id 1581 loss 0.13377569615840912 train acc 0.9184851359898798\n",
            "epoch 16 batch id 1591 loss 0.24417747557163239 train acc 0.9185260842237586\n",
            "epoch 16 batch id 1601 loss 0.4986238479614258 train acc 0.9185079637726421\n",
            "epoch 16 batch id 1611 loss 0.31834566593170166 train acc 0.9184706703910615\n",
            "epoch 16 batch id 1621 loss 0.1348700076341629 train acc 0.9184434762492288\n",
            "epoch 16 batch id 1631 loss 0.21612949669361115 train acc 0.9184549356223176\n",
            "epoch 16 batch id 1641 loss 0.2340448796749115 train acc 0.9184567336989641\n",
            "epoch 16 batch id 1651 loss 0.17999055981636047 train acc 0.918411190187765\n",
            "epoch 16 batch id 1661 loss 0.32704398036003113 train acc 0.9183756020469597\n",
            "epoch 16 batch id 1671 loss 0.2572658360004425 train acc 0.9184152453620587\n",
            "epoch 16 batch id 1681 loss 0.3189854919910431 train acc 0.9182685157644259\n",
            "epoch 16 batch id 1691 loss 0.2694533169269562 train acc 0.918317563571851\n",
            "epoch 16 batch id 1701 loss 0.2803958058357239 train acc 0.9183292915931804\n",
            "epoch 16 batch id 1711 loss 0.20555560290813446 train acc 0.9182586937463472\n",
            "epoch 16 batch id 1721 loss 0.1542923003435135 train acc 0.9183069436374202\n",
            "epoch 16 batch id 1731 loss 0.14501389861106873 train acc 0.9183546360485269\n",
            "epoch 16 batch id 1741 loss 0.2050066590309143 train acc 0.9183210080413555\n",
            "epoch 16 batch id 1751 loss 0.23761211335659027 train acc 0.9182788406624786\n",
            "epoch 16 batch id 1761 loss 0.25832870602607727 train acc 0.9183613713798978\n",
            "epoch 16 batch id 1771 loss 0.19038811326026917 train acc 0.9184341473743648\n",
            "epoch 16 batch id 1781 loss 0.11365804076194763 train acc 0.9184973329590118\n",
            "epoch 16 batch id 1791 loss 0.5452600121498108 train acc 0.9185249162479062\n",
            "epoch 16 batch id 1801 loss 0.2824099659919739 train acc 0.9186215991116047\n",
            "epoch 16 batch id 1811 loss 0.10923881828784943 train acc 0.91868270292656\n",
            "epoch 16 batch id 1821 loss 0.18426614999771118 train acc 0.9186744920373421\n",
            "epoch 16 batch id 1831 loss 0.2427581548690796 train acc 0.9186919716002184\n",
            "epoch 16 batch id 1841 loss 0.18357330560684204 train acc 0.9186074144486692\n",
            "epoch 16 batch id 1851 loss 0.22159436345100403 train acc 0.9185575364667747\n",
            "epoch 16 batch id 1861 loss 0.2504340410232544 train acc 0.9185585706609349\n",
            "epoch 16 batch id 1871 loss 0.49091681838035583 train acc 0.9185846472474613\n",
            "epoch 16 batch id 1881 loss 0.17701607942581177 train acc 0.9186187533227007\n",
            "epoch 16 batch id 1891 loss 0.2555461823940277 train acc 0.9187599153886833\n",
            "epoch 16 batch id 1901 loss 0.3785874843597412 train acc 0.9187516438716465\n",
            "epoch 16 batch id 1911 loss 0.14196732640266418 train acc 0.9187843406593407\n",
            "epoch 16 batch id 1921 loss 0.3379971385002136 train acc 0.9186865564809995\n",
            "epoch 16 batch id 1931 loss 0.3554309904575348 train acc 0.9187273433454168\n",
            "epoch 16 batch id 1941 loss 0.16789579391479492 train acc 0.9187033101494075\n",
            "epoch 16 batch id 1951 loss 0.20896606147289276 train acc 0.9187516017426961\n",
            "epoch 16 batch id 1961 loss 0.14805743098258972 train acc 0.9186798827129016\n",
            "epoch 16 batch id 1971 loss 0.23112456500530243 train acc 0.9187595129375952\n",
            "epoch 16 batch id 1981 loss 0.2179879993200302 train acc 0.9187989020696617\n",
            "epoch 16 batch id 1991 loss 0.2795460820198059 train acc 0.9187358739326972\n",
            "epoch 16 batch id 2001 loss 0.3557853102684021 train acc 0.9187437531234383\n",
            "epoch 16 batch id 2011 loss 0.22852154076099396 train acc 0.9187127051218299\n",
            "epoch 16 batch id 2021 loss 0.3982734680175781 train acc 0.9186510390895596\n",
            "epoch 16 batch id 2031 loss 0.1655685156583786 train acc 0.918659219596258\n",
            "epoch 16 batch id 2041 loss 0.11911315470933914 train acc 0.9185831087702107\n",
            "epoch 16 batch id 2051 loss 0.29014620184898376 train acc 0.9185991589468552\n",
            "epoch 16 batch id 2061 loss 0.21786853671073914 train acc 0.9185847282872392\n",
            "epoch 16 batch id 2071 loss 0.18931171298027039 train acc 0.9185327136648962\n",
            "epoch 16 batch id 2081 loss 0.3242540657520294 train acc 0.9185187409899087\n",
            "epoch 16 batch id 2091 loss 0.16054104268550873 train acc 0.9184376494500239\n",
            "epoch 16 batch id 2101 loss 0.1364450454711914 train acc 0.9184316991908615\n",
            "epoch 16 batch id 2111 loss 0.20166781544685364 train acc 0.9184480104216012\n",
            "epoch 16 batch id 2121 loss 0.1409113109111786 train acc 0.9184568010372466\n",
            "epoch 16 batch id 2131 loss 0.2964737117290497 train acc 0.9184655091506335\n",
            "epoch 16 batch id 2141 loss 0.16560013592243195 train acc 0.9184230499766465\n",
            "epoch 16 batch id 2151 loss 0.4052640497684479 train acc 0.9184318340306834\n",
            "epoch 16 batch id 2161 loss 0.28765639662742615 train acc 0.9184043845441925\n",
            "epoch 16 batch id 2171 loss 0.0989295095205307 train acc 0.918499539382773\n",
            "epoch 16 batch id 2181 loss 0.2933293581008911 train acc 0.9184147180192572\n",
            "epoch 16 batch id 2191 loss 0.16599710285663605 train acc 0.9184590369694203\n",
            "epoch 16 batch id 2201 loss 0.2219540923833847 train acc 0.918424863698319\n",
            "epoch 16 batch id 2211 loss 0.1356474906206131 train acc 0.9183697987336047\n",
            "epoch 16 batch id 2221 loss 0.2130039632320404 train acc 0.9184137212967132\n",
            "epoch 16 batch id 2231 loss 0.1742563545703888 train acc 0.9184782608695652\n",
            "epoch 16 batch id 2241 loss 0.21453268826007843 train acc 0.9185003904506917\n",
            "epoch 16 batch id 2251 loss 0.0911872386932373 train acc 0.9184112616614838\n",
            "epoch 16 batch id 2261 loss 0.13754211366176605 train acc 0.9184680451127819\n",
            "epoch 16 batch id 2271 loss 0.26545679569244385 train acc 0.9184555261999119\n",
            "epoch 16 batch id 2281 loss 0.30585646629333496 train acc 0.9185664182376151\n",
            "epoch 16 batch id 2291 loss 0.1474810689687729 train acc 0.9186422413793104\n",
            "epoch 16 batch id 2301 loss 0.16667675971984863 train acc 0.9186902433724468\n",
            "epoch 16 batch id 2311 loss 0.3175782561302185 train acc 0.9187378299437473\n",
            "epoch 16 batch id 2321 loss 0.17711475491523743 train acc 0.9187513464024127\n",
            "epoch 16 batch id 2331 loss 0.20879192650318146 train acc 0.918717824967825\n",
            "epoch 16 batch id 2341 loss 0.312197208404541 train acc 0.9186912644169158\n",
            "epoch 16 batch id 2351 loss 0.2129746824502945 train acc 0.9186981603572948\n",
            "epoch 16 batch id 2361 loss 0.1863613873720169 train acc 0.9187182337992376\n",
            "epoch 16 batch id 2371 loss 0.09943881630897522 train acc 0.9188040383804302\n",
            "epoch 16 batch id 2381 loss 0.0805191695690155 train acc 0.918830060898782\n",
            "epoch 16 batch id 2391 loss 0.2432669848203659 train acc 0.9188558657465495\n",
            "epoch 16 batch id 2401 loss 0.21357658505439758 train acc 0.9188228862973761\n",
            "epoch 16 batch id 2411 loss 0.15431980788707733 train acc 0.9188614682704273\n",
            "epoch 16 batch id 2421 loss 0.10781866312026978 train acc 0.9188803696819496\n",
            "epoch 16 batch id 2431 loss 0.15067856013774872 train acc 0.9188541238173591\n",
            "epoch 16 batch id 2441 loss 0.17740961909294128 train acc 0.9188664993854977\n",
            "epoch 16 batch id 2451 loss 0.11439625173807144 train acc 0.9188723990208079\n",
            "epoch 16 batch id 2461 loss 0.18921634554862976 train acc 0.9188338073953677\n",
            "epoch 16 batch id 2471 loss 0.4375356435775757 train acc 0.918814498178875\n",
            "epoch 16 batch id 2481 loss 0.2126292735338211 train acc 0.9188646211205159\n",
            "epoch 16 batch id 2491 loss 0.2915492057800293 train acc 0.9188265254917704\n",
            "epoch 16 batch id 2501 loss 0.10667434334754944 train acc 0.9187824870051979\n",
            "epoch 16 batch id 2511 loss 0.2382183074951172 train acc 0.9188508064516129\n",
            "epoch 16 batch id 2521 loss 0.348226934671402 train acc 0.918732645775486\n",
            "epoch 16 batch id 2531 loss 0.2702513039112091 train acc 0.9187388877913868\n",
            "epoch 16 batch id 2541 loss 0.3368019759654999 train acc 0.918732782369146\n",
            "epoch 16 batch id 2551 loss 0.18234498798847198 train acc 0.9187941003528028\n",
            "epoch 16 batch id 2561 loss 0.24859531223773956 train acc 0.9188061304178056\n",
            "epoch 16 batch id 2571 loss 0.24793514609336853 train acc 0.9188423765071957\n",
            "epoch 16 batch id 2581 loss 0.17836284637451172 train acc 0.9188178031770632\n",
            "epoch 16 batch id 2591 loss 0.17375311255455017 train acc 0.9187451756078734\n",
            "epoch 16 batch id 2601 loss 0.4135594069957733 train acc 0.9187451941560938\n",
            "epoch 16 batch id 2611 loss 0.24986961483955383 train acc 0.9187811183454615\n",
            "epoch 16 batch id 2621 loss 0.15800578892230988 train acc 0.9187571537581076\n",
            "epoch 16 batch id 2631 loss 0.49581992626190186 train acc 0.9186977385024705\n",
            "epoch 16 batch id 2641 loss 0.1664476990699768 train acc 0.9186979363877319\n",
            "epoch 16 batch id 2651 loss 0.06395043432712555 train acc 0.9187865428140325\n",
            "epoch 16 batch id 2661 loss 0.2251472920179367 train acc 0.9188568677189026\n",
            "epoch 16 batch id 2671 loss 0.2824589014053345 train acc 0.9188330681392737\n",
            "epoch 16 batch id 2681 loss 0.1498841941356659 train acc 0.918733681462141\n",
            "epoch 16 batch id 2691 loss 0.19243407249450684 train acc 0.9187337421033073\n",
            "epoch 16 batch id 2701 loss 0.2517596483230591 train acc 0.9187858663457978\n",
            "epoch 16 batch id 2711 loss 0.1637483686208725 train acc 0.9187338620435264\n",
            "epoch 16 batch id 2721 loss 0.17560787498950958 train acc 0.9187511484748254\n",
            "epoch 16 batch id 2731 loss 0.15612928569316864 train acc 0.9187225375320396\n",
            "epoch 16 batch id 2741 loss 0.2802973687648773 train acc 0.9187226377234586\n",
            "epoch 16 batch id 2751 loss 0.23103158175945282 train acc 0.9186659396583061\n",
            "epoch 16 batch id 2761 loss 0.06137057766318321 train acc 0.9187228359290113\n",
            "epoch 16 batch id 2771 loss 0.30267077684402466 train acc 0.9186834626488632\n",
            "epoch 16 batch id 2781 loss 0.17830222845077515 train acc 0.9187679791441927\n",
            "epoch 16 batch id 2791 loss 0.2639329731464386 train acc 0.9187847097814403\n",
            "epoch 16 batch id 2801 loss 0.17728032171726227 train acc 0.9187957425919314\n",
            "epoch 16 batch id 2811 loss 0.22283270955085754 train acc 0.918806696905016\n",
            "epoch 16 batch id 2821 loss 0.2679414749145508 train acc 0.9187898794753634\n",
            "epoch 16 batch id 2831 loss 0.15106503665447235 train acc 0.9188283733663016\n",
            "epoch 16 batch id 2841 loss 0.19174860417842865 train acc 0.918888595564942\n",
            "epoch 16 batch id 2851 loss 0.17867693305015564 train acc 0.9188442651701157\n",
            "epoch 16 batch id 2861 loss 0.15391892194747925 train acc 0.9188548584411045\n",
            "epoch 16 batch id 2871 loss 0.2557139992713928 train acc 0.9189089167537443\n",
            "epoch 16 batch id 2881 loss 0.16998204588890076 train acc 0.9188975182228393\n",
            "epoch 16 batch id 2891 loss 0.1506180614233017 train acc 0.9189078173642339\n",
            "epoch 16 batch id 2901 loss 0.18537317216396332 train acc 0.9188965012064805\n",
            "epoch 16 batch id 2911 loss 0.11177901923656464 train acc 0.9188959979388527\n",
            "epoch 16 batch id 2921 loss 0.14377278089523315 train acc 0.9188901489216021\n",
            "epoch 16 batch id 2931 loss 0.1374495029449463 train acc 0.9189269873763221\n",
            "epoch 16 batch id 2941 loss 0.15130075812339783 train acc 0.9189157599455967\n",
            "epoch 16 batch id 2951 loss 0.22158801555633545 train acc 0.9189999152829549\n",
            "epoch 16 batch id 2961 loss 0.1246243342757225 train acc 0.9190940560621411\n",
            "epoch 16 batch id 2971 loss 0.24233277142047882 train acc 0.9190876388421407\n",
            "epoch 16 batch id 2981 loss 0.2937401235103607 train acc 0.9190760231465951\n",
            "epoch 16 batch id 2991 loss 0.19559034705162048 train acc 0.9191010531594784\n",
            "epoch 16 batch id 3001 loss 0.2576907277107239 train acc 0.9191207097634122\n",
            "epoch 16 batch id 3011 loss 0.2186513990163803 train acc 0.9191246678844238\n",
            "epoch 16 batch id 3021 loss 0.16298064589500427 train acc 0.9191648047004303\n",
            "epoch 16 batch id 3031 loss 0.31945887207984924 train acc 0.9191221956450016\n",
            "epoch 16 batch id 3041 loss 0.2924093008041382 train acc 0.919100419269977\n",
            "epoch 16 batch id 3051 loss 0.22649787366390228 train acc 0.9191146345460505\n",
            "epoch 16 batch id 3061 loss 0.20982681214809418 train acc 0.9191236524011761\n",
            "epoch 16 batch id 3071 loss 0.09951405227184296 train acc 0.9191275236079453\n",
            "epoch 16 batch id 3081 loss 0.32975590229034424 train acc 0.9191668695228822\n",
            "epoch 16 batch id 3091 loss 0.1708291471004486 train acc 0.919160465868651\n",
            "epoch 16 batch id 3101 loss 0.26347672939300537 train acc 0.919118832634634\n",
            "epoch 16 batch id 3111 loss 0.20739611983299255 train acc 0.9191076020572163\n",
            "epoch 16 batch id 3121 loss 0.18941539525985718 train acc 0.9191465075296379\n",
            "epoch 16 batch id 3131 loss 0.13278928399085999 train acc 0.9192200974129671\n",
            "epoch 16 batch id 3141 loss 0.2286626696586609 train acc 0.9192335243553008\n",
            "epoch 16 batch id 3151 loss 0.211378276348114 train acc 0.9192914947635671\n",
            "epoch 16 batch id 3161 loss 0.14205338060855865 train acc 0.9192897817146473\n",
            "epoch 16 batch id 3171 loss 0.28102344274520874 train acc 0.9192979344055503\n",
            "epoch 16 batch id 3181 loss 0.09495313465595245 train acc 0.9193011238604213\n",
            "epoch 16 batch id 3191 loss 0.09387003630399704 train acc 0.9193630523346913\n",
            "epoch 16 batch id 3201 loss 0.33267977833747864 train acc 0.9193318494220556\n",
            "epoch 16 batch id 3211 loss 0.21850286424160004 train acc 0.9193251712862037\n",
            "epoch 16 batch id 3221 loss 0.18695871531963348 train acc 0.9193039816827072\n",
            "epoch 16 batch id 3231 loss 0.14932844042778015 train acc 0.9193312828845559\n",
            "epoch 16 batch id 3241 loss 0.13491690158843994 train acc 0.9193439524838013\n",
            "epoch 16 batch id 3251 loss 0.1954832673072815 train acc 0.9194094124884651\n",
            "epoch 16 batch id 3261 loss 0.17223753035068512 train acc 0.9194025988960441\n",
            "epoch 16 batch id 3271 loss 0.2823066711425781 train acc 0.919348058697646\n",
            "epoch 16 batch id 3281 loss 0.3296472728252411 train acc 0.9193319491008839\n",
            "epoch 16 batch id 3291 loss 0.24014611542224884 train acc 0.9193824065633546\n",
            "epoch 16 batch id 3301 loss 0.16264450550079346 train acc 0.9193378900333232\n",
            "epoch 16 batch id 3311 loss 0.2476048469543457 train acc 0.9193361144669284\n",
            "epoch 16 batch id 3321 loss 0.250955730676651 train acc 0.9193014152363745\n",
            "epoch 16 batch id 3331 loss 0.29834476113319397 train acc 0.9192387796457521\n",
            "epoch 16 batch id 3341 loss 0.394866019487381 train acc 0.919265377132595\n",
            "epoch 16 batch id 3351 loss 0.19079475104808807 train acc 0.9192498507908087\n",
            "epoch 16 batch id 3361 loss 0.268507182598114 train acc 0.9192204700981851\n",
            "epoch 16 batch id 3371 loss 0.3133607804775238 train acc 0.9192283447048354\n",
            "epoch 16 batch id 3381 loss 0.053010180592536926 train acc 0.9192084442472641\n",
            "epoch 16 batch id 3391 loss 0.2705865502357483 train acc 0.9192946402241227\n",
            "epoch 16 batch id 3401 loss 0.3219231069087982 train acc 0.9193160099970596\n",
            "epoch 16 batch id 3411 loss 0.30296480655670166 train acc 0.9193235121665201\n",
            "epoch 16 batch id 3421 loss 0.22225244343280792 train acc 0.9193675095001461\n",
            "epoch 16 batch id 3431 loss 0.3629317581653595 train acc 0.9193247231127951\n",
            "epoch 16 batch id 3441 loss 0.1495286375284195 train acc 0.9193502978785237\n",
            "epoch 16 batch id 3451 loss 0.1960534304380417 train acc 0.9193259200231817\n",
            "epoch 16 batch id 3461 loss 0.15123200416564941 train acc 0.9193603727246461\n",
            "epoch 16 batch id 3471 loss 0.1894116848707199 train acc 0.9193406078939786\n",
            "epoch 16 batch id 3481 loss 0.2833041846752167 train acc 0.9193344225797184\n",
            "epoch 16 batch id 3491 loss 0.0831209048628807 train acc 0.9193819822400459\n",
            "epoch 16 batch id 3501 loss 0.3195945918560028 train acc 0.9194337332190803\n",
            "epoch 16 batch id 3511 loss 0.2950395345687866 train acc 0.9194451367131872\n",
            "epoch 16 batch id 3521 loss 0.3447285592556 train acc 0.9194564754331156\n",
            "epoch 16 batch id 3531 loss 0.4606896638870239 train acc 0.9194500495610308\n",
            "epoch 16 batch id 3541 loss 0.11533304303884506 train acc 0.9194966111268004\n",
            "epoch 16 batch id 3551 loss 0.09987211227416992 train acc 0.9195209096029288\n",
            "epoch 16 batch id 3561 loss 0.10125084966421127 train acc 0.9195538472339231\n",
            "epoch 16 batch id 3571 loss 0.2959693968296051 train acc 0.919525343041165\n",
            "epoch 16 batch id 3581 loss 0.32641729712486267 train acc 0.9195319044959509\n",
            "epoch 16 batch id 3591 loss 0.3667367696762085 train acc 0.9195384294068505\n",
            "epoch 16 batch id 3601 loss 0.30307820439338684 train acc 0.9195709525131908\n",
            "epoch 16 batch id 3611 loss 0.13666962087154388 train acc 0.9195816602049294\n",
            "epoch 16 batch id 3621 loss 0.1788794845342636 train acc 0.9195707332228666\n",
            "epoch 16 batch id 3631 loss 0.2819024622440338 train acc 0.919516834205453\n",
            "epoch 16 batch id 3641 loss 0.30902040004730225 train acc 0.9195276023070585\n",
            "epoch 16 batch id 3651 loss 0.3189769387245178 train acc 0.919491235278006\n",
            "epoch 16 batch id 3661 loss 0.3089474141597748 train acc 0.91953615815351\n",
            "epoch 16 batch id 3671 loss 0.09394598007202148 train acc 0.9195212476164533\n",
            "epoch 16 batch id 3681 loss 0.2511935830116272 train acc 0.9195616001086662\n",
            "epoch 16 batch id 3691 loss 0.15331850945949554 train acc 0.9195805675968572\n",
            "epoch 16 batch id 3701 loss 0.22613713145256042 train acc 0.919578323426101\n",
            "epoch 16 batch id 3711 loss 0.21789270639419556 train acc 0.9195845122608461\n",
            "epoch 16 batch id 3721 loss 0.2078632414340973 train acc 0.9196074643912927\n",
            "epoch 16 batch id 3731 loss 0.19343610107898712 train acc 0.9196219177164299\n",
            "epoch 16 batch id 3741 loss 0.20657652616500854 train acc 0.9196112336273724\n",
            "epoch 16 batch id 3751 loss 0.3156307339668274 train acc 0.9196047720607838\n",
            "epoch 16 batch id 3761 loss 0.14912627637386322 train acc 0.9196648165381548\n",
            "epoch 16 batch id 3771 loss 0.2601456344127655 train acc 0.9196623906125696\n",
            "epoch 16 batch id 3781 loss 0.34452104568481445 train acc 0.9196517125099181\n",
            "epoch 16 batch id 3791 loss 0.12691038846969604 train acc 0.9197070364020048\n",
            "epoch 16 batch id 3801 loss 0.2289896011352539 train acc 0.9197127400684031\n",
            "epoch 16 batch id 3811 loss 0.2173386961221695 train acc 0.9197430136447127\n",
            "epoch 16 batch id 3821 loss 0.2962890863418579 train acc 0.9197322363255692\n",
            "epoch 16 batch id 3831 loss 0.2910589873790741 train acc 0.9197255938397285\n",
            "epoch 16 batch id 3841 loss 0.059889331459999084 train acc 0.91972712184327\n",
            "epoch 16 batch id 3851 loss 0.12515121698379517 train acc 0.9197529862373409\n",
            "epoch 16 batch id 3861 loss 0.1436537802219391 train acc 0.9197625291375291\n",
            "epoch 16 batch id 3871 loss 0.26375994086265564 train acc 0.9197679863084475\n",
            "epoch 16 batch id 3881 loss 0.3295830488204956 train acc 0.9198056235506313\n",
            "epoch 16 batch id 3891 loss 0.18325424194335938 train acc 0.9198069262400411\n",
            "epoch 16 batch id 3901 loss 0.18743768334388733 train acc 0.9197641630351192\n",
            "epoch 16 batch id 3911 loss 0.057230714708566666 train acc 0.9197735553566863\n",
            "epoch 16 batch id 3921 loss 0.20409467816352844 train acc 0.919790869676103\n",
            "epoch 16 batch id 3931 loss 0.12612099945545197 train acc 0.9197961714576444\n",
            "epoch 16 batch id 3941 loss 0.16098852455615997 train acc 0.9198014463334179\n",
            "epoch 16 batch id 3951 loss 0.12410697340965271 train acc 0.9198304226778031\n",
            "epoch 16 batch id 3961 loss 0.2516891658306122 train acc 0.9198355844483717\n",
            "epoch 16 batch id 3971 loss 0.23899586498737335 train acc 0.9198210463359355\n",
            "epoch 16 batch id 3981 loss 0.2858039438724518 train acc 0.9198183559407184\n",
            "epoch 16 batch id 3991 loss 0.24115915596485138 train acc 0.9198548296166374\n",
            "epoch 16 batch id 4001 loss 0.30934521555900574 train acc 0.9198442576855786\n",
            "epoch 16 batch id 4011 loss 0.22379431128501892 train acc 0.9197986786337572\n",
            "epoch 16 batch id 4021 loss 0.16387498378753662 train acc 0.9197882989306143\n",
            "epoch 16 batch id 4031 loss 0.21241049468517303 train acc 0.9198477424956586\n",
            "epoch 16 batch id 4041 loss 0.3900097608566284 train acc 0.9198140930462757\n",
            "epoch 16 batch id 4051 loss 0.2717871367931366 train acc 0.9198114663046162\n",
            "epoch 16 batch id 4061 loss 0.22721220552921295 train acc 0.9198473282442748\n",
            "epoch 16 batch id 4071 loss 0.3992432951927185 train acc 0.919817765905183\n",
            "epoch 16 batch id 4081 loss 0.2088112235069275 train acc 0.919864922813036\n",
            "epoch 16 batch id 4091 loss 0.2107378989458084 train acc 0.9199194879002689\n",
            "epoch 16 batch id 4101 loss 0.2937825620174408 train acc 0.9199204462326261\n",
            "epoch 16 batch id 4111 loss 0.10580087453126907 train acc 0.9199137983459013\n",
            "epoch 16 batch id 4121 loss 0.21991847455501556 train acc 0.919895808056297\n",
            "epoch 16 batch id 4131 loss 0.260921448469162 train acc 0.9199157286371339\n",
            "epoch 16 batch id 4141 loss 0.43293285369873047 train acc 0.9199242332769862\n",
            "epoch 16 batch id 4151 loss 0.21405187249183655 train acc 0.9199101120211998\n",
            "epoch 16 batch id 4161 loss 0.17730197310447693 train acc 0.9198960586397501\n",
            "epoch 16 batch id 4171 loss 0.18780101835727692 train acc 0.9199232797890194\n",
            "epoch 16 batch id 4181 loss 0.019354339689016342 train acc 0.919980267878498\n",
            "epoch 16 batch id 4191 loss 0.28431957960128784 train acc 0.9199624194702934\n",
            "epoch 16 batch id 4201 loss 0.24047307670116425 train acc 0.9199855689121638\n",
            "epoch 16 batch id 4211 loss 0.2649702727794647 train acc 0.9199677926858228\n",
            "epoch 16 batch id 4221 loss 0.21700312197208405 train acc 0.919920486851457\n",
            "epoch 16 batch id 4231 loss 0.15570077300071716 train acc 0.919925106357835\n",
            "epoch 16 batch id 4241 loss 0.15054883062839508 train acc 0.9199223355340721\n",
            "epoch 16 batch id 4251 loss 0.26074618101119995 train acc 0.9199600094095507\n",
            "epoch 16 batch id 4261 loss 0.0842340812087059 train acc 0.9199791715559728\n",
            "epoch 16 batch id 4271 loss 0.1667638123035431 train acc 0.9199945855771482\n",
            "epoch 16 batch id 4281 loss 0.37410759925842285 train acc 0.9199916783461808\n",
            "epoch 16 batch id 4291 loss 0.08488063514232635 train acc 0.9199887846655791\n",
            "epoch 16 batch id 4301 loss 0.17154720425605774 train acc 0.919989537316903\n",
            "epoch 16 batch id 4311 loss 0.15691401064395905 train acc 0.9200229065182093\n",
            "epoch 16 batch id 4321 loss 0.1698872447013855 train acc 0.920052505207128\n",
            "epoch 16 batch id 4331 loss 0.14043882489204407 train acc 0.9200639286538905\n",
            "epoch 16 batch id 4341 loss 0.10270018130540848 train acc 0.9200645012669891\n",
            "epoch 16 batch id 4351 loss 0.18109947443008423 train acc 0.9200902091473224\n",
            "epoch 16 batch id 4361 loss 0.17325414717197418 train acc 0.9201516280669572\n",
            "epoch 16 batch id 4371 loss 0.27837079763412476 train acc 0.920173444291924\n",
            "epoch 16 batch id 4381 loss 0.34733134508132935 train acc 0.9201416628623602\n",
            "epoch 16 batch id 4391 loss 0.20461010932922363 train acc 0.9201456103393304\n",
            "epoch 16 batch id 4401 loss 0.23896454274654388 train acc 0.9201140365825948\n",
            "epoch 16 batch id 4411 loss 0.23244443535804749 train acc 0.9201074019496712\n",
            "epoch 16 batch id 4421 loss 0.20295505225658417 train acc 0.9201326057453065\n",
            "epoch 16 batch id 4431 loss 0.31502825021743774 train acc 0.9201541694877003\n",
            "epoch 16 batch id 4441 loss 0.07379361242055893 train acc 0.920179154469714\n",
            "epoch 16 batch id 4451 loss 0.1020740196108818 train acc 0.9201689227139969\n",
            "epoch 16 batch id 4461 loss 0.20654910802841187 train acc 0.9201377213629232\n",
            "epoch 16 batch id 4471 loss 0.2993934750556946 train acc 0.9201206385596064\n",
            "epoch 16 batch id 4481 loss 0.18333621323108673 train acc 0.9201071189466636\n",
            "epoch 16 batch id 4491 loss 0.19513662159442902 train acc 0.9200971387218883\n",
            "epoch 16 batch id 4501 loss 0.0673752948641777 train acc 0.9200906742946012\n",
            "epoch 16 batch id 4511 loss 0.22568075358867645 train acc 0.9200703835069829\n",
            "epoch 16 batch id 4521 loss 0.11292780190706253 train acc 0.9200432702941828\n",
            "epoch 16 batch id 4531 loss 0.15721234679222107 train acc 0.9200197252262193\n",
            "epoch 16 batch id 4541 loss 0.2260647416114807 train acc 0.9200444560669456\n",
            "epoch 16 batch id 4551 loss 0.2403399795293808 train acc 0.9200416117336849\n",
            "epoch 16 batch id 4561 loss 0.30318060517311096 train acc 0.9200422056566543\n",
            "epoch 16 batch id 4571 loss 0.14417964220046997 train acc 0.9200906530299715\n",
            "epoch 16 batch id 4581 loss 0.24813103675842285 train acc 0.9200740831696136\n",
            "epoch 16 batch id 4591 loss 0.2748804986476898 train acc 0.920047375299499\n",
            "epoch 16 batch id 4601 loss 0.16540616750717163 train acc 0.9200581395348837\n",
            "epoch 16 batch id 4611 loss 0.18453001976013184 train acc 0.9200858002602472\n",
            "epoch 16 batch id 4621 loss 0.18405179679393768 train acc 0.9200862908461372\n",
            "epoch 16 batch id 4631 loss 0.28004515171051025 train acc 0.9201036493198014\n",
            "epoch 16 batch id 4641 loss 0.18077798187732697 train acc 0.9201074660633484\n",
            "epoch 16 batch id 4651 loss 0.38654327392578125 train acc 0.9200843904536659\n",
            "epoch 16 batch id 4661 loss 0.23072168231010437 train acc 0.9201150504183652\n",
            "epoch 16 batch id 4671 loss 0.16953735053539276 train acc 0.9201422339970028\n",
            "epoch 16 batch id 4681 loss 0.4342319071292877 train acc 0.9201592875453963\n",
            "epoch 16 batch id 4691 loss 0.12060973048210144 train acc 0.920156283308463\n",
            "epoch 16 batch id 4701 loss 0.24505780637264252 train acc 0.9201699106573069\n",
            "epoch 16 batch id 4711 loss 0.09090545773506165 train acc 0.9202166472086606\n",
            "epoch 16 batch id 4721 loss 0.13832171261310577 train acc 0.9202102308832875\n",
            "epoch 16 batch id 4731 loss 0.10892748087644577 train acc 0.9201840255759882\n",
            "epoch 16 batch id 4741 loss 0.16655278205871582 train acc 0.9202139580257329\n",
            "epoch 16 batch id 4751 loss 0.1861923187971115 train acc 0.9202371869080194\n",
            "epoch 16 batch id 4761 loss 0.3580559492111206 train acc 0.9202439088426801\n",
            "epoch 16 batch id 4771 loss 0.2056339532136917 train acc 0.9202768025571159\n",
            "epoch 16 batch id 4781 loss 0.31135571002960205 train acc 0.9202409276302029\n",
            "epoch 16 batch id 4791 loss 0.36980825662612915 train acc 0.9201921571696932\n",
            "epoch 16 batch id 4801 loss 0.2482486516237259 train acc 0.9201663715892522\n",
            "epoch 16 batch id 4811 loss 0.20164553821086884 train acc 0.9201666753273747\n",
            "epoch 16 batch id 4821 loss 0.2590835988521576 train acc 0.920141049574777\n",
            "epoch 16 batch id 4831 loss 0.0774165466427803 train acc 0.9201963879114055\n",
            "epoch 16 batch id 4841 loss 0.2063283771276474 train acc 0.9201934001239414\n",
            "epoch 16 batch id 4851 loss 0.23187586665153503 train acc 0.9202129715522572\n",
            "epoch 16 batch id 4861 loss 0.2867906987667084 train acc 0.92023567681547\n",
            "epoch 16 batch id 4871 loss 0.2970339357852936 train acc 0.9202422500513242\n",
            "epoch 16 batch id 4881 loss 0.1960933357477188 train acc 0.9202840094242983\n",
            "epoch 16 batch id 4891 loss 0.13159728050231934 train acc 0.9202617051727663\n",
            "epoch 16 batch id 4901 loss 0.16658784449100494 train acc 0.9202809375637625\n",
            "epoch 16 batch id 4911 loss 0.3818396329879761 train acc 0.9202587304011403\n",
            "epoch 16 batch id 4921 loss 0.17630541324615479 train acc 0.9202715403373298\n",
            "epoch 16 batch id 4931 loss 0.2170495092868805 train acc 0.920252611032245\n",
            "epoch 16 batch id 4941 loss 0.15859095752239227 train acc 0.920255894555758\n",
            "epoch 16 batch id 4951 loss 0.22407472133636475 train acc 0.9202749444556655\n",
            "epoch 16 train acc 0.9202992324903259\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81fb6326eae74a9dba841281f19f2002",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 16 loss 0.689504086971283 test acc 0.8130842192082112\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b010dd3476124185998a26d4024017f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17 batch id 1 loss 0.4026084542274475 train acc 0.875\n",
            "epoch 17 batch id 11 loss 0.15929357707500458 train acc 0.9403409090909091\n",
            "epoch 17 batch id 21 loss 0.11584416031837463 train acc 0.9345238095238095\n",
            "epoch 17 batch id 31 loss 0.19646789133548737 train acc 0.9248991935483871\n",
            "epoch 17 batch id 41 loss 0.17543256282806396 train acc 0.926829268292683\n",
            "epoch 17 batch id 51 loss 0.34763815999031067 train acc 0.930453431372549\n",
            "epoch 17 batch id 61 loss 0.1681174486875534 train acc 0.9285348360655737\n",
            "epoch 17 batch id 71 loss 0.3502855598926544 train acc 0.9286971830985915\n",
            "epoch 17 batch id 81 loss 0.26240217685699463 train acc 0.9286265432098766\n",
            "epoch 17 batch id 91 loss 0.20680028200149536 train acc 0.9270260989010989\n",
            "epoch 17 batch id 101 loss 0.2148815244436264 train acc 0.9258972772277227\n",
            "epoch 17 batch id 111 loss 0.16289757192134857 train acc 0.9276463963963963\n",
            "epoch 17 batch id 121 loss 0.23240692913532257 train acc 0.9271694214876033\n",
            "epoch 17 batch id 131 loss 0.3029002547264099 train acc 0.9268845419847328\n",
            "epoch 17 batch id 141 loss 0.20139889419078827 train acc 0.9263076241134752\n",
            "epoch 17 batch id 151 loss 0.14479467272758484 train acc 0.925807119205298\n",
            "epoch 17 batch id 161 loss 0.23189516365528107 train acc 0.9270186335403726\n",
            "epoch 17 batch id 171 loss 0.3641357421875 train acc 0.9258040935672515\n",
            "epoch 17 batch id 181 loss 0.14473956823349 train acc 0.9267092541436464\n",
            "epoch 17 batch id 191 loss 0.07480082660913467 train acc 0.9274378272251309\n",
            "epoch 17 batch id 201 loss 0.3176584839820862 train acc 0.9282493781094527\n",
            "epoch 17 batch id 211 loss 0.2906109690666199 train acc 0.9275770142180095\n",
            "epoch 17 batch id 221 loss 0.46669599413871765 train acc 0.9268240950226244\n",
            "epoch 17 batch id 231 loss 0.11131033301353455 train acc 0.9272186147186147\n",
            "epoch 17 batch id 241 loss 0.23141539096832275 train acc 0.9264782157676349\n",
            "epoch 17 batch id 251 loss 0.2647547721862793 train acc 0.9256723107569721\n",
            "epoch 17 batch id 261 loss 0.289345383644104 train acc 0.9262452107279694\n",
            "epoch 17 batch id 271 loss 0.0870889201760292 train acc 0.9267181734317343\n",
            "epoch 17 batch id 281 loss 0.15149039030075073 train acc 0.9263790035587188\n",
            "epoch 17 batch id 291 loss 0.2654868960380554 train acc 0.9264390034364262\n",
            "epoch 17 batch id 301 loss 0.23500607907772064 train acc 0.9268583887043189\n",
            "epoch 17 batch id 311 loss 0.2711453139781952 train acc 0.926899115755627\n",
            "epoch 17 batch id 321 loss 0.2657836079597473 train acc 0.9269373052959502\n",
            "epoch 17 batch id 331 loss 0.24528001248836517 train acc 0.9270203927492447\n",
            "epoch 17 batch id 341 loss 0.33482152223587036 train acc 0.9272818914956011\n",
            "epoch 17 batch id 351 loss 0.27520623803138733 train acc 0.9271723646723646\n",
            "epoch 17 batch id 361 loss 0.19885851442813873 train acc 0.9273286011080333\n",
            "epoch 17 batch id 371 loss 0.14166438579559326 train acc 0.9274764150943396\n",
            "epoch 17 batch id 381 loss 0.06098885089159012 train acc 0.9280675853018373\n",
            "epoch 17 batch id 391 loss 0.3239319324493408 train acc 0.9283088235294118\n",
            "epoch 17 batch id 401 loss 0.15288898348808289 train acc 0.9279535536159601\n",
            "epoch 17 batch id 411 loss 0.21460747718811035 train acc 0.9279577250608273\n",
            "epoch 17 batch id 421 loss 0.39657965302467346 train acc 0.9277761282660333\n",
            "epoch 17 batch id 431 loss 0.27050408720970154 train acc 0.927820475638051\n",
            "epoch 17 batch id 441 loss 0.3342428207397461 train acc 0.9278628117913832\n",
            "epoch 17 batch id 451 loss 0.32000917196273804 train acc 0.9278339800443459\n",
            "epoch 17 batch id 461 loss 0.10248735547065735 train acc 0.9275691431670282\n",
            "epoch 17 batch id 471 loss 0.17483949661254883 train acc 0.9280453821656051\n",
            "epoch 17 batch id 481 loss 0.29383596777915955 train acc 0.9283069126819127\n",
            "epoch 17 batch id 491 loss 0.08754346519708633 train acc 0.9284941446028513\n",
            "epoch 17 batch id 501 loss 0.4680820405483246 train acc 0.9281749001996008\n",
            "epoch 17 batch id 511 loss 0.28486740589141846 train acc 0.9282045009784736\n",
            "epoch 17 batch id 521 loss 0.19529493153095245 train acc 0.9282629558541267\n",
            "epoch 17 batch id 531 loss 0.1109711304306984 train acc 0.927995527306968\n",
            "epoch 17 batch id 541 loss 0.156878262758255 train acc 0.9279401571164511\n",
            "epoch 17 batch id 551 loss 0.15470393002033234 train acc 0.9280852994555354\n",
            "epoch 17 batch id 561 loss 0.17103110253810883 train acc 0.9280581550802139\n",
            "epoch 17 batch id 571 loss 0.23353007435798645 train acc 0.928114054290718\n",
            "epoch 17 batch id 581 loss 0.35153326392173767 train acc 0.9280335628227194\n",
            "epoch 17 batch id 591 loss 0.16307850182056427 train acc 0.9283259306260575\n",
            "epoch 17 batch id 601 loss 0.12160719186067581 train acc 0.9284005823627288\n",
            "epoch 17 batch id 611 loss 0.1804768294095993 train acc 0.928217062193126\n",
            "epoch 17 batch id 621 loss 0.20976175367832184 train acc 0.9282659017713365\n",
            "epoch 17 batch id 631 loss 0.21497759222984314 train acc 0.9279912836767037\n",
            "epoch 17 batch id 641 loss 0.11941169947385788 train acc 0.9282127535101404\n",
            "epoch 17 batch id 651 loss 0.14669926464557648 train acc 0.9282594086021505\n",
            "epoch 17 batch id 661 loss 0.2441636025905609 train acc 0.9282573751891074\n",
            "epoch 17 batch id 671 loss 0.13213154673576355 train acc 0.928488263785395\n",
            "epoch 17 batch id 681 loss 0.1886097639799118 train acc 0.9283223201174743\n",
            "epoch 17 batch id 691 loss 0.2808484435081482 train acc 0.9282968523878437\n",
            "epoch 17 batch id 701 loss 0.30024757981300354 train acc 0.9281829529243937\n",
            "epoch 17 batch id 711 loss 0.18803471326828003 train acc 0.9280283052039381\n",
            "epoch 17 batch id 721 loss 0.22129715979099274 train acc 0.9278129334257975\n",
            "epoch 17 batch id 731 loss 0.17387251555919647 train acc 0.9276462038303693\n",
            "epoch 17 batch id 741 loss 0.26876839995384216 train acc 0.9276737516869096\n",
            "epoch 17 batch id 751 loss 0.3379896581172943 train acc 0.9275341211717709\n",
            "epoch 17 batch id 761 loss 0.132979154586792 train acc 0.9275418856767411\n",
            "epoch 17 batch id 771 loss 0.37803399562835693 train acc 0.9274481193255513\n",
            "epoch 17 batch id 781 loss 0.19059470295906067 train acc 0.9274767925736236\n",
            "epoch 17 batch id 791 loss 0.13038800656795502 train acc 0.9274652338811631\n",
            "epoch 17 batch id 801 loss 0.2864101529121399 train acc 0.927414950062422\n",
            "epoch 17 batch id 811 loss 0.18643420934677124 train acc 0.9276356350184957\n",
            "epoch 17 batch id 821 loss 0.06765946745872498 train acc 0.9277367539585871\n",
            "epoch 17 batch id 831 loss 0.3978312313556671 train acc 0.927797833935018\n",
            "epoch 17 batch id 841 loss 0.1413218080997467 train acc 0.9277459869203329\n",
            "epoch 17 batch id 851 loss 0.2632576823234558 train acc 0.9276402761457109\n",
            "epoch 17 batch id 861 loss 0.41785112023353577 train acc 0.9275188734030198\n",
            "epoch 17 batch id 871 loss 0.1829422116279602 train acc 0.9274540757749713\n",
            "epoch 17 batch id 881 loss 0.2079513818025589 train acc 0.9274084846765039\n",
            "epoch 17 batch id 891 loss 0.17372223734855652 train acc 0.9275042087542088\n",
            "epoch 17 batch id 901 loss 0.2081751823425293 train acc 0.9273203385127636\n",
            "epoch 17 batch id 911 loss 0.2806812822818756 train acc 0.9272605653128431\n",
            "epoch 17 batch id 921 loss 0.20245611667633057 train acc 0.9272360206297503\n",
            "epoch 17 batch id 931 loss 0.13982956111431122 train acc 0.9270106068743287\n",
            "epoch 17 batch id 941 loss 0.14035861194133759 train acc 0.9272217056323061\n",
            "epoch 17 batch id 951 loss 0.1401735097169876 train acc 0.927165483701367\n",
            "epoch 17 batch id 961 loss 0.31849586963653564 train acc 0.9271266909469302\n",
            "epoch 17 batch id 971 loss 0.12388691306114197 train acc 0.9271691555097837\n",
            "epoch 17 batch id 981 loss 0.3469139337539673 train acc 0.9272744648318043\n",
            "epoch 17 batch id 991 loss 0.2639250159263611 train acc 0.9273303481331988\n",
            "epoch 17 batch id 1001 loss 0.39751702547073364 train acc 0.9274787712287712\n",
            "epoch 17 batch id 1011 loss 0.24960041046142578 train acc 0.9274078882294757\n",
            "epoch 17 batch id 1021 loss 0.11177090555429459 train acc 0.9274455190989226\n",
            "epoch 17 batch id 1031 loss 0.3267558515071869 train acc 0.9273308680892337\n",
            "epoch 17 batch id 1041 loss 0.2678144574165344 train acc 0.9272634486071085\n",
            "epoch 17 batch id 1051 loss 0.134431853890419 train acc 0.92710811132255\n",
            "epoch 17 batch id 1061 loss 0.23856136202812195 train acc 0.9271176955702167\n",
            "epoch 17 batch id 1071 loss 0.05790131911635399 train acc 0.9271854575163399\n",
            "epoch 17 batch id 1081 loss 0.2592293322086334 train acc 0.9271074236817761\n",
            "epoch 17 batch id 1091 loss 0.6156982183456421 train acc 0.9270737855178736\n",
            "epoch 17 batch id 1101 loss 0.1307704895734787 train acc 0.9270975249772934\n",
            "epoch 17 batch id 1111 loss 0.25424739718437195 train acc 0.9270083258325833\n",
            "epoch 17 batch id 1121 loss 0.16227342188358307 train acc 0.9271716101694916\n",
            "epoch 17 batch id 1131 loss 0.318174809217453 train acc 0.9269728116710876\n",
            "epoch 17 batch id 1141 loss 0.20430819690227509 train acc 0.9270513803680982\n",
            "epoch 17 batch id 1151 loss 0.2204173058271408 train acc 0.9271014335360556\n",
            "epoch 17 batch id 1161 loss 0.1932431310415268 train acc 0.9270295004306632\n",
            "epoch 17 batch id 1171 loss 0.11965576559305191 train acc 0.927025512382579\n",
            "epoch 17 batch id 1181 loss 0.27443644404411316 train acc 0.9270215918712955\n",
            "epoch 17 batch id 1191 loss 0.14396235346794128 train acc 0.9271620486985727\n",
            "epoch 17 batch id 1201 loss 0.1770247370004654 train acc 0.9272871565362198\n",
            "epoch 17 batch id 1211 loss 0.1898326873779297 train acc 0.9272166597853014\n",
            "epoch 17 batch id 1221 loss 0.1749410778284073 train acc 0.927236895986896\n",
            "epoch 17 batch id 1231 loss 0.2028781622648239 train acc 0.9270664094232332\n",
            "epoch 17 batch id 1241 loss 0.24508537352085114 train acc 0.926986804995971\n",
            "epoch 17 batch id 1251 loss 0.1970399022102356 train acc 0.9270333733013589\n",
            "epoch 17 batch id 1261 loss 0.20397581160068512 train acc 0.9270915939730373\n",
            "epoch 17 batch id 1271 loss 0.16914501786231995 train acc 0.9271980723839497\n",
            "epoch 17 batch id 1281 loss 0.1957555115222931 train acc 0.9272297033567526\n",
            "epoch 17 batch id 1291 loss 0.14101070165634155 train acc 0.9273092563903951\n",
            "epoch 17 batch id 1301 loss 0.27639538049697876 train acc 0.9273035165257494\n",
            "epoch 17 batch id 1311 loss 0.30451956391334534 train acc 0.9272978642257819\n",
            "epoch 17 batch id 1321 loss 0.06958334892988205 train acc 0.9272686411809236\n",
            "epoch 17 batch id 1331 loss 0.3088831901550293 train acc 0.9271694214876033\n",
            "epoch 17 batch id 1341 loss 0.32523250579833984 train acc 0.9272115026099925\n",
            "epoch 17 batch id 1351 loss 0.20278263092041016 train acc 0.9272529607698001\n",
            "epoch 17 batch id 1361 loss 0.16172999143600464 train acc 0.9272593681116826\n",
            "epoch 17 batch id 1371 loss 0.06922038644552231 train acc 0.9272998723559446\n",
            "epoch 17 batch id 1381 loss 0.20775461196899414 train acc 0.9273284757422158\n",
            "epoch 17 batch id 1391 loss 0.10497071593999863 train acc 0.92724433860532\n",
            "epoch 17 batch id 1401 loss 0.05223909765481949 train acc 0.9272729300499644\n",
            "epoch 17 batch id 1411 loss 0.11272025853395462 train acc 0.927400779588944\n",
            "epoch 17 batch id 1421 loss 0.3207172751426697 train acc 0.9272959183673469\n",
            "epoch 17 batch id 1431 loss 0.26408150792121887 train acc 0.9272471174004193\n",
            "epoch 17 batch id 1441 loss 0.29359444975852966 train acc 0.9271230916030534\n",
            "epoch 17 batch id 1451 loss 0.11544870585203171 train acc 0.9272484493452792\n",
            "epoch 17 batch id 1461 loss 0.1635318249464035 train acc 0.9272223648186174\n",
            "epoch 17 batch id 1471 loss 0.33173868060112 train acc 0.9272391230455472\n",
            "epoch 17 batch id 1481 loss 0.09001989662647247 train acc 0.9272662052667117\n",
            "epoch 17 batch id 1491 loss 0.2246319055557251 train acc 0.9273138832997988\n",
            "epoch 17 batch id 1501 loss 0.3040531575679779 train acc 0.9274754330446369\n",
            "epoch 17 batch id 1511 loss 0.1238044947385788 train acc 0.927521095301125\n",
            "epoch 17 batch id 1521 loss 0.30119505524635315 train acc 0.9276072485207101\n",
            "epoch 17 batch id 1531 loss 0.13309216499328613 train acc 0.9275698073154801\n",
            "epoch 17 batch id 1541 loss 0.11744676530361176 train acc 0.9275531310837118\n",
            "epoch 17 batch id 1551 loss 0.1716243326663971 train acc 0.9275265957446809\n",
            "epoch 17 batch id 1561 loss 0.19420930743217468 train acc 0.927500400384369\n",
            "epoch 17 batch id 1571 loss 0.13025091588497162 train acc 0.9275143220878421\n",
            "epoch 17 batch id 1581 loss 0.17294950783252716 train acc 0.9275083017077799\n",
            "epoch 17 batch id 1591 loss 0.28442272543907166 train acc 0.9275514613450659\n",
            "epoch 17 batch id 1601 loss 0.3766476511955261 train acc 0.9275452841973766\n",
            "epoch 17 batch id 1611 loss 0.127279132604599 train acc 0.9274809900682806\n",
            "epoch 17 batch id 1621 loss 0.1531057357788086 train acc 0.9275331585441086\n",
            "epoch 17 batch id 1631 loss 0.060607705265283585 train acc 0.9275080472103004\n",
            "epoch 17 batch id 1641 loss 0.20588494837284088 train acc 0.9275784582571602\n",
            "epoch 17 batch id 1651 loss 0.19855085015296936 train acc 0.9275912325863114\n",
            "epoch 17 batch id 1661 loss 0.23744826018810272 train acc 0.9276602950030103\n",
            "epoch 17 batch id 1671 loss 0.2589084208011627 train acc 0.9276443746259725\n",
            "epoch 17 batch id 1681 loss 0.22168311476707458 train acc 0.9275728732897085\n",
            "epoch 17 batch id 1691 loss 0.20196494460105896 train acc 0.9276130987581312\n",
            "epoch 17 batch id 1701 loss 0.1131243109703064 train acc 0.9275885508524397\n",
            "epoch 17 batch id 1711 loss 0.22820430994033813 train acc 0.9275368936294565\n",
            "epoch 17 batch id 1721 loss 0.19748303294181824 train acc 0.9276311011040093\n",
            "epoch 17 batch id 1731 loss 0.056189876049757004 train acc 0.9276971403812825\n",
            "epoch 17 batch id 1741 loss 0.33761492371559143 train acc 0.9276367748420448\n",
            "epoch 17 batch id 1751 loss 0.0933210700750351 train acc 0.9276663335237008\n",
            "epoch 17 batch id 1761 loss 0.18608316779136658 train acc 0.9277399204997161\n",
            "epoch 17 batch id 1771 loss 0.16714581847190857 train acc 0.9277509175607002\n",
            "epoch 17 batch id 1781 loss 0.14475470781326294 train acc 0.9277881106120157\n",
            "epoch 17 batch id 1791 loss 0.36611393094062805 train acc 0.9277725432719152\n",
            "epoch 17 batch id 1801 loss 0.288779079914093 train acc 0.9278265546918378\n",
            "epoch 17 batch id 1811 loss 0.12485780566930771 train acc 0.9278454583103258\n",
            "epoch 17 batch id 1821 loss 0.21092434227466583 train acc 0.9277440280065898\n",
            "epoch 17 batch id 1831 loss 0.15120205283164978 train acc 0.9277375750955762\n",
            "epoch 17 batch id 1841 loss 0.1314469873905182 train acc 0.927773628462792\n",
            "epoch 17 batch id 1851 loss 0.14316076040267944 train acc 0.9278683819556997\n",
            "epoch 17 batch id 1861 loss 0.2900763154029846 train acc 0.9278697608812466\n",
            "epoch 17 batch id 1871 loss 0.2297758311033249 train acc 0.927929583110636\n",
            "epoch 17 batch id 1881 loss 0.1380576491355896 train acc 0.9279804625199362\n",
            "epoch 17 batch id 1891 loss 0.14534661173820496 train acc 0.9281216948704389\n",
            "epoch 17 batch id 1901 loss 0.23409995436668396 train acc 0.928138150973172\n",
            "epoch 17 batch id 1911 loss 0.14173877239227295 train acc 0.9281707875457875\n",
            "epoch 17 batch id 1921 loss 0.3103633522987366 train acc 0.9281054789172306\n",
            "epoch 17 batch id 1931 loss 0.09595336019992828 train acc 0.9281864966338684\n",
            "epoch 17 batch id 1941 loss 0.15759733319282532 train acc 0.928226429675425\n",
            "epoch 17 batch id 1951 loss 0.3211922347545624 train acc 0.92818586622245\n",
            "epoch 17 batch id 1961 loss 0.09739639610052109 train acc 0.9281696200917899\n",
            "epoch 17 batch id 1971 loss 0.17389051616191864 train acc 0.9280901192288179\n",
            "epoch 17 batch id 1981 loss 0.323004812002182 train acc 0.9280824078748107\n",
            "epoch 17 batch id 1991 loss 0.3505438566207886 train acc 0.9280276870919136\n",
            "epoch 17 batch id 2001 loss 0.2241598218679428 train acc 0.9279891304347826\n",
            "epoch 17 batch id 2011 loss 0.18426404893398285 train acc 0.9279664967677772\n",
            "epoch 17 batch id 2021 loss 0.39446187019348145 train acc 0.9279363557644731\n",
            "epoch 17 batch id 2031 loss 0.13972057402133942 train acc 0.9279603643525357\n",
            "epoch 17 batch id 2041 loss 0.1761084496974945 train acc 0.9278922709456149\n",
            "epoch 17 batch id 2051 loss 0.3342634439468384 train acc 0.9278705509507558\n",
            "epoch 17 batch id 2061 loss 0.18085794150829315 train acc 0.9279096918971373\n",
            "epoch 17 batch id 2071 loss 0.2609727084636688 train acc 0.9279031868662482\n",
            "epoch 17 batch id 2081 loss 0.22816650569438934 train acc 0.9278892359442575\n",
            "epoch 17 batch id 2091 loss 0.13448253273963928 train acc 0.9278754184600669\n",
            "epoch 17 batch id 2101 loss 0.15159255266189575 train acc 0.9278766063779152\n",
            "epoch 17 batch id 2111 loss 0.25611940026283264 train acc 0.9278407745144481\n",
            "epoch 17 batch id 2121 loss 0.2710798978805542 train acc 0.927790546911834\n",
            "epoch 17 batch id 2131 loss 0.19213895499706268 train acc 0.9278801032379165\n",
            "epoch 17 batch id 2141 loss 0.24566076695919037 train acc 0.9278447571228398\n",
            "epoch 17 batch id 2151 loss 0.2087518274784088 train acc 0.9278533240353324\n",
            "epoch 17 batch id 2161 loss 0.18287226557731628 train acc 0.9277750462748727\n",
            "epoch 17 batch id 2171 loss 0.155902698636055 train acc 0.9277838553661907\n",
            "epoch 17 batch id 2181 loss 0.12953874468803406 train acc 0.9277639270976616\n",
            "epoch 17 batch id 2191 loss 0.09518830478191376 train acc 0.927779837973528\n",
            "epoch 17 batch id 2201 loss 0.18517106771469116 train acc 0.927703316674239\n",
            "epoch 17 batch id 2211 loss 0.08367473632097244 train acc 0.9276557553143374\n",
            "epoch 17 batch id 2221 loss 0.3363856375217438 train acc 0.927594552003602\n",
            "epoch 17 batch id 2231 loss 0.1530071496963501 train acc 0.9275969296279695\n",
            "epoch 17 batch id 2241 loss 0.308229923248291 train acc 0.9274947010263276\n",
            "epoch 17 batch id 2251 loss 0.18667569756507874 train acc 0.9274350288760551\n",
            "epoch 17 batch id 2261 loss 0.0752767026424408 train acc 0.927438080495356\n",
            "epoch 17 batch id 2271 loss 0.31972336769104004 train acc 0.9274479854689565\n",
            "epoch 17 batch id 2281 loss 0.3218366503715515 train acc 0.927540004384042\n",
            "epoch 17 batch id 2291 loss 0.18829955160617828 train acc 0.9276653208206024\n",
            "epoch 17 batch id 2301 loss 0.17257875204086304 train acc 0.9277012711864406\n",
            "epoch 17 batch id 2311 loss 0.24502547085285187 train acc 0.9276895824318477\n",
            "epoch 17 batch id 2321 loss 0.1876799762248993 train acc 0.9276577983627746\n",
            "epoch 17 batch id 2331 loss 0.22868070006370544 train acc 0.9276128807378807\n",
            "epoch 17 batch id 2341 loss 0.24177081882953644 train acc 0.9275883703545493\n",
            "epoch 17 batch id 2351 loss 0.2991619110107422 train acc 0.9274843151850276\n",
            "epoch 17 batch id 2361 loss 0.1979733109474182 train acc 0.9275399724692927\n",
            "epoch 17 batch id 2371 loss 0.1282358020544052 train acc 0.9276083403627161\n",
            "epoch 17 batch id 2381 loss 0.1783990114927292 train acc 0.9276039479210416\n",
            "epoch 17 batch id 2391 loss 0.16635940968990326 train acc 0.9276453366792137\n",
            "epoch 17 batch id 2401 loss 0.14225667715072632 train acc 0.9276408267388588\n",
            "epoch 17 batch id 2411 loss 0.28978559374809265 train acc 0.9276493156366653\n",
            "epoch 17 batch id 2421 loss 0.12335747480392456 train acc 0.9276835501858736\n",
            "epoch 17 batch id 2431 loss 0.33198282122612 train acc 0.9276403743315508\n",
            "epoch 17 batch id 2441 loss 0.16894137859344482 train acc 0.927635958623515\n",
            "epoch 17 batch id 2451 loss 0.1539449244737625 train acc 0.9276379538963688\n",
            "epoch 17 batch id 2461 loss 0.14175677299499512 train acc 0.927652631044291\n",
            "epoch 17 batch id 2471 loss 0.26714080572128296 train acc 0.9276039558883044\n",
            "epoch 17 batch id 2481 loss 0.3039768636226654 train acc 0.9276627367996776\n",
            "epoch 17 batch id 2491 loss 0.22796718776226044 train acc 0.9276081393014853\n",
            "epoch 17 batch id 2501 loss 0.10387736558914185 train acc 0.927578968412635\n",
            "epoch 17 batch id 2511 loss 0.14570346474647522 train acc 0.927668259657507\n",
            "epoch 17 batch id 2521 loss 0.2441110908985138 train acc 0.9276142899642998\n",
            "epoch 17 batch id 2531 loss 0.23046326637268066 train acc 0.9276163077834848\n",
            "epoch 17 batch id 2541 loss 0.1440536379814148 train acc 0.927612160566706\n",
            "epoch 17 batch id 2551 loss 0.28452441096305847 train acc 0.9276019208153665\n",
            "epoch 17 batch id 2561 loss 0.26956865191459656 train acc 0.927579558766107\n",
            "epoch 17 batch id 2571 loss 0.20185284316539764 train acc 0.9276424542979386\n",
            "epoch 17 batch id 2581 loss 0.13672247529029846 train acc 0.9277109163115071\n",
            "epoch 17 batch id 2591 loss 0.16635845601558685 train acc 0.9277607583944423\n",
            "epoch 17 batch id 2601 loss 0.18810178339481354 train acc 0.9277561514802\n",
            "epoch 17 batch id 2611 loss 0.2944203019142151 train acc 0.927757564151666\n",
            "epoch 17 batch id 2621 loss 0.1245058923959732 train acc 0.9277708889736742\n",
            "epoch 17 batch id 2631 loss 0.25537383556365967 train acc 0.9277484796655264\n",
            "epoch 17 batch id 2641 loss 0.1948184221982956 train acc 0.927738072699735\n",
            "epoch 17 batch id 2651 loss 0.053949423134326935 train acc 0.9277984722746133\n",
            "epoch 17 batch id 2661 loss 0.19495369493961334 train acc 0.9278231867718902\n",
            "epoch 17 batch id 2671 loss 0.23119224607944489 train acc 0.9278067671284164\n",
            "epoch 17 batch id 2681 loss 0.14791692793369293 train acc 0.9277613297277135\n",
            "epoch 17 batch id 2691 loss 0.36147791147232056 train acc 0.9277162300260127\n",
            "epoch 17 batch id 2701 loss 0.31608811020851135 train acc 0.927752452795261\n",
            "epoch 17 batch id 2711 loss 0.10070917010307312 train acc 0.9277480634452232\n",
            "epoch 17 batch id 2721 loss 0.08013873547315598 train acc 0.9277379639838295\n",
            "epoch 17 batch id 2731 loss 0.07544372975826263 train acc 0.927699331746613\n",
            "epoch 17 batch id 2741 loss 0.2951575815677643 train acc 0.9276837832907698\n",
            "epoch 17 batch id 2751 loss 0.3449777364730835 train acc 0.9276285896037805\n",
            "epoch 17 batch id 2761 loss 0.11935973167419434 train acc 0.927698297718218\n",
            "epoch 17 batch id 2771 loss 0.0850481390953064 train acc 0.9277449476723204\n",
            "epoch 17 batch id 2781 loss 0.38297709822654724 train acc 0.9277463142754405\n",
            "epoch 17 batch id 2791 loss 0.19931958615779877 train acc 0.9277588677893228\n",
            "epoch 17 batch id 2801 loss 0.2773594856262207 train acc 0.9277378614780436\n",
            "epoch 17 batch id 2811 loss 0.23740491271018982 train acc 0.9277614727854856\n",
            "epoch 17 batch id 2821 loss 0.2972736358642578 train acc 0.9277461449840482\n",
            "epoch 17 batch id 2831 loss 0.30528178811073303 train acc 0.9277640409749205\n",
            "epoch 17 batch id 2841 loss 0.29315853118896484 train acc 0.9277543118620204\n",
            "epoch 17 batch id 2851 loss 0.15759755671024323 train acc 0.9277227288670642\n",
            "epoch 17 batch id 2861 loss 0.0817660391330719 train acc 0.9277350576721426\n",
            "epoch 17 batch id 2871 loss 0.2962588369846344 train acc 0.9277255311738071\n",
            "epoch 17 batch id 2881 loss 0.1537483036518097 train acc 0.9277594585213468\n",
            "epoch 17 batch id 2891 loss 0.1402302235364914 train acc 0.9277445088204773\n",
            "epoch 17 batch id 2901 loss 0.24576100707054138 train acc 0.9276919596690796\n",
            "epoch 17 batch id 2911 loss 0.08525897562503815 train acc 0.9276773445551357\n",
            "epoch 17 batch id 2921 loss 0.05573175102472305 train acc 0.9276200359465936\n",
            "epoch 17 batch id 2931 loss 0.06540297716856003 train acc 0.9276270897304674\n",
            "epoch 17 batch id 2941 loss 0.23957332968711853 train acc 0.9276075314518871\n",
            "epoch 17 batch id 2951 loss 0.18863505125045776 train acc 0.9276781175872586\n",
            "epoch 17 batch id 2961 loss 0.14405661821365356 train acc 0.9277218422830125\n",
            "epoch 17 batch id 2971 loss 0.19518214464187622 train acc 0.9277284584315045\n",
            "epoch 17 batch id 2981 loss 0.3323342800140381 train acc 0.9277035810130828\n",
            "epoch 17 batch id 2991 loss 0.15949106216430664 train acc 0.9277311099966566\n",
            "epoch 17 batch id 3001 loss 0.17829634249210358 train acc 0.9276855631456181\n",
            "epoch 17 batch id 3011 loss 0.27283623814582825 train acc 0.9276247509133179\n",
            "epoch 17 batch id 3021 loss 0.1710200160741806 train acc 0.9276160625620655\n",
            "epoch 17 batch id 3031 loss 0.2412123680114746 train acc 0.927617741669416\n",
            "epoch 17 batch id 3041 loss 0.350767582654953 train acc 0.9276091335087142\n",
            "epoch 17 batch id 3051 loss 0.1649821549654007 train acc 0.9276415519501803\n",
            "epoch 17 batch id 3061 loss 0.1781749725341797 train acc 0.9276073995426332\n",
            "epoch 17 batch id 3071 loss 0.10486697405576706 train acc 0.9276141729078476\n",
            "epoch 17 batch id 3081 loss 0.4303273856639862 train acc 0.9276462593313859\n",
            "epoch 17 batch id 3091 loss 0.13023601472377777 train acc 0.9276124231640246\n",
            "epoch 17 batch id 3101 loss 0.2336871176958084 train acc 0.9275888826185101\n",
            "epoch 17 batch id 3111 loss 0.23104532063007355 train acc 0.9275805609128898\n",
            "epoch 17 batch id 3121 loss 0.41398414969444275 train acc 0.9275572733098366\n",
            "epoch 17 batch id 3131 loss 0.08840465545654297 train acc 0.9275590865538167\n",
            "epoch 17 batch id 3141 loss 0.35073041915893555 train acc 0.9275409901305317\n",
            "epoch 17 batch id 3151 loss 0.1220056489109993 train acc 0.92759738971755\n",
            "epoch 17 batch id 3161 loss 0.27369925379753113 train acc 0.927564457450174\n",
            "epoch 17 batch id 3171 loss 0.09474214911460876 train acc 0.9275612976978871\n",
            "epoch 17 batch id 3181 loss 0.1937483251094818 train acc 0.9274844781515247\n",
            "epoch 17 batch id 3191 loss 0.08076658844947815 train acc 0.9275452444374804\n",
            "epoch 17 batch id 3201 loss 0.18294334411621094 train acc 0.927537293033427\n",
            "epoch 17 batch id 3211 loss 0.4241276979446411 train acc 0.9275147928994083\n",
            "epoch 17 batch id 3221 loss 0.2935144603252411 train acc 0.9274875814964296\n",
            "epoch 17 batch id 3231 loss 0.11901114881038666 train acc 0.9275040622098422\n",
            "epoch 17 batch id 3241 loss 0.08607112616300583 train acc 0.9275156201789572\n",
            "epoch 17 batch id 3251 loss 0.19127735495567322 train acc 0.9275511381113504\n",
            "epoch 17 batch id 3261 loss 0.15289418399333954 train acc 0.9275624808341\n",
            "epoch 17 batch id 3271 loss 0.15601085126399994 train acc 0.9275068786303883\n",
            "epoch 17 batch id 3281 loss 0.26966655254364014 train acc 0.9274992380371838\n",
            "epoch 17 batch id 3291 loss 0.09617102146148682 train acc 0.9274916438772409\n",
            "epoch 17 batch id 3301 loss 0.07570763677358627 train acc 0.9275361632838534\n",
            "epoch 17 batch id 3311 loss 0.1210399940609932 train acc 0.9275190652370885\n",
            "epoch 17 batch id 3321 loss 0.19453959167003632 train acc 0.9275161848840711\n",
            "epoch 17 batch id 3331 loss 0.285603404045105 train acc 0.9274664139897929\n",
            "epoch 17 batch id 3341 loss 0.281388521194458 train acc 0.9274590317270278\n",
            "epoch 17 batch id 3351 loss 0.18186070024967194 train acc 0.9274610190987765\n",
            "epoch 17 batch id 3361 loss 0.11881808191537857 train acc 0.9275048348705742\n",
            "epoch 17 batch id 3371 loss 0.20089377462863922 train acc 0.9275066745772768\n",
            "epoch 17 batch id 3381 loss 0.08529157191514969 train acc 0.927536231884058\n",
            "epoch 17 batch id 3391 loss 0.2380109578371048 train acc 0.9275886537894427\n",
            "epoch 17 batch id 3401 loss 0.2873185873031616 train acc 0.9275994192884446\n",
            "epoch 17 batch id 3411 loss 0.2628974914550781 train acc 0.9276009601289944\n",
            "epoch 17 batch id 3421 loss 0.22205758094787598 train acc 0.9276298962291728\n",
            "epoch 17 batch id 3431 loss 0.1854729950428009 train acc 0.9275949067327309\n",
            "epoch 17 batch id 3441 loss 0.08900406956672668 train acc 0.9276236922406277\n",
            "epoch 17 batch id 3451 loss 0.23724186420440674 train acc 0.9276206172124022\n",
            "epoch 17 batch id 3461 loss 0.1067558228969574 train acc 0.9276852788211499\n",
            "epoch 17 batch id 3471 loss 0.25383463501930237 train acc 0.9277225583405359\n",
            "epoch 17 batch id 3481 loss 0.2833211123943329 train acc 0.9276743392703246\n",
            "epoch 17 batch id 3491 loss 0.15459556877613068 train acc 0.9276398238327127\n",
            "epoch 17 batch id 3501 loss 0.3699110448360443 train acc 0.9276813767495001\n",
            "epoch 17 batch id 3511 loss 0.0818372443318367 train acc 0.9277048917687268\n",
            "epoch 17 batch id 3521 loss 0.31874045729637146 train acc 0.9277460238568589\n",
            "epoch 17 batch id 3531 loss 0.3715851306915283 train acc 0.927773647691872\n",
            "epoch 17 batch id 3541 loss 0.10264450311660767 train acc 0.9278320036712793\n",
            "epoch 17 batch id 3551 loss 0.2762020528316498 train acc 0.9277932272599267\n",
            "epoch 17 batch id 3561 loss 0.030252652242779732 train acc 0.9279038542544229\n",
            "epoch 17 batch id 3571 loss 0.2742910087108612 train acc 0.9279044735368244\n",
            "epoch 17 batch id 3581 loss 0.29115670919418335 train acc 0.9279487224239039\n",
            "epoch 17 batch id 3591 loss 0.3509371280670166 train acc 0.9279274575327207\n",
            "epoch 17 batch id 3601 loss 0.35379528999328613 train acc 0.9279193279644543\n",
            "epoch 17 batch id 3611 loss 0.07640762627124786 train acc 0.927928551647743\n",
            "epoch 17 batch id 3621 loss 0.06155635043978691 train acc 0.9279679301297984\n",
            "epoch 17 batch id 3631 loss 0.2147110104560852 train acc 0.9278952079316992\n",
            "epoch 17 batch id 3641 loss 0.3172200322151184 train acc 0.9279044218621257\n",
            "epoch 17 batch id 3651 loss 0.16660311818122864 train acc 0.9278964667214462\n",
            "epoch 17 batch id 3661 loss 0.26641663908958435 train acc 0.9279056268779022\n",
            "epoch 17 batch id 3671 loss 0.1254262626171112 train acc 0.9279402751293926\n",
            "epoch 17 batch id 3681 loss 0.2060115933418274 train acc 0.9279577560445531\n",
            "epoch 17 batch id 3691 loss 0.2765117287635803 train acc 0.9279624424275265\n",
            "epoch 17 batch id 3701 loss 0.19339236617088318 train acc 0.9279839908132937\n",
            "epoch 17 batch id 3711 loss 0.20301249623298645 train acc 0.9280391067097817\n",
            "epoch 17 batch id 3721 loss 0.2410763055086136 train acc 0.9280393375436711\n",
            "epoch 17 batch id 3731 loss 0.11328893899917603 train acc 0.9280437550254623\n",
            "epoch 17 batch id 3741 loss 0.17743836343288422 train acc 0.9280356188184977\n",
            "epoch 17 batch id 3751 loss 0.21261022984981537 train acc 0.9280441882164756\n",
            "epoch 17 batch id 3761 loss 0.16893735527992249 train acc 0.9280859478862005\n",
            "epoch 17 batch id 3771 loss 0.08537276089191437 train acc 0.9280653341288783\n",
            "epoch 17 batch id 3781 loss 0.32785844802856445 train acc 0.928086154456493\n",
            "epoch 17 batch id 3791 loss 0.1613132208585739 train acc 0.9281233513584806\n",
            "epoch 17 batch id 3801 loss 0.19804689288139343 train acc 0.9281356879768482\n",
            "epoch 17 batch id 3811 loss 0.07716814428567886 train acc 0.9281643597480976\n",
            "epoch 17 batch id 3821 loss 0.2548745572566986 train acc 0.9281724352263805\n",
            "epoch 17 batch id 3831 loss 0.11045664548873901 train acc 0.9282008613938919\n",
            "epoch 17 batch id 3841 loss 0.11568152159452438 train acc 0.9282006638896121\n",
            "epoch 17 batch id 3851 loss 0.1402353197336197 train acc 0.9282045247987536\n",
            "epoch 17 batch id 3861 loss 0.0815906971693039 train acc 0.928232646982647\n",
            "epoch 17 batch id 3871 loss 0.42828652262687683 train acc 0.9282283324722294\n",
            "epoch 17 batch id 3881 loss 0.220855250954628 train acc 0.9282079360989436\n",
            "epoch 17 batch id 3891 loss 0.18739613890647888 train acc 0.928215754304806\n",
            "epoch 17 batch id 3901 loss 0.07818707823753357 train acc 0.9281714624455268\n",
            "epoch 17 batch id 3911 loss 0.0825376808643341 train acc 0.9281793339299412\n",
            "epoch 17 batch id 3921 loss 0.08027425408363342 train acc 0.9281712254526906\n",
            "epoch 17 batch id 3931 loss 0.34545016288757324 train acc 0.9281790574917324\n",
            "epoch 17 batch id 3941 loss 0.3838336169719696 train acc 0.9281749555950266\n",
            "epoch 17 batch id 3951 loss 0.1088503897190094 train acc 0.9281392369020501\n",
            "epoch 17 batch id 3961 loss 0.16340060532093048 train acc 0.9281155326937642\n",
            "epoch 17 batch id 3971 loss 0.208817258477211 train acc 0.9281273608662806\n",
            "epoch 17 batch id 3981 loss 0.06146073713898659 train acc 0.9281626789751318\n",
            "epoch 17 batch id 3991 loss 0.207434743642807 train acc 0.928244800801804\n",
            "epoch 17 batch id 4001 loss 0.1218978762626648 train acc 0.9282562171957011\n",
            "epoch 17 batch id 4011 loss 0.1690974235534668 train acc 0.9282169346796311\n",
            "epoch 17 batch id 4021 loss 0.15784120559692383 train acc 0.9281778475503606\n",
            "epoch 17 batch id 4031 loss 0.20560908317565918 train acc 0.9281893450756636\n",
            "epoch 17 batch id 4041 loss 0.28451940417289734 train acc 0.9281814526107399\n",
            "epoch 17 batch id 4051 loss 0.056686289608478546 train acc 0.9282198839792644\n",
            "epoch 17 batch id 4061 loss 0.21106842160224915 train acc 0.9282350406303866\n",
            "epoch 17 batch id 4071 loss 0.2499527782201767 train acc 0.9282923421763695\n",
            "epoch 17 batch id 4081 loss 0.1700667440891266 train acc 0.9283034182798334\n",
            "epoch 17 batch id 4091 loss 0.18801087141036987 train acc 0.92834117575165\n",
            "epoch 17 batch id 4101 loss 0.19656185805797577 train acc 0.9283444586686174\n",
            "epoch 17 batch id 4111 loss 0.1067948043346405 train acc 0.9283249209438093\n",
            "epoch 17 batch id 4121 loss 0.22561630606651306 train acc 0.9283016864838631\n",
            "epoch 17 batch id 4131 loss 0.11913210898637772 train acc 0.9283088235294118\n",
            "epoch 17 batch id 4141 loss 0.20158636569976807 train acc 0.9282895134025597\n",
            "epoch 17 batch id 4151 loss 0.19459611177444458 train acc 0.9283117019995182\n",
            "epoch 17 batch id 4161 loss 0.2620494067668915 train acc 0.9282661920211488\n",
            "epoch 17 batch id 4171 loss 0.22092626988887787 train acc 0.928303314552865\n",
            "epoch 17 batch id 4181 loss 0.07586750388145447 train acc 0.9282879394881607\n",
            "epoch 17 batch id 4191 loss 0.3226359784603119 train acc 0.9283061918396565\n",
            "epoch 17 batch id 4201 loss 0.2070872038602829 train acc 0.9283503927636277\n",
            "epoch 17 batch id 4211 loss 0.23225602507591248 train acc 0.9283498575160295\n",
            "epoch 17 batch id 4221 loss 0.2695346176624298 train acc 0.9283160092395167\n",
            "epoch 17 batch id 4231 loss 0.1470840871334076 train acc 0.9283007858662254\n",
            "epoch 17 batch id 4241 loss 0.14167645573616028 train acc 0.928337214100448\n",
            "epoch 17 batch id 4251 loss 0.1880849301815033 train acc 0.9283844977652317\n",
            "epoch 17 batch id 4261 loss 0.13579915463924408 train acc 0.9284022236564187\n",
            "epoch 17 batch id 4271 loss 0.07699979096651077 train acc 0.9284308417232499\n",
            "epoch 17 batch id 4281 loss 0.2478456050157547 train acc 0.9284556762438683\n",
            "epoch 17 batch id 4291 loss 0.06800637394189835 train acc 0.9284767536704731\n",
            "epoch 17 batch id 4301 loss 0.11579535156488419 train acc 0.9284686700767263\n",
            "epoch 17 batch id 4311 loss 0.10780569165945053 train acc 0.9285294885177453\n",
            "epoch 17 batch id 4321 loss 0.09708044677972794 train acc 0.9285177042351308\n",
            "epoch 17 batch id 4331 loss 0.16737408936023712 train acc 0.9285095820826599\n",
            "epoch 17 batch id 4341 loss 0.0780334621667862 train acc 0.9284978979497811\n",
            "epoch 17 batch id 4351 loss 0.13008753955364227 train acc 0.9285006320386118\n",
            "epoch 17 batch id 4361 loss 0.23058770596981049 train acc 0.9285355996331117\n",
            "epoch 17 batch id 4371 loss 0.17588628828525543 train acc 0.928581131320064\n",
            "epoch 17 batch id 4381 loss 0.2561798393726349 train acc 0.9285836566993837\n",
            "epoch 17 batch id 4391 loss 0.024587785825133324 train acc 0.9286039626508767\n",
            "epoch 17 batch id 4401 loss 0.22779138386249542 train acc 0.9285709213815042\n",
            "epoch 17 batch id 4411 loss 0.13071297109127045 train acc 0.9285699104511449\n",
            "epoch 17 batch id 4421 loss 0.1878005564212799 train acc 0.9286219181180728\n",
            "epoch 17 batch id 4431 loss 0.16185733675956726 train acc 0.9286490069961634\n",
            "epoch 17 batch id 4441 loss 0.05315861105918884 train acc 0.9286900472866472\n",
            "epoch 17 batch id 4451 loss 0.17166051268577576 train acc 0.9286922882498315\n",
            "epoch 17 batch id 4461 loss 0.14787138998508453 train acc 0.9286980217440036\n",
            "epoch 17 batch id 4471 loss 0.4117698073387146 train acc 0.9286548031760232\n",
            "epoch 17 batch id 4481 loss 0.06274797767400742 train acc 0.9286361861191699\n",
            "epoch 17 batch id 4491 loss 0.24012620747089386 train acc 0.9286280895123581\n",
            "epoch 17 batch id 4501 loss 0.05276178941130638 train acc 0.9286408575872028\n",
            "epoch 17 batch id 4511 loss 0.2767009139060974 train acc 0.9286362502771004\n",
            "epoch 17 batch id 4521 loss 0.41558554768562317 train acc 0.9286178389736784\n",
            "epoch 17 batch id 4531 loss 0.12635445594787598 train acc 0.9285926120061796\n",
            "epoch 17 batch id 4541 loss 0.15832477807998657 train acc 0.9286363135873156\n",
            "epoch 17 batch id 4551 loss 0.3308100402355194 train acc 0.9286592232476378\n",
            "epoch 17 batch id 4561 loss 0.23661693930625916 train acc 0.928658051962289\n",
            "epoch 17 batch id 4571 loss 0.18944303691387177 train acc 0.9286705589586524\n",
            "epoch 17 batch id 4581 loss 0.16373136639595032 train acc 0.928665957214582\n",
            "epoch 17 batch id 4591 loss 0.18627658486366272 train acc 0.9286545687214115\n",
            "epoch 17 batch id 4601 loss 0.20734001696109772 train acc 0.9286432297326668\n",
            "epoch 17 batch id 4611 loss 0.21216298639774323 train acc 0.9286759921925829\n",
            "epoch 17 batch id 4621 loss 0.1059613823890686 train acc 0.9287086128543606\n",
            "epoch 17 batch id 4631 loss 0.1484300196170807 train acc 0.9287141006262146\n",
            "epoch 17 batch id 4641 loss 0.24050848186016083 train acc 0.9287195647489765\n",
            "epoch 17 batch id 4651 loss 0.43368107080459595 train acc 0.9287216458826059\n",
            "epoch 17 batch id 4661 loss 0.23520544171333313 train acc 0.9287170135164128\n",
            "epoch 17 batch id 4671 loss 0.31141695380210876 train acc 0.9287291265253693\n",
            "epoch 17 batch id 4681 loss 0.4793066680431366 train acc 0.9287178220465713\n",
            "epoch 17 batch id 4691 loss 0.19950906932353973 train acc 0.9287565284587508\n",
            "epoch 17 batch id 4701 loss 0.23564696311950684 train acc 0.928758508827909\n",
            "epoch 17 batch id 4711 loss 0.11882713437080383 train acc 0.9287903311398854\n",
            "epoch 17 batch id 4721 loss 0.11410363763570786 train acc 0.9287823024782885\n",
            "epoch 17 batch id 4731 loss 0.10841816663742065 train acc 0.9288007292327204\n",
            "epoch 17 batch id 4741 loss 0.16473498940467834 train acc 0.928819078253533\n",
            "epoch 17 batch id 4751 loss 0.22087612748146057 train acc 0.9288044622184803\n",
            "epoch 17 batch id 4761 loss 0.17036683857440948 train acc 0.9287866257088847\n",
            "epoch 17 batch id 4771 loss 0.1679805964231491 train acc 0.9287950639278977\n",
            "epoch 17 batch id 4781 loss 0.3425658047199249 train acc 0.928744640242627\n",
            "epoch 17 batch id 4791 loss 0.3893173635005951 train acc 0.9287009496973492\n",
            "epoch 17 batch id 4801 loss 0.1667979657649994 train acc 0.9286997500520725\n",
            "epoch 17 batch id 4811 loss 0.2398044914007187 train acc 0.9287147942215755\n",
            "epoch 17 batch id 4821 loss 0.265407919883728 train acc 0.9286811605476042\n",
            "epoch 17 batch id 4831 loss 0.2361784130334854 train acc 0.9287123525150073\n",
            "epoch 17 batch id 4841 loss 0.2595442831516266 train acc 0.9286788628382565\n",
            "epoch 17 batch id 4851 loss 0.1429007649421692 train acc 0.928684163059163\n",
            "epoch 17 batch id 4861 loss 0.25269052386283875 train acc 0.928654083521909\n",
            "epoch 17 batch id 4871 loss 0.15147747099399567 train acc 0.9286914904537056\n",
            "epoch 17 batch id 4881 loss 0.2134096473455429 train acc 0.9287063357918459\n",
            "epoch 17 batch id 4891 loss 0.18795692920684814 train acc 0.928695563279493\n",
            "epoch 17 batch id 4901 loss 0.20739218592643738 train acc 0.9287071516017139\n",
            "epoch 17 batch id 4911 loss 0.2897160053253174 train acc 0.92868369476685\n",
            "epoch 17 batch id 4921 loss 0.2565467059612274 train acc 0.9286571580979476\n",
            "epoch 17 batch id 4931 loss 0.4160626530647278 train acc 0.9286148854187791\n",
            "epoch 17 batch id 4941 loss 0.14288584887981415 train acc 0.9285791084800648\n",
            "epoch 17 batch id 4951 loss 0.25340738892555237 train acc 0.9285624116340133\n",
            "epoch 17 train acc 0.9285798205476186\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b806c362b94465cb49461c9bc29ac22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17 loss 0.4616341292858124 test acc 0.8155287756598241\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b3d323ae915480e854fc18662de8bd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 18 batch id 1 loss 0.17640265822410583 train acc 0.9375\n",
            "epoch 18 batch id 11 loss 0.11494135856628418 train acc 0.9332386363636364\n",
            "epoch 18 batch id 21 loss 0.1517249196767807 train acc 0.9382440476190477\n",
            "epoch 18 batch id 31 loss 0.13566847145557404 train acc 0.9334677419354839\n",
            "epoch 18 batch id 41 loss 0.11960392445325851 train acc 0.9363567073170732\n",
            "epoch 18 batch id 51 loss 0.20199799537658691 train acc 0.9375\n",
            "epoch 18 batch id 61 loss 0.18984220921993256 train acc 0.9346823770491803\n",
            "epoch 18 batch id 71 loss 0.3352842330932617 train acc 0.9328785211267606\n",
            "epoch 18 batch id 81 loss 0.21040333807468414 train acc 0.9315200617283951\n",
            "epoch 18 batch id 91 loss 0.1354922205209732 train acc 0.9313186813186813\n",
            "epoch 18 batch id 101 loss 0.19735433161258698 train acc 0.9305383663366337\n",
            "epoch 18 batch id 111 loss 0.1155833899974823 train acc 0.9303209459459459\n",
            "epoch 18 batch id 121 loss 0.20368802547454834 train acc 0.9318181818181818\n",
            "epoch 18 batch id 131 loss 0.25090378522872925 train acc 0.9322519083969466\n",
            "epoch 18 batch id 141 loss 0.12195335328578949 train acc 0.9307402482269503\n",
            "epoch 18 batch id 151 loss 0.3698928654193878 train acc 0.9309809602649006\n",
            "epoch 18 batch id 161 loss 0.3620336651802063 train acc 0.9314829192546584\n",
            "epoch 18 batch id 171 loss 0.18619471788406372 train acc 0.9319261695906432\n",
            "epoch 18 batch id 181 loss 0.11185847967863083 train acc 0.9334426795580111\n",
            "epoch 18 batch id 191 loss 0.1739119291305542 train acc 0.9333278795811518\n",
            "epoch 18 batch id 201 loss 0.22348475456237793 train acc 0.9340018656716418\n",
            "epoch 18 batch id 211 loss 0.1406983882188797 train acc 0.9342417061611374\n",
            "epoch 18 batch id 221 loss 0.3643825352191925 train acc 0.9336114253393665\n",
            "epoch 18 batch id 231 loss 0.04540375620126724 train acc 0.9344561688311688\n",
            "epoch 18 batch id 241 loss 0.3234061896800995 train acc 0.9332209543568465\n",
            "epoch 18 batch id 251 loss 0.21824564039707184 train acc 0.9333291832669323\n",
            "epoch 18 batch id 261 loss 0.30369916558265686 train acc 0.9334889846743295\n",
            "epoch 18 batch id 271 loss 0.06237747147679329 train acc 0.9338676199261993\n",
            "epoch 18 batch id 281 loss 0.1046164482831955 train acc 0.9337744661921709\n",
            "epoch 18 batch id 291 loss 0.21194979548454285 train acc 0.9341172680412371\n",
            "epoch 18 batch id 301 loss 0.25589486956596375 train acc 0.9340739202657807\n",
            "epoch 18 batch id 311 loss 0.21070647239685059 train acc 0.9338826366559485\n",
            "epoch 18 batch id 321 loss 0.2659067213535309 train acc 0.9339953271028038\n",
            "epoch 18 batch id 331 loss 0.23237788677215576 train acc 0.9336291540785498\n",
            "epoch 18 batch id 341 loss 0.3529176712036133 train acc 0.9337884897360704\n",
            "epoch 18 batch id 351 loss 0.40428662300109863 train acc 0.9336716524216524\n",
            "epoch 18 batch id 361 loss 0.1567676067352295 train acc 0.9338209833795014\n",
            "epoch 18 batch id 371 loss 0.18364956974983215 train acc 0.9340043800539084\n",
            "epoch 18 batch id 381 loss 0.05831129103899002 train acc 0.9341371391076115\n",
            "epoch 18 batch id 391 loss 0.3308262526988983 train acc 0.9340632992327366\n",
            "epoch 18 batch id 401 loss 0.026833826676011086 train acc 0.933876246882793\n",
            "epoch 18 batch id 411 loss 0.08752892911434174 train acc 0.9343065693430657\n",
            "epoch 18 batch id 421 loss 0.1700417846441269 train acc 0.9341597387173397\n",
            "epoch 18 batch id 431 loss 0.20110809803009033 train acc 0.9343822505800464\n",
            "epoch 18 batch id 441 loss 0.3721925914287567 train acc 0.9340986394557823\n",
            "epoch 18 batch id 451 loss 0.18996769189834595 train acc 0.9343472838137472\n",
            "epoch 18 batch id 461 loss 0.06041552871465683 train acc 0.9341784164859002\n",
            "epoch 18 batch id 471 loss 0.18375569581985474 train acc 0.934381634819533\n",
            "epoch 18 batch id 481 loss 0.345743328332901 train acc 0.934706340956341\n",
            "epoch 18 batch id 491 loss 0.16398844122886658 train acc 0.9348268839103869\n",
            "epoch 18 batch id 501 loss 0.30114081501960754 train acc 0.934942614770459\n",
            "epoch 18 batch id 511 loss 0.2639998197555542 train acc 0.9344422700587084\n",
            "epoch 18 batch id 521 loss 0.38027361035346985 train acc 0.9344709692898272\n",
            "epoch 18 batch id 531 loss 0.15755411982536316 train acc 0.9345280131826742\n",
            "epoch 18 batch id 541 loss 0.09716005623340607 train acc 0.9342941312384473\n",
            "epoch 18 batch id 551 loss 0.2121344655752182 train acc 0.9342955989110708\n",
            "epoch 18 batch id 561 loss 0.109872967004776 train acc 0.9344084224598931\n",
            "epoch 18 batch id 571 loss 0.29904672503471375 train acc 0.9347635726795096\n",
            "epoch 18 batch id 581 loss 0.25526076555252075 train acc 0.9349720309810671\n",
            "epoch 18 batch id 591 loss 0.13665582239627838 train acc 0.9352263113367174\n",
            "epoch 18 batch id 601 loss 0.16176658868789673 train acc 0.9353941347753744\n",
            "epoch 18 batch id 611 loss 0.2294444441795349 train acc 0.9354286006546645\n",
            "epoch 18 batch id 621 loss 0.2351590245962143 train acc 0.9355122785829307\n",
            "epoch 18 batch id 631 loss 0.40722745656967163 train acc 0.9356428288431062\n",
            "epoch 18 batch id 641 loss 0.25858640670776367 train acc 0.9357693057722309\n",
            "epoch 18 batch id 651 loss 0.14470252394676208 train acc 0.935723886328725\n",
            "epoch 18 batch id 661 loss 0.1304987370967865 train acc 0.935821671709531\n",
            "epoch 18 batch id 671 loss 0.09726448357105255 train acc 0.9358466840536512\n",
            "epoch 18 batch id 681 loss 0.19065631926059723 train acc 0.9358021292217328\n",
            "epoch 18 batch id 691 loss 0.2444029450416565 train acc 0.9358040882778582\n",
            "epoch 18 batch id 701 loss 0.2192084789276123 train acc 0.935739122681883\n",
            "epoch 18 batch id 711 loss 0.4223718047142029 train acc 0.9357638888888888\n",
            "epoch 18 batch id 721 loss 0.19125641882419586 train acc 0.9355929264909847\n",
            "epoch 18 batch id 731 loss 0.147362619638443 train acc 0.9354907660738714\n",
            "epoch 18 batch id 741 loss 0.16953012347221375 train acc 0.9353702766531714\n",
            "epoch 18 batch id 751 loss 0.2369695007801056 train acc 0.9353986351531292\n",
            "epoch 18 batch id 761 loss 0.07745198160409927 train acc 0.9354467805519053\n",
            "epoch 18 batch id 771 loss 0.1705971658229828 train acc 0.9353518158236057\n",
            "epoch 18 batch id 781 loss 0.18991036713123322 train acc 0.9352192701664532\n",
            "epoch 18 batch id 791 loss 0.17941579222679138 train acc 0.9352876106194691\n",
            "epoch 18 batch id 801 loss 0.1083117425441742 train acc 0.9352567103620475\n",
            "epoch 18 batch id 811 loss 0.31663036346435547 train acc 0.9352651048088779\n",
            "epoch 18 batch id 821 loss 0.12361161410808563 train acc 0.9352732947624848\n",
            "epoch 18 batch id 831 loss 0.27965036034584045 train acc 0.9353188929001204\n",
            "epoch 18 batch id 841 loss 0.2200150191783905 train acc 0.9355306183115338\n",
            "epoch 18 batch id 851 loss 0.26885682344436646 train acc 0.9353701527614571\n",
            "epoch 18 batch id 861 loss 0.10600320994853973 train acc 0.9354130371660859\n",
            "epoch 18 batch id 871 loss 0.10118946433067322 train acc 0.935526693455798\n",
            "epoch 18 batch id 881 loss 0.16468225419521332 train acc 0.9354958853575482\n",
            "epoch 18 batch id 891 loss 0.12226971983909607 train acc 0.93565867003367\n",
            "epoch 18 batch id 901 loss 0.21615254878997803 train acc 0.935644422863485\n",
            "epoch 18 batch id 911 loss 0.31914815306663513 train acc 0.935544731064764\n",
            "epoch 18 batch id 921 loss 0.4315205216407776 train acc 0.9355489956568946\n",
            "epoch 18 batch id 931 loss 0.07616590708494186 train acc 0.9353182062298604\n",
            "epoch 18 batch id 941 loss 0.12389540672302246 train acc 0.9354244155154091\n",
            "epoch 18 batch id 951 loss 0.15108713507652283 train acc 0.9353476603575184\n",
            "epoch 18 batch id 961 loss 0.23785555362701416 train acc 0.9355001300728408\n",
            "epoch 18 batch id 971 loss 0.3171260952949524 train acc 0.9355046343975283\n",
            "epoch 18 batch id 981 loss 0.27923285961151123 train acc 0.9356683231396534\n",
            "epoch 18 batch id 991 loss 0.16796955466270447 train acc 0.9356552724520686\n",
            "epoch 18 batch id 1001 loss 0.14964298903942108 train acc 0.9357361388611388\n",
            "epoch 18 batch id 1011 loss 0.20865823328495026 train acc 0.9357381305637982\n",
            "epoch 18 batch id 1021 loss 0.18269780278205872 train acc 0.935847208619001\n",
            "epoch 18 batch id 1031 loss 0.1512010395526886 train acc 0.9356965324927256\n",
            "epoch 18 batch id 1041 loss 0.19258403778076172 train acc 0.9355337415946205\n",
            "epoch 18 batch id 1051 loss 0.10507569462060928 train acc 0.9353145813510942\n",
            "epoch 18 batch id 1061 loss 0.23739005625247955 train acc 0.9351879123468426\n",
            "epoch 18 batch id 1071 loss 0.06703968346118927 train acc 0.9352824463118581\n",
            "epoch 18 batch id 1081 loss 0.4677702486515045 train acc 0.9351150555041629\n",
            "epoch 18 batch id 1091 loss 0.3841501474380493 train acc 0.9350653070577452\n",
            "epoch 18 batch id 1101 loss 0.16447733342647552 train acc 0.9351299954586739\n",
            "epoch 18 batch id 1111 loss 0.18595808744430542 train acc 0.9351653915391539\n",
            "epoch 18 batch id 1121 loss 0.12401304394006729 train acc 0.9352001561106156\n",
            "epoch 18 batch id 1131 loss 0.24603889882564545 train acc 0.9351099690539346\n",
            "epoch 18 batch id 1141 loss 0.3184764087200165 train acc 0.9350761393514461\n",
            "epoch 18 batch id 1151 loss 0.1894655078649521 train acc 0.934988596872285\n",
            "epoch 18 batch id 1161 loss 0.1256592869758606 train acc 0.9349294788975021\n",
            "epoch 18 batch id 1171 loss 0.18039561808109283 train acc 0.9349647736976943\n",
            "epoch 18 batch id 1181 loss 0.26856428384780884 train acc 0.934946549534293\n",
            "epoch 18 batch id 1191 loss 0.22662784159183502 train acc 0.9349811083123426\n",
            "epoch 18 batch id 1201 loss 0.1242765486240387 train acc 0.9351972314737719\n",
            "epoch 18 batch id 1211 loss 0.04794773459434509 train acc 0.9352162469033857\n",
            "epoch 18 batch id 1221 loss 0.26656532287597656 train acc 0.9352349508599509\n",
            "epoch 18 batch id 1231 loss 0.1627691686153412 train acc 0.9351264216084484\n",
            "epoch 18 batch id 1241 loss 0.16706393659114838 train acc 0.9351833199033038\n",
            "epoch 18 batch id 1251 loss 0.2046176791191101 train acc 0.9351144084732215\n",
            "epoch 18 batch id 1261 loss 0.07743845880031586 train acc 0.9351828905630452\n",
            "epoch 18 batch id 1271 loss 0.13364122807979584 train acc 0.9353240558615263\n",
            "epoch 18 batch id 1281 loss 0.16222795844078064 train acc 0.9354264246682279\n",
            "epoch 18 batch id 1291 loss 0.21429407596588135 train acc 0.9355151045701007\n",
            "epoch 18 batch id 1301 loss 0.13684765994548798 train acc 0.9354583013066872\n",
            "epoch 18 batch id 1311 loss 0.21344627439975739 train acc 0.9354262013729977\n",
            "epoch 18 batch id 1321 loss 0.02893080748617649 train acc 0.9354537282361847\n",
            "epoch 18 batch id 1331 loss 0.3085174262523651 train acc 0.9353869271224643\n",
            "epoch 18 batch id 1341 loss 0.2905071973800659 train acc 0.9353793810589113\n",
            "epoch 18 batch id 1351 loss 0.20281830430030823 train acc 0.93534881569208\n",
            "epoch 18 batch id 1361 loss 0.30513137578964233 train acc 0.9351579720793534\n",
            "epoch 18 batch id 1371 loss 0.0796939879655838 train acc 0.9352206418672502\n",
            "epoch 18 batch id 1381 loss 0.3645094931125641 train acc 0.9350900615496017\n",
            "epoch 18 batch id 1391 loss 0.042323049157857895 train acc 0.9351073867721064\n",
            "epoch 18 batch id 1401 loss 0.10540629923343658 train acc 0.9351467701641685\n",
            "epoch 18 batch id 1411 loss 0.04113553464412689 train acc 0.9352852586817859\n",
            "epoch 18 batch id 1421 loss 0.14999830722808838 train acc 0.9352238740323716\n",
            "epoch 18 batch id 1431 loss 0.2401459664106369 train acc 0.9351415094339622\n",
            "epoch 18 batch id 1441 loss 0.15201665461063385 train acc 0.9350602879944483\n",
            "epoch 18 batch id 1451 loss 0.14241169393062592 train acc 0.9351417126119917\n",
            "epoch 18 batch id 1461 loss 0.1009620949625969 train acc 0.9351043805612594\n",
            "epoch 18 batch id 1471 loss 0.24279960989952087 train acc 0.9351100441876274\n",
            "epoch 18 batch id 1481 loss 0.21421188116073608 train acc 0.9351050810263336\n",
            "epoch 18 batch id 1491 loss 0.20038671791553497 train acc 0.9351421026156942\n",
            "epoch 18 batch id 1501 loss 0.19133853912353516 train acc 0.935261908727515\n",
            "epoch 18 batch id 1511 loss 0.35894086956977844 train acc 0.9352663798808736\n",
            "epoch 18 batch id 1521 loss 0.17679478228092194 train acc 0.9352913379355687\n",
            "epoch 18 batch id 1531 loss 0.0918358564376831 train acc 0.9354180274330502\n",
            "epoch 18 batch id 1541 loss 0.13340307772159576 train acc 0.9353808403634004\n",
            "epoch 18 batch id 1551 loss 0.1303548961877823 train acc 0.9353542069632496\n",
            "epoch 18 batch id 1561 loss 0.3388468325138092 train acc 0.9353279147982063\n",
            "epoch 18 batch id 1571 loss 0.24186287820339203 train acc 0.9352721196690006\n",
            "epoch 18 batch id 1581 loss 0.10093189775943756 train acc 0.9353356261859582\n",
            "epoch 18 batch id 1591 loss 0.32998043298721313 train acc 0.9353001257071024\n",
            "epoch 18 batch id 1601 loss 0.34656599164009094 train acc 0.9352845877576514\n",
            "epoch 18 batch id 1611 loss 0.1821494996547699 train acc 0.9352498448168839\n",
            "epoch 18 batch id 1621 loss 0.07351738214492798 train acc 0.9352058914250463\n",
            "epoch 18 batch id 1631 loss 0.04142032936215401 train acc 0.9353157572041693\n",
            "epoch 18 batch id 1641 loss 0.1458294838666916 train acc 0.9353195460085314\n",
            "epoch 18 batch id 1651 loss 0.2309122085571289 train acc 0.9353895366444579\n",
            "epoch 18 batch id 1661 loss 0.19183477759361267 train acc 0.9353363937387116\n",
            "epoch 18 batch id 1671 loss 0.09445282816886902 train acc 0.9352838868940754\n",
            "epoch 18 batch id 1681 loss 0.3600272834300995 train acc 0.9351483491969066\n",
            "epoch 18 batch id 1691 loss 0.3320953845977783 train acc 0.9352176966292135\n",
            "epoch 18 batch id 1701 loss 0.301798939704895 train acc 0.9351851851851852\n",
            "epoch 18 batch id 1711 loss 0.22720488905906677 train acc 0.935134789596727\n",
            "epoch 18 batch id 1721 loss 0.1272454410791397 train acc 0.9351939279488669\n",
            "epoch 18 batch id 1731 loss 0.0202545914798975 train acc 0.9352614095898325\n",
            "epoch 18 batch id 1741 loss 0.16772447526454926 train acc 0.9352473434807582\n",
            "epoch 18 batch id 1751 loss 0.10833737999200821 train acc 0.9351888206739006\n",
            "epoch 18 batch id 1761 loss 0.25593283772468567 train acc 0.9352729273140261\n",
            "epoch 18 batch id 1771 loss 0.13382166624069214 train acc 0.9353649068322981\n",
            "epoch 18 batch id 1781 loss 0.11350590735673904 train acc 0.9354032144862436\n",
            "epoch 18 batch id 1791 loss 0.3552893400192261 train acc 0.9354061976549414\n",
            "epoch 18 batch id 1801 loss 0.0827476903796196 train acc 0.9354959050527485\n",
            "epoch 18 batch id 1811 loss 0.09098199009895325 train acc 0.9354983434566538\n",
            "epoch 18 batch id 1821 loss 0.18114781379699707 train acc 0.9355264964305327\n",
            "epoch 18 batch id 1831 loss 0.1043146625161171 train acc 0.9355458083014746\n",
            "epoch 18 batch id 1841 loss 0.12006403505802155 train acc 0.9355309614340033\n",
            "epoch 18 batch id 1851 loss 0.23817557096481323 train acc 0.9355753646677472\n",
            "epoch 18 batch id 1861 loss 0.14082849025726318 train acc 0.9355185384202042\n",
            "epoch 18 batch id 1871 loss 0.2040204256772995 train acc 0.9356460448957776\n",
            "epoch 18 batch id 1881 loss 0.1413814276456833 train acc 0.9357306618819776\n",
            "epoch 18 batch id 1891 loss 0.07396486401557922 train acc 0.9358309095716552\n",
            "epoch 18 batch id 1901 loss 0.22077739238739014 train acc 0.9357821541294056\n",
            "epoch 18 batch id 1911 loss 0.1447683721780777 train acc 0.935782967032967\n",
            "epoch 18 batch id 1921 loss 0.2281508594751358 train acc 0.9356942998438313\n",
            "epoch 18 batch id 1931 loss 0.35664817690849304 train acc 0.9357036509580529\n",
            "epoch 18 batch id 1941 loss 0.14246387779712677 train acc 0.9357048557444616\n",
            "epoch 18 batch id 1951 loss 0.35459938645362854 train acc 0.9357541004613019\n",
            "epoch 18 batch id 1961 loss 0.09045793861150742 train acc 0.9357789393166752\n",
            "epoch 18 batch id 1971 loss 0.12537766993045807 train acc 0.9358114535768646\n",
            "epoch 18 batch id 1981 loss 0.1993023008108139 train acc 0.9357647652700656\n",
            "epoch 18 batch id 1991 loss 0.3258020281791687 train acc 0.9357263937719739\n",
            "epoch 18 batch id 2001 loss 0.3763035535812378 train acc 0.9357040229885057\n",
            "epoch 18 batch id 2011 loss 0.18565750122070312 train acc 0.9356663351566384\n",
            "epoch 18 batch id 2021 loss 0.2910752594470978 train acc 0.9356290202869867\n",
            "epoch 18 batch id 2031 loss 0.03800689056515694 train acc 0.9356382323978336\n",
            "epoch 18 batch id 2041 loss 0.16425184905529022 train acc 0.935563143067124\n",
            "epoch 18 batch id 2051 loss 0.4067842960357666 train acc 0.935580204778157\n",
            "epoch 18 batch id 2061 loss 0.2044668346643448 train acc 0.9355516132945172\n",
            "epoch 18 batch id 2071 loss 0.14425502717494965 train acc 0.9355308425881217\n",
            "epoch 18 batch id 2081 loss 0.33571577072143555 train acc 0.9354952546852475\n",
            "epoch 18 batch id 2091 loss 0.16296015679836273 train acc 0.9354375896700143\n",
            "epoch 18 batch id 2101 loss 0.0480097159743309 train acc 0.935492027605902\n",
            "epoch 18 batch id 2111 loss 0.18561944365501404 train acc 0.9354941378493605\n",
            "epoch 18 batch id 2121 loss 0.07339326292276382 train acc 0.9355109618104668\n",
            "epoch 18 batch id 2131 loss 0.2747068703174591 train acc 0.9355202956358517\n",
            "epoch 18 batch id 2141 loss 0.17313511669635773 train acc 0.9354930523120037\n",
            "epoch 18 batch id 2151 loss 0.17125488817691803 train acc 0.9354878544862855\n",
            "epoch 18 batch id 2161 loss 0.27052271366119385 train acc 0.9354248611753818\n",
            "epoch 18 batch id 2171 loss 0.14694252610206604 train acc 0.9354560110548135\n",
            "epoch 18 batch id 2181 loss 0.1976739466190338 train acc 0.9353794131132508\n",
            "epoch 18 batch id 2191 loss 0.09368609637022018 train acc 0.9354318804198996\n",
            "epoch 18 batch id 2201 loss 0.13430184125900269 train acc 0.9354128805088596\n",
            "epoch 18 batch id 2211 loss 0.08359011262655258 train acc 0.9354081863410222\n",
            "epoch 18 batch id 2221 loss 0.3168565630912781 train acc 0.9353613237280505\n",
            "epoch 18 batch id 2231 loss 0.07531319558620453 train acc 0.9353779134917077\n",
            "epoch 18 batch id 2241 loss 0.22636471688747406 train acc 0.9353664658634538\n",
            "epoch 18 batch id 2251 loss 0.16951338946819305 train acc 0.9352718236339405\n",
            "epoch 18 batch id 2261 loss 0.16514480113983154 train acc 0.9353231424148607\n",
            "epoch 18 batch id 2271 loss 0.2155347466468811 train acc 0.9353464883311317\n",
            "epoch 18 batch id 2281 loss 0.23358765244483948 train acc 0.9353764796142043\n",
            "epoch 18 batch id 2291 loss 0.14649002254009247 train acc 0.9354812309035355\n",
            "epoch 18 batch id 2301 loss 0.19465245306491852 train acc 0.9355375380269448\n",
            "epoch 18 batch id 2311 loss 0.17498037219047546 train acc 0.9355189852877542\n",
            "epoch 18 batch id 2321 loss 0.2219863384962082 train acc 0.9354467363205515\n",
            "epoch 18 batch id 2331 loss 0.14567409455776215 train acc 0.9353751072501072\n",
            "epoch 18 batch id 2341 loss 0.26709574460983276 train acc 0.9354242310978215\n",
            "epoch 18 batch id 2351 loss 0.2554734945297241 train acc 0.9353200765631646\n",
            "epoch 18 batch id 2361 loss 0.2815053164958954 train acc 0.9352962198221093\n",
            "epoch 18 batch id 2371 loss 0.16571073234081268 train acc 0.9353252846900042\n",
            "epoch 18 batch id 2381 loss 0.07160989940166473 train acc 0.9353869172616548\n",
            "epoch 18 batch id 2391 loss 0.07864566147327423 train acc 0.9354218946047679\n",
            "epoch 18 batch id 2401 loss 0.11051580309867859 train acc 0.9354045189504373\n",
            "epoch 18 batch id 2411 loss 0.19301556050777435 train acc 0.9354067295727914\n",
            "epoch 18 batch id 2421 loss 0.12387923151254654 train acc 0.9354282837670385\n",
            "epoch 18 batch id 2431 loss 0.15162406861782074 train acc 0.9353918140682846\n",
            "epoch 18 batch id 2441 loss 0.18564577400684357 train acc 0.935381247439574\n",
            "epoch 18 batch id 2451 loss 0.08037446439266205 train acc 0.9354026417788658\n",
            "epoch 18 batch id 2461 loss 0.1459563970565796 train acc 0.9353921170255993\n",
            "epoch 18 batch id 2471 loss 0.2509719431400299 train acc 0.9353121205989477\n",
            "epoch 18 batch id 2481 loss 0.25739365816116333 train acc 0.9353650241837969\n",
            "epoch 18 batch id 2491 loss 0.1890009194612503 train acc 0.9353234142914492\n",
            "epoch 18 batch id 2501 loss 0.027049649506807327 train acc 0.935375849660136\n",
            "epoch 18 batch id 2511 loss 0.22961188852787018 train acc 0.9354278673835126\n",
            "epoch 18 batch id 2521 loss 0.15955156087875366 train acc 0.9353988992463308\n",
            "epoch 18 batch id 2531 loss 0.2348107546567917 train acc 0.9353639865665745\n",
            "epoch 18 batch id 2541 loss 0.18376775085926056 train acc 0.9353354978354979\n",
            "epoch 18 batch id 2551 loss 0.16460639238357544 train acc 0.9352766072128577\n",
            "epoch 18 batch id 2561 loss 0.1720345914363861 train acc 0.935248682155408\n",
            "epoch 18 batch id 2571 loss 0.29545050859451294 train acc 0.9352756709451575\n",
            "epoch 18 batch id 2581 loss 0.101565882563591 train acc 0.9353024506005424\n",
            "epoch 18 batch id 2591 loss 0.2220344990491867 train acc 0.9352988710922424\n",
            "epoch 18 batch id 2601 loss 0.2505033016204834 train acc 0.9353133410226836\n",
            "epoch 18 batch id 2611 loss 0.1575862169265747 train acc 0.9353336844121026\n",
            "epoch 18 batch id 2621 loss 0.2742864489555359 train acc 0.9353300267073636\n",
            "epoch 18 batch id 2631 loss 0.27432140707969666 train acc 0.9353382744203724\n",
            "epoch 18 batch id 2641 loss 0.11538119614124298 train acc 0.9353464596743658\n",
            "epoch 18 batch id 2651 loss 0.03901282325387001 train acc 0.9353604771784232\n",
            "epoch 18 batch id 2661 loss 0.19803032279014587 train acc 0.9353626456219466\n",
            "epoch 18 batch id 2671 loss 0.14400650560855865 train acc 0.9353062991388993\n",
            "epoch 18 batch id 2681 loss 0.10632680356502533 train acc 0.93531448153674\n",
            "epoch 18 batch id 2691 loss 0.1892090141773224 train acc 0.9353051839464883\n",
            "epoch 18 batch id 2701 loss 0.2710300087928772 train acc 0.9353480192521288\n",
            "epoch 18 batch id 2711 loss 0.15776598453521729 train acc 0.935275267428993\n",
            "epoch 18 batch id 2721 loss 0.10695483535528183 train acc 0.9352777012127894\n",
            "epoch 18 batch id 2731 loss 0.08073284476995468 train acc 0.9352686744782132\n",
            "epoch 18 batch id 2741 loss 0.2927860915660858 train acc 0.935293916453849\n",
            "epoch 18 batch id 2751 loss 0.17047186195850372 train acc 0.9352678571428571\n",
            "epoch 18 batch id 2761 loss 0.08895625919103622 train acc 0.9352702825063383\n",
            "epoch 18 batch id 2771 loss 0.21296577155590057 train acc 0.9352219415373512\n",
            "epoch 18 batch id 2781 loss 0.3071691691875458 train acc 0.9352750809061489\n",
            "epoch 18 batch id 2791 loss 0.20041540265083313 train acc 0.9352886510211393\n",
            "epoch 18 batch id 2801 loss 0.23353703320026398 train acc 0.9352742324169939\n",
            "epoch 18 batch id 2811 loss 0.223337322473526 train acc 0.9352877090003557\n",
            "epoch 18 batch id 2821 loss 0.20077738165855408 train acc 0.9353121676710386\n",
            "epoch 18 batch id 2831 loss 0.0988350659608841 train acc 0.9353143765453903\n",
            "epoch 18 batch id 2841 loss 0.3737766444683075 train acc 0.9352780711017248\n",
            "epoch 18 batch id 2851 loss 0.07795976847410202 train acc 0.9352584619431779\n",
            "epoch 18 batch id 2861 loss 0.05266239121556282 train acc 0.93527721950367\n",
            "epoch 18 batch id 2871 loss 0.2726995050907135 train acc 0.9353448275862069\n",
            "epoch 18 batch id 2881 loss 0.2767035961151123 train acc 0.9353306143700104\n",
            "epoch 18 batch id 2891 loss 0.05360628664493561 train acc 0.935359737115185\n",
            "epoch 18 batch id 2901 loss 0.08577461540699005 train acc 0.9352970958290244\n",
            "epoch 18 batch id 2911 loss 0.09763559699058533 train acc 0.9353100309172105\n",
            "epoch 18 batch id 2921 loss 0.06425223499536514 train acc 0.9352854330708661\n",
            "epoch 18 batch id 2931 loss 0.11040782928466797 train acc 0.9353036506311839\n",
            "epoch 18 batch id 2941 loss 0.14829805493354797 train acc 0.9352579904794288\n",
            "epoch 18 batch id 2951 loss 0.18070565164089203 train acc 0.9353026516435107\n",
            "epoch 18 batch id 2961 loss 0.09612616151571274 train acc 0.9353522880783519\n",
            "epoch 18 batch id 2971 loss 0.44124388694763184 train acc 0.9353963312016156\n",
            "epoch 18 batch id 2981 loss 0.1635333150625229 train acc 0.9354138711841664\n",
            "epoch 18 batch id 2991 loss 0.1155482828617096 train acc 0.935446965897693\n",
            "epoch 18 batch id 3001 loss 0.28967222571372986 train acc 0.9354538070643119\n",
            "epoch 18 batch id 3011 loss 0.27453508973121643 train acc 0.935439845566257\n",
            "epoch 18 batch id 3021 loss 0.15339308977127075 train acc 0.9354208043694141\n",
            "epoch 18 batch id 3031 loss 0.2694930136203766 train acc 0.9354431293302541\n",
            "epoch 18 batch id 3041 loss 0.2807791233062744 train acc 0.9354498931272608\n",
            "epoch 18 batch id 3051 loss 0.16239430010318756 train acc 0.9354617338577516\n",
            "epoch 18 batch id 3061 loss 0.12089511752128601 train acc 0.93546839268213\n",
            "epoch 18 batch id 3071 loss 0.11844681203365326 train acc 0.935439392705959\n",
            "epoch 18 batch id 3081 loss 0.28902506828308105 train acc 0.935456223628692\n",
            "epoch 18 batch id 3091 loss 0.16868378221988678 train acc 0.9354780006470398\n",
            "epoch 18 batch id 3101 loss 0.18630680441856384 train acc 0.9354593276362464\n",
            "epoch 18 batch id 3111 loss 0.35541683435440063 train acc 0.9354307296689167\n",
            "epoch 18 batch id 3121 loss 0.21906396746635437 train acc 0.9354473726369753\n",
            "epoch 18 batch id 3131 loss 0.07385460287332535 train acc 0.9354738901309486\n",
            "epoch 18 batch id 3141 loss 0.1488141417503357 train acc 0.9355499840815027\n",
            "epoch 18 batch id 3151 loss 0.13336141407489777 train acc 0.9355214614408124\n",
            "epoch 18 batch id 3161 loss 0.09463849663734436 train acc 0.9354832331540651\n",
            "epoch 18 batch id 3171 loss 0.10484974831342697 train acc 0.9354846657205929\n",
            "epoch 18 batch id 3181 loss 0.10787402838468552 train acc 0.9354762653253694\n",
            "epoch 18 batch id 3191 loss 0.04058872535824776 train acc 0.9355168834221247\n",
            "epoch 18 batch id 3201 loss 0.187948077917099 train acc 0.935508434864105\n",
            "epoch 18 batch id 3211 loss 0.2162049114704132 train acc 0.9354951728433509\n",
            "epoch 18 batch id 3221 loss 0.21827347576618195 train acc 0.9354868441477802\n",
            "epoch 18 batch id 3231 loss 0.1504524052143097 train acc 0.9355075827917053\n",
            "epoch 18 batch id 3241 loss 0.25446009635925293 train acc 0.935537835544585\n",
            "epoch 18 batch id 3251 loss 0.21433831751346588 train acc 0.9355486773300523\n",
            "epoch 18 batch id 3261 loss 0.16312746703624725 train acc 0.9355354952468568\n",
            "epoch 18 batch id 3271 loss 0.10495658218860626 train acc 0.9355080632833995\n",
            "epoch 18 batch id 3281 loss 0.09318810701370239 train acc 0.9354998476074368\n",
            "epoch 18 batch id 3291 loss 0.16262570023536682 train acc 0.9355106730477059\n",
            "epoch 18 batch id 3301 loss 0.053746480494737625 train acc 0.9355166994850046\n",
            "epoch 18 batch id 3311 loss 0.24249255657196045 train acc 0.9355368468740561\n",
            "epoch 18 batch id 3321 loss 0.16593177616596222 train acc 0.9355427582053598\n",
            "epoch 18 batch id 3331 loss 0.24541084468364716 train acc 0.9355111077754428\n",
            "epoch 18 batch id 3341 loss 0.4897691309452057 train acc 0.9354890002993116\n",
            "epoch 18 batch id 3351 loss 0.15759874880313873 train acc 0.9354716875559534\n",
            "epoch 18 batch id 3361 loss 0.11138295382261276 train acc 0.9355102648021422\n",
            "epoch 18 batch id 3371 loss 0.19313682615756989 train acc 0.9355393429249481\n",
            "epoch 18 batch id 3381 loss 0.020886139944195747 train acc 0.935563627624963\n",
            "epoch 18 batch id 3391 loss 0.20039492845535278 train acc 0.9355923768799764\n",
            "epoch 18 batch id 3401 loss 0.27795541286468506 train acc 0.9355796089385475\n",
            "epoch 18 batch id 3411 loss 0.17877352237701416 train acc 0.9356447889182058\n",
            "epoch 18 batch id 3421 loss 0.21923872828483582 train acc 0.935641077170418\n",
            "epoch 18 batch id 3431 loss 0.20841576159000397 train acc 0.9356464951909065\n",
            "epoch 18 batch id 3441 loss 0.052528634667396545 train acc 0.9356836675385063\n",
            "epoch 18 batch id 3451 loss 0.20379051566123962 train acc 0.935675347725297\n",
            "epoch 18 batch id 3461 loss 0.19625021517276764 train acc 0.9356670759895984\n",
            "epoch 18 batch id 3471 loss 0.2577369511127472 train acc 0.9356633535004322\n",
            "epoch 18 batch id 3481 loss 0.14214996993541718 train acc 0.9356327204826199\n",
            "epoch 18 batch id 3491 loss 0.08639968186616898 train acc 0.9356693998854196\n",
            "epoch 18 batch id 3501 loss 0.19125254452228546 train acc 0.9356969437303627\n",
            "epoch 18 batch id 3511 loss 0.1586865335702896 train acc 0.9356353246938194\n",
            "epoch 18 batch id 3521 loss 0.2728135585784912 train acc 0.9356317452428288\n",
            "epoch 18 batch id 3531 loss 0.29945072531700134 train acc 0.9356591617105636\n",
            "epoch 18 batch id 3541 loss 0.12793530523777008 train acc 0.9356511225642474\n",
            "epoch 18 batch id 3551 loss 0.09326379746198654 train acc 0.9356607293720078\n",
            "epoch 18 batch id 3561 loss 0.08961829543113708 train acc 0.9357097725358046\n",
            "epoch 18 batch id 3571 loss 0.23381035029888153 train acc 0.9356710305236628\n",
            "epoch 18 batch id 3581 loss 0.31821170449256897 train acc 0.9356674113376152\n",
            "epoch 18 batch id 3591 loss 0.18063394725322723 train acc 0.9357247284878863\n",
            "epoch 18 batch id 3601 loss 0.21587008237838745 train acc 0.9357556928630936\n",
            "epoch 18 batch id 3611 loss 0.19716590642929077 train acc 0.93576052340072\n",
            "epoch 18 batch id 3621 loss 0.29510632157325745 train acc 0.9357825876829605\n",
            "epoch 18 batch id 3631 loss 0.11392924189567566 train acc 0.9357399820985954\n",
            "epoch 18 batch id 3641 loss 0.156645730137825 train acc 0.9357576901950013\n",
            "epoch 18 batch id 3651 loss 0.21596378087997437 train acc 0.935758182689674\n",
            "epoch 18 batch id 3661 loss 0.3534695506095886 train acc 0.935762940453428\n",
            "epoch 18 batch id 3671 loss 0.13318441808223724 train acc 0.9358187482974666\n",
            "epoch 18 batch id 3681 loss 0.23025541007518768 train acc 0.9358360499864168\n",
            "epoch 18 batch id 3691 loss 0.22903627157211304 train acc 0.9358701910051477\n",
            "epoch 18 batch id 3701 loss 0.179188534617424 train acc 0.935887260199946\n",
            "epoch 18 batch id 3711 loss 0.24874594807624817 train acc 0.9359084478577203\n",
            "epoch 18 batch id 3721 loss 0.1668471246957779 train acc 0.9359421190540177\n",
            "epoch 18 batch id 3731 loss 0.061110615730285645 train acc 0.9359588582149557\n",
            "epoch 18 batch id 3741 loss 0.13405360281467438 train acc 0.9359379176690724\n",
            "epoch 18 batch id 3751 loss 0.07230526953935623 train acc 0.9359254198880299\n",
            "epoch 18 batch id 3761 loss 0.20156024396419525 train acc 0.9359296064876362\n",
            "epoch 18 batch id 3771 loss 0.08516719192266464 train acc 0.9358881927870591\n",
            "epoch 18 batch id 3781 loss 0.23247785866260529 train acc 0.9358841906902936\n",
            "epoch 18 batch id 3791 loss 0.09519743174314499 train acc 0.9359173041413875\n",
            "epoch 18 batch id 3801 loss 0.17722102999687195 train acc 0.9359009142330965\n",
            "epoch 18 batch id 3811 loss 0.1300806850194931 train acc 0.9359256100760955\n",
            "epoch 18 batch id 3821 loss 0.21205247938632965 train acc 0.9359419981680188\n",
            "epoch 18 batch id 3831 loss 0.08388448506593704 train acc 0.9359542221352127\n",
            "epoch 18 batch id 3841 loss 0.11776955425739288 train acc 0.9359460426972143\n",
            "epoch 18 batch id 3851 loss 0.18800635635852814 train acc 0.9359297909633861\n",
            "epoch 18 batch id 3861 loss 0.12274330854415894 train acc 0.9359014827764828\n",
            "epoch 18 batch id 3871 loss 0.10551343113183975 train acc 0.9359177215189873\n",
            "epoch 18 batch id 3881 loss 0.27239614725112915 train acc 0.9358895903117753\n",
            "epoch 18 batch id 3891 loss 0.18690761923789978 train acc 0.9358816820868672\n",
            "epoch 18 batch id 3901 loss 0.1629152148962021 train acc 0.9358698090233274\n",
            "epoch 18 batch id 3911 loss 0.07990279793739319 train acc 0.9358939529532089\n",
            "epoch 18 batch id 3921 loss 0.07100703567266464 train acc 0.9358821091558276\n",
            "epoch 18 batch id 3931 loss 0.15275448560714722 train acc 0.9358782752480285\n",
            "epoch 18 batch id 3941 loss 0.2179717868566513 train acc 0.9358903197158082\n",
            "epoch 18 batch id 3951 loss 0.08408218622207642 train acc 0.9358785750442926\n",
            "epoch 18 batch id 3961 loss 0.15076924860477448 train acc 0.9358668896743246\n",
            "epoch 18 batch id 3971 loss 0.24970124661922455 train acc 0.9358631327121631\n",
            "epoch 18 batch id 3981 loss 0.08834534883499146 train acc 0.9359025684501382\n",
            "epoch 18 batch id 3991 loss 0.2566380202770233 train acc 0.9359144011525934\n",
            "epoch 18 batch id 4001 loss 0.27790284156799316 train acc 0.9359027430642339\n",
            "epoch 18 batch id 4011 loss 0.22513224184513092 train acc 0.9358794564946398\n",
            "epoch 18 batch id 4021 loss 0.10065627843141556 train acc 0.9358368565033574\n",
            "epoch 18 batch id 4031 loss 0.10119018703699112 train acc 0.93583322996775\n",
            "epoch 18 batch id 4041 loss 0.28110089898109436 train acc 0.9358296213808464\n",
            "epoch 18 batch id 4051 loss 0.16587191820144653 train acc 0.9358376018267095\n",
            "epoch 18 batch id 4061 loss 0.13959847390651703 train acc 0.9358378478207338\n",
            "epoch 18 batch id 4071 loss 0.37092605233192444 train acc 0.9358035494964382\n",
            "epoch 18 batch id 4081 loss 0.16202180087566376 train acc 0.9358306787552071\n",
            "epoch 18 batch id 4091 loss 0.2283637374639511 train acc 0.9358576753849914\n",
            "epoch 18 batch id 4101 loss 0.1777408868074417 train acc 0.93586930017069\n",
            "epoch 18 batch id 4111 loss 0.06700360029935837 train acc 0.9358428606178545\n",
            "epoch 18 batch id 4121 loss 0.1935662180185318 train acc 0.935805174714875\n",
            "epoch 18 batch id 4131 loss 0.1860155463218689 train acc 0.9357676712660373\n",
            "epoch 18 batch id 4141 loss 0.11449971795082092 train acc 0.9358058138130886\n",
            "epoch 18 batch id 4151 loss 0.13583317399024963 train acc 0.9358098952059745\n",
            "epoch 18 batch id 4161 loss 0.24192336201667786 train acc 0.9357951814467677\n",
            "epoch 18 batch id 4171 loss 0.20817625522613525 train acc 0.9358142531766962\n",
            "epoch 18 batch id 4181 loss 0.051327504217624664 train acc 0.9358294965319301\n",
            "epoch 18 batch id 4191 loss 0.20677708089351654 train acc 0.9358670365068003\n",
            "epoch 18 batch id 4201 loss 0.1417052447795868 train acc 0.9359043977624375\n",
            "epoch 18 batch id 4211 loss 0.23785340785980225 train acc 0.9358859237710757\n",
            "epoch 18 batch id 4221 loss 0.1772373467683792 train acc 0.9358416252072969\n",
            "epoch 18 batch id 4231 loss 0.3330363929271698 train acc 0.935849237768849\n",
            "epoch 18 batch id 4241 loss 0.1792946308851242 train acc 0.9358715515208678\n",
            "epoch 18 batch id 4251 loss 0.2674436569213867 train acc 0.9358827334744766\n",
            "epoch 18 batch id 4261 loss 0.07778273522853851 train acc 0.9358681940858953\n",
            "epoch 18 batch id 4271 loss 0.11516394466161728 train acc 0.935886648325919\n",
            "epoch 18 batch id 4281 loss 0.18682679533958435 train acc 0.9359086661994861\n",
            "epoch 18 batch id 4291 loss 0.13498789072036743 train acc 0.9359123747378234\n",
            "epoch 18 batch id 4301 loss 0.09331373870372772 train acc 0.9359233317833062\n",
            "epoch 18 batch id 4311 loss 0.19501805305480957 train acc 0.9359632335884945\n",
            "epoch 18 batch id 4321 loss 0.1268785297870636 train acc 0.9359595579726915\n",
            "epoch 18 batch id 4331 loss 0.2088765949010849 train acc 0.9359198222119602\n",
            "epoch 18 batch id 4341 loss 0.13425087928771973 train acc 0.9359306611379866\n",
            "epoch 18 batch id 4351 loss 0.23627103865146637 train acc 0.9359306768558951\n",
            "epoch 18 batch id 4361 loss 0.21653898060321808 train acc 0.9359486069708782\n",
            "epoch 18 batch id 4371 loss 0.20297092199325562 train acc 0.9359593056508808\n",
            "epoch 18 batch id 4381 loss 0.2941744923591614 train acc 0.935952122803013\n",
            "epoch 18 batch id 4391 loss 0.05689891800284386 train acc 0.9359485310863129\n",
            "epoch 18 batch id 4401 loss 0.1501503437757492 train acc 0.9359343047034765\n",
            "epoch 18 batch id 4411 loss 0.08256079256534576 train acc 0.9359095159827704\n",
            "epoch 18 batch id 4421 loss 0.15083688497543335 train acc 0.9359237163537661\n",
            "epoch 18 batch id 4431 loss 0.25907406210899353 train acc 0.9359308000451365\n",
            "epoch 18 batch id 4441 loss 0.07075309008359909 train acc 0.9359343334834497\n",
            "epoch 18 batch id 4451 loss 0.14876718819141388 train acc 0.935930830150528\n",
            "epoch 18 batch id 4461 loss 0.04245566204190254 train acc 0.9359343476798924\n",
            "epoch 18 batch id 4471 loss 0.17681202292442322 train acc 0.9359133862670543\n",
            "epoch 18 batch id 4481 loss 0.12775279581546783 train acc 0.9359343617496094\n",
            "epoch 18 batch id 4491 loss 0.17943254113197327 train acc 0.9359065352928079\n",
            "epoch 18 batch id 4501 loss 0.04834261164069176 train acc 0.9359274327927127\n",
            "epoch 18 batch id 4511 loss 0.2618032693862915 train acc 0.9359136000886721\n",
            "epoch 18 batch id 4521 loss 0.1751352995634079 train acc 0.9359101968591019\n",
            "epoch 18 batch id 4531 loss 0.1908852756023407 train acc 0.9358826693886559\n",
            "epoch 18 batch id 4541 loss 0.1453009396791458 train acc 0.9358999944946047\n",
            "epoch 18 batch id 4551 loss 0.21073603630065918 train acc 0.9358966435948143\n",
            "epoch 18 batch id 4561 loss 0.29804527759552 train acc 0.9359104363078272\n",
            "epoch 18 batch id 4571 loss 0.15700781345367432 train acc 0.9359310052504922\n",
            "epoch 18 batch id 4581 loss 0.2939918339252472 train acc 0.9359139652914211\n",
            "epoch 18 batch id 4591 loss 0.30609503388404846 train acc 0.9358969995643651\n",
            "epoch 18 batch id 4601 loss 0.11453171819448471 train acc 0.9358868995870463\n",
            "epoch 18 batch id 4611 loss 0.11701199412345886 train acc 0.9359073411407504\n",
            "epoch 18 batch id 4621 loss 0.1414683759212494 train acc 0.9359310755247782\n",
            "epoch 18 batch id 4631 loss 0.11664274334907532 train acc 0.9359614554091988\n",
            "epoch 18 batch id 4641 loss 0.19934570789337158 train acc 0.9359883376427494\n",
            "epoch 18 batch id 4651 loss 0.31173083186149597 train acc 0.9360050258009031\n",
            "epoch 18 batch id 4661 loss 0.22878390550613403 train acc 0.936014937781592\n",
            "epoch 18 batch id 4671 loss 0.14311447739601135 train acc 0.9360381877542282\n",
            "epoch 18 batch id 4681 loss 0.4134346842765808 train acc 0.9360847041230507\n",
            "epoch 18 batch id 4691 loss 0.11272818595170975 train acc 0.9360677360903858\n",
            "epoch 18 batch id 4701 loss 0.2616950571537018 train acc 0.9360674590512656\n",
            "epoch 18 batch id 4711 loss 0.13418413698673248 train acc 0.9361003502441095\n",
            "epoch 18 batch id 4721 loss 0.064357228577137 train acc 0.9360966956153357\n",
            "epoch 18 batch id 4731 loss 0.17012012004852295 train acc 0.9361029644895371\n",
            "epoch 18 batch id 4741 loss 0.18350952863693237 train acc 0.9361190940729803\n",
            "epoch 18 batch id 4751 loss 0.18421390652656555 train acc 0.9361351557566828\n",
            "epoch 18 batch id 4761 loss 0.2028747797012329 train acc 0.9361774049569418\n",
            "epoch 18 batch id 4771 loss 0.17739829421043396 train acc 0.9362129270593167\n",
            "epoch 18 batch id 4781 loss 0.29540079832077026 train acc 0.9361796695252039\n",
            "epoch 18 batch id 4791 loss 0.5097768902778625 train acc 0.9361661187643499\n",
            "epoch 18 batch id 4801 loss 0.17814290523529053 train acc 0.9361656425744637\n",
            "epoch 18 batch id 4811 loss 0.07717428356409073 train acc 0.9361391862398669\n",
            "epoch 18 batch id 4821 loss 0.17034606635570526 train acc 0.9361225627463182\n",
            "epoch 18 batch id 4831 loss 0.08478701114654541 train acc 0.9361545228731112\n",
            "epoch 18 batch id 4841 loss 0.38307470083236694 train acc 0.9361088876265234\n",
            "epoch 18 batch id 4851 loss 0.11654596775770187 train acc 0.9361181972789115\n",
            "epoch 18 batch id 4861 loss 0.21866071224212646 train acc 0.9361178255502983\n",
            "epoch 18 batch id 4871 loss 0.14446298778057098 train acc 0.9361302863888319\n",
            "epoch 18 batch id 4881 loss 0.23454922437667847 train acc 0.9361138854742881\n",
            "epoch 18 batch id 4891 loss 0.13956813514232635 train acc 0.936103940911879\n",
            "epoch 18 batch id 4901 loss 0.11975036561489105 train acc 0.9361322944297082\n",
            "epoch 18 batch id 4911 loss 0.2355429232120514 train acc 0.9361191712482183\n",
            "epoch 18 batch id 4921 loss 0.3192550539970398 train acc 0.9360965758992075\n",
            "epoch 18 batch id 4931 loss 0.27025526762008667 train acc 0.9360518910971405\n",
            "epoch 18 batch id 4941 loss 0.13336655497550964 train acc 0.936042172637118\n",
            "epoch 18 batch id 4951 loss 0.2077588587999344 train acc 0.9360230256513835\n",
            "epoch 18 train acc 0.9360359890512957\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd765c50e2f742b092e9cfed97a5fd79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 18 loss 0.9147039651870728 test acc 0.8197248442082112\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d423a73fc1fb4400baca29cbbfa53480",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 19 batch id 1 loss 0.12446365505456924 train acc 0.96875\n",
            "epoch 19 batch id 11 loss 0.05105128884315491 train acc 0.9474431818181818\n",
            "epoch 19 batch id 21 loss 0.13021862506866455 train acc 0.9464285714285714\n",
            "epoch 19 batch id 31 loss 0.16630300879478455 train acc 0.9415322580645161\n",
            "epoch 19 batch id 41 loss 0.18335387110710144 train acc 0.9413109756097561\n",
            "epoch 19 batch id 51 loss 0.13733896613121033 train acc 0.9414828431372549\n",
            "epoch 19 batch id 61 loss 0.2817912995815277 train acc 0.9377561475409836\n",
            "epoch 19 batch id 71 loss 0.455431193113327 train acc 0.9379401408450704\n",
            "epoch 19 batch id 81 loss 0.2865276634693146 train acc 0.9371141975308642\n",
            "epoch 19 batch id 91 loss 0.11986162513494492 train acc 0.939217032967033\n",
            "epoch 19 batch id 101 loss 0.22462494671344757 train acc 0.9375\n",
            "epoch 19 batch id 111 loss 0.11015719175338745 train acc 0.9384853603603603\n",
            "epoch 19 batch id 121 loss 0.2362968623638153 train acc 0.9380165289256198\n",
            "epoch 19 batch id 131 loss 0.09534607082605362 train acc 0.9375\n",
            "epoch 19 batch id 141 loss 0.13928470015525818 train acc 0.9379432624113475\n",
            "epoch 19 batch id 151 loss 0.29324284195899963 train acc 0.937396523178808\n",
            "epoch 19 batch id 161 loss 0.09778176248073578 train acc 0.9386645962732919\n",
            "epoch 19 batch id 171 loss 0.3154734969139099 train acc 0.9385964912280702\n",
            "epoch 19 batch id 181 loss 0.08618894219398499 train acc 0.9387948895027625\n",
            "epoch 19 batch id 191 loss 0.17335619032382965 train acc 0.9391361256544503\n",
            "epoch 19 batch id 201 loss 0.24048161506652832 train acc 0.9395988805970149\n",
            "epoch 19 batch id 211 loss 0.26748448610305786 train acc 0.9393513033175356\n",
            "epoch 19 batch id 221 loss 0.26113319396972656 train acc 0.938560520361991\n",
            "epoch 19 batch id 231 loss 0.09713797271251678 train acc 0.9387851731601732\n",
            "epoch 19 batch id 241 loss 0.2603403329849243 train acc 0.9386021784232366\n",
            "epoch 19 batch id 251 loss 0.23030510544776917 train acc 0.9386827689243028\n",
            "epoch 19 batch id 261 loss 0.1554265171289444 train acc 0.9391163793103449\n",
            "epoch 19 batch id 271 loss 0.17341971397399902 train acc 0.9394026752767528\n",
            "epoch 19 batch id 281 loss 0.09156956523656845 train acc 0.9393905693950177\n",
            "epoch 19 batch id 291 loss 0.23405826091766357 train acc 0.9395403780068728\n",
            "epoch 19 batch id 301 loss 0.3349452018737793 train acc 0.9391611295681063\n",
            "epoch 19 batch id 311 loss 0.111371248960495 train acc 0.9393086816720257\n",
            "epoch 19 batch id 321 loss 0.30767393112182617 train acc 0.9387655763239875\n",
            "epoch 19 batch id 331 loss 0.19588340818881989 train acc 0.9388689577039275\n",
            "epoch 19 batch id 341 loss 0.2891508638858795 train acc 0.9388288123167156\n",
            "epoch 19 batch id 351 loss 0.2829476594924927 train acc 0.9385683760683761\n",
            "epoch 19 batch id 361 loss 0.16285254061222076 train acc 0.9388850415512465\n",
            "epoch 19 batch id 371 loss 0.20689983665943146 train acc 0.9395215633423181\n",
            "epoch 19 batch id 381 loss 0.06526509672403336 train acc 0.9399196194225722\n",
            "epoch 19 batch id 391 loss 0.2595588266849518 train acc 0.9401774296675192\n",
            "epoch 19 batch id 401 loss 0.07995902746915817 train acc 0.9398768703241895\n",
            "epoch 19 batch id 411 loss 0.2098669707775116 train acc 0.9398950729927007\n",
            "epoch 19 batch id 421 loss 0.18565474450588226 train acc 0.939875296912114\n",
            "epoch 19 batch id 431 loss 0.17484180629253387 train acc 0.9398201856148491\n",
            "epoch 19 batch id 441 loss 0.3467719256877899 train acc 0.940015589569161\n",
            "epoch 19 batch id 451 loss 0.11175308376550674 train acc 0.9400637472283814\n",
            "epoch 19 batch id 461 loss 0.15916728973388672 train acc 0.9392624728850325\n",
            "epoch 19 batch id 471 loss 0.13148337602615356 train acc 0.9396894904458599\n",
            "epoch 19 batch id 481 loss 0.21799755096435547 train acc 0.9400012993762994\n",
            "epoch 19 batch id 491 loss 0.2207050323486328 train acc 0.9401412932790224\n",
            "epoch 19 batch id 501 loss 0.29662343859672546 train acc 0.9401509481037924\n",
            "epoch 19 batch id 511 loss 0.20284831523895264 train acc 0.9402213796477495\n",
            "epoch 19 batch id 521 loss 0.16277313232421875 train acc 0.9403190978886756\n",
            "epoch 19 batch id 531 loss 0.2834312617778778 train acc 0.940118879472693\n",
            "epoch 19 batch id 541 loss 0.07192414253950119 train acc 0.9401282347504621\n",
            "epoch 19 batch id 551 loss 0.32463258504867554 train acc 0.9400521778584392\n",
            "epoch 19 batch id 561 loss 0.07283396273851395 train acc 0.9400902406417112\n",
            "epoch 19 batch id 571 loss 0.15164023637771606 train acc 0.9402364273204904\n",
            "epoch 19 batch id 581 loss 0.4702109396457672 train acc 0.940270008605852\n",
            "epoch 19 batch id 591 loss 0.10333339869976044 train acc 0.9404346446700508\n",
            "epoch 19 batch id 601 loss 0.09792460501194 train acc 0.9406198003327787\n",
            "epoch 19 batch id 611 loss 0.2075989842414856 train acc 0.9403897299509002\n",
            "epoch 19 batch id 621 loss 0.1956312358379364 train acc 0.9402677133655395\n",
            "epoch 19 batch id 631 loss 0.1608370989561081 train acc 0.9400009904912837\n",
            "epoch 19 batch id 641 loss 0.3259502947330475 train acc 0.9401569812792512\n",
            "epoch 19 batch id 651 loss 0.1426548957824707 train acc 0.9400441628264209\n",
            "epoch 19 batch id 661 loss 0.13819746673107147 train acc 0.940123865355522\n",
            "epoch 19 batch id 671 loss 0.14521744847297668 train acc 0.9399916169895678\n",
            "epoch 19 batch id 681 loss 0.2252303808927536 train acc 0.9400468061674009\n",
            "epoch 19 batch id 691 loss 0.2522169351577759 train acc 0.9400325615050651\n",
            "epoch 19 batch id 701 loss 0.1743389517068863 train acc 0.9398404065620543\n",
            "epoch 19 batch id 711 loss 0.28509625792503357 train acc 0.9397415611814346\n",
            "epoch 19 batch id 721 loss 0.17135584354400635 train acc 0.9398188280166435\n",
            "epoch 19 batch id 731 loss 0.08895476907491684 train acc 0.9399153556771546\n",
            "epoch 19 batch id 741 loss 0.11550076305866241 train acc 0.9399881916329285\n",
            "epoch 19 batch id 751 loss 0.31859704852104187 train acc 0.9396221704394141\n",
            "epoch 19 batch id 761 loss 0.13450317084789276 train acc 0.9397174770039421\n",
            "epoch 19 batch id 771 loss 0.1583511233329773 train acc 0.939708981841764\n",
            "epoch 19 batch id 781 loss 0.1604176014661789 train acc 0.9396006722151088\n",
            "epoch 19 batch id 791 loss 0.040983185172080994 train acc 0.9398901706700379\n",
            "epoch 19 batch id 801 loss 0.05727861821651459 train acc 0.9400163857677902\n",
            "epoch 19 batch id 811 loss 0.2400103211402893 train acc 0.9401780209617756\n",
            "epoch 19 batch id 821 loss 0.1323155015707016 train acc 0.940069275274056\n",
            "epoch 19 batch id 831 loss 0.1837131530046463 train acc 0.940113567990373\n",
            "epoch 19 batch id 841 loss 0.08945776522159576 train acc 0.9403054399524375\n",
            "epoch 19 batch id 851 loss 0.2537364065647125 train acc 0.9402541128084606\n",
            "epoch 19 batch id 861 loss 0.11925657093524933 train acc 0.9402765679442509\n",
            "epoch 19 batch id 871 loss 0.20328253507614136 train acc 0.9400832376578645\n",
            "epoch 19 batch id 881 loss 0.1271388679742813 train acc 0.9399297673098751\n",
            "epoch 19 batch id 891 loss 0.16229751706123352 train acc 0.9400427890011224\n",
            "epoch 19 batch id 901 loss 0.17289920151233673 train acc 0.9398931742508324\n",
            "epoch 19 batch id 911 loss 0.21060815453529358 train acc 0.9398669045005489\n",
            "epoch 19 batch id 921 loss 0.18805372714996338 train acc 0.9397733441910966\n",
            "epoch 19 batch id 931 loss 0.2566487491130829 train acc 0.939547529538131\n",
            "epoch 19 batch id 941 loss 0.17151868343353271 train acc 0.9396420031880978\n",
            "epoch 19 batch id 951 loss 0.1349504441022873 train acc 0.9396523396424816\n",
            "epoch 19 batch id 961 loss 0.12858766317367554 train acc 0.9396949791883454\n",
            "epoch 19 batch id 971 loss 0.29782918095588684 train acc 0.939720648815654\n",
            "epoch 19 batch id 981 loss 0.12087329477071762 train acc 0.9399050713557594\n",
            "epoch 19 batch id 991 loss 0.31472110748291016 train acc 0.9400227043390514\n",
            "epoch 19 batch id 1001 loss 0.1679619997739792 train acc 0.9401223776223776\n",
            "epoch 19 batch id 1011 loss 0.1780494600534439 train acc 0.9402046241345203\n",
            "epoch 19 batch id 1021 loss 0.2598761022090912 train acc 0.9402699559255632\n",
            "epoch 19 batch id 1031 loss 0.06408744305372238 train acc 0.9402733996120272\n",
            "epoch 19 batch id 1041 loss 0.3808083236217499 train acc 0.9402467579250721\n",
            "epoch 19 batch id 1051 loss 0.10559634864330292 train acc 0.940101688867745\n",
            "epoch 19 batch id 1061 loss 0.21136941015720367 train acc 0.939988807728558\n",
            "epoch 19 batch id 1071 loss 0.05903105065226555 train acc 0.9399801587301587\n",
            "epoch 19 batch id 1081 loss 0.1466585099697113 train acc 0.9397548566142461\n",
            "epoch 19 batch id 1091 loss 0.4531652331352234 train acc 0.939605293308891\n",
            "epoch 19 batch id 1101 loss 0.12616197764873505 train acc 0.9395861716621253\n",
            "epoch 19 batch id 1111 loss 0.3177401125431061 train acc 0.9394830108010801\n",
            "epoch 19 batch id 1121 loss 0.13744699954986572 train acc 0.9395210749330954\n",
            "epoch 19 batch id 1131 loss 0.2506667971611023 train acc 0.9393926834659593\n",
            "epoch 19 batch id 1141 loss 0.18480129539966583 train acc 0.9394719544259421\n",
            "epoch 19 batch id 1151 loss 0.13842067122459412 train acc 0.9394276715899218\n",
            "epoch 19 batch id 1161 loss 0.106587715446949 train acc 0.939437984496124\n",
            "epoch 19 batch id 1171 loss 0.1003020778298378 train acc 0.9393680614859095\n",
            "epoch 19 batch id 1181 loss 0.1639203578233719 train acc 0.9394183954276037\n",
            "epoch 19 batch id 1191 loss 0.16677148640155792 train acc 0.9395334802686818\n",
            "epoch 19 batch id 1201 loss 0.283994197845459 train acc 0.9396726686094921\n",
            "epoch 19 batch id 1211 loss 0.03087596409022808 train acc 0.9397450454170108\n",
            "epoch 19 batch id 1221 loss 0.15970662236213684 train acc 0.9397394553644554\n",
            "epoch 19 batch id 1231 loss 0.17518866062164307 train acc 0.9396958773354996\n",
            "epoch 19 batch id 1241 loss 0.2032512128353119 train acc 0.939552276390008\n",
            "epoch 19 batch id 1251 loss 0.2618129551410675 train acc 0.9394484412470024\n",
            "epoch 19 batch id 1261 loss 0.15958791971206665 train acc 0.939395816812054\n",
            "epoch 19 batch id 1271 loss 0.1623835563659668 train acc 0.9393686073957513\n",
            "epoch 19 batch id 1281 loss 0.14044183492660522 train acc 0.9395247853239657\n",
            "epoch 19 batch id 1291 loss 0.13461732864379883 train acc 0.9395696165762975\n",
            "epoch 19 batch id 1301 loss 0.14045029878616333 train acc 0.9395416986933128\n",
            "epoch 19 batch id 1311 loss 0.2282155156135559 train acc 0.9395618802440885\n",
            "epoch 19 batch id 1321 loss 0.05549877509474754 train acc 0.9396763815291446\n",
            "epoch 19 batch id 1331 loss 0.17820926010608673 train acc 0.9396365514650639\n",
            "epoch 19 batch id 1341 loss 0.18719521164894104 train acc 0.939748788217748\n",
            "epoch 19 batch id 1351 loss 0.06718461215496063 train acc 0.9398477979274611\n",
            "epoch 19 batch id 1361 loss 0.10071209073066711 train acc 0.9397846252755327\n",
            "epoch 19 batch id 1371 loss 0.06457007676362991 train acc 0.9398021517140773\n",
            "epoch 19 batch id 1381 loss 0.4912444055080414 train acc 0.93963839608979\n",
            "epoch 19 batch id 1391 loss 0.12329719215631485 train acc 0.9396230230050323\n",
            "epoch 19 batch id 1401 loss 0.20393109321594238 train acc 0.9396413276231264\n",
            "epoch 19 batch id 1411 loss 0.04437195882201195 train acc 0.9397258150248051\n",
            "epoch 19 batch id 1421 loss 0.08808551728725433 train acc 0.9396991555242786\n",
            "epoch 19 batch id 1431 loss 0.23931290209293365 train acc 0.9397056254367575\n",
            "epoch 19 batch id 1441 loss 0.17891813814640045 train acc 0.9397228487161693\n",
            "epoch 19 batch id 1451 loss 0.2026885449886322 train acc 0.9397829083390765\n",
            "epoch 19 batch id 1461 loss 0.15749742090702057 train acc 0.9397993668720055\n",
            "epoch 19 batch id 1471 loss 0.18360911309719086 train acc 0.939911199864038\n",
            "epoch 19 batch id 1481 loss 0.11781411617994308 train acc 0.939831617150574\n",
            "epoch 19 batch id 1491 loss 0.2760215103626251 train acc 0.9398893360160966\n",
            "epoch 19 batch id 1501 loss 0.1497257947921753 train acc 0.9399566955363091\n",
            "epoch 19 batch id 1511 loss 0.14351530373096466 train acc 0.9399714592984778\n",
            "epoch 19 batch id 1521 loss 0.17666050791740417 train acc 0.9399346646942801\n",
            "epoch 19 batch id 1531 loss 0.24617928266525269 train acc 0.9399697909862835\n",
            "epoch 19 batch id 1541 loss 0.10584896057844162 train acc 0.940014600908501\n",
            "epoch 19 batch id 1551 loss 0.06709864735603333 train acc 0.9398774983881367\n",
            "epoch 19 batch id 1561 loss 0.20620453357696533 train acc 0.9398622677770659\n",
            "epoch 19 batch id 1571 loss 0.08495938777923584 train acc 0.9399168523233609\n",
            "epoch 19 batch id 1581 loss 0.07442685961723328 train acc 0.9399213314358001\n",
            "epoch 19 batch id 1591 loss 0.12476404011249542 train acc 0.9399846794468888\n",
            "epoch 19 batch id 1601 loss 0.2609885036945343 train acc 0.9400960337289195\n",
            "epoch 19 batch id 1611 loss 0.13688544929027557 train acc 0.9400217256362507\n",
            "epoch 19 batch id 1621 loss 0.11716175824403763 train acc 0.9400158081431216\n",
            "epoch 19 batch id 1631 loss 0.1582401841878891 train acc 0.9400770232985898\n",
            "epoch 19 batch id 1641 loss 0.09941807389259338 train acc 0.9401279707495429\n",
            "epoch 19 batch id 1651 loss 0.06885263323783875 train acc 0.9401309812235009\n",
            "epoch 19 batch id 1661 loss 0.18825547397136688 train acc 0.9401621763997592\n",
            "epoch 19 batch id 1671 loss 0.1660933792591095 train acc 0.9401742968282466\n",
            "epoch 19 batch id 1681 loss 0.20698951184749603 train acc 0.9400747323022011\n",
            "epoch 19 batch id 1691 loss 0.27811935544013977 train acc 0.9401519071555293\n",
            "epoch 19 batch id 1701 loss 0.10370594263076782 train acc 0.9401914315108759\n",
            "epoch 19 batch id 1711 loss 0.11687132716178894 train acc 0.9400752483927528\n",
            "epoch 19 batch id 1721 loss 0.09777787327766418 train acc 0.9401419959325973\n",
            "epoch 19 batch id 1731 loss 0.05734182149171829 train acc 0.9401628393991912\n",
            "epoch 19 batch id 1741 loss 0.2071387618780136 train acc 0.9402013928776565\n",
            "epoch 19 batch id 1751 loss 0.07085062563419342 train acc 0.9401235008566533\n",
            "epoch 19 batch id 1761 loss 0.16390743851661682 train acc 0.9401707126632595\n",
            "epoch 19 batch id 1771 loss 0.1618768870830536 train acc 0.940235036702428\n",
            "epoch 19 batch id 1781 loss 0.044715605676174164 train acc 0.9403425042111173\n",
            "epoch 19 batch id 1791 loss 0.3011300265789032 train acc 0.9402830122836404\n",
            "epoch 19 batch id 1801 loss 0.09363415092229843 train acc 0.9403803442531927\n",
            "epoch 19 batch id 1811 loss 0.07012078911066055 train acc 0.9404075786858089\n",
            "epoch 19 batch id 1821 loss 0.11921284347772598 train acc 0.9403487095002746\n",
            "epoch 19 batch id 1831 loss 0.11012919247150421 train acc 0.9403672856362644\n",
            "epoch 19 batch id 1841 loss 0.056825343519449234 train acc 0.9404026344378056\n",
            "epoch 19 batch id 1851 loss 0.1494409590959549 train acc 0.940420718530524\n",
            "epoch 19 batch id 1861 loss 0.30304741859436035 train acc 0.9403294599677593\n",
            "epoch 19 batch id 1871 loss 0.13747760653495789 train acc 0.9404145510422234\n",
            "epoch 19 batch id 1881 loss 0.0638199970126152 train acc 0.9404655103668261\n",
            "epoch 19 batch id 1891 loss 0.09351662546396255 train acc 0.9405655076679006\n",
            "epoch 19 batch id 1901 loss 0.22379295527935028 train acc 0.9404918463966333\n",
            "epoch 19 batch id 1911 loss 0.06856326758861542 train acc 0.9404680141287284\n",
            "epoch 19 batch id 1921 loss 0.2955225706100464 train acc 0.9403956272774596\n",
            "epoch 19 batch id 1931 loss 0.11724359542131424 train acc 0.9404453650958052\n",
            "epoch 19 batch id 1941 loss 0.10141538083553314 train acc 0.9404704404945904\n",
            "epoch 19 batch id 1951 loss 0.20816819369792938 train acc 0.9404071629933367\n",
            "epoch 19 batch id 1961 loss 0.056222155690193176 train acc 0.9403445308516063\n",
            "epoch 19 batch id 1971 loss 0.09376829117536545 train acc 0.9403776636225266\n",
            "epoch 19 batch id 1981 loss 0.15967358648777008 train acc 0.9403789121655729\n",
            "epoch 19 batch id 1991 loss 0.2860226631164551 train acc 0.9403252134605726\n",
            "epoch 19 batch id 2001 loss 0.1696092188358307 train acc 0.9403423288355822\n",
            "epoch 19 batch id 2011 loss 0.267311155796051 train acc 0.9402504972650423\n",
            "epoch 19 batch id 2021 loss 0.22529861330986023 train acc 0.940205962394854\n",
            "epoch 19 batch id 2031 loss 0.062227606773376465 train acc 0.9401233998030527\n",
            "epoch 19 batch id 2041 loss 0.05393289774656296 train acc 0.9401488241058304\n",
            "epoch 19 batch id 2051 loss 0.28167179226875305 train acc 0.9401816187225743\n",
            "epoch 19 batch id 2061 loss 0.08806837350130081 train acc 0.9402520014556041\n",
            "epoch 19 batch id 2071 loss 0.191171333193779 train acc 0.940223623853211\n",
            "epoch 19 batch id 2081 loss 0.1436430960893631 train acc 0.9402105358000961\n",
            "epoch 19 batch id 2091 loss 0.10234705358743668 train acc 0.9401826279292205\n",
            "epoch 19 batch id 2101 loss 0.16094191372394562 train acc 0.9401475487862923\n",
            "epoch 19 batch id 2111 loss 0.04905867576599121 train acc 0.9401572122216959\n",
            "epoch 19 batch id 2121 loss 0.1527419239282608 train acc 0.9401078500707214\n",
            "epoch 19 batch id 2131 loss 0.17315323650836945 train acc 0.9401835992491788\n",
            "epoch 19 batch id 2141 loss 0.12090923637151718 train acc 0.9401053829985988\n",
            "epoch 19 batch id 2151 loss 0.11244148015975952 train acc 0.9401731752673175\n",
            "epoch 19 batch id 2161 loss 0.17908157408237457 train acc 0.9400812702452568\n",
            "epoch 19 batch id 2171 loss 0.34965550899505615 train acc 0.9401557461999078\n",
            "epoch 19 batch id 2181 loss 0.11628775298595428 train acc 0.9401220770288858\n",
            "epoch 19 batch id 2191 loss 0.07951684296131134 train acc 0.9401600296668188\n",
            "epoch 19 batch id 2201 loss 0.09189897030591965 train acc 0.9401692412539755\n",
            "epoch 19 batch id 2211 loss 0.08072880655527115 train acc 0.9401289009497965\n",
            "epoch 19 batch id 2221 loss 0.18946406245231628 train acc 0.9400185727149932\n",
            "epoch 19 batch id 2231 loss 0.1441863775253296 train acc 0.9399722658000896\n",
            "epoch 19 batch id 2241 loss 0.16957822442054749 train acc 0.9399821508255243\n",
            "epoch 19 batch id 2251 loss 0.0617363378405571 train acc 0.9399780653043092\n",
            "epoch 19 batch id 2261 loss 0.09225991368293762 train acc 0.9399671052631579\n",
            "epoch 19 batch id 2271 loss 0.31111130118370056 train acc 0.9399424812857772\n",
            "epoch 19 batch id 2281 loss 0.2362978458404541 train acc 0.9400002740026304\n",
            "epoch 19 batch id 2291 loss 0.14809219539165497 train acc 0.9400984831951114\n",
            "epoch 19 batch id 2301 loss 0.1320016235113144 train acc 0.940195838765754\n",
            "epoch 19 batch id 2311 loss 0.21704500913619995 train acc 0.9402585460839463\n",
            "epoch 19 batch id 2321 loss 0.15155842900276184 train acc 0.94019953683757\n",
            "epoch 19 batch id 2331 loss 0.13588163256645203 train acc 0.9401008151008151\n",
            "epoch 19 batch id 2341 loss 0.22635115683078766 train acc 0.9400830307560871\n",
            "epoch 19 batch id 2351 loss 0.17161977291107178 train acc 0.9399922905146746\n",
            "epoch 19 batch id 2361 loss 0.15620125830173492 train acc 0.940034678102499\n",
            "epoch 19 batch id 2371 loss 0.016458306461572647 train acc 0.9401228384647828\n",
            "epoch 19 batch id 2381 loss 0.03414897248148918 train acc 0.9401840088198236\n",
            "epoch 19 batch id 2391 loss 0.08851074427366257 train acc 0.9402185278126307\n",
            "epoch 19 batch id 2401 loss 0.17370334267616272 train acc 0.9402137130362349\n",
            "epoch 19 batch id 2411 loss 0.060806047171354294 train acc 0.9402218996267109\n",
            "epoch 19 batch id 2421 loss 0.16385290026664734 train acc 0.9402493804213135\n",
            "epoch 19 batch id 2431 loss 0.29425543546676636 train acc 0.940270207733443\n",
            "epoch 19 batch id 2441 loss 0.1579573154449463 train acc 0.9402908643998361\n",
            "epoch 19 batch id 2451 loss 0.1059974953532219 train acc 0.9402858527131783\n",
            "epoch 19 batch id 2461 loss 0.11819063127040863 train acc 0.9402999288906948\n",
            "epoch 19 batch id 2471 loss 0.3344990611076355 train acc 0.9402316875758802\n",
            "epoch 19 batch id 2481 loss 0.2746240198612213 train acc 0.9403214429665457\n",
            "epoch 19 batch id 2491 loss 0.22079025208950043 train acc 0.9402787535126456\n",
            "epoch 19 batch id 2501 loss 0.19270971417427063 train acc 0.9402239104358256\n",
            "epoch 19 batch id 2511 loss 0.07778207212686539 train acc 0.9402752887295898\n",
            "epoch 19 batch id 2521 loss 0.116519995033741 train acc 0.9402270924236414\n",
            "epoch 19 batch id 2531 loss 0.10507040470838547 train acc 0.9402533583563809\n",
            "epoch 19 batch id 2541 loss 0.1771366000175476 train acc 0.9402179260133806\n",
            "epoch 19 batch id 2551 loss 0.17715869843959808 train acc 0.9401950215601724\n",
            "epoch 19 batch id 2561 loss 0.1528312712907791 train acc 0.9401722959781336\n",
            "epoch 19 batch id 2571 loss 0.240879625082016 train acc 0.9402348308051341\n",
            "epoch 19 batch id 2581 loss 0.12577779591083527 train acc 0.9402787194885703\n",
            "epoch 19 batch id 2591 loss 0.14362949132919312 train acc 0.9403102084137399\n",
            "epoch 19 batch id 2601 loss 0.28050777316093445 train acc 0.9403714917339485\n",
            "epoch 19 batch id 2611 loss 0.11781040579080582 train acc 0.9404023841440061\n",
            "epoch 19 batch id 2621 loss 0.14010144770145416 train acc 0.9403913105684854\n",
            "epoch 19 batch id 2631 loss 0.14679256081581116 train acc 0.9404218928164196\n",
            "epoch 19 batch id 2641 loss 0.11248530447483063 train acc 0.9404463271488073\n",
            "epoch 19 batch id 2651 loss 0.062038928270339966 train acc 0.9404882591474916\n",
            "epoch 19 batch id 2661 loss 0.10704359412193298 train acc 0.9405005167230365\n",
            "epoch 19 batch id 2671 loss 0.3123601973056793 train acc 0.9405126825159117\n",
            "epoch 19 batch id 2681 loss 0.15283764898777008 train acc 0.9404489929130921\n",
            "epoch 19 batch id 2691 loss 0.14030204713344574 train acc 0.9404206150130063\n",
            "epoch 19 batch id 2701 loss 0.21762312948703766 train acc 0.9404155868196964\n",
            "epoch 19 batch id 2711 loss 0.15496717393398285 train acc 0.9404221228329029\n",
            "epoch 19 batch id 2721 loss 0.019556140527129173 train acc 0.940457322675487\n",
            "epoch 19 batch id 2731 loss 0.15259034931659698 train acc 0.9404407726107653\n",
            "epoch 19 batch id 2741 loss 0.18438570201396942 train acc 0.940424343305363\n",
            "epoch 19 batch id 2751 loss 0.24971161782741547 train acc 0.9403796346782988\n",
            "epoch 19 batch id 2761 loss 0.05946491286158562 train acc 0.940454092720029\n",
            "epoch 19 batch id 2771 loss 0.18110553920269012 train acc 0.940482903284013\n",
            "epoch 19 batch id 2781 loss 0.10904021561145782 train acc 0.9405452175476448\n",
            "epoch 19 batch id 2791 loss 0.09889663755893707 train acc 0.940573495163024\n",
            "epoch 19 batch id 2801 loss 0.18488675355911255 train acc 0.940590414137808\n",
            "epoch 19 batch id 2811 loss 0.16749313473701477 train acc 0.9406183297758804\n",
            "epoch 19 batch id 2821 loss 0.14755411446094513 train acc 0.9406792803970223\n",
            "epoch 19 batch id 2831 loss 0.12717121839523315 train acc 0.9407011656658425\n",
            "epoch 19 batch id 2841 loss 0.15122148394584656 train acc 0.9407283966913059\n",
            "epoch 19 batch id 2851 loss 0.2156057357788086 train acc 0.9407280340231498\n",
            "epoch 19 batch id 2861 loss 0.1860702782869339 train acc 0.9407222125131073\n",
            "epoch 19 batch id 2871 loss 0.22405779361724854 train acc 0.9407109892023685\n",
            "epoch 19 batch id 2881 loss 0.10805702954530716 train acc 0.9407378080527594\n",
            "epoch 19 batch id 2891 loss 0.07147226482629776 train acc 0.9407482272570045\n",
            "epoch 19 batch id 2901 loss 0.12537714838981628 train acc 0.9407478024819028\n",
            "epoch 19 batch id 2911 loss 0.048072922974824905 train acc 0.9407473806252147\n",
            "epoch 19 batch id 2921 loss 0.03545749932527542 train acc 0.9407095172885998\n",
            "epoch 19 batch id 2931 loss 0.06541488319635391 train acc 0.9407412146025247\n",
            "epoch 19 batch id 2941 loss 0.17412938177585602 train acc 0.9406823784427065\n",
            "epoch 19 batch id 2951 loss 0.2532255947589874 train acc 0.9407245425279567\n",
            "epoch 19 batch id 2961 loss 0.04422493651509285 train acc 0.9407558679500169\n",
            "epoch 19 batch id 2971 loss 0.13930167257785797 train acc 0.940739649949512\n",
            "epoch 19 batch id 2981 loss 0.22355470061302185 train acc 0.9407392653471989\n",
            "epoch 19 batch id 2991 loss 0.19059152901172638 train acc 0.940785899364761\n",
            "epoch 19 batch id 3001 loss 0.11227506399154663 train acc 0.9407593302232589\n",
            "epoch 19 batch id 3011 loss 0.1605979949235916 train acc 0.9407433161740285\n",
            "epoch 19 batch id 3021 loss 0.12224037945270538 train acc 0.9407532687851705\n",
            "epoch 19 batch id 3031 loss 0.3381763994693756 train acc 0.9407528455955131\n",
            "epoch 19 batch id 3041 loss 0.22321978211402893 train acc 0.9407164584018415\n",
            "epoch 19 batch id 3051 loss 0.24731308221817017 train acc 0.94070079482137\n",
            "epoch 19 batch id 3061 loss 0.0932772308588028 train acc 0.9407056517477949\n",
            "epoch 19 batch id 3071 loss 0.13511142134666443 train acc 0.9407155649625529\n",
            "epoch 19 batch id 3081 loss 0.32701176404953003 train acc 0.9407710564751705\n",
            "epoch 19 batch id 3091 loss 0.0699026882648468 train acc 0.9407756389517955\n",
            "epoch 19 batch id 3101 loss 0.17032897472381592 train acc 0.9407348435988391\n",
            "epoch 19 batch id 3111 loss 0.18198886513710022 train acc 0.9407093780135005\n",
            "epoch 19 batch id 3121 loss 0.2876029312610626 train acc 0.9406840756167895\n",
            "epoch 19 batch id 3131 loss 0.146028071641922 train acc 0.9406988581922708\n",
            "epoch 19 batch id 3141 loss 0.18065005540847778 train acc 0.9407035975803885\n",
            "epoch 19 batch id 3151 loss 0.12098588049411774 train acc 0.9407281418597271\n",
            "epoch 19 batch id 3161 loss 0.13067348301410675 train acc 0.9406882711167353\n",
            "epoch 19 batch id 3171 loss 0.16812333464622498 train acc 0.9406683617155471\n",
            "epoch 19 batch id 3181 loss 0.12208288908004761 train acc 0.9406780493555486\n",
            "epoch 19 batch id 3191 loss 0.03119237907230854 train acc 0.9407317455343153\n",
            "epoch 19 batch id 3201 loss 0.17619773745536804 train acc 0.9406923617619494\n",
            "epoch 19 batch id 3211 loss 0.32310566306114197 train acc 0.9406483572095921\n",
            "epoch 19 batch id 3221 loss 0.1837407797574997 train acc 0.9406482846941944\n",
            "epoch 19 batch id 3231 loss 0.13171331584453583 train acc 0.9406482126276694\n",
            "epoch 19 batch id 3241 loss 0.08962072432041168 train acc 0.9406481410058624\n",
            "epoch 19 batch id 3251 loss 0.14642466604709625 train acc 0.9406528760381421\n",
            "epoch 19 batch id 3261 loss 0.15120138227939606 train acc 0.9406336246550138\n",
            "epoch 19 batch id 3271 loss 0.0865618884563446 train acc 0.9405906068480587\n",
            "epoch 19 batch id 3281 loss 0.20795124769210815 train acc 0.9405954739408717\n",
            "epoch 19 batch id 3291 loss 0.1309344470500946 train acc 0.9405860680644181\n",
            "epoch 19 batch id 3301 loss 0.03981241211295128 train acc 0.9406098530748258\n",
            "epoch 19 batch id 3311 loss 0.3007822632789612 train acc 0.9406193370582906\n",
            "epoch 19 batch id 3321 loss 0.09087609499692917 train acc 0.940624059018368\n",
            "epoch 19 batch id 3331 loss 0.11133245378732681 train acc 0.9406334434103872\n",
            "epoch 19 batch id 3341 loss 0.22789344191551208 train acc 0.9406521251122418\n",
            "epoch 19 batch id 3351 loss 0.1223280057311058 train acc 0.940675358102059\n",
            "epoch 19 batch id 3361 loss 0.2072242945432663 train acc 0.9406938039274025\n",
            "epoch 19 batch id 3371 loss 0.2775815725326538 train acc 0.9406843295757935\n",
            "epoch 19 batch id 3381 loss 0.08191636949777603 train acc 0.9407072611653357\n",
            "epoch 19 batch id 3391 loss 0.27266553044319153 train acc 0.9406931952226482\n",
            "epoch 19 batch id 3401 loss 0.17918641865253448 train acc 0.9406700235224934\n",
            "epoch 19 batch id 3411 loss 0.2598857879638672 train acc 0.9406927953679273\n",
            "epoch 19 batch id 3421 loss 0.11424187570810318 train acc 0.9407154340836013\n",
            "epoch 19 batch id 3431 loss 0.24509425461292267 train acc 0.9406832920431362\n",
            "epoch 19 batch id 3441 loss 0.13554076850414276 train acc 0.9407012859633828\n",
            "epoch 19 batch id 3451 loss 0.09696561098098755 train acc 0.9406920095624457\n",
            "epoch 19 batch id 3461 loss 0.22359228134155273 train acc 0.940723418087258\n",
            "epoch 19 batch id 3471 loss 0.10511735081672668 train acc 0.9407411408815903\n",
            "epoch 19 batch id 3481 loss 0.1840960532426834 train acc 0.9406869434070669\n",
            "epoch 19 batch id 3491 loss 0.16443999111652374 train acc 0.9406688627900315\n",
            "epoch 19 batch id 3501 loss 0.1815153807401657 train acc 0.9406955155669808\n",
            "epoch 19 batch id 3511 loss 0.09179919958114624 train acc 0.9406953147251496\n",
            "epoch 19 batch id 3521 loss 0.29079073667526245 train acc 0.9406995526838966\n",
            "epoch 19 batch id 3531 loss 0.26101672649383545 train acc 0.9407037666383461\n",
            "epoch 19 batch id 3541 loss 0.17876608669757843 train acc 0.9407123693871787\n",
            "epoch 19 batch id 3551 loss 0.07546771317720413 train acc 0.9407121233455364\n",
            "epoch 19 batch id 3561 loss 0.10651971399784088 train acc 0.9407469811850604\n",
            "epoch 19 batch id 3571 loss 0.20712406933307648 train acc 0.9407116353962476\n",
            "epoch 19 batch id 3581 loss 0.20583407580852509 train acc 0.9407550265289025\n",
            "epoch 19 batch id 3591 loss 0.13236099481582642 train acc 0.9407851225285436\n",
            "epoch 19 batch id 3601 loss 0.1859973967075348 train acc 0.9408020341571786\n",
            "epoch 19 batch id 3611 loss 0.08783414214849472 train acc 0.9408101980060924\n",
            "epoch 19 batch id 3621 loss 0.04912848770618439 train acc 0.940796741231704\n",
            "epoch 19 batch id 3631 loss 0.1121632382273674 train acc 0.9407661456898926\n",
            "epoch 19 batch id 3641 loss 0.2673977017402649 train acc 0.940778632243889\n",
            "epoch 19 batch id 3651 loss 0.12054542452096939 train acc 0.9407653725006847\n",
            "epoch 19 batch id 3661 loss 0.1302967369556427 train acc 0.940786328871893\n",
            "epoch 19 batch id 3671 loss 0.08446650207042694 train acc 0.9408454780713702\n",
            "epoch 19 batch id 3681 loss 0.09285947680473328 train acc 0.9408788372724803\n",
            "epoch 19 batch id 3691 loss 0.08759444952011108 train acc 0.9408993159035491\n",
            "epoch 19 batch id 3701 loss 0.15509407222270966 train acc 0.9409323493650364\n",
            "epoch 19 batch id 3711 loss 0.18872103095054626 train acc 0.940990467528968\n",
            "epoch 19 batch id 3721 loss 0.10382555425167084 train acc 0.9410104810534803\n",
            "epoch 19 batch id 3731 loss 0.1322692632675171 train acc 0.9410220115250603\n",
            "epoch 19 batch id 3741 loss 0.14891678094863892 train acc 0.9409917134456027\n",
            "epoch 19 batch id 3751 loss 0.09936192631721497 train acc 0.9409740735803785\n",
            "epoch 19 batch id 3761 loss 0.1941070258617401 train acc 0.9410063812815741\n",
            "epoch 19 batch id 3771 loss 0.14601708948612213 train acc 0.9409597918324052\n",
            "epoch 19 batch id 3781 loss 0.23609519004821777 train acc 0.9409589063739752\n",
            "epoch 19 batch id 3791 loss 0.0582013763487339 train acc 0.9409703903983118\n",
            "epoch 19 batch id 3801 loss 0.20716741681098938 train acc 0.940985924756643\n",
            "epoch 19 batch id 3811 loss 0.18518298864364624 train acc 0.9409931776436631\n",
            "epoch 19 batch id 3821 loss 0.10499264299869537 train acc 0.9409963033237373\n",
            "epoch 19 batch id 3831 loss 0.031854625791311264 train acc 0.941011648394675\n",
            "epoch 19 batch id 3841 loss 0.03732514753937721 train acc 0.941006573808904\n",
            "epoch 19 batch id 3851 loss 0.026310624554753304 train acc 0.9410218125162295\n",
            "epoch 19 batch id 3861 loss 0.09164686501026154 train acc 0.9410329254079254\n",
            "epoch 19 batch id 3871 loss 0.11998174339532852 train acc 0.9410359080340998\n",
            "epoch 19 batch id 3881 loss 0.1621743142604828 train acc 0.9410469273383149\n",
            "epoch 19 batch id 3891 loss 0.03479749336838722 train acc 0.94105789000257\n",
            "epoch 19 batch id 3901 loss 0.13301625847816467 train acc 0.9410127210971546\n",
            "epoch 19 batch id 3911 loss 0.08135149627923965 train acc 0.9410237151623626\n",
            "epoch 19 batch id 3921 loss 0.10599559545516968 train acc 0.9410067584799796\n",
            "epoch 19 batch id 3931 loss 0.18917244672775269 train acc 0.9410256614093107\n",
            "epoch 19 batch id 3941 loss 0.13499771058559418 train acc 0.9410087858411571\n",
            "epoch 19 batch id 3951 loss 0.04127400368452072 train acc 0.9409959503923058\n",
            "epoch 19 batch id 3961 loss 0.19058647751808167 train acc 0.9409634561979299\n",
            "epoch 19 batch id 3971 loss 0.30043449997901917 train acc 0.9409586691009821\n",
            "epoch 19 batch id 3981 loss 0.15450650453567505 train acc 0.9409382064807837\n",
            "epoch 19 batch id 3991 loss 0.1717355102300644 train acc 0.9409844024054121\n",
            "epoch 19 batch id 4001 loss 0.2772299349308014 train acc 0.9409913146713321\n",
            "epoch 19 batch id 4011 loss 0.3019901216030121 train acc 0.9409514460234355\n",
            "epoch 19 batch id 4021 loss 0.11420551687479019 train acc 0.9409350907734394\n",
            "epoch 19 batch id 4031 loss 0.04062862694263458 train acc 0.9409847122302158\n",
            "epoch 19 batch id 4041 loss 0.17072714865207672 train acc 0.9409915553080921\n",
            "epoch 19 batch id 4051 loss 0.053719110786914825 train acc 0.9409829363120217\n",
            "epoch 19 batch id 4061 loss 0.0957040786743164 train acc 0.9410128355084955\n",
            "epoch 19 batch id 4071 loss 0.2823682427406311 train acc 0.9409888540899042\n",
            "epoch 19 batch id 4081 loss 0.09174270182847977 train acc 0.9410568794413134\n",
            "epoch 19 batch id 4091 loss 0.14079827070236206 train acc 0.9410520043999022\n",
            "epoch 19 batch id 4101 loss 0.11834152042865753 train acc 0.941043343087052\n",
            "epoch 19 batch id 4111 loss 0.07345885038375854 train acc 0.9410765324738506\n",
            "epoch 19 batch id 4121 loss 0.11929299682378769 train acc 0.9410792283426352\n",
            "epoch 19 batch id 4131 loss 0.1746894121170044 train acc 0.9411046054224159\n",
            "epoch 19 batch id 4141 loss 0.0844840556383133 train acc 0.9411487261531031\n",
            "epoch 19 batch id 4151 loss 0.1585068702697754 train acc 0.9411700493856902\n",
            "epoch 19 batch id 4161 loss 0.2949201166629791 train acc 0.9411462088440279\n",
            "epoch 19 batch id 4171 loss 0.18079987168312073 train acc 0.941159943658595\n",
            "epoch 19 batch id 4181 loss 0.013679884374141693 train acc 0.9411511899067209\n",
            "epoch 19 batch id 4191 loss 0.20366312563419342 train acc 0.9411536626103555\n",
            "epoch 19 batch id 4201 loss 0.14613141119480133 train acc 0.9411710009521542\n",
            "epoch 19 batch id 4211 loss 0.13987630605697632 train acc 0.9411697043457611\n",
            "epoch 19 batch id 4221 loss 0.21134410798549652 train acc 0.9411425017768301\n",
            "epoch 19 batch id 4231 loss 0.17734627425670624 train acc 0.941133892696762\n",
            "epoch 19 batch id 4241 loss 0.13890919089317322 train acc 0.9411695354869135\n",
            "epoch 19 batch id 4251 loss 0.39019399881362915 train acc 0.9411425252881674\n",
            "epoch 19 batch id 4261 loss 0.20020978152751923 train acc 0.9411266428068529\n",
            "epoch 19 batch id 4271 loss 0.03678864613175392 train acc 0.941187660969328\n",
            "epoch 19 batch id 4281 loss 0.13700534403324127 train acc 0.9412009460406447\n",
            "epoch 19 batch id 4291 loss 0.0501948781311512 train acc 0.9412323759030529\n",
            "epoch 19 batch id 4301 loss 0.16940511763095856 train acc 0.9412309637293652\n",
            "epoch 19 batch id 4311 loss 0.06145097687840462 train acc 0.9412839248434238\n",
            "epoch 19 batch id 4321 loss 0.09576084464788437 train acc 0.9412787838463319\n",
            "epoch 19 batch id 4331 loss 0.18118472397327423 train acc 0.9412953128607712\n",
            "epoch 19 batch id 4341 loss 0.12012448161840439 train acc 0.9412865699147662\n",
            "epoch 19 batch id 4351 loss 0.1055411547422409 train acc 0.9412778671569754\n",
            "epoch 19 batch id 4361 loss 0.14936478435993195 train acc 0.9412835358862646\n",
            "epoch 19 batch id 4371 loss 0.21461538970470428 train acc 0.9412999027682453\n",
            "epoch 19 batch id 4381 loss 0.2632756531238556 train acc 0.9413054953207031\n",
            "epoch 19 batch id 4391 loss 0.04559936746954918 train acc 0.9413075039854247\n",
            "epoch 19 batch id 4401 loss 0.13822804391384125 train acc 0.9413059531924562\n",
            "epoch 19 batch id 4411 loss 0.03133120760321617 train acc 0.9413256631149399\n",
            "epoch 19 batch id 4421 loss 0.14456844329833984 train acc 0.941377092286813\n",
            "epoch 19 batch id 4431 loss 0.04868568107485771 train acc 0.9413965526969081\n",
            "epoch 19 batch id 4441 loss 0.07582900673151016 train acc 0.9413983337086241\n",
            "epoch 19 batch id 4451 loss 0.030896753072738647 train acc 0.9414457425297685\n",
            "epoch 19 batch id 4461 loss 0.10285419970750809 train acc 0.9414684207576777\n",
            "epoch 19 batch id 4471 loss 0.22061684727668762 train acc 0.9414490606128383\n",
            "epoch 19 batch id 4481 loss 0.09987712651491165 train acc 0.9414332738228074\n",
            "epoch 19 batch id 4491 loss 0.1845337301492691 train acc 0.9414140781563126\n",
            "epoch 19 batch id 4501 loss 0.04583164304494858 train acc 0.941415796489669\n",
            "epoch 19 batch id 4511 loss 0.3216007649898529 train acc 0.9414278984704056\n",
            "epoch 19 batch id 4521 loss 0.05284147709608078 train acc 0.9414434030081841\n",
            "epoch 19 batch id 4531 loss 0.06513673812150955 train acc 0.9414553906422424\n",
            "epoch 19 batch id 4541 loss 0.20435228943824768 train acc 0.9414432393745871\n",
            "epoch 19 batch id 4551 loss 0.24967968463897705 train acc 0.9414586079982421\n",
            "epoch 19 batch id 4561 loss 0.21151694655418396 train acc 0.9414567803113353\n",
            "epoch 19 batch id 4571 loss 0.2034582942724228 train acc 0.9414788886458105\n",
            "epoch 19 batch id 4581 loss 0.07178597152233124 train acc 0.9414804354944335\n",
            "epoch 19 batch id 4591 loss 0.17949342727661133 train acc 0.941471765410586\n",
            "epoch 19 batch id 4601 loss 0.10895442962646484 train acc 0.9414699250163008\n",
            "epoch 19 batch id 4611 loss 0.10591976344585419 train acc 0.9414782585122533\n",
            "epoch 19 batch id 4621 loss 0.1371344029903412 train acc 0.9414764120320277\n",
            "epoch 19 batch id 4631 loss 0.22685779631137848 train acc 0.941484695530123\n",
            "epoch 19 batch id 4641 loss 0.26206979155540466 train acc 0.9415198771816419\n",
            "epoch 19 batch id 4651 loss 0.25758153200149536 train acc 0.9415616265319287\n",
            "epoch 19 batch id 4661 loss 0.22323329746723175 train acc 0.9415629693198885\n",
            "epoch 19 batch id 4671 loss 0.11557670682668686 train acc 0.9415944123314065\n",
            "epoch 19 batch id 4681 loss 0.3264766335487366 train acc 0.9415956793420209\n",
            "epoch 19 batch id 4691 loss 0.14826971292495728 train acc 0.9416069334896611\n",
            "epoch 19 batch id 4701 loss 0.26552706956863403 train acc 0.9416114922356945\n",
            "epoch 19 batch id 4711 loss 0.07998475432395935 train acc 0.9416591488006792\n",
            "epoch 19 batch id 4721 loss 0.04838354140520096 train acc 0.9416536485914001\n",
            "epoch 19 batch id 4731 loss 0.09696182608604431 train acc 0.9416613823715916\n",
            "epoch 19 batch id 4741 loss 0.2872980535030365 train acc 0.9416624920902763\n",
            "epoch 19 batch id 4751 loss 0.17068207263946533 train acc 0.9416504420122079\n",
            "epoch 19 batch id 4761 loss 0.13868270814418793 train acc 0.9416482881747532\n",
            "epoch 19 batch id 4771 loss 0.22841109335422516 train acc 0.9416690683294907\n",
            "epoch 19 batch id 4781 loss 0.25667625665664673 train acc 0.9416734208324619\n",
            "epoch 19 batch id 4791 loss 0.348156601190567 train acc 0.9416288353162179\n",
            "epoch 19 batch id 4801 loss 0.14636239409446716 train acc 0.9416007081857947\n",
            "epoch 19 batch id 4811 loss 0.03667784109711647 train acc 0.9416149189357722\n",
            "epoch 19 batch id 4821 loss 0.215011328458786 train acc 0.9415901783862269\n",
            "epoch 19 batch id 4831 loss 0.0924791693687439 train acc 0.9416075864210308\n",
            "epoch 19 batch id 4841 loss 0.265131413936615 train acc 0.9416152396199132\n",
            "epoch 19 batch id 4851 loss 0.10452204942703247 train acc 0.9416389661925376\n",
            "epoch 19 batch id 4861 loss 0.04665442183613777 train acc 0.9416400946307344\n",
            "epoch 19 batch id 4871 loss 0.14946958422660828 train acc 0.9416540494764936\n",
            "epoch 19 batch id 4881 loss 0.2505210340023041 train acc 0.9416807518951035\n",
            "epoch 19 batch id 4891 loss 0.11435297131538391 train acc 0.9416658147618074\n",
            "epoch 19 batch id 4901 loss 0.12678223848342896 train acc 0.9416732554580698\n",
            "epoch 19 batch id 4911 loss 0.32845503091812134 train acc 0.9416647576868254\n",
            "epoch 19 batch id 4921 loss 0.20771390199661255 train acc 0.9416594696199959\n",
            "epoch 19 batch id 4931 loss 0.22369521856307983 train acc 0.9416605404583249\n",
            "epoch 19 batch id 4941 loss 0.08543458580970764 train acc 0.941652120016191\n",
            "epoch 19 batch id 4951 loss 0.306173712015152 train acc 0.9416563573015553\n",
            "epoch 19 train acc 0.9416501801859629\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "703ed4b812ac400d94e0a01beab685a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 19 loss 0.33576148748397827 test acc 0.8240698313782991\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d18b0f9f43bb4eb894e173b649370c6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20 batch id 1 loss 0.1525055319070816 train acc 0.9375\n",
            "epoch 20 batch id 11 loss 0.18328799307346344 train acc 0.9460227272727273\n",
            "epoch 20 batch id 21 loss 0.14955265820026398 train acc 0.9419642857142857\n",
            "epoch 20 batch id 31 loss 0.10631772130727768 train acc 0.9354838709677419\n",
            "epoch 20 batch id 41 loss 0.03751442953944206 train acc 0.9420731707317073\n",
            "epoch 20 batch id 51 loss 0.1618877649307251 train acc 0.9439338235294118\n",
            "epoch 20 batch id 61 loss 0.17152102291584015 train acc 0.9415983606557377\n",
            "epoch 20 batch id 71 loss 0.2678702771663666 train acc 0.9423415492957746\n",
            "epoch 20 batch id 81 loss 0.19915777444839478 train acc 0.9417438271604939\n",
            "epoch 20 batch id 91 loss 0.18258728086948395 train acc 0.9440247252747253\n",
            "epoch 20 batch id 101 loss 0.18807896971702576 train acc 0.942759900990099\n",
            "epoch 20 batch id 111 loss 0.1178286001086235 train acc 0.9434121621621622\n",
            "epoch 20 batch id 121 loss 0.1230049654841423 train acc 0.9440857438016529\n",
            "epoch 20 batch id 131 loss 0.11011870205402374 train acc 0.9447757633587787\n",
            "epoch 20 batch id 141 loss 0.26919543743133545 train acc 0.9442597517730497\n",
            "epoch 20 batch id 151 loss 0.17305521667003632 train acc 0.9443294701986755\n",
            "epoch 20 batch id 161 loss 0.22819554805755615 train acc 0.9452639751552795\n",
            "epoch 20 batch id 171 loss 0.2525251805782318 train acc 0.9449013157894737\n",
            "epoch 20 batch id 181 loss 0.1341860294342041 train acc 0.9458736187845304\n",
            "epoch 20 batch id 191 loss 0.07480711489915848 train acc 0.9458442408376964\n",
            "epoch 20 batch id 201 loss 0.21081402897834778 train acc 0.9464396766169154\n",
            "epoch 20 batch id 211 loss 0.16569088399410248 train acc 0.9460159952606635\n",
            "epoch 20 batch id 221 loss 0.20079384744167328 train acc 0.9451357466063348\n",
            "epoch 20 batch id 231 loss 0.1354014277458191 train acc 0.9451433982683982\n",
            "epoch 20 batch id 241 loss 0.22325821220874786 train acc 0.9445020746887967\n",
            "epoch 20 batch id 251 loss 0.19318310916423798 train acc 0.9447833665338645\n",
            "epoch 20 batch id 261 loss 0.13863995671272278 train acc 0.9448635057471264\n",
            "epoch 20 batch id 271 loss 0.10118015855550766 train acc 0.9455719557195572\n",
            "epoch 20 batch id 281 loss 0.016162604093551636 train acc 0.9457295373665481\n",
            "epoch 20 batch id 291 loss 0.18148808181285858 train acc 0.9458225945017182\n",
            "epoch 20 batch id 301 loss 0.28573375940322876 train acc 0.9455980066445183\n",
            "epoch 20 batch id 311 loss 0.15964947640895844 train acc 0.9457395498392283\n",
            "epoch 20 batch id 321 loss 0.22794556617736816 train acc 0.9455315420560748\n",
            "epoch 20 batch id 331 loss 0.15393970906734467 train acc 0.9460913897280967\n",
            "epoch 20 batch id 341 loss 0.3670961558818817 train acc 0.9461601906158358\n",
            "epoch 20 batch id 351 loss 0.23070041835308075 train acc 0.9457353988603988\n",
            "epoch 20 batch id 361 loss 0.10504399240016937 train acc 0.9460699445983379\n",
            "epoch 20 batch id 371 loss 0.14366021752357483 train acc 0.9464706873315364\n",
            "epoch 20 batch id 381 loss 0.08500256389379501 train acc 0.9463582677165354\n",
            "epoch 20 batch id 391 loss 0.16530567407608032 train acc 0.9464114450127877\n",
            "epoch 20 batch id 401 loss 0.07311969250440598 train acc 0.9463061097256857\n",
            "epoch 20 batch id 411 loss 0.12106442451477051 train acc 0.946205900243309\n",
            "epoch 20 batch id 421 loss 0.24635036289691925 train acc 0.9457764251781473\n",
            "epoch 20 batch id 431 loss 0.20670142769813538 train acc 0.9458744199535963\n",
            "epoch 20 batch id 441 loss 0.2650008201599121 train acc 0.9459679705215419\n",
            "epoch 20 batch id 451 loss 0.09011033922433853 train acc 0.9461613082039911\n",
            "epoch 20 batch id 461 loss 0.07772041112184525 train acc 0.9461767895878525\n",
            "epoch 20 batch id 471 loss 0.06784974783658981 train acc 0.9465897027600849\n",
            "epoch 20 batch id 481 loss 0.1662004441022873 train acc 0.9465306652806653\n",
            "epoch 20 batch id 491 loss 0.19339871406555176 train acc 0.9466649694501018\n",
            "epoch 20 batch id 501 loss 0.535790205001831 train acc 0.9466691616766467\n",
            "epoch 20 batch id 511 loss 0.14775119721889496 train acc 0.9465508806262231\n",
            "epoch 20 batch id 521 loss 0.14056167006492615 train acc 0.9465870921305183\n",
            "epoch 20 batch id 531 loss 0.17734190821647644 train acc 0.9462688323917138\n",
            "epoch 20 batch id 541 loss 0.11649840325117111 train acc 0.9464244454713494\n",
            "epoch 20 batch id 551 loss 0.150609090924263 train acc 0.9465176950998185\n",
            "epoch 20 batch id 561 loss 0.17429819703102112 train acc 0.9464126559714795\n",
            "epoch 20 batch id 571 loss 0.24786968529224396 train acc 0.9463933887915937\n",
            "epoch 20 batch id 581 loss 0.2002997249364853 train acc 0.9462941049913941\n",
            "epoch 20 batch id 591 loss 0.08195683360099792 train acc 0.9464361252115059\n",
            "epoch 20 batch id 601 loss 0.07892622798681259 train acc 0.9465214226289518\n",
            "epoch 20 batch id 611 loss 0.19262227416038513 train acc 0.9465527823240589\n",
            "epoch 20 batch id 621 loss 0.2675149738788605 train acc 0.9465579710144928\n",
            "epoch 20 batch id 631 loss 0.13471505045890808 train acc 0.9465877575277337\n",
            "epoch 20 batch id 641 loss 0.17792147397994995 train acc 0.9466897425897036\n",
            "epoch 20 batch id 651 loss 0.1295032650232315 train acc 0.9465965821812596\n",
            "epoch 20 batch id 661 loss 0.08111481368541718 train acc 0.9466007942511346\n",
            "epoch 20 batch id 671 loss 0.03355253115296364 train acc 0.9466980253353204\n",
            "epoch 20 batch id 681 loss 0.2269940823316574 train acc 0.9466088472834068\n",
            "epoch 20 batch id 691 loss 0.41018691658973694 train acc 0.9467031476121563\n",
            "epoch 20 batch id 701 loss 0.2380990982055664 train acc 0.9464827032810271\n",
            "epoch 20 batch id 711 loss 0.2853506803512573 train acc 0.9464882208157525\n",
            "epoch 20 batch id 721 loss 0.1780034601688385 train acc 0.9464719140083218\n",
            "epoch 20 batch id 731 loss 0.17878970503807068 train acc 0.9465201778385773\n",
            "epoch 20 batch id 741 loss 0.09584309160709381 train acc 0.9465038798920378\n",
            "epoch 20 batch id 751 loss 0.2548984885215759 train acc 0.9463839880159787\n",
            "epoch 20 batch id 761 loss 0.11230262368917465 train acc 0.9464520367936925\n",
            "epoch 20 batch id 771 loss 0.31116050481796265 train acc 0.9462143320363164\n",
            "epoch 20 batch id 781 loss 0.24522437155246735 train acc 0.9459827144686299\n",
            "epoch 20 batch id 791 loss 0.030890749767422676 train acc 0.9460927623261695\n",
            "epoch 20 batch id 801 loss 0.0991271585226059 train acc 0.9460245006242197\n",
            "epoch 20 batch id 811 loss 0.22896845638751984 train acc 0.9459001233045623\n",
            "epoch 20 batch id 821 loss 0.06918077915906906 train acc 0.9459500609013398\n",
            "epoch 20 batch id 831 loss 0.2242392897605896 train acc 0.9459987966305656\n",
            "epoch 20 batch id 841 loss 0.06114303693175316 train acc 0.9462135850178359\n",
            "epoch 20 batch id 851 loss 0.26758086681365967 train acc 0.9459459459459459\n",
            "epoch 20 batch id 861 loss 0.15408955514431 train acc 0.945902293844367\n",
            "epoch 20 batch id 871 loss 0.16703695058822632 train acc 0.9457878874856487\n",
            "epoch 20 batch id 881 loss 0.048212870955467224 train acc 0.9457470204313281\n",
            "epoch 20 batch id 891 loss 0.028528010472655296 train acc 0.9458298260381593\n",
            "epoch 20 batch id 901 loss 0.14182987809181213 train acc 0.9459107935627081\n",
            "epoch 20 batch id 911 loss 0.2166256159543991 train acc 0.9458699231613611\n",
            "epoch 20 batch id 921 loss 0.08408273756504059 train acc 0.9458638707926167\n",
            "epoch 20 batch id 931 loss 0.08862778544425964 train acc 0.9457404672395274\n",
            "epoch 20 batch id 941 loss 0.08812013268470764 train acc 0.9459849893730075\n",
            "epoch 20 batch id 951 loss 0.3018489181995392 train acc 0.9458464773922187\n",
            "epoch 20 batch id 961 loss 0.1769619733095169 train acc 0.9459059573361083\n",
            "epoch 20 batch id 971 loss 0.12475193291902542 train acc 0.945883753861998\n",
            "epoch 20 batch id 981 loss 0.09706972539424896 train acc 0.9460372069317023\n",
            "epoch 20 batch id 991 loss 0.2621332108974457 train acc 0.9460929616548941\n",
            "epoch 20 batch id 1001 loss 0.13771271705627441 train acc 0.9462724775224776\n",
            "epoch 20 batch id 1011 loss 0.10439439117908478 train acc 0.9463248021760633\n",
            "epoch 20 batch id 1021 loss 0.08029511570930481 train acc 0.9463607982370226\n",
            "epoch 20 batch id 1031 loss 0.12050540745258331 train acc 0.9462900096993211\n",
            "epoch 20 batch id 1041 loss 0.3573349118232727 train acc 0.9461605427473583\n",
            "epoch 20 batch id 1051 loss 0.08924929052591324 train acc 0.9459889391056137\n",
            "epoch 20 batch id 1061 loss 0.1945117563009262 train acc 0.9459531102733271\n",
            "epoch 20 batch id 1071 loss 0.03819624334573746 train acc 0.94609302054155\n",
            "epoch 20 batch id 1081 loss 0.25063556432724 train acc 0.9457822617946346\n",
            "epoch 20 batch id 1091 loss 0.4450732171535492 train acc 0.9456920256645279\n",
            "epoch 20 batch id 1101 loss 0.13333645462989807 train acc 0.945716961852861\n",
            "epoch 20 batch id 1111 loss 0.22376908361911774 train acc 0.945685193519352\n",
            "epoch 20 batch id 1121 loss 0.2509795129299164 train acc 0.9457515611061552\n",
            "epoch 20 batch id 1131 loss 0.22509552538394928 train acc 0.9456509725906278\n",
            "epoch 20 batch id 1141 loss 0.23190726339817047 train acc 0.9456480061349694\n",
            "epoch 20 batch id 1151 loss 0.133221834897995 train acc 0.9455772154648132\n",
            "epoch 20 batch id 1161 loss 0.05834285914897919 train acc 0.9455076442721791\n",
            "epoch 20 batch id 1171 loss 0.21604417264461517 train acc 0.9454659479077712\n",
            "epoch 20 batch id 1181 loss 0.08643858134746552 train acc 0.945530800169348\n",
            "epoch 20 batch id 1191 loss 0.08008449524641037 train acc 0.9455420864819479\n",
            "epoch 20 batch id 1201 loss 0.22395718097686768 train acc 0.9457223147377186\n",
            "epoch 20 batch id 1211 loss 0.10060694813728333 train acc 0.9458221511147812\n",
            "epoch 20 batch id 1221 loss 0.14264284074306488 train acc 0.9459203521703522\n",
            "epoch 20 batch id 1231 loss 0.11414359509944916 train acc 0.9457630991064175\n",
            "epoch 20 batch id 1241 loss 0.12489606440067291 train acc 0.9457720588235294\n",
            "epoch 20 batch id 1251 loss 0.09211256355047226 train acc 0.945818345323741\n",
            "epoch 20 batch id 1261 loss 0.16221481561660767 train acc 0.9459258524980174\n",
            "epoch 20 batch id 1271 loss 0.09455966204404831 train acc 0.9459087332808812\n",
            "epoch 20 batch id 1281 loss 0.13933785259723663 train acc 0.9459162763466042\n",
            "epoch 20 batch id 1291 loss 0.14786840975284576 train acc 0.945935805577072\n",
            "epoch 20 batch id 1301 loss 0.20045575499534607 train acc 0.945798904688701\n",
            "epoch 20 batch id 1311 loss 0.10760511457920074 train acc 0.945807112890923\n",
            "epoch 20 batch id 1321 loss 0.03871418163180351 train acc 0.9458151968205905\n",
            "epoch 20 batch id 1331 loss 0.23690226674079895 train acc 0.9457879413974455\n",
            "epoch 20 batch id 1341 loss 0.2336389720439911 train acc 0.9456678784489188\n",
            "epoch 20 batch id 1351 loss 0.10676615685224533 train acc 0.9456305514433753\n",
            "epoch 20 batch id 1361 loss 0.09378952533006668 train acc 0.9455363703159442\n",
            "epoch 20 batch id 1371 loss 0.12816831469535828 train acc 0.9454777534646244\n",
            "epoch 20 batch id 1381 loss 0.20109380781650543 train acc 0.9455218139029689\n",
            "epoch 20 batch id 1391 loss 0.1880825310945511 train acc 0.9455427749820273\n",
            "epoch 20 batch id 1401 loss 0.11692660301923752 train acc 0.945652658815132\n",
            "epoch 20 batch id 1411 loss 0.035829946398735046 train acc 0.9457720588235294\n",
            "epoch 20 batch id 1421 loss 0.2053847312927246 train acc 0.9457358374384236\n",
            "epoch 20 batch id 1431 loss 0.1263028234243393 train acc 0.9458093116701607\n",
            "epoch 20 batch id 1441 loss 0.12461111694574356 train acc 0.9456432165163081\n",
            "epoch 20 batch id 1451 loss 0.1938650757074356 train acc 0.945716316333563\n",
            "epoch 20 batch id 1461 loss 0.17630554735660553 train acc 0.9456921629021219\n",
            "epoch 20 batch id 1471 loss 0.0953252911567688 train acc 0.9457745581237254\n",
            "epoch 20 batch id 1481 loss 0.10410478711128235 train acc 0.9457608879135719\n",
            "epoch 20 batch id 1491 loss 0.20916321873664856 train acc 0.9457788397048961\n",
            "epoch 20 batch id 1501 loss 0.1206832304596901 train acc 0.945786142571619\n",
            "epoch 20 batch id 1511 loss 0.09022447466850281 train acc 0.9458347121111846\n",
            "epoch 20 batch id 1521 loss 0.06031516566872597 train acc 0.9459031886916502\n",
            "epoch 20 batch id 1531 loss 0.07836627960205078 train acc 0.9459503592423253\n",
            "epoch 20 batch id 1541 loss 0.1882510781288147 train acc 0.9459969175859831\n",
            "epoch 20 batch id 1551 loss 0.09251171350479126 train acc 0.9458615409413281\n",
            "epoch 20 batch id 1561 loss 0.25397300720214844 train acc 0.9458780429212044\n",
            "epoch 20 batch id 1571 loss 0.04951540380716324 train acc 0.9460037396562699\n",
            "epoch 20 batch id 1581 loss 0.17861798405647278 train acc 0.9460290164452878\n",
            "epoch 20 batch id 1591 loss 0.21724042296409607 train acc 0.9461325424261471\n",
            "epoch 20 batch id 1601 loss 0.26475775241851807 train acc 0.9461859775140538\n",
            "epoch 20 batch id 1611 loss 0.2498980164527893 train acc 0.9460932650527623\n",
            "epoch 20 batch id 1621 loss 0.04647492617368698 train acc 0.9460980876002467\n",
            "epoch 20 batch id 1631 loss 0.04864572361111641 train acc 0.946189071122011\n",
            "epoch 20 batch id 1641 loss 0.18094290792942047 train acc 0.9461837294332724\n",
            "epoch 20 batch id 1651 loss 0.10533061623573303 train acc 0.9461311326468806\n",
            "epoch 20 batch id 1661 loss 0.23534415662288666 train acc 0.9461732390126429\n",
            "epoch 20 batch id 1671 loss 0.2008952498435974 train acc 0.9460932824655894\n",
            "epoch 20 batch id 1681 loss 0.2814560532569885 train acc 0.945958506841166\n",
            "epoch 20 batch id 1691 loss 0.20711790025234222 train acc 0.9459084861028977\n",
            "epoch 20 batch id 1701 loss 0.10639671981334686 train acc 0.945923353909465\n",
            "epoch 20 batch id 1711 loss 0.2175343781709671 train acc 0.945864991233197\n",
            "epoch 20 batch id 1721 loss 0.12514351308345795 train acc 0.9458890180127832\n",
            "epoch 20 batch id 1731 loss 0.09006426483392715 train acc 0.9458495811669555\n",
            "epoch 20 batch id 1741 loss 0.16898171603679657 train acc 0.9457926479035037\n",
            "epoch 20 batch id 1751 loss 0.08269713073968887 train acc 0.9458166761850372\n",
            "epoch 20 batch id 1761 loss 0.18513819575309753 train acc 0.9458315587734242\n",
            "epoch 20 batch id 1771 loss 0.13009920716285706 train acc 0.9458374505928854\n",
            "epoch 20 batch id 1781 loss 0.208757221698761 train acc 0.945860822571589\n",
            "epoch 20 batch id 1791 loss 0.43254056572914124 train acc 0.9457617950865438\n",
            "epoch 20 batch id 1801 loss 0.06699337810277939 train acc 0.9457940033314826\n",
            "epoch 20 batch id 1811 loss 0.12960422039031982 train acc 0.9457827167310878\n",
            "epoch 20 batch id 1821 loss 0.12302806228399277 train acc 0.9457629736408567\n",
            "epoch 20 batch id 1831 loss 0.22358396649360657 train acc 0.9457605133806664\n",
            "epoch 20 batch id 1841 loss 0.12708517909049988 train acc 0.9457920287887018\n",
            "epoch 20 batch id 1851 loss 0.1308496594429016 train acc 0.9457809967585089\n",
            "epoch 20 batch id 1861 loss 0.10840943455696106 train acc 0.9458540435249866\n",
            "epoch 20 batch id 1871 loss 0.22272971272468567 train acc 0.9459847675040085\n",
            "epoch 20 batch id 1881 loss 0.07173579186201096 train acc 0.9460227272727273\n",
            "epoch 20 batch id 1891 loss 0.13947534561157227 train acc 0.9460768112109995\n",
            "epoch 20 batch id 1901 loss 0.15886741876602173 train acc 0.9461056680694372\n",
            "epoch 20 batch id 1911 loss 0.11296314001083374 train acc 0.9460442830978545\n",
            "epoch 20 batch id 1921 loss 0.14322762191295624 train acc 0.9459591358667361\n",
            "epoch 20 batch id 1931 loss 0.14760373532772064 train acc 0.9459557871569135\n",
            "epoch 20 batch id 1941 loss 0.19554691016674042 train acc 0.9459363730036063\n",
            "epoch 20 batch id 1951 loss 0.18436507880687714 train acc 0.9459491927216812\n",
            "epoch 20 batch id 1961 loss 0.09228187054395676 train acc 0.9459857853136155\n",
            "epoch 20 batch id 1971 loss 0.03793029114603996 train acc 0.9460854261796042\n",
            "epoch 20 batch id 1981 loss 0.2004968822002411 train acc 0.9460341998990409\n",
            "epoch 20 batch id 1991 loss 0.17488861083984375 train acc 0.9460227272727273\n",
            "epoch 20 batch id 2001 loss 0.15358780324459076 train acc 0.9460504122938531\n",
            "epoch 20 batch id 2011 loss 0.10505703836679459 train acc 0.9460234336151169\n",
            "epoch 20 batch id 2021 loss 0.17121367156505585 train acc 0.946012184562098\n",
            "epoch 20 batch id 2031 loss 0.06551225483417511 train acc 0.946031819300837\n",
            "epoch 20 batch id 2041 loss 0.078990638256073 train acc 0.9459900171484567\n",
            "epoch 20 batch id 2051 loss 0.30846843123435974 train acc 0.9459638590931253\n",
            "epoch 20 batch id 2061 loss 0.1423799693584442 train acc 0.9459606986899564\n",
            "epoch 20 batch id 2071 loss 0.24458330869674683 train acc 0.9459349348140995\n",
            "epoch 20 batch id 2081 loss 0.22249393165111542 train acc 0.9459019101393561\n",
            "epoch 20 batch id 2091 loss 0.13785725831985474 train acc 0.9458692013390723\n",
            "epoch 20 batch id 2101 loss 0.033754218369722366 train acc 0.9459706687291766\n",
            "epoch 20 batch id 2111 loss 0.09259818494319916 train acc 0.945952747513027\n",
            "epoch 20 batch id 2121 loss 0.07106824219226837 train acc 0.9459644625176803\n",
            "epoch 20 batch id 2131 loss 0.23782792687416077 train acc 0.9459980642890662\n",
            "epoch 20 batch id 2141 loss 0.1512499898672104 train acc 0.9459583722559551\n",
            "epoch 20 batch id 2151 loss 0.049796417355537415 train acc 0.9460425383542538\n",
            "epoch 20 batch id 2161 loss 0.21612875163555145 train acc 0.9459957774178621\n",
            "epoch 20 batch id 2171 loss 0.16304585337638855 train acc 0.9460286158452326\n",
            "epoch 20 batch id 2181 loss 0.08582133799791336 train acc 0.9459751834021092\n",
            "epoch 20 batch id 2191 loss 0.07634321600198746 train acc 0.9460149475125513\n",
            "epoch 20 batch id 2201 loss 0.08266814053058624 train acc 0.9460117560199909\n",
            "epoch 20 batch id 2211 loss 0.06642550975084305 train acc 0.9460085933966531\n",
            "epoch 20 batch id 2221 loss 0.26245278120040894 train acc 0.9459913890139576\n",
            "epoch 20 batch id 2231 loss 0.06585658341646194 train acc 0.9459533281039892\n",
            "epoch 20 batch id 2241 loss 0.12071280926465988 train acc 0.9459783578759482\n",
            "epoch 20 batch id 2251 loss 0.08130724728107452 train acc 0.9459892825410928\n",
            "epoch 20 batch id 2261 loss 0.055364940315485 train acc 0.946000110570544\n",
            "epoch 20 batch id 2271 loss 0.37484779953956604 train acc 0.945997082782915\n",
            "epoch 20 batch id 2281 loss 0.22708627581596375 train acc 0.946028331871986\n",
            "epoch 20 batch id 2291 loss 0.16275741159915924 train acc 0.946066128328241\n",
            "epoch 20 batch id 2301 loss 0.13042311370372772 train acc 0.9461171773142112\n",
            "epoch 20 batch id 2311 loss 0.21793600916862488 train acc 0.9461407399394202\n",
            "epoch 20 batch id 2321 loss 0.26306697726249695 train acc 0.94611697544162\n",
            "epoch 20 batch id 2331 loss 0.1276908814907074 train acc 0.946053196053196\n",
            "epoch 20 batch id 2341 loss 0.17976145446300507 train acc 0.946070055531824\n",
            "epoch 20 batch id 2351 loss 0.17429406940937042 train acc 0.9460668332624416\n",
            "epoch 20 batch id 2361 loss 0.13551868498325348 train acc 0.9460239305379077\n",
            "epoch 20 batch id 2371 loss 0.020908745005726814 train acc 0.9460538802193167\n",
            "epoch 20 batch id 2381 loss 0.014321181923151016 train acc 0.9460835783284335\n",
            "epoch 20 batch id 2391 loss 0.12093138694763184 train acc 0.9460934232538687\n",
            "epoch 20 batch id 2401 loss 0.18002839386463165 train acc 0.9460901707621824\n",
            "epoch 20 batch id 2411 loss 0.14814099669456482 train acc 0.9460675031107424\n",
            "epoch 20 batch id 2421 loss 0.18216122686862946 train acc 0.9461676476662536\n",
            "epoch 20 batch id 2431 loss 0.09292656928300858 train acc 0.9461898395721925\n",
            "epoch 20 batch id 2441 loss 0.14113084971904755 train acc 0.9461734432609586\n",
            "epoch 20 batch id 2451 loss 0.08053027093410492 train acc 0.9461699306405549\n",
            "epoch 20 batch id 2461 loss 0.2681339383125305 train acc 0.9461156542056075\n",
            "epoch 20 batch id 2471 loss 0.23107916116714478 train acc 0.946055493727236\n",
            "epoch 20 batch id 2481 loss 0.14713720977306366 train acc 0.9461217754937525\n",
            "epoch 20 batch id 2491 loss 0.13693727552890778 train acc 0.94608089120835\n",
            "epoch 20 batch id 2501 loss 0.029331253841519356 train acc 0.9460965613754498\n",
            "epoch 20 batch id 2511 loss 0.05415932834148407 train acc 0.9461245519713262\n",
            "epoch 20 batch id 2521 loss 0.13857828080654144 train acc 0.9460593514478381\n",
            "epoch 20 batch id 2531 loss 0.13605090975761414 train acc 0.9460502271829316\n",
            "epoch 20 batch id 2541 loss 0.1719886064529419 train acc 0.9460842188114915\n",
            "epoch 20 batch id 2551 loss 0.20739804208278656 train acc 0.9460505684045473\n",
            "epoch 20 batch id 2561 loss 0.20563869178295135 train acc 0.9460659898477157\n",
            "epoch 20 batch id 2571 loss 0.17974556982517242 train acc 0.9460812913263321\n",
            "epoch 20 batch id 2581 loss 0.1280873864889145 train acc 0.9460783126695079\n",
            "epoch 20 batch id 2591 loss 0.10805735737085342 train acc 0.9460572655345426\n",
            "epoch 20 batch id 2601 loss 0.23998673260211945 train acc 0.9461024605920799\n",
            "epoch 20 batch id 2611 loss 0.08746309578418732 train acc 0.946057545001915\n",
            "epoch 20 batch id 2621 loss 0.14183944463729858 train acc 0.9460010492178558\n",
            "epoch 20 batch id 2631 loss 0.31834858655929565 train acc 0.9459806157354618\n",
            "epoch 20 batch id 2641 loss 0.1678960770368576 train acc 0.9460254165088982\n",
            "epoch 20 batch id 2651 loss 0.06782911717891693 train acc 0.9460875612976235\n",
            "epoch 20 batch id 2661 loss 0.14229658246040344 train acc 0.9461081360390831\n",
            "epoch 20 batch id 2671 loss 0.09090843796730042 train acc 0.9460993073755148\n",
            "epoch 20 batch id 2681 loss 0.0917772427201271 train acc 0.9460672323759791\n",
            "epoch 20 batch id 2691 loss 0.18921053409576416 train acc 0.9460818468970643\n",
            "epoch 20 batch id 2701 loss 0.17352525889873505 train acc 0.946102138097001\n",
            "epoch 20 batch id 2711 loss 0.06460005044937134 train acc 0.9460761711545556\n",
            "epoch 20 batch id 2721 loss 0.09022586047649384 train acc 0.9461078188166115\n",
            "epoch 20 batch id 2731 loss 0.07403875887393951 train acc 0.9460934639326254\n",
            "epoch 20 batch id 2741 loss 0.1531694382429123 train acc 0.9460963152134257\n",
            "epoch 20 batch id 2751 loss 0.27666473388671875 train acc 0.9460764267539077\n",
            "epoch 20 batch id 2761 loss 0.09435982257127762 train acc 0.9461528884462151\n",
            "epoch 20 batch id 2771 loss 0.05213138088583946 train acc 0.946121661854926\n",
            "epoch 20 batch id 2781 loss 0.24898891150951385 train acc 0.9461637001078749\n",
            "epoch 20 batch id 2791 loss 0.0983356311917305 train acc 0.9461886420637764\n",
            "epoch 20 batch id 2801 loss 0.25512459874153137 train acc 0.946230141021064\n",
            "epoch 20 batch id 2811 loss 0.23487508296966553 train acc 0.9461824083955888\n",
            "epoch 20 batch id 2821 loss 0.1078801080584526 train acc 0.9462014799716413\n",
            "epoch 20 batch id 2831 loss 0.06590026617050171 train acc 0.9462259360649947\n",
            "epoch 20 batch id 2841 loss 0.24415621161460876 train acc 0.9462557198169659\n",
            "epoch 20 batch id 2851 loss 0.1121545359492302 train acc 0.9462359698351456\n",
            "epoch 20 batch id 2861 loss 0.08005574345588684 train acc 0.9462491261796575\n",
            "epoch 20 batch id 2871 loss 0.16680631041526794 train acc 0.9462132096830372\n",
            "epoch 20 batch id 2881 loss 0.12183666974306107 train acc 0.946231777160708\n",
            "epoch 20 batch id 2891 loss 0.18408027291297913 train acc 0.9462123832583881\n",
            "epoch 20 batch id 2901 loss 0.16259829699993134 train acc 0.9462092812823164\n",
            "epoch 20 batch id 2911 loss 0.16643361747264862 train acc 0.9461900979045001\n",
            "epoch 20 batch id 2921 loss 0.08104322850704193 train acc 0.9461175539198905\n",
            "epoch 20 batch id 2931 loss 0.16307039558887482 train acc 0.9460988143978164\n",
            "epoch 20 batch id 2941 loss 0.1689731329679489 train acc 0.9460802023121387\n",
            "epoch 20 batch id 2951 loss 0.1956484466791153 train acc 0.9461305489664521\n",
            "epoch 20 batch id 2961 loss 0.12782512605190277 train acc 0.9461172323539345\n",
            "epoch 20 batch id 2971 loss 0.10924691706895828 train acc 0.9460882278694043\n",
            "epoch 20 batch id 2981 loss 0.179594025015831 train acc 0.9461065917477357\n",
            "epoch 20 batch id 2991 loss 0.1914130002260208 train acc 0.9461300568371782\n",
            "epoch 20 batch id 3001 loss 0.08732765167951584 train acc 0.946142952349217\n",
            "epoch 20 batch id 3011 loss 0.23748208582401276 train acc 0.946093490534706\n",
            "epoch 20 batch id 3021 loss 0.10165733844041824 train acc 0.9461167659715326\n",
            "epoch 20 batch id 3031 loss 0.22372445464134216 train acc 0.9461501979544704\n",
            "epoch 20 batch id 3041 loss 0.10887464880943298 train acc 0.9461782719500165\n",
            "epoch 20 batch id 3051 loss 0.04679624363780022 train acc 0.9462061619141265\n",
            "epoch 20 batch id 3061 loss 0.17495056986808777 train acc 0.9461828242404443\n",
            "epoch 20 batch id 3071 loss 0.15003816783428192 train acc 0.946225781504396\n",
            "epoch 20 batch id 3081 loss 0.30375880002975464 train acc 0.9462076030509575\n",
            "epoch 20 batch id 3091 loss 0.0899648517370224 train acc 0.9462198722096409\n",
            "epoch 20 batch id 3101 loss 0.11471474170684814 train acc 0.9461715978716543\n",
            "epoch 20 batch id 3111 loss 0.19598360359668732 train acc 0.9461738588878175\n",
            "epoch 20 batch id 3121 loss 0.17407271265983582 train acc 0.9462061438641461\n",
            "epoch 20 batch id 3131 loss 0.2351199984550476 train acc 0.9462132705206004\n",
            "epoch 20 batch id 3141 loss 0.2858072817325592 train acc 0.9462253263291945\n",
            "epoch 20 batch id 3151 loss 0.17476050555706024 train acc 0.9462422643605205\n",
            "epoch 20 batch id 3161 loss 0.20313936471939087 train acc 0.9461800063271116\n",
            "epoch 20 batch id 3171 loss 0.07488487660884857 train acc 0.9461723431094292\n",
            "epoch 20 batch id 3181 loss 0.1045764610171318 train acc 0.9461106963219114\n",
            "epoch 20 batch id 3191 loss 0.03813677653670311 train acc 0.9461473675963648\n",
            "epoch 20 batch id 3201 loss 0.23540186882019043 train acc 0.9460861840049984\n",
            "epoch 20 batch id 3211 loss 0.1891041249036789 train acc 0.9460448458424167\n",
            "epoch 20 batch id 3221 loss 0.23700258135795593 train acc 0.9459843604470661\n",
            "epoch 20 batch id 3231 loss 0.05548267439007759 train acc 0.9460306406685237\n",
            "epoch 20 batch id 3241 loss 0.024572115391492844 train acc 0.9460573511261956\n",
            "epoch 20 batch id 3251 loss 0.11492960155010223 train acc 0.9460742848354352\n",
            "epoch 20 batch id 3261 loss 0.160588800907135 train acc 0.9460911146887457\n",
            "epoch 20 batch id 3271 loss 0.15721552073955536 train acc 0.9460887343320086\n",
            "epoch 20 batch id 3281 loss 0.2940031886100769 train acc 0.9460530326120086\n",
            "epoch 20 batch id 3291 loss 0.24006368219852448 train acc 0.9460602780309936\n",
            "epoch 20 batch id 3301 loss 0.1435384750366211 train acc 0.946067479551651\n",
            "epoch 20 batch id 3311 loss 0.06849325448274612 train acc 0.9461171096345515\n",
            "epoch 20 batch id 3321 loss 0.22704726457595825 train acc 0.9460958672086721\n",
            "epoch 20 batch id 3331 loss 0.1296040117740631 train acc 0.9460888246772741\n",
            "epoch 20 batch id 3341 loss 0.21006807684898376 train acc 0.9460958545345705\n",
            "epoch 20 batch id 3351 loss 0.09999790787696838 train acc 0.9461121680095493\n",
            "epoch 20 batch id 3361 loss 0.19844768941402435 train acc 0.9461376822374293\n",
            "epoch 20 batch id 3371 loss 0.2164846807718277 train acc 0.9461398694749332\n",
            "epoch 20 batch id 3381 loss 0.10225307196378708 train acc 0.9461559080153801\n",
            "epoch 20 batch id 3391 loss 0.24780847132205963 train acc 0.9461626363904453\n",
            "epoch 20 batch id 3401 loss 0.17395752668380737 train acc 0.9461555424875037\n",
            "epoch 20 batch id 3411 loss 0.19170734286308289 train acc 0.9462172017003811\n",
            "epoch 20 batch id 3421 loss 0.08484004437923431 train acc 0.9462328266588717\n",
            "epoch 20 batch id 3431 loss 0.1846040040254593 train acc 0.9462756849315068\n",
            "epoch 20 batch id 3441 loss 0.1707741767168045 train acc 0.9462819674513223\n",
            "epoch 20 batch id 3451 loss 0.17098960280418396 train acc 0.946297268907563\n",
            "epoch 20 batch id 3461 loss 0.0804663822054863 train acc 0.9463305403062698\n",
            "epoch 20 batch id 3471 loss 0.15738141536712646 train acc 0.9463501152405647\n",
            "epoch 20 batch id 3481 loss 0.20424193143844604 train acc 0.9462708273484631\n",
            "epoch 20 batch id 3491 loss 0.05023829638957977 train acc 0.9462636064164995\n",
            "epoch 20 batch id 3501 loss 0.15554170310497284 train acc 0.9462564267352185\n",
            "epoch 20 batch id 3511 loss 0.12556087970733643 train acc 0.9462670891483907\n",
            "epoch 20 batch id 3521 loss 0.16504789888858795 train acc 0.9462821286566316\n",
            "epoch 20 batch id 3531 loss 0.2667132019996643 train acc 0.9462572571509488\n",
            "epoch 20 batch id 3541 loss 0.15710057318210602 train acc 0.9462369387178763\n",
            "epoch 20 batch id 3551 loss 0.0831163227558136 train acc 0.9462167347226134\n",
            "epoch 20 batch id 3561 loss 0.03304893895983696 train acc 0.9462624613872508\n",
            "epoch 20 batch id 3571 loss 0.27017083764076233 train acc 0.946216045925511\n",
            "epoch 20 batch id 3581 loss 0.20041313767433167 train acc 0.9462266126780229\n",
            "epoch 20 batch id 3591 loss 0.16590017080307007 train acc 0.9462327694235589\n",
            "epoch 20 batch id 3601 loss 0.22766053676605225 train acc 0.9462692654818106\n",
            "epoch 20 batch id 3611 loss 0.0645415410399437 train acc 0.9462709429520908\n",
            "epoch 20 batch id 3621 loss 0.14403805136680603 train acc 0.9462682960508146\n",
            "epoch 20 batch id 3631 loss 0.11827012896537781 train acc 0.946248450839989\n",
            "epoch 20 batch id 3641 loss 0.3605520725250244 train acc 0.9462802114803626\n",
            "epoch 20 batch id 3651 loss 0.12311258912086487 train acc 0.9462732812927965\n",
            "epoch 20 batch id 3661 loss 0.15179891884326935 train acc 0.9462877287626331\n",
            "epoch 20 batch id 3671 loss 0.18189947307109833 train acc 0.9463020975211114\n",
            "epoch 20 batch id 3681 loss 0.09550110250711441 train acc 0.9463333672914969\n",
            "epoch 20 batch id 3691 loss 0.1637151688337326 train acc 0.9463221349227852\n",
            "epoch 20 batch id 3701 loss 0.22593146562576294 train acc 0.9462982977573628\n",
            "epoch 20 batch id 3711 loss 0.16753333806991577 train acc 0.9463040622473726\n",
            "epoch 20 batch id 3721 loss 0.10065945237874985 train acc 0.9463307914539102\n",
            "epoch 20 batch id 3731 loss 0.037838295102119446 train acc 0.9463783168051461\n",
            "epoch 20 batch id 3741 loss 0.20283830165863037 train acc 0.9463712910986367\n",
            "epoch 20 batch id 3751 loss 0.14303281903266907 train acc 0.9464017928552386\n",
            "epoch 20 batch id 3761 loss 0.22971011698246002 train acc 0.9464279779314012\n",
            "epoch 20 batch id 3771 loss 0.15390430390834808 train acc 0.9463794417926279\n",
            "epoch 20 batch id 3781 loss 0.3754380941390991 train acc 0.9463683549325576\n",
            "epoch 20 batch id 3791 loss 0.104280985891819 train acc 0.9463861777895014\n",
            "epoch 20 batch id 3801 loss 0.08472936600446701 train acc 0.9463956853459616\n",
            "epoch 20 batch id 3811 loss 0.058748867362737656 train acc 0.9464502427184466\n",
            "epoch 20 batch id 3821 loss 0.18445344269275665 train acc 0.946459532844805\n",
            "epoch 20 batch id 3831 loss 0.10652138292789459 train acc 0.9464565387627252\n",
            "epoch 20 batch id 3841 loss 0.0681212916970253 train acc 0.9464413564175996\n",
            "epoch 20 batch id 3851 loss 0.017377164214849472 train acc 0.9464343676967022\n",
            "epoch 20 batch id 3861 loss 0.10816109925508499 train acc 0.9464436026936027\n",
            "epoch 20 batch id 3871 loss 0.11829975992441177 train acc 0.9464527899767502\n",
            "epoch 20 batch id 3881 loss 0.16718944907188416 train acc 0.9464498518423087\n",
            "epoch 20 batch id 3891 loss 0.1841205656528473 train acc 0.9464388974556669\n",
            "epoch 20 batch id 3901 loss 0.1090051531791687 train acc 0.9464199884644963\n",
            "epoch 20 batch id 3911 loss 0.0806744322180748 train acc 0.9464331373050371\n",
            "epoch 20 batch id 3921 loss 0.14677521586418152 train acc 0.9464342642183117\n",
            "epoch 20 batch id 3931 loss 0.15833605825901031 train acc 0.9464353853981176\n",
            "epoch 20 batch id 3941 loss 0.2220323383808136 train acc 0.9464365008880995\n",
            "epoch 20 batch id 3951 loss 0.10600447654724121 train acc 0.9464297013414326\n",
            "epoch 20 batch id 3961 loss 0.16226790845394135 train acc 0.9464268808381722\n",
            "epoch 20 batch id 3971 loss 0.1433296650648117 train acc 0.946459487534626\n",
            "epoch 20 batch id 3981 loss 0.08105681091547012 train acc 0.9464801557397639\n",
            "epoch 20 batch id 3991 loss 0.1947617530822754 train acc 0.946481145076422\n",
            "epoch 20 batch id 4001 loss 0.1806492805480957 train acc 0.9464782241939516\n",
            "epoch 20 batch id 4011 loss 0.27772432565689087 train acc 0.946424675891299\n",
            "epoch 20 batch id 4021 loss 0.046806592494249344 train acc 0.9464063665754787\n",
            "epoch 20 batch id 4031 loss 0.2717641592025757 train acc 0.9464346626147357\n",
            "epoch 20 batch id 4041 loss 0.20478969812393188 train acc 0.9464473521405593\n",
            "epoch 20 batch id 4051 loss 0.05221721902489662 train acc 0.94647155023451\n",
            "epoch 20 batch id 4061 loss 0.24738946557044983 train acc 0.9464648485594681\n",
            "epoch 20 batch id 4071 loss 0.4284120202064514 train acc 0.9464428273151559\n",
            "epoch 20 batch id 4081 loss 0.13163655996322632 train acc 0.9464821734868905\n",
            "epoch 20 batch id 4091 loss 0.11082769185304642 train acc 0.9464678562698606\n",
            "epoch 20 batch id 4101 loss 0.15736587345600128 train acc 0.9464993294318459\n",
            "epoch 20 batch id 4111 loss 0.11586567014455795 train acc 0.9464622354658234\n",
            "epoch 20 batch id 4121 loss 0.09970363229513168 train acc 0.9464821948556176\n",
            "epoch 20 batch id 4131 loss 0.2619000971317291 train acc 0.9465096223674655\n",
            "epoch 20 batch id 4141 loss 0.08359534293413162 train acc 0.9465482371407873\n",
            "epoch 20 batch id 4151 loss 0.09432833641767502 train acc 0.9465866658636473\n",
            "epoch 20 batch id 4161 loss 0.08507202565670013 train acc 0.9465911139149243\n",
            "epoch 20 batch id 4171 loss 0.11621826887130737 train acc 0.9466180172620474\n",
            "epoch 20 batch id 4181 loss 0.0799332931637764 train acc 0.9465887347524515\n",
            "epoch 20 batch id 4191 loss 0.21526552736759186 train acc 0.9465819613457409\n",
            "epoch 20 batch id 4201 loss 0.22531186044216156 train acc 0.9466012556534159\n",
            "epoch 20 batch id 4211 loss 0.181161031126976 train acc 0.9465981952030397\n",
            "epoch 20 batch id 4221 loss 0.16020476818084717 train acc 0.9465988509831793\n",
            "epoch 20 batch id 4231 loss 0.13576366007328033 train acc 0.9466031966438194\n",
            "epoch 20 batch id 4241 loss 0.06973281502723694 train acc 0.9466001532657392\n",
            "epoch 20 batch id 4251 loss 0.21089258790016174 train acc 0.9466081510232887\n",
            "epoch 20 batch id 4261 loss 0.11523750424385071 train acc 0.9466124442619104\n",
            "epoch 20 batch id 4271 loss 0.04106400907039642 train acc 0.946653301334582\n",
            "epoch 20 batch id 4281 loss 0.1412738412618637 train acc 0.9466684185937865\n",
            "epoch 20 batch id 4291 loss 0.03416917845606804 train acc 0.9466871067350268\n",
            "epoch 20 batch id 4301 loss 0.10631629079580307 train acc 0.9466621134619856\n",
            "epoch 20 batch id 4311 loss 0.1600254774093628 train acc 0.9466734806309441\n",
            "epoch 20 batch id 4321 loss 0.05391916632652283 train acc 0.9466884112473964\n",
            "epoch 20 batch id 4331 loss 0.20375560224056244 train acc 0.9467032729161856\n",
            "epoch 20 batch id 4341 loss 0.06792846322059631 train acc 0.94670366850956\n",
            "epoch 20 batch id 4351 loss 0.21823552250862122 train acc 0.9466932888991036\n",
            "epoch 20 batch id 4361 loss 0.08101020008325577 train acc 0.9466972884659481\n",
            "epoch 20 batch id 4371 loss 0.15568596124649048 train acc 0.9466905456417296\n",
            "epoch 20 batch id 4381 loss 0.228643998503685 train acc 0.9466909666742753\n",
            "epoch 20 batch id 4391 loss 0.08624105155467987 train acc 0.9466949442040538\n",
            "epoch 20 batch id 4401 loss 0.11873186379671097 train acc 0.9466740513519655\n",
            "epoch 20 batch id 4411 loss 0.028577320277690887 train acc 0.9466886760371798\n",
            "epoch 20 batch id 4421 loss 0.2294447273015976 train acc 0.9466997002940511\n",
            "epoch 20 batch id 4431 loss 0.04190143570303917 train acc 0.9467177273753103\n",
            "epoch 20 batch id 4441 loss 0.06646232306957245 train acc 0.946749746678676\n",
            "epoch 20 batch id 4451 loss 0.1893848031759262 train acc 0.946732475848124\n",
            "epoch 20 batch id 4461 loss 0.09854942560195923 train acc 0.946771323694239\n",
            "epoch 20 batch id 4471 loss 0.2474524825811386 train acc 0.946750587116976\n",
            "epoch 20 batch id 4481 loss 0.12020112574100494 train acc 0.9467473778174514\n",
            "epoch 20 batch id 4491 loss 0.20846037566661835 train acc 0.9467546203518148\n",
            "epoch 20 batch id 4501 loss 0.18961411714553833 train acc 0.9467514163519218\n",
            "epoch 20 batch id 4511 loss 0.17500771582126617 train acc 0.946727444025715\n",
            "epoch 20 batch id 4521 loss 0.17868484556674957 train acc 0.9467346825923468\n",
            "epoch 20 batch id 4531 loss 0.08794878423213959 train acc 0.9467177499448245\n",
            "epoch 20 batch id 4541 loss 0.11930648982524872 train acc 0.9467249779784188\n",
            "epoch 20 batch id 4551 loss 0.20913904905319214 train acc 0.9467596407382993\n",
            "epoch 20 batch id 4561 loss 0.17844174802303314 train acc 0.9467427647445735\n",
            "epoch 20 batch id 4571 loss 0.07519543170928955 train acc 0.9467704003500328\n",
            "epoch 20 batch id 4581 loss 0.11694207787513733 train acc 0.946777450338354\n",
            "epoch 20 batch id 4591 loss 0.15152089297771454 train acc 0.946784469614463\n",
            "epoch 20 batch id 4601 loss 0.09090609103441238 train acc 0.9467846663768746\n",
            "epoch 20 batch id 4611 loss 0.22872813045978546 train acc 0.9468018054651919\n",
            "epoch 20 batch id 4621 loss 0.09452946484088898 train acc 0.9468256329798744\n",
            "epoch 20 batch id 4631 loss 0.16402198374271393 train acc 0.9468358615849708\n",
            "epoch 20 batch id 4641 loss 0.19147564470767975 train acc 0.9468426793794441\n",
            "epoch 20 batch id 4651 loss 0.21603143215179443 train acc 0.9468393893786282\n",
            "epoch 20 batch id 4661 loss 0.0989903062582016 train acc 0.9468294089251234\n",
            "epoch 20 batch id 4671 loss 0.08069471269845963 train acc 0.9468562673945622\n",
            "epoch 20 batch id 4681 loss 0.2899908125400543 train acc 0.9468629833368938\n",
            "epoch 20 batch id 4691 loss 0.08133780211210251 train acc 0.9468530164144106\n",
            "epoch 20 batch id 4701 loss 0.14095938205718994 train acc 0.9468829770261646\n",
            "epoch 20 batch id 4711 loss 0.11545169353485107 train acc 0.9468895935045638\n",
            "epoch 20 batch id 4721 loss 0.0761328712105751 train acc 0.9468928722728236\n",
            "epoch 20 batch id 4731 loss 0.07002013921737671 train acc 0.946899439864722\n",
            "epoch 20 batch id 4741 loss 0.16656678915023804 train acc 0.946919162623919\n",
            "epoch 20 batch id 4751 loss 0.17574690282344818 train acc 0.946915780888234\n",
            "epoch 20 batch id 4761 loss 0.1524847447872162 train acc 0.9469386683469859\n",
            "epoch 20 batch id 4771 loss 0.0860460177063942 train acc 0.9469483598826242\n",
            "epoch 20 batch id 4781 loss 0.4145372211933136 train acc 0.9469155249947709\n",
            "epoch 20 batch id 4791 loss 0.24149425327777863 train acc 0.9468763045293258\n",
            "epoch 20 batch id 4801 loss 0.22700221836566925 train acc 0.9468437565090606\n",
            "epoch 20 batch id 4811 loss 0.11887870728969574 train acc 0.946814591561006\n",
            "epoch 20 batch id 4821 loss 0.30670472979545593 train acc 0.946811475834889\n",
            "epoch 20 batch id 4831 loss 0.2463211864233017 train acc 0.9468213102877251\n",
            "epoch 20 batch id 4841 loss 0.22089189291000366 train acc 0.9468020553604627\n",
            "epoch 20 batch id 4851 loss 0.09602807462215424 train acc 0.9468150896722325\n",
            "epoch 20 batch id 4861 loss 0.14132721722126007 train acc 0.9468216416375231\n",
            "epoch 20 batch id 4871 loss 0.1086403876543045 train acc 0.9468506210223774\n",
            "epoch 20 batch id 4881 loss 0.09608591347932816 train acc 0.9468570733456259\n",
            "epoch 20 batch id 4891 loss 0.1639561653137207 train acc 0.9468475260682887\n",
            "epoch 20 batch id 4901 loss 0.06568518280982971 train acc 0.946863522750459\n",
            "epoch 20 batch id 4911 loss 0.1892269253730774 train acc 0.9468317297902668\n",
            "epoch 20 batch id 4921 loss 0.26938843727111816 train acc 0.9468191170493802\n",
            "epoch 20 batch id 4931 loss 0.27471914887428284 train acc 0.9467843743662543\n",
            "epoch 20 batch id 4941 loss 0.1569080352783203 train acc 0.9467782331511839\n",
            "epoch 20 batch id 4951 loss 0.326808363199234 train acc 0.9467878963845687\n",
            "epoch 20 train acc 0.9468018710913859\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3136510bd0c43639dcb853ba2a1bb20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20 loss 0.08803270012140274 test acc 0.8261466733870968\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd2d377cfb8346f682916bae35d3e546",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 21 batch id 1 loss 0.0584617555141449 train acc 0.984375\n",
            "epoch 21 batch id 11 loss 0.1720483899116516 train acc 0.9630681818181818\n",
            "epoch 21 batch id 21 loss 0.15003469586372375 train acc 0.9568452380952381\n",
            "epoch 21 batch id 31 loss 0.24466300010681152 train acc 0.9501008064516129\n",
            "epoch 21 batch id 41 loss 0.14526161551475525 train acc 0.9527439024390244\n",
            "epoch 21 batch id 51 loss 0.06680367141962051 train acc 0.9528186274509803\n",
            "epoch 21 batch id 61 loss 0.1476173996925354 train acc 0.9505635245901639\n",
            "epoch 21 batch id 71 loss 0.23190701007843018 train acc 0.9496038732394366\n",
            "epoch 21 batch id 81 loss 0.2974882125854492 train acc 0.9498456790123457\n",
            "epoch 21 batch id 91 loss 0.11064986884593964 train acc 0.9505494505494505\n",
            "epoch 21 batch id 101 loss 0.15555043518543243 train acc 0.9486386138613861\n",
            "epoch 21 batch id 111 loss 0.08361069113016129 train acc 0.9487612612612613\n",
            "epoch 21 batch id 121 loss 0.10800182819366455 train acc 0.9497675619834711\n",
            "epoch 21 batch id 131 loss 0.1632557064294815 train acc 0.9489503816793893\n",
            "epoch 21 batch id 141 loss 0.0635669156908989 train acc 0.9488031914893617\n",
            "epoch 21 batch id 151 loss 0.1005953848361969 train acc 0.9487789735099338\n",
            "epoch 21 batch id 161 loss 0.07196252048015594 train acc 0.9493400621118012\n",
            "epoch 21 batch id 171 loss 0.21540594100952148 train acc 0.9483735380116959\n",
            "epoch 21 batch id 181 loss 0.09371060132980347 train acc 0.9486360497237569\n",
            "epoch 21 batch id 191 loss 0.09449992328882217 train acc 0.9488710732984293\n",
            "epoch 21 batch id 201 loss 0.2529621422290802 train acc 0.9488495024875622\n",
            "epoch 21 batch id 211 loss 0.15410949289798737 train acc 0.9485337677725119\n",
            "epoch 21 batch id 221 loss 0.3214668333530426 train acc 0.9472567873303167\n",
            "epoch 21 batch id 231 loss 0.05331775173544884 train acc 0.9475784632034632\n",
            "epoch 21 batch id 241 loss 0.2928580343723297 train acc 0.9469009336099585\n",
            "epoch 21 batch id 251 loss 0.21123480796813965 train acc 0.947335657370518\n",
            "epoch 21 batch id 261 loss 0.22946062684059143 train acc 0.9474377394636015\n",
            "epoch 21 batch id 271 loss 0.04055040329694748 train acc 0.947705258302583\n",
            "epoch 21 batch id 281 loss 0.04508426412940025 train acc 0.947786921708185\n",
            "epoch 21 batch id 291 loss 0.20422209799289703 train acc 0.9477555841924399\n",
            "epoch 21 batch id 301 loss 0.2150498777627945 train acc 0.947985880398671\n",
            "epoch 21 batch id 311 loss 0.23764817416667938 train acc 0.9480506430868167\n",
            "epoch 21 batch id 321 loss 0.30127474665641785 train acc 0.9483060747663551\n",
            "epoch 21 batch id 331 loss 0.1646239161491394 train acc 0.9484988670694864\n",
            "epoch 21 batch id 341 loss 0.3527437150478363 train acc 0.9488178152492669\n",
            "epoch 21 batch id 351 loss 0.18568332493305206 train acc 0.9490295584045584\n",
            "epoch 21 batch id 361 loss 0.0488499291241169 train acc 0.9494459833795014\n",
            "epoch 21 batch id 371 loss 0.2037300020456314 train acc 0.9498820754716981\n",
            "epoch 21 batch id 381 loss 0.06888150423765182 train acc 0.9498031496062992\n",
            "epoch 21 batch id 391 loss 0.2131129503250122 train acc 0.9498481457800512\n",
            "epoch 21 batch id 401 loss 0.008541404269635677 train acc 0.95008572319202\n",
            "epoch 21 batch id 411 loss 0.032651834189891815 train acc 0.9500456204379562\n",
            "epoch 21 batch id 421 loss 0.2226385623216629 train acc 0.9499331947743468\n",
            "epoch 21 batch id 431 loss 0.15568536520004272 train acc 0.9499347447795824\n",
            "epoch 21 batch id 441 loss 0.16737598180770874 train acc 0.9501842403628118\n",
            "epoch 21 batch id 451 loss 0.2804633378982544 train acc 0.950180155210643\n",
            "epoch 21 batch id 461 loss 0.055679649114608765 train acc 0.9500745661605207\n",
            "epoch 21 batch id 471 loss 0.11085233092308044 train acc 0.9503052016985138\n",
            "epoch 21 batch id 481 loss 0.15034602582454681 train acc 0.9505262474012474\n",
            "epoch 21 batch id 491 loss 0.15353435277938843 train acc 0.9504837067209776\n",
            "epoch 21 batch id 501 loss 0.36431780457496643 train acc 0.9502869261477046\n",
            "epoch 21 batch id 511 loss 0.18674114346504211 train acc 0.9502507338551859\n",
            "epoch 21 batch id 521 loss 0.14070898294448853 train acc 0.9502459213051824\n",
            "epoch 21 batch id 531 loss 0.12441806495189667 train acc 0.9501530131826742\n",
            "epoch 21 batch id 541 loss 0.08545093983411789 train acc 0.9500635397412199\n",
            "epoch 21 batch id 551 loss 0.21534191071987152 train acc 0.9500907441016334\n",
            "epoch 21 batch id 561 loss 0.10081897675991058 train acc 0.9500612745098039\n",
            "epoch 21 batch id 571 loss 0.30104076862335205 train acc 0.9500875656742557\n",
            "epoch 21 batch id 581 loss 0.15167440474033356 train acc 0.9501129518072289\n",
            "epoch 21 batch id 591 loss 0.12451944500207901 train acc 0.9502696700507615\n",
            "epoch 21 batch id 601 loss 0.08649048954248428 train acc 0.9504731697171381\n",
            "epoch 21 batch id 611 loss 0.12817582488059998 train acc 0.950439852700491\n",
            "epoch 21 batch id 621 loss 0.2384643405675888 train acc 0.9503321256038647\n",
            "epoch 21 batch id 631 loss 0.19159576296806335 train acc 0.9503516244057052\n",
            "epoch 21 batch id 641 loss 0.08753480017185211 train acc 0.9503948907956318\n",
            "epoch 21 batch id 651 loss 0.06520766019821167 train acc 0.9503168202764977\n",
            "epoch 21 batch id 661 loss 0.07941712439060211 train acc 0.9504538577912254\n",
            "epoch 21 batch id 671 loss 0.118204265832901 train acc 0.9505169523099851\n",
            "epoch 21 batch id 681 loss 0.14221502840518951 train acc 0.9505781938325991\n",
            "epoch 21 batch id 691 loss 0.2516244947910309 train acc 0.950501989869754\n",
            "epoch 21 batch id 701 loss 0.06444432586431503 train acc 0.9503833808844507\n",
            "epoch 21 batch id 711 loss 0.18733666837215424 train acc 0.9503120604781997\n",
            "epoch 21 batch id 721 loss 0.2016746997833252 train acc 0.9502427184466019\n",
            "epoch 21 batch id 731 loss 0.038920655846595764 train acc 0.9503462722298222\n",
            "epoch 21 batch id 741 loss 0.2604147791862488 train acc 0.9502783400809717\n",
            "epoch 21 batch id 751 loss 0.17693325877189636 train acc 0.9501498002663116\n",
            "epoch 21 batch id 761 loss 0.09568990767002106 train acc 0.9503531537450722\n",
            "epoch 21 batch id 771 loss 0.11630448698997498 train acc 0.9502675097276264\n",
            "epoch 21 batch id 781 loss 0.21281053125858307 train acc 0.9501840588988476\n",
            "epoch 21 batch id 791 loss 0.13639643788337708 train acc 0.950240992414665\n",
            "epoch 21 batch id 801 loss 0.11043987423181534 train acc 0.950296504369538\n",
            "epoch 21 batch id 811 loss 0.1295146644115448 train acc 0.9502543156596794\n",
            "epoch 21 batch id 821 loss 0.1154332235455513 train acc 0.9503654080389768\n",
            "epoch 21 batch id 831 loss 0.14206445217132568 train acc 0.9503798134777377\n",
            "epoch 21 batch id 841 loss 0.05365171283483505 train acc 0.950282401902497\n",
            "epoch 21 batch id 851 loss 0.3441295027732849 train acc 0.9501872796709753\n",
            "epoch 21 batch id 861 loss 0.07372674345970154 train acc 0.9502395470383276\n",
            "epoch 21 batch id 871 loss 0.05379139631986618 train acc 0.9502547359357061\n",
            "epoch 21 batch id 881 loss 0.07956098020076752 train acc 0.9501809023836549\n",
            "epoch 21 batch id 891 loss 0.030035780742764473 train acc 0.9503542368125701\n",
            "epoch 21 batch id 901 loss 0.11935194581747055 train acc 0.9503503052164262\n",
            "epoch 21 batch id 911 loss 0.19816389679908752 train acc 0.9504493688254665\n",
            "epoch 21 batch id 921 loss 0.14975610375404358 train acc 0.9502578718783931\n",
            "epoch 21 batch id 931 loss 0.051028333604335785 train acc 0.9501544038668098\n",
            "epoch 21 batch id 941 loss 0.08408927172422409 train acc 0.9502856004250797\n",
            "epoch 21 batch id 951 loss 0.075391486287117 train acc 0.9502004468980021\n",
            "epoch 21 batch id 961 loss 0.11109024286270142 train acc 0.9501821019771072\n",
            "epoch 21 batch id 971 loss 0.13396641612052917 train acc 0.9503250514933058\n",
            "epoch 21 batch id 981 loss 0.07352976500988007 train acc 0.9505925076452599\n",
            "epoch 21 batch id 991 loss 0.15974313020706177 train acc 0.9506180625630676\n",
            "epoch 21 batch id 1001 loss 0.2591080367565155 train acc 0.9506431068931069\n",
            "epoch 21 batch id 1011 loss 0.15139901638031006 train acc 0.9506367457962414\n",
            "epoch 21 batch id 1021 loss 0.25419533252716064 train acc 0.9506917238001958\n",
            "epoch 21 batch id 1031 loss 0.10541047900915146 train acc 0.9505789282250242\n",
            "epoch 21 batch id 1041 loss 0.15985256433486938 train acc 0.9506183957732949\n",
            "epoch 21 batch id 1051 loss 0.1139904335141182 train acc 0.9504787107516651\n",
            "epoch 21 batch id 1061 loss 0.19114340841770172 train acc 0.9504594721960414\n",
            "epoch 21 batch id 1071 loss 0.08947999030351639 train acc 0.950484360410831\n",
            "epoch 21 batch id 1081 loss 0.2526922821998596 train acc 0.9503931544865865\n",
            "epoch 21 batch id 1091 loss 0.50058513879776 train acc 0.9503609074243813\n",
            "epoch 21 batch id 1101 loss 0.0906301736831665 train acc 0.9503434377838329\n",
            "epoch 21 batch id 1111 loss 0.18306629359722137 train acc 0.9502700270027002\n",
            "epoch 21 batch id 1121 loss 0.21749559044837952 train acc 0.9502676181980375\n",
            "epoch 21 batch id 1131 loss 0.12686440348625183 train acc 0.9502652519893899\n",
            "epoch 21 batch id 1141 loss 0.15847325325012207 train acc 0.9503040096406661\n",
            "epoch 21 batch id 1151 loss 0.2231924682855606 train acc 0.9502199174630755\n",
            "epoch 21 batch id 1161 loss 0.05282638967037201 train acc 0.9501911068044789\n",
            "epoch 21 batch id 1171 loss 0.08277341723442078 train acc 0.9501361016225448\n",
            "epoch 21 batch id 1181 loss 0.12355529516935349 train acc 0.9501746401354784\n",
            "epoch 21 batch id 1191 loss 0.09717858582735062 train acc 0.9501862930310663\n",
            "epoch 21 batch id 1201 loss 0.056225117295980453 train acc 0.9504189217318901\n",
            "epoch 21 batch id 1211 loss 0.09571389108896255 train acc 0.9505186829066887\n",
            "epoch 21 batch id 1221 loss 0.09116178005933762 train acc 0.9505528255528255\n",
            "epoch 21 batch id 1231 loss 0.2610049545764923 train acc 0.9504594841592201\n",
            "epoch 21 batch id 1241 loss 0.1857120245695114 train acc 0.9504306003223207\n",
            "epoch 21 batch id 1251 loss 0.13521817326545715 train acc 0.9503896882494005\n",
            "epoch 21 batch id 1261 loss 0.043972648680210114 train acc 0.9504981165741475\n",
            "epoch 21 batch id 1271 loss 0.0851900577545166 train acc 0.9505679583005507\n",
            "epoch 21 batch id 1281 loss 0.04300664737820625 train acc 0.9506733021077284\n",
            "epoch 21 batch id 1291 loss 0.0855681374669075 train acc 0.9508012199845082\n",
            "epoch 21 batch id 1301 loss 0.1012452021241188 train acc 0.950662951575711\n",
            "epoch 21 batch id 1311 loss 0.08710934221744537 train acc 0.9506578947368421\n",
            "epoch 21 batch id 1321 loss 0.02807058021426201 train acc 0.9505582891748675\n",
            "epoch 21 batch id 1331 loss 0.17787079513072968 train acc 0.950565833959429\n",
            "epoch 21 batch id 1341 loss 0.2644793689250946 train acc 0.9504567486950037\n",
            "epoch 21 batch id 1351 loss 0.06416958570480347 train acc 0.9505343264248705\n",
            "epoch 21 batch id 1361 loss 0.2258395552635193 train acc 0.9504729977957385\n",
            "epoch 21 batch id 1371 loss 0.06717145442962646 train acc 0.9503783734500365\n",
            "epoch 21 batch id 1381 loss 0.19241191446781158 train acc 0.9503303765387401\n",
            "epoch 21 batch id 1391 loss 0.08008022606372833 train acc 0.9502493709561467\n",
            "epoch 21 batch id 1401 loss 0.06863771378993988 train acc 0.9502587437544611\n",
            "epoch 21 batch id 1411 loss 0.1042594462633133 train acc 0.950290131112686\n",
            "epoch 21 batch id 1421 loss 0.10678008198738098 train acc 0.9502441062631949\n",
            "epoch 21 batch id 1431 loss 0.13018646836280823 train acc 0.9503188329839273\n",
            "epoch 21 batch id 1441 loss 0.1880437582731247 train acc 0.9501756592643997\n",
            "epoch 21 batch id 1451 loss 0.13589657843112946 train acc 0.9501959855272226\n",
            "epoch 21 batch id 1461 loss 0.06641169637441635 train acc 0.9501625598904859\n",
            "epoch 21 batch id 1471 loss 0.21926574409008026 train acc 0.9502889191026512\n",
            "epoch 21 batch id 1481 loss 0.10671866685152054 train acc 0.9503080688723835\n",
            "epoch 21 batch id 1491 loss 0.11399161070585251 train acc 0.950379359490275\n",
            "epoch 21 batch id 1501 loss 0.19520048797130585 train acc 0.9504288807461693\n",
            "epoch 21 batch id 1511 loss 0.10282885283231735 train acc 0.9504363831899404\n",
            "epoch 21 batch id 1521 loss 0.05419398844242096 train acc 0.9504232412886259\n",
            "epoch 21 batch id 1531 loss 0.17280666530132294 train acc 0.9504000653167864\n",
            "epoch 21 batch id 1541 loss 0.05455294996500015 train acc 0.9503771901362752\n",
            "epoch 21 batch id 1551 loss 0.07497009634971619 train acc 0.9502941650548034\n",
            "epoch 21 batch id 1561 loss 0.13212640583515167 train acc 0.950272261370916\n",
            "epoch 21 batch id 1571 loss 0.06107674911618233 train acc 0.9503003660089115\n",
            "epoch 21 batch id 1581 loss 0.14150486886501312 train acc 0.9501601043643264\n",
            "epoch 21 batch id 1591 loss 0.23889558017253876 train acc 0.950168918918919\n",
            "epoch 21 batch id 1601 loss 0.3734467923641205 train acc 0.950138585259213\n",
            "epoch 21 batch id 1611 loss 0.13369467854499817 train acc 0.9500989292364991\n",
            "epoch 21 batch id 1621 loss 0.08294661343097687 train acc 0.9500886798272671\n",
            "epoch 21 batch id 1631 loss 0.09373117238283157 train acc 0.9501168761496015\n",
            "epoch 21 batch id 1641 loss 0.16062283515930176 train acc 0.9501256855575868\n",
            "epoch 21 batch id 1651 loss 0.09355040639638901 train acc 0.9501533161720169\n",
            "epoch 21 batch id 1661 loss 0.13146431744098663 train acc 0.9501712071041541\n",
            "epoch 21 batch id 1671 loss 0.1286102682352066 train acc 0.9501608318372232\n",
            "epoch 21 batch id 1681 loss 0.14633966982364655 train acc 0.9501041046995836\n",
            "epoch 21 batch id 1691 loss 0.11258667707443237 train acc 0.9501496895328209\n",
            "epoch 21 batch id 1701 loss 0.07086175680160522 train acc 0.9502039241622575\n",
            "epoch 21 batch id 1711 loss 0.0975969135761261 train acc 0.950230128579778\n",
            "epoch 21 batch id 1721 loss 0.13370150327682495 train acc 0.9502469494479954\n",
            "epoch 21 batch id 1731 loss 0.012656102888286114 train acc 0.9502906556903524\n",
            "epoch 21 batch id 1741 loss 0.21502654254436493 train acc 0.9502979609419874\n",
            "epoch 21 batch id 1751 loss 0.07377517223358154 train acc 0.9503051827527127\n",
            "epoch 21 batch id 1761 loss 0.07269502431154251 train acc 0.9503300681431005\n",
            "epoch 21 batch id 1771 loss 0.07667835801839828 train acc 0.9503370271033315\n",
            "epoch 21 batch id 1781 loss 0.018222764134407043 train acc 0.950414093206064\n",
            "epoch 21 batch id 1791 loss 0.23536266386508942 train acc 0.9504205053042992\n",
            "epoch 21 batch id 1801 loss 0.06688801944255829 train acc 0.950461549139367\n",
            "epoch 21 batch id 1811 loss 0.057984087616205215 train acc 0.9505107675317505\n",
            "epoch 21 batch id 1821 loss 0.12139797955751419 train acc 0.9505337040087863\n",
            "epoch 21 batch id 1831 loss 0.16774632036685944 train acc 0.9505051884216276\n",
            "epoch 21 batch id 1841 loss 0.017822347581386566 train acc 0.9505109315589354\n",
            "epoch 21 batch id 1851 loss 0.15772560238838196 train acc 0.9504828471096705\n",
            "epoch 21 batch id 1861 loss 0.1309625506401062 train acc 0.9504634605051048\n",
            "epoch 21 batch id 1871 loss 0.1645955741405487 train acc 0.950519441475147\n",
            "epoch 21 batch id 1881 loss 0.2243887186050415 train acc 0.9505166799574695\n",
            "epoch 21 batch id 1891 loss 0.12178418785333633 train acc 0.9505387361184559\n",
            "epoch 21 batch id 1901 loss 0.12677207589149475 train acc 0.9505769989479221\n",
            "epoch 21 batch id 1911 loss 0.144637331366539 train acc 0.9505167451596023\n",
            "epoch 21 batch id 1921 loss 0.05995414778590202 train acc 0.950481520041645\n",
            "epoch 21 batch id 1931 loss 0.24119558930397034 train acc 0.9504223847747281\n",
            "epoch 21 batch id 1941 loss 0.048663072288036346 train acc 0.9504041087068521\n",
            "epoch 21 batch id 1951 loss 0.18624842166900635 train acc 0.950482124551512\n",
            "epoch 21 batch id 1961 loss 0.05448027700185776 train acc 0.9504796659867415\n",
            "epoch 21 batch id 1971 loss 0.05971493571996689 train acc 0.9505247970573313\n",
            "epoch 21 batch id 1981 loss 0.06752172857522964 train acc 0.950530035335689\n",
            "epoch 21 batch id 1991 loss 0.2841576933860779 train acc 0.9505116775489704\n",
            "epoch 21 batch id 2001 loss 0.11931592226028442 train acc 0.950555972013993\n",
            "epoch 21 batch id 2011 loss 0.19671498239040375 train acc 0.95050658876181\n",
            "epoch 21 batch id 2021 loss 0.30511438846588135 train acc 0.950473156853043\n",
            "epoch 21 batch id 2031 loss 0.01814182661473751 train acc 0.9504785204332841\n",
            "epoch 21 batch id 2041 loss 0.047277022153139114 train acc 0.9504455536501715\n",
            "epoch 21 batch id 2051 loss 0.29141923785209656 train acc 0.950450999512433\n",
            "epoch 21 batch id 2061 loss 0.028477409854531288 train acc 0.9504791363415818\n",
            "epoch 21 batch id 2071 loss 0.17624656856060028 train acc 0.9504768227909223\n",
            "epoch 21 batch id 2081 loss 0.1666853129863739 train acc 0.9504745314752523\n",
            "epoch 21 batch id 2091 loss 0.11031109094619751 train acc 0.9503900645624104\n",
            "epoch 21 batch id 2101 loss 0.04458147659897804 train acc 0.95043282960495\n",
            "epoch 21 batch id 2111 loss 0.08682914078235626 train acc 0.9503715656087163\n",
            "epoch 21 batch id 2121 loss 0.11355040222406387 train acc 0.9503477133427628\n",
            "epoch 21 batch id 2131 loss 0.2052168846130371 train acc 0.9503607461285781\n",
            "epoch 21 batch id 2141 loss 0.10624934732913971 train acc 0.9504247431106959\n",
            "epoch 21 batch id 2151 loss 0.12267647683620453 train acc 0.9504590887959089\n",
            "epoch 21 batch id 2161 loss 0.22669199109077454 train acc 0.950456964368348\n",
            "epoch 21 batch id 2171 loss 0.0829889178276062 train acc 0.9504692538000922\n",
            "epoch 21 batch id 2181 loss 0.10516324639320374 train acc 0.9504742663915635\n",
            "epoch 21 batch id 2191 loss 0.10472438484430313 train acc 0.9505148904609767\n",
            "epoch 21 batch id 2201 loss 0.0923129990696907 train acc 0.9504557587460245\n",
            "epoch 21 batch id 2211 loss 0.10392932593822479 train acc 0.9504678312980552\n",
            "epoch 21 batch id 2221 loss 0.168618306517601 train acc 0.9504235141828006\n",
            "epoch 21 batch id 2231 loss 0.09334465116262436 train acc 0.9503795943523083\n",
            "epoch 21 batch id 2241 loss 0.16372019052505493 train acc 0.9503918451584115\n",
            "epoch 21 batch id 2251 loss 0.12479386478662491 train acc 0.9503484562416704\n",
            "epoch 21 batch id 2261 loss 0.030578594654798508 train acc 0.9502985404688191\n",
            "epoch 21 batch id 2271 loss 0.11154238879680634 train acc 0.9503797886393659\n",
            "epoch 21 batch id 2281 loss 0.12140676379203796 train acc 0.9503644234984656\n",
            "epoch 21 batch id 2291 loss 0.11189326643943787 train acc 0.9504310344827587\n",
            "epoch 21 batch id 2301 loss 0.10317783057689667 train acc 0.9504902759669709\n",
            "epoch 21 batch id 2311 loss 0.22689513862133026 train acc 0.9504813933362181\n",
            "epoch 21 batch id 2321 loss 0.11818534135818481 train acc 0.9504995152951314\n",
            "epoch 21 batch id 2331 loss 0.20084838569164276 train acc 0.9504437473187474\n",
            "epoch 21 batch id 2341 loss 0.32870906591415405 train acc 0.9504685497650577\n",
            "epoch 21 batch id 2351 loss 0.07917418330907822 train acc 0.9503934495959167\n",
            "epoch 21 batch id 2361 loss 0.14570850133895874 train acc 0.9504513447691656\n",
            "epoch 21 batch id 2371 loss 0.0420408733189106 train acc 0.9505417018135808\n",
            "epoch 21 batch id 2381 loss 0.0730220302939415 train acc 0.9505853632927341\n",
            "epoch 21 batch id 2391 loss 0.09496255218982697 train acc 0.9505698452530322\n",
            "epoch 21 batch id 2401 loss 0.11464681476354599 train acc 0.9505869950020824\n",
            "epoch 21 batch id 2411 loss 0.15396642684936523 train acc 0.9506040024885939\n",
            "epoch 21 batch id 2421 loss 0.15340647101402283 train acc 0.9506660470879802\n",
            "epoch 21 batch id 2431 loss 0.09078395366668701 train acc 0.9506697346770876\n",
            "epoch 21 batch id 2441 loss 0.11270289868116379 train acc 0.9506477877918885\n",
            "epoch 21 batch id 2451 loss 0.04635082557797432 train acc 0.9506451448388413\n",
            "epoch 21 batch id 2461 loss 0.33600008487701416 train acc 0.9506679195449005\n",
            "epoch 21 batch id 2471 loss 0.23255789279937744 train acc 0.9506335997571833\n",
            "epoch 21 batch id 2481 loss 0.1459948569536209 train acc 0.9506814288593309\n",
            "epoch 21 batch id 2491 loss 0.2215808480978012 train acc 0.9506724207145725\n",
            "epoch 21 batch id 2501 loss 0.045415982604026794 train acc 0.9506634846061576\n",
            "epoch 21 batch id 2511 loss 0.10568036884069443 train acc 0.9506608422939068\n",
            "epoch 21 batch id 2521 loss 0.19251461327075958 train acc 0.9506024395081317\n",
            "epoch 21 batch id 2531 loss 0.043674688786268234 train acc 0.950593885815883\n",
            "epoch 21 batch id 2541 loss 0.2616792321205139 train acc 0.9505608028335301\n",
            "epoch 21 batch id 2551 loss 0.14524415135383606 train acc 0.9505708545668365\n",
            "epoch 21 batch id 2561 loss 0.23108915984630585 train acc 0.9505259176103085\n",
            "epoch 21 batch id 2571 loss 0.2675662636756897 train acc 0.9505299494360171\n",
            "epoch 21 batch id 2581 loss 0.13688653707504272 train acc 0.950558165439752\n",
            "epoch 21 batch id 2591 loss 0.06917096674442291 train acc 0.9505499807024315\n",
            "epoch 21 batch id 2601 loss 0.2437453716993332 train acc 0.9505959246443676\n",
            "epoch 21 batch id 2611 loss 0.17602437734603882 train acc 0.9505996265798544\n",
            "epoch 21 batch id 2621 loss 0.14614251255989075 train acc 0.9505973388019839\n",
            "epoch 21 batch id 2631 loss 0.1580447107553482 train acc 0.9505416191562144\n",
            "epoch 21 batch id 2641 loss 0.16473230719566345 train acc 0.9505395683453237\n",
            "epoch 21 batch id 2651 loss 0.05225229263305664 train acc 0.9505846850245191\n",
            "epoch 21 batch id 2661 loss 0.06709340214729309 train acc 0.9506177189026682\n",
            "epoch 21 batch id 2671 loss 0.13856948912143707 train acc 0.9506388056907525\n",
            "epoch 21 batch id 2681 loss 0.12615349888801575 train acc 0.9505781424841477\n",
            "epoch 21 batch id 2691 loss 0.16102387011051178 train acc 0.9505353493125233\n",
            "epoch 21 batch id 2701 loss 0.3321884870529175 train acc 0.950556506849315\n",
            "epoch 21 batch id 2711 loss 0.016103973612189293 train acc 0.9504968185171524\n",
            "epoch 21 batch id 2721 loss 0.033474989235401154 train acc 0.9504720231532525\n",
            "epoch 21 batch id 2731 loss 0.11148626357316971 train acc 0.9504817374588063\n",
            "epoch 21 batch id 2741 loss 0.13921710848808289 train acc 0.9504799799343305\n",
            "epoch 21 batch id 2751 loss 0.14706304669380188 train acc 0.9505066339512904\n",
            "epoch 21 batch id 2761 loss 0.06610796600580215 train acc 0.9506010050706266\n",
            "epoch 21 batch id 2771 loss 0.07092548906803131 train acc 0.9505819198845182\n",
            "epoch 21 batch id 2781 loss 0.19424724578857422 train acc 0.9506528676734988\n",
            "epoch 21 batch id 2791 loss 0.05865037068724632 train acc 0.9506729218917951\n",
            "epoch 21 batch id 2801 loss 0.506184458732605 train acc 0.9506537843627276\n",
            "epoch 21 batch id 2811 loss 0.10899883508682251 train acc 0.950695926716471\n",
            "epoch 21 batch id 2821 loss 0.09978492558002472 train acc 0.9507156150301311\n",
            "epoch 21 batch id 2831 loss 0.06046917289495468 train acc 0.9507737990109502\n",
            "epoch 21 batch id 2841 loss 0.22281019389629364 train acc 0.950809574093629\n",
            "epoch 21 batch id 2851 loss 0.04015909135341644 train acc 0.9508176955454226\n",
            "epoch 21 batch id 2861 loss 0.06773574650287628 train acc 0.950825760223698\n",
            "epoch 21 batch id 2871 loss 0.17551647126674652 train acc 0.9508446534308603\n",
            "epoch 21 batch id 2881 loss 0.14747782051563263 train acc 0.950836298160361\n",
            "epoch 21 batch id 2891 loss 0.06720472872257233 train acc 0.9508117865790384\n",
            "epoch 21 batch id 2901 loss 0.11979757249355316 train acc 0.9508089882799035\n",
            "epoch 21 batch id 2911 loss 0.0709717869758606 train acc 0.950816944349021\n",
            "epoch 21 batch id 2921 loss 0.00799423549324274 train acc 0.9508034491612462\n",
            "epoch 21 batch id 2931 loss 0.01938531920313835 train acc 0.950832693619925\n",
            "epoch 21 batch id 2941 loss 0.10707900673151016 train acc 0.9508032981978919\n",
            "epoch 21 batch id 2951 loss 0.1303614228963852 train acc 0.9508852931209759\n",
            "epoch 21 batch id 2961 loss 0.14223147928714752 train acc 0.9508928571428571\n",
            "epoch 21 batch id 2971 loss 0.21837599575519562 train acc 0.9508582968697409\n",
            "epoch 21 batch id 2981 loss 0.08184384554624557 train acc 0.9508396930560215\n",
            "epoch 21 batch id 2991 loss 0.09888944774866104 train acc 0.9508630056837178\n",
            "epoch 21 batch id 3001 loss 0.07053865492343903 train acc 0.9508965761412862\n",
            "epoch 21 batch id 3011 loss 0.10980983078479767 train acc 0.9508987877781468\n",
            "epoch 21 batch id 3021 loss 0.15494002401828766 train acc 0.9509165011585567\n",
            "epoch 21 batch id 3031 loss 0.18045493960380554 train acc 0.9509083223358628\n",
            "epoch 21 batch id 3041 loss 0.25248071551322937 train acc 0.9508488161788885\n",
            "epoch 21 batch id 3051 loss 0.13298718631267548 train acc 0.9508460340871845\n",
            "epoch 21 batch id 3061 loss 0.14210401475429535 train acc 0.9508024338451486\n",
            "epoch 21 batch id 3071 loss 0.01309964619576931 train acc 0.9508303484207099\n",
            "epoch 21 batch id 3081 loss 0.26026651263237 train acc 0.9508175105485233\n",
            "epoch 21 batch id 3091 loss 0.1299055516719818 train acc 0.9508350857327725\n",
            "epoch 21 batch id 3101 loss 0.18608637154102325 train acc 0.9507870445017736\n",
            "epoch 21 batch id 3111 loss 0.09253304451704025 train acc 0.9508146496303439\n",
            "epoch 21 batch id 3121 loss 0.15243582427501678 train acc 0.9508070330022429\n",
            "epoch 21 batch id 3131 loss 0.08270645886659622 train acc 0.9508343979559246\n",
            "epoch 21 batch id 3141 loss 0.1823718547821045 train acc 0.9508068688315823\n",
            "epoch 21 batch id 3151 loss 0.09757976233959198 train acc 0.9508340606156775\n",
            "epoch 21 batch id 3161 loss 0.09400993585586548 train acc 0.9507770484024043\n",
            "epoch 21 batch id 3171 loss 0.08846402168273926 train acc 0.9507795253863135\n",
            "epoch 21 batch id 3181 loss 0.10044917464256287 train acc 0.9507770748192392\n",
            "epoch 21 batch id 3191 loss 0.09422814846038818 train acc 0.9508187088686932\n",
            "epoch 21 batch id 3201 loss 0.2029823660850525 train acc 0.9508063886285536\n",
            "epoch 21 batch id 3211 loss 0.19477704167366028 train acc 0.9507406181874806\n",
            "epoch 21 batch id 3221 loss 0.10525867342948914 train acc 0.9506995110214219\n",
            "epoch 21 batch id 3231 loss 0.1719360500574112 train acc 0.9506780021665119\n",
            "epoch 21 batch id 3241 loss 0.03321567550301552 train acc 0.9507048364702252\n",
            "epoch 21 batch id 3251 loss 0.14524400234222412 train acc 0.9507363119040295\n",
            "epoch 21 batch id 3261 loss 0.14491422474384308 train acc 0.9506861392210978\n",
            "epoch 21 batch id 3271 loss 0.10578352212905884 train acc 0.9506744879241822\n",
            "epoch 21 batch id 3281 loss 0.19989177584648132 train acc 0.950681956720512\n",
            "epoch 21 batch id 3291 loss 0.07451487332582474 train acc 0.9506798845335764\n",
            "epoch 21 batch id 3301 loss 0.05037979409098625 train acc 0.950677824901545\n",
            "epoch 21 batch id 3311 loss 0.19693249464035034 train acc 0.9507182497734823\n",
            "epoch 21 batch id 3321 loss 0.11291567236185074 train acc 0.9507254968383018\n",
            "epoch 21 batch id 3331 loss 0.08726644515991211 train acc 0.9506576478534975\n",
            "epoch 21 batch id 3341 loss 0.13504524528980255 train acc 0.9506603561807841\n",
            "epoch 21 batch id 3351 loss 0.09397649765014648 train acc 0.9506583855565502\n",
            "epoch 21 batch id 3361 loss 0.20087501406669617 train acc 0.9506285331746503\n",
            "epoch 21 batch id 3371 loss 0.14355282485485077 train acc 0.9506544793829724\n",
            "epoch 21 batch id 3381 loss 0.08968598395586014 train acc 0.9506664078674948\n",
            "epoch 21 batch id 3391 loss 0.21960295736789703 train acc 0.9506966971394869\n",
            "epoch 21 batch id 3401 loss 0.19413933157920837 train acc 0.9506670832108204\n",
            "epoch 21 batch id 3411 loss 0.11814192682504654 train acc 0.9506926121372031\n",
            "epoch 21 batch id 3421 loss 0.09364757686853409 train acc 0.950727126571178\n",
            "epoch 21 batch id 3431 loss 0.237010195851326 train acc 0.9507022369571554\n",
            "epoch 21 batch id 3441 loss 0.04986332729458809 train acc 0.950713818657367\n",
            "epoch 21 batch id 3451 loss 0.1010672003030777 train acc 0.95070722254419\n",
            "epoch 21 batch id 3461 loss 0.02788311243057251 train acc 0.9507458104594048\n",
            "epoch 21 batch id 3471 loss 0.13527743518352509 train acc 0.9507211538461539\n",
            "epoch 21 batch id 3481 loss 0.16428735852241516 train acc 0.9506652183280666\n",
            "epoch 21 batch id 3491 loss 0.0948677808046341 train acc 0.95068569177886\n",
            "epoch 21 batch id 3501 loss 0.23363542556762695 train acc 0.9506926592402171\n",
            "epoch 21 batch id 3511 loss 0.052483201026916504 train acc 0.950690686414127\n",
            "epoch 21 batch id 3521 loss 0.2173791378736496 train acc 0.9506798494745811\n",
            "epoch 21 batch id 3531 loss 0.24268124997615814 train acc 0.9507133248371566\n",
            "epoch 21 batch id 3541 loss 0.1043117567896843 train acc 0.950720135554928\n",
            "epoch 21 batch id 3551 loss 0.0345047190785408 train acc 0.9507269079132639\n",
            "epoch 21 batch id 3561 loss 0.0335627980530262 train acc 0.9507687447346251\n",
            "epoch 21 batch id 3571 loss 0.12091737240552902 train acc 0.9507359633155978\n",
            "epoch 21 batch id 3581 loss 0.2255687266588211 train acc 0.9507426347388998\n",
            "epoch 21 batch id 3591 loss 0.09375403076410294 train acc 0.9507666736285157\n",
            "epoch 21 batch id 3601 loss 0.1681383103132248 train acc 0.9507732227159122\n",
            "epoch 21 batch id 3611 loss 0.043619196861982346 train acc 0.9508056978676267\n",
            "epoch 21 batch id 3621 loss 0.0941852480173111 train acc 0.9507818972659486\n",
            "epoch 21 batch id 3631 loss 0.1688060462474823 train acc 0.9507625309832002\n",
            "epoch 21 batch id 3641 loss 0.10860075056552887 train acc 0.9507904765174403\n",
            "epoch 21 batch id 3651 loss 0.07996159791946411 train acc 0.9507883114215283\n",
            "epoch 21 batch id 3661 loss 0.1312764436006546 train acc 0.9508074979513794\n",
            "epoch 21 batch id 3671 loss 0.08639105409383774 train acc 0.9508350926178153\n",
            "epoch 21 batch id 3681 loss 0.1641838103532791 train acc 0.9508710268948656\n",
            "epoch 21 batch id 3691 loss 0.047363828867673874 train acc 0.9508940666486048\n",
            "epoch 21 batch id 3701 loss 0.11783703416585922 train acc 0.9508874290732234\n",
            "epoch 21 batch id 3711 loss 0.08604617416858673 train acc 0.9508934586364861\n",
            "epoch 21 batch id 3721 loss 0.14762510359287262 train acc 0.9509120532115023\n",
            "epoch 21 batch id 3731 loss 0.033826012164354324 train acc 0.9509389238809971\n",
            "epoch 21 batch id 3741 loss 0.15667004883289337 train acc 0.9509489441325849\n",
            "epoch 21 batch id 3751 loss 0.027848606929183006 train acc 0.9509547454012264\n",
            "epoch 21 batch id 3761 loss 0.15910813212394714 train acc 0.9509729792608349\n",
            "epoch 21 batch id 3771 loss 0.07716143876314163 train acc 0.9509331079289313\n",
            "epoch 21 batch id 3781 loss 0.2732263505458832 train acc 0.9509182425284316\n",
            "epoch 21 batch id 3791 loss 0.11752929538488388 train acc 0.9509116987602215\n",
            "epoch 21 batch id 3801 loss 0.11020556092262268 train acc 0.9509134109444883\n",
            "epoch 21 batch id 3811 loss 0.04836112633347511 train acc 0.9509233140907898\n",
            "epoch 21 batch id 3821 loss 0.20189185440540314 train acc 0.9509208976707668\n",
            "epoch 21 batch id 3831 loss 0.10867603868246078 train acc 0.950898101018011\n",
            "epoch 21 batch id 3841 loss 0.04985145106911659 train acc 0.9508835589690184\n",
            "epoch 21 batch id 3851 loss 0.04705808311700821 train acc 0.9509015515450532\n",
            "epoch 21 batch id 3861 loss 0.04090023413300514 train acc 0.9509356384356384\n",
            "epoch 21 batch id 3871 loss 0.3541804850101471 train acc 0.9509332213898217\n",
            "epoch 21 batch id 3881 loss 0.2979890704154968 train acc 0.9509348428240144\n",
            "epoch 21 batch id 3891 loss 0.14527159929275513 train acc 0.9509525186327422\n",
            "epoch 21 batch id 3901 loss 0.12113091349601746 train acc 0.950942066136888\n",
            "epoch 21 batch id 3911 loss 0.13321024179458618 train acc 0.9509636282280747\n",
            "epoch 21 batch id 3921 loss 0.0331244021654129 train acc 0.9509771104310125\n",
            "epoch 21 batch id 3931 loss 0.20824916660785675 train acc 0.9509825744085475\n",
            "epoch 21 batch id 3941 loss 0.20401446521282196 train acc 0.9509681870083735\n",
            "epoch 21 batch id 3951 loss 0.05856858566403389 train acc 0.9509894646924829\n",
            "epoch 21 batch id 3961 loss 0.28192126750946045 train acc 0.9509790772532188\n",
            "epoch 21 batch id 3971 loss 0.11367813497781754 train acc 0.9510120246789222\n",
            "epoch 21 batch id 3981 loss 0.1262914538383484 train acc 0.9509898580758603\n",
            "epoch 21 batch id 3991 loss 0.20952554047107697 train acc 0.9509873778501629\n",
            "epoch 21 batch id 4001 loss 0.1210905909538269 train acc 0.950988815296176\n",
            "epoch 21 batch id 4011 loss 0.11448105424642563 train acc 0.9509357080528547\n",
            "epoch 21 batch id 4021 loss 0.09097437560558319 train acc 0.9509294951504601\n",
            "epoch 21 batch id 4031 loss 0.057167910039424896 train acc 0.9509388179111883\n",
            "epoch 21 batch id 4041 loss 0.20601174235343933 train acc 0.9509364946795348\n",
            "epoch 21 batch id 4051 loss 0.1085052341222763 train acc 0.9509264687731425\n",
            "epoch 21 batch id 4061 loss 0.10424023866653442 train acc 0.9509588155626693\n",
            "epoch 21 batch id 4071 loss 0.17044246196746826 train acc 0.9509718128224024\n",
            "epoch 21 batch id 4081 loss 0.11467426270246506 train acc 0.9510306910071061\n",
            "epoch 21 batch id 4091 loss 0.15804795920848846 train acc 0.9510205328770471\n",
            "epoch 21 batch id 4101 loss 0.21207916736602783 train acc 0.9510409046574007\n",
            "epoch 21 batch id 4111 loss 0.03675680607557297 train acc 0.9510345718803211\n",
            "epoch 21 batch id 4121 loss 0.14122599363327026 train acc 0.9510320613928658\n",
            "epoch 21 batch id 4131 loss 0.23789508640766144 train acc 0.9510371278140886\n",
            "epoch 21 batch id 4141 loss 0.19221067428588867 train acc 0.951064809224825\n",
            "epoch 21 batch id 4151 loss 0.09154374897480011 train acc 0.9510810648036617\n",
            "epoch 21 batch id 4161 loss 0.08886464685201645 train acc 0.9510634462869503\n",
            "epoch 21 batch id 4171 loss 0.10065113753080368 train acc 0.9510908654998801\n",
            "epoch 21 batch id 4181 loss 0.043579526245594025 train acc 0.951084519253767\n",
            "epoch 21 batch id 4191 loss 0.18183444440364838 train acc 0.9510670186113099\n",
            "epoch 21 batch id 4201 loss 0.1856566071510315 train acc 0.9510681980480837\n",
            "epoch 21 batch id 4211 loss 0.11849864572286606 train acc 0.9510174246022323\n",
            "epoch 21 batch id 4221 loss 0.17337705194950104 train acc 0.9510150142146411\n",
            "epoch 21 batch id 4231 loss 0.11061647534370422 train acc 0.9510310801229024\n",
            "epoch 21 batch id 4241 loss 0.08341953158378601 train acc 0.9510507545390238\n",
            "epoch 21 batch id 4251 loss 0.13397258520126343 train acc 0.9510740119971771\n",
            "epoch 21 batch id 4261 loss 0.2129807323217392 train acc 0.9510788253931002\n",
            "epoch 21 batch id 4271 loss 0.02112203650176525 train acc 0.9510872746429407\n",
            "epoch 21 batch id 4281 loss 0.05291800573468208 train acc 0.9511394825975239\n",
            "epoch 21 batch id 4291 loss 0.05843157693743706 train acc 0.9511623164763459\n",
            "epoch 21 batch id 4301 loss 0.05850357934832573 train acc 0.9511668797953964\n",
            "epoch 21 batch id 4311 loss 0.13674283027648926 train acc 0.9512040419856181\n",
            "epoch 21 batch id 4321 loss 0.07102924585342407 train acc 0.9511723270076371\n",
            "epoch 21 batch id 4331 loss 0.09171265363693237 train acc 0.9511732278919418\n",
            "epoch 21 batch id 4341 loss 0.04681212827563286 train acc 0.9511921216309606\n",
            "epoch 21 batch id 4351 loss 0.1735524833202362 train acc 0.9511570615950357\n",
            "epoch 21 batch id 4361 loss 0.1571750044822693 train acc 0.9511436597110754\n",
            "epoch 21 batch id 4371 loss 0.14651769399642944 train acc 0.9511481926332647\n",
            "epoch 21 batch id 4381 loss 0.21642976999282837 train acc 0.9511455717872632\n",
            "epoch 21 batch id 4391 loss 0.03269219025969505 train acc 0.9511322876337964\n",
            "epoch 21 batch id 4401 loss 0.1197885051369667 train acc 0.9511474664848898\n",
            "epoch 21 batch id 4411 loss 0.06620527803897858 train acc 0.9511484073906143\n",
            "epoch 21 batch id 4421 loss 0.17616602778434753 train acc 0.9511882209907261\n",
            "epoch 21 batch id 4431 loss 0.19477683305740356 train acc 0.9512031708417964\n",
            "epoch 21 batch id 4441 loss 0.11762536317110062 train acc 0.9512145350146364\n",
            "epoch 21 batch id 4451 loss 0.052647948265075684 train acc 0.9512153167827454\n",
            "epoch 21 batch id 4461 loss 0.03441077843308449 train acc 0.9512160950459538\n",
            "epoch 21 batch id 4471 loss 0.1645590364933014 train acc 0.9512098803399687\n",
            "epoch 21 batch id 4481 loss 0.05776551738381386 train acc 0.9512071803168936\n",
            "epoch 21 batch id 4491 loss 0.11945180594921112 train acc 0.9512044923179692\n",
            "epoch 21 batch id 4501 loss 0.02985508181154728 train acc 0.9512261164185737\n",
            "epoch 21 batch id 4511 loss 0.24535539746284485 train acc 0.9512233983595655\n",
            "epoch 21 batch id 4521 loss 0.10608698427677155 train acc 0.951220692324707\n",
            "epoch 21 batch id 4531 loss 0.1063709706068039 train acc 0.951200755903774\n",
            "epoch 21 batch id 4541 loss 0.14207753539085388 train acc 0.9512118751376348\n",
            "epoch 21 batch id 4551 loss 0.2322143018245697 train acc 0.9512195121951219\n",
            "epoch 21 batch id 4561 loss 0.1724247932434082 train acc 0.9511860063582548\n",
            "epoch 21 batch id 4571 loss 0.0725378692150116 train acc 0.9512073397506016\n",
            "epoch 21 batch id 4581 loss 0.0677780881524086 train acc 0.9511978825583933\n",
            "epoch 21 batch id 4591 loss 0.23848730325698853 train acc 0.951185063167066\n",
            "epoch 21 batch id 4601 loss 0.04632459208369255 train acc 0.9512164475114105\n",
            "epoch 21 batch id 4611 loss 0.2439027726650238 train acc 0.9512510843634786\n",
            "epoch 21 batch id 4621 loss 0.07988813519477844 train acc 0.9512652834884224\n",
            "epoch 21 batch id 4631 loss 0.1784716546535492 train acc 0.951259177283524\n",
            "epoch 21 batch id 4641 loss 0.17009931802749634 train acc 0.9512800312432665\n",
            "epoch 21 batch id 4651 loss 0.33363643288612366 train acc 0.9512672006020211\n",
            "epoch 21 batch id 4661 loss 0.13683485984802246 train acc 0.9512946524350998\n",
            "epoch 21 batch id 4671 loss 0.03704923391342163 train acc 0.9513219867266111\n",
            "epoch 21 batch id 4681 loss 0.20606739819049835 train acc 0.9513425283059176\n",
            "epoch 21 batch id 4691 loss 0.11622549593448639 train acc 0.9513629823065445\n",
            "epoch 21 batch id 4701 loss 0.16974565386772156 train acc 0.9513800255264837\n",
            "epoch 21 batch id 4711 loss 0.11542227864265442 train acc 0.9513804128635109\n",
            "epoch 21 batch id 4721 loss 0.11615937203168869 train acc 0.9513609404787121\n",
            "epoch 21 batch id 4731 loss 0.28110963106155396 train acc 0.9513514584654407\n",
            "epoch 21 batch id 4741 loss 0.1511731892824173 train acc 0.9513420164522253\n",
            "epoch 21 batch id 4751 loss 0.20001478493213654 train acc 0.951349058093033\n",
            "epoch 21 batch id 4761 loss 0.10812745988368988 train acc 0.951375761394665\n",
            "epoch 21 batch id 4771 loss 0.13024240732192993 train acc 0.9513925277719556\n",
            "epoch 21 batch id 4781 loss 0.2656700611114502 train acc 0.9513863469985359\n",
            "epoch 21 batch id 4791 loss 0.39173051714897156 train acc 0.9513573627635149\n",
            "epoch 21 batch id 4801 loss 0.12949028611183167 train acc 0.9513708081649657\n",
            "epoch 21 batch id 4811 loss 0.059240084141492844 train acc 0.9513712066098524\n",
            "epoch 21 batch id 4821 loss 0.3818846046924591 train acc 0.9513521572287907\n",
            "epoch 21 batch id 4831 loss 0.07754813879728317 train acc 0.9513655299109915\n",
            "epoch 21 batch id 4841 loss 0.13358628749847412 train acc 0.9513723920677546\n",
            "epoch 21 batch id 4851 loss 0.060670431703329086 train acc 0.9513695629767058\n",
            "epoch 21 batch id 4861 loss 0.07846707105636597 train acc 0.9513860316807241\n",
            "epoch 21 batch id 4871 loss 0.10286762565374374 train acc 0.951421679326627\n",
            "epoch 21 batch id 4881 loss 0.20521214604377747 train acc 0.9514251690227412\n",
            "epoch 21 batch id 4891 loss 0.12385907769203186 train acc 0.9514318390922102\n",
            "epoch 21 batch id 4901 loss 0.03872627392411232 train acc 0.951467175066313\n",
            "epoch 21 batch id 4911 loss 0.31371408700942993 train acc 0.9514673691712482\n",
            "epoch 21 batch id 4921 loss 0.1518581658601761 train acc 0.9514612121520016\n",
            "epoch 21 batch id 4931 loss 0.2159876823425293 train acc 0.951458248833908\n",
            "epoch 21 batch id 4941 loss 0.11535803973674774 train acc 0.9514394859340215\n",
            "epoch 21 batch id 4951 loss 0.12587402760982513 train acc 0.9514523581094728\n",
            "epoch 21 train acc 0.9514795995561832\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dcc6442dfad46ba8cc9c38fc484ea7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 21 loss 0.5632191896438599 test acc 0.8275854563782992\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26574e3ce74144ac9c2181bfed5dc431",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 22 batch id 1 loss 0.048052575439214706 train acc 0.96875\n",
            "epoch 22 batch id 11 loss 0.038524795323610306 train acc 0.9673295454545454\n",
            "epoch 22 batch id 21 loss 0.1284635365009308 train acc 0.9642857142857143\n",
            "epoch 22 batch id 31 loss 0.19718967378139496 train acc 0.9581653225806451\n",
            "epoch 22 batch id 41 loss 0.023940978571772575 train acc 0.959984756097561\n",
            "epoch 22 batch id 51 loss 0.03724321722984314 train acc 0.960171568627451\n",
            "epoch 22 batch id 61 loss 0.17689478397369385 train acc 0.9561987704918032\n",
            "epoch 22 batch id 71 loss 0.21581825613975525 train acc 0.9555457746478874\n",
            "epoch 22 batch id 81 loss 0.22640570998191833 train acc 0.9552469135802469\n",
            "epoch 22 batch id 91 loss 0.1075248271226883 train acc 0.9560439560439561\n",
            "epoch 22 batch id 101 loss 0.12184229493141174 train acc 0.9545173267326733\n",
            "epoch 22 batch id 111 loss 0.10807675868272781 train acc 0.9546734234234234\n",
            "epoch 22 batch id 121 loss 0.15408623218536377 train acc 0.9555785123966942\n",
            "epoch 22 batch id 131 loss 0.13566139340400696 train acc 0.9549141221374046\n",
            "epoch 22 batch id 141 loss 0.04737810045480728 train acc 0.9552304964539007\n",
            "epoch 22 batch id 151 loss 0.10741063952445984 train acc 0.9546771523178808\n",
            "epoch 22 batch id 161 loss 0.14684723317623138 train acc 0.9551630434782609\n",
            "epoch 22 batch id 171 loss 0.17977988719940186 train acc 0.9550438596491229\n",
            "epoch 22 batch id 181 loss 0.07995155453681946 train acc 0.9554558011049724\n",
            "epoch 22 batch id 191 loss 0.15599475800991058 train acc 0.9556609947643979\n",
            "epoch 22 batch id 201 loss 0.36447495222091675 train acc 0.9555348258706468\n",
            "epoch 22 batch id 211 loss 0.10222340375185013 train acc 0.955568720379147\n",
            "epoch 22 batch id 221 loss 0.1872536540031433 train acc 0.9557409502262444\n",
            "epoch 22 batch id 231 loss 0.06948398053646088 train acc 0.9560335497835498\n",
            "epoch 22 batch id 241 loss 0.21891586482524872 train acc 0.9556535269709544\n",
            "epoch 22 batch id 251 loss 0.20062214136123657 train acc 0.9559885458167331\n",
            "epoch 22 batch id 261 loss 0.17596028745174408 train acc 0.9560584291187739\n",
            "epoch 22 batch id 271 loss 0.037574511021375656 train acc 0.9567573800738007\n",
            "epoch 22 batch id 281 loss 0.027482951059937477 train acc 0.9567949288256228\n",
            "epoch 22 batch id 291 loss 0.13886281847953796 train acc 0.9567762027491409\n",
            "epoch 22 batch id 301 loss 0.11794719099998474 train acc 0.9570701827242525\n",
            "epoch 22 batch id 311 loss 0.16429156064987183 train acc 0.9569935691318328\n",
            "epoch 22 batch id 321 loss 0.1970098316669464 train acc 0.9566783489096573\n",
            "epoch 22 batch id 331 loss 0.08025786280632019 train acc 0.9568542296072508\n",
            "epoch 22 batch id 341 loss 0.3417316973209381 train acc 0.9570197947214076\n",
            "epoch 22 batch id 351 loss 0.11725430935621262 train acc 0.9568198005698005\n",
            "epoch 22 batch id 361 loss 0.08273852616548538 train acc 0.956847299168975\n",
            "epoch 22 batch id 371 loss 0.23165860772132874 train acc 0.9566627358490566\n",
            "epoch 22 batch id 381 loss 0.04446474835276604 train acc 0.9570209973753281\n",
            "epoch 22 batch id 391 loss 0.24760033190250397 train acc 0.9570012787723785\n",
            "epoch 22 batch id 401 loss 0.00286433775909245 train acc 0.956943578553616\n",
            "epoch 22 batch id 411 loss 0.03684891760349274 train acc 0.9568886861313869\n",
            "epoch 22 batch id 421 loss 0.22841067612171173 train acc 0.9567250593824228\n",
            "epoch 22 batch id 431 loss 0.11628738790750504 train acc 0.9566052784222738\n",
            "epoch 22 batch id 441 loss 0.13586923480033875 train acc 0.9567389455782312\n",
            "epoch 22 batch id 451 loss 0.13431885838508606 train acc 0.9566241685144125\n",
            "epoch 22 batch id 461 loss 0.058538611978292465 train acc 0.9565482646420824\n",
            "epoch 22 batch id 471 loss 0.06469853222370148 train acc 0.9567741507430998\n",
            "epoch 22 batch id 481 loss 0.08471227437257767 train acc 0.9567307692307693\n",
            "epoch 22 batch id 491 loss 0.059746723622083664 train acc 0.9566573319755601\n",
            "epoch 22 batch id 501 loss 0.29988497495651245 train acc 0.9565556387225549\n",
            "epoch 22 batch id 511 loss 0.1540612280368805 train acc 0.9564273483365949\n",
            "epoch 22 batch id 521 loss 0.14827321469783783 train acc 0.956753838771593\n",
            "epoch 22 batch id 531 loss 0.16746816039085388 train acc 0.9565383709981168\n",
            "epoch 22 batch id 541 loss 0.03924354538321495 train acc 0.9565330406654344\n",
            "epoch 22 batch id 551 loss 0.2471846044063568 train acc 0.956641333938294\n",
            "epoch 22 batch id 561 loss 0.1473001092672348 train acc 0.9563279857397504\n",
            "epoch 22 batch id 571 loss 0.2696194052696228 train acc 0.9564634413309983\n",
            "epoch 22 batch id 581 loss 0.20772571861743927 train acc 0.9564059810671256\n",
            "epoch 22 batch id 591 loss 0.05772984400391579 train acc 0.9564562182741116\n",
            "epoch 22 batch id 601 loss 0.04589630290865898 train acc 0.956634775374376\n",
            "epoch 22 batch id 611 loss 0.12855321168899536 train acc 0.9565006137479541\n",
            "epoch 22 batch id 621 loss 0.31768059730529785 train acc 0.9565217391304348\n",
            "epoch 22 batch id 631 loss 0.16451172530651093 train acc 0.9563936212361331\n",
            "epoch 22 batch id 641 loss 0.17468833923339844 train acc 0.9563670046801872\n",
            "epoch 22 batch id 651 loss 0.04759443551301956 train acc 0.9564132104454686\n",
            "epoch 22 batch id 661 loss 0.16391968727111816 train acc 0.9563634644478064\n",
            "epoch 22 batch id 671 loss 0.086870938539505 train acc 0.9563152011922503\n",
            "epoch 22 batch id 681 loss 0.19169636070728302 train acc 0.9562683553597651\n",
            "epoch 22 batch id 691 loss 0.20423361659049988 train acc 0.9562228654124457\n",
            "epoch 22 batch id 701 loss 0.14167995750904083 train acc 0.9561340941512125\n",
            "epoch 22 batch id 711 loss 0.16191726922988892 train acc 0.9561137482419128\n",
            "epoch 22 batch id 721 loss 0.2359091192483902 train acc 0.9558339112343966\n",
            "epoch 22 batch id 731 loss 0.03568928316235542 train acc 0.9560106019151847\n",
            "epoch 22 batch id 741 loss 0.05110950395464897 train acc 0.9559927462887989\n",
            "epoch 22 batch id 751 loss 0.23049086332321167 train acc 0.9557881158455392\n",
            "epoch 22 batch id 761 loss 0.12697359919548035 train acc 0.9559995072273325\n",
            "epoch 22 batch id 771 loss 0.17624524235725403 train acc 0.9558203631647212\n",
            "epoch 22 batch id 781 loss 0.1061287447810173 train acc 0.9557058258642765\n",
            "epoch 22 batch id 791 loss 0.1915391981601715 train acc 0.9558114728192162\n",
            "epoch 22 batch id 801 loss 0.11243849992752075 train acc 0.9556999063670412\n",
            "epoch 22 batch id 811 loss 0.12174874544143677 train acc 0.9557837546239211\n",
            "epoch 22 batch id 821 loss 0.038759998977184296 train acc 0.9560178136419001\n",
            "epoch 22 batch id 831 loss 0.27605903148651123 train acc 0.9560018050541517\n",
            "epoch 22 batch id 841 loss 0.03204343467950821 train acc 0.9560233353151011\n",
            "epoch 22 batch id 851 loss 0.1535152941942215 train acc 0.9558791128084606\n",
            "epoch 22 batch id 861 loss 0.24049705266952515 train acc 0.955756387921022\n",
            "epoch 22 batch id 871 loss 0.03514409437775612 train acc 0.9556723593570609\n",
            "epoch 22 batch id 881 loss 0.12476605921983719 train acc 0.9554660896708286\n",
            "epoch 22 batch id 891 loss 0.25454196333885193 train acc 0.9555099607182941\n",
            "epoch 22 batch id 901 loss 0.10053854435682297 train acc 0.9554314650388457\n",
            "epoch 22 batch id 911 loss 0.1071491613984108 train acc 0.9554061470911087\n",
            "epoch 22 batch id 921 loss 0.1547289937734604 train acc 0.9553813789359392\n",
            "epoch 22 batch id 931 loss 0.011867858469486237 train acc 0.9553571428571429\n",
            "epoch 22 batch id 941 loss 0.19510364532470703 train acc 0.9555658873538788\n",
            "epoch 22 batch id 951 loss 0.11670541763305664 train acc 0.955556650893796\n",
            "epoch 22 batch id 961 loss 0.09007918834686279 train acc 0.955596383975026\n",
            "epoch 22 batch id 971 loss 0.1616855412721634 train acc 0.9556835736354274\n",
            "epoch 22 batch id 981 loss 0.11928220838308334 train acc 0.9558167686034659\n",
            "epoch 22 batch id 991 loss 0.2637244164943695 train acc 0.9558684409687185\n",
            "epoch 22 batch id 1001 loss 0.133385568857193 train acc 0.9557785964035964\n",
            "epoch 22 batch id 1011 loss 0.1277066469192505 train acc 0.9556905291790306\n",
            "epoch 22 batch id 1021 loss 0.1485845446586609 train acc 0.95575722331048\n",
            "epoch 22 batch id 1031 loss 0.08502359688282013 train acc 0.9557923132880698\n",
            "epoch 22 batch id 1041 loss 0.15103119611740112 train acc 0.9558267291066282\n",
            "epoch 22 batch id 1051 loss 0.0615832544863224 train acc 0.955726688867745\n",
            "epoch 22 batch id 1061 loss 0.14088286459445953 train acc 0.9557168944392083\n",
            "epoch 22 batch id 1071 loss 0.005907343700528145 train acc 0.955765639589169\n",
            "epoch 22 batch id 1081 loss 0.3445057272911072 train acc 0.9555966697502313\n",
            "epoch 22 batch id 1091 loss 0.3147379755973816 train acc 0.9555167277726856\n",
            "epoch 22 batch id 1101 loss 0.16487132012844086 train acc 0.9556085376930064\n",
            "epoch 22 batch id 1111 loss 0.22615937888622284 train acc 0.9556143114311431\n",
            "epoch 22 batch id 1121 loss 0.1745004653930664 train acc 0.9556757359500446\n",
            "epoch 22 batch id 1131 loss 0.21297986805438995 train acc 0.9555979221927497\n",
            "epoch 22 batch id 1141 loss 0.20006665587425232 train acc 0.9556310254163015\n",
            "epoch 22 batch id 1151 loss 0.12892083823680878 train acc 0.9555685273675065\n",
            "epoch 22 batch id 1161 loss 0.09128525853157043 train acc 0.955547480620155\n",
            "epoch 22 batch id 1171 loss 0.15875458717346191 train acc 0.9554600768573869\n",
            "epoch 22 batch id 1181 loss 0.212840273976326 train acc 0.9554667654530059\n",
            "epoch 22 batch id 1191 loss 0.08094621449708939 train acc 0.9554864609571788\n",
            "epoch 22 batch id 1201 loss 0.22452302277088165 train acc 0.9555838884263114\n",
            "epoch 22 batch id 1211 loss 0.020925838500261307 train acc 0.9556926094137077\n",
            "epoch 22 batch id 1221 loss 0.13791146874427795 train acc 0.9557739557739557\n",
            "epoch 22 batch id 1231 loss 0.19122551381587982 train acc 0.9557905158407799\n",
            "epoch 22 batch id 1241 loss 0.12288106977939606 train acc 0.9558319903303787\n",
            "epoch 22 batch id 1251 loss 0.13458804786205292 train acc 0.955847821742606\n",
            "epoch 22 batch id 1261 loss 0.08072816580533981 train acc 0.9558386201427439\n",
            "epoch 22 batch id 1271 loss 0.1448015421628952 train acc 0.9559156176239182\n",
            "epoch 22 batch id 1281 loss 0.14307184517383575 train acc 0.9559792154566745\n",
            "epoch 22 batch id 1291 loss 0.24777507781982422 train acc 0.9560539310611929\n",
            "epoch 22 batch id 1301 loss 0.17714115977287292 train acc 0.9559833781706379\n",
            "epoch 22 batch id 1311 loss 0.14596901834011078 train acc 0.9559258199847445\n",
            "epoch 22 batch id 1321 loss 0.034898772835731506 train acc 0.9558336487509462\n",
            "epoch 22 batch id 1331 loss 0.11027165502309799 train acc 0.9558837340345605\n",
            "epoch 22 batch id 1341 loss 0.28713804483413696 train acc 0.955828206562267\n",
            "epoch 22 batch id 1351 loss 0.12454017996788025 train acc 0.9558428941524797\n",
            "epoch 22 batch id 1361 loss 0.07471531629562378 train acc 0.9557884827332843\n",
            "epoch 22 batch id 1371 loss 0.08763164281845093 train acc 0.9558032458059811\n",
            "epoch 22 batch id 1381 loss 0.2169552445411682 train acc 0.9557046524257784\n",
            "epoch 22 batch id 1391 loss 0.08613310009241104 train acc 0.9556524083393242\n",
            "epoch 22 batch id 1401 loss 0.24458825588226318 train acc 0.9556009100642399\n",
            "epoch 22 batch id 1411 loss 0.04841731861233711 train acc 0.9557273210489015\n",
            "epoch 22 batch id 1421 loss 0.1786462515592575 train acc 0.9557309992962703\n",
            "epoch 22 batch id 1431 loss 0.12594881653785706 train acc 0.9557673829489868\n",
            "epoch 22 batch id 1441 loss 0.16216988861560822 train acc 0.9556731436502429\n",
            "epoch 22 batch id 1451 loss 0.18734848499298096 train acc 0.9556986560992419\n",
            "epoch 22 batch id 1461 loss 0.13439024984836578 train acc 0.9556917351129364\n",
            "epoch 22 batch id 1471 loss 0.21411040425300598 train acc 0.9557592624065262\n",
            "epoch 22 batch id 1481 loss 0.1406993865966797 train acc 0.9557520256583389\n",
            "epoch 22 batch id 1491 loss 0.16983330249786377 train acc 0.9558077632461436\n",
            "epoch 22 batch id 1501 loss 0.2579914331436157 train acc 0.9558419387075283\n",
            "epoch 22 batch id 1511 loss 0.10394759476184845 train acc 0.9558653209794837\n",
            "epoch 22 batch id 1521 loss 0.09299692511558533 train acc 0.9557753944773175\n",
            "epoch 22 batch id 1531 loss 0.0777992531657219 train acc 0.9557784944480732\n",
            "epoch 22 batch id 1541 loss 0.06820398569107056 train acc 0.9558322517845554\n",
            "epoch 22 batch id 1551 loss 0.19208817183971405 train acc 0.9557644261766602\n",
            "epoch 22 batch id 1561 loss 0.07573282718658447 train acc 0.9558476137091608\n",
            "epoch 22 batch id 1571 loss 0.06274672597646713 train acc 0.9558899586250795\n",
            "epoch 22 batch id 1581 loss 0.08321502804756165 train acc 0.9558823529411765\n",
            "epoch 22 batch id 1591 loss 0.3069261610507965 train acc 0.9558355593966059\n",
            "epoch 22 batch id 1601 loss 0.1992267370223999 train acc 0.9558381480324797\n",
            "epoch 22 batch id 1611 loss 0.09927286207675934 train acc 0.9558310055865922\n",
            "epoch 22 batch id 1621 loss 0.06848294287919998 train acc 0.9557564774830352\n",
            "epoch 22 batch id 1631 loss 0.0058339727111160755 train acc 0.9557882434089515\n",
            "epoch 22 batch id 1641 loss 0.16256560385227203 train acc 0.9557910572821451\n",
            "epoch 22 batch id 1651 loss 0.0582343228161335 train acc 0.9557654451847365\n",
            "epoch 22 batch id 1661 loss 0.10160156339406967 train acc 0.9557871763997592\n",
            "epoch 22 batch id 1671 loss 0.08130980283021927 train acc 0.9557618940754039\n",
            "epoch 22 batch id 1681 loss 0.05326009541749954 train acc 0.9556811421772754\n",
            "epoch 22 batch id 1691 loss 0.2561523914337158 train acc 0.9556567859254879\n",
            "epoch 22 batch id 1701 loss 0.22150923311710358 train acc 0.9557245737801293\n",
            "epoch 22 batch id 1711 loss 0.1696735918521881 train acc 0.9557459088252483\n",
            "epoch 22 batch id 1721 loss 0.2175130397081375 train acc 0.9557397588611273\n",
            "epoch 22 batch id 1731 loss 0.007985950447618961 train acc 0.9557246533795494\n",
            "epoch 22 batch id 1741 loss 0.15979856252670288 train acc 0.9557725445146468\n",
            "epoch 22 batch id 1751 loss 0.13819848001003265 train acc 0.9557128069674472\n",
            "epoch 22 batch id 1761 loss 0.10322606563568115 train acc 0.9557779670641681\n",
            "epoch 22 batch id 1771 loss 0.13548977673053741 train acc 0.9558335686053078\n",
            "epoch 22 batch id 1781 loss 0.06197064369916916 train acc 0.9558709994385177\n",
            "epoch 22 batch id 1791 loss 0.28999778628349304 train acc 0.9557597012841987\n",
            "epoch 22 batch id 1801 loss 0.06364146620035172 train acc 0.9558318295391449\n",
            "epoch 22 batch id 1811 loss 0.13564787805080414 train acc 0.9557823716178907\n",
            "epoch 22 batch id 1821 loss 0.15860243141651154 train acc 0.9557077155409116\n",
            "epoch 22 batch id 1831 loss 0.05399845167994499 train acc 0.9556936100491534\n",
            "epoch 22 batch id 1841 loss 0.04651394486427307 train acc 0.9557220939706681\n",
            "epoch 22 batch id 1851 loss 0.08645474910736084 train acc 0.9557418287412209\n",
            "epoch 22 batch id 1861 loss 0.18168847262859344 train acc 0.9558033315421817\n",
            "epoch 22 batch id 1871 loss 0.220513716340065 train acc 0.9558641769107429\n",
            "epoch 22 batch id 1881 loss 0.09211637824773788 train acc 0.955882841573631\n",
            "epoch 22 batch id 1891 loss 0.1215897798538208 train acc 0.9558765203595981\n",
            "epoch 22 batch id 1901 loss 0.1365707516670227 train acc 0.9559031430825881\n",
            "epoch 22 batch id 1911 loss 0.07347472757101059 train acc 0.9558313710099424\n",
            "epoch 22 batch id 1921 loss 0.22243353724479675 train acc 0.9558254164497657\n",
            "epoch 22 batch id 1931 loss 0.05587207153439522 train acc 0.9558195235629208\n",
            "epoch 22 batch id 1941 loss 0.14944174885749817 train acc 0.9558378413189078\n",
            "epoch 22 batch id 1951 loss 0.23117224872112274 train acc 0.9558079190158892\n",
            "epoch 22 batch id 1961 loss 0.10544741898775101 train acc 0.9558261091279959\n",
            "epoch 22 batch id 1971 loss 0.04108083248138428 train acc 0.9558282597666159\n",
            "epoch 22 batch id 1981 loss 0.11095277965068817 train acc 0.9557751766784452\n",
            "epoch 22 batch id 1991 loss 0.23185209929943085 train acc 0.9557854093420391\n",
            "epoch 22 batch id 2001 loss 0.141509011387825 train acc 0.9558580084957521\n",
            "epoch 22 batch id 2011 loss 0.12261445820331573 train acc 0.9558754972650423\n",
            "epoch 22 batch id 2021 loss 0.10962802916765213 train acc 0.9559082756061356\n",
            "epoch 22 batch id 2031 loss 0.0989246591925621 train acc 0.9558561053668144\n",
            "epoch 22 batch id 2041 loss 0.027148230001330376 train acc 0.9558503797158255\n",
            "epoch 22 batch id 2051 loss 0.27370038628578186 train acc 0.955821855192589\n",
            "epoch 22 batch id 2061 loss 0.06428738683462143 train acc 0.9558542576419214\n",
            "epoch 22 batch id 2071 loss 0.12452710419893265 train acc 0.9559014365041043\n",
            "epoch 22 batch id 2081 loss 0.07841195911169052 train acc 0.9558956030754445\n",
            "epoch 22 batch id 2091 loss 0.12796567380428314 train acc 0.9558674079387852\n",
            "epoch 22 batch id 2101 loss 0.04914029315114021 train acc 0.9559287244169443\n",
            "epoch 22 batch id 2111 loss 0.14896588027477264 train acc 0.9558932378019895\n",
            "epoch 22 batch id 2121 loss 0.04703546687960625 train acc 0.9559096534653465\n",
            "epoch 22 batch id 2131 loss 0.07902688533067703 train acc 0.9559332473017362\n",
            "epoch 22 batch id 2141 loss 0.11333177238702774 train acc 0.9559493227463802\n",
            "epoch 22 batch id 2151 loss 0.11241575330495834 train acc 0.955950720595072\n",
            "epoch 22 batch id 2161 loss 0.13190698623657227 train acc 0.9558798010180471\n",
            "epoch 22 batch id 2171 loss 0.06983809918165207 train acc 0.9559174919391985\n",
            "epoch 22 batch id 2181 loss 0.15011869370937347 train acc 0.9558545392022009\n",
            "epoch 22 batch id 2191 loss 0.07929122447967529 train acc 0.9559133957097216\n",
            "epoch 22 batch id 2201 loss 0.15173286199569702 train acc 0.9559362221717401\n",
            "epoch 22 batch id 2211 loss 0.06787842512130737 train acc 0.9559305744007237\n",
            "epoch 22 batch id 2221 loss 0.13417768478393555 train acc 0.9558968370103557\n",
            "epoch 22 batch id 2231 loss 0.12234924733638763 train acc 0.9559054235768714\n",
            "epoch 22 batch id 2241 loss 0.1459156721830368 train acc 0.9559209058456046\n",
            "epoch 22 batch id 2251 loss 0.1414717733860016 train acc 0.9559015437583296\n",
            "epoch 22 batch id 2261 loss 0.13534267246723175 train acc 0.9559030849181778\n",
            "epoch 22 batch id 2271 loss 0.06526824086904526 train acc 0.9559252531924263\n",
            "epoch 22 batch id 2281 loss 0.12837526202201843 train acc 0.9559746273564226\n",
            "epoch 22 batch id 2291 loss 0.04906608164310455 train acc 0.9560235704932344\n",
            "epoch 22 batch id 2301 loss 0.03974470496177673 train acc 0.9560856692742286\n",
            "epoch 22 batch id 2311 loss 0.24496594071388245 train acc 0.9560660969277369\n",
            "epoch 22 batch id 2321 loss 0.06885156035423279 train acc 0.9560668892718656\n",
            "epoch 22 batch id 2331 loss 0.07920080423355103 train acc 0.956081081081081\n",
            "epoch 22 batch id 2341 loss 0.11021428555250168 train acc 0.9560818026484409\n",
            "epoch 22 batch id 2351 loss 0.06950028240680695 train acc 0.9560359953211399\n",
            "epoch 22 batch id 2361 loss 0.23240210115909576 train acc 0.9560435196950444\n",
            "epoch 22 batch id 2371 loss 0.032570794224739075 train acc 0.9561037009700548\n",
            "epoch 22 batch id 2381 loss 0.1331285834312439 train acc 0.956064941201176\n",
            "epoch 22 batch id 2391 loss 0.22350884974002838 train acc 0.9560199707235466\n",
            "epoch 22 batch id 2401 loss 0.06384064257144928 train acc 0.9560144210745523\n",
            "epoch 22 batch id 2411 loss 0.0781804695725441 train acc 0.956028359601825\n",
            "epoch 22 batch id 2421 loss 0.13620619475841522 train acc 0.9560228211482859\n",
            "epoch 22 batch id 2431 loss 0.06398496776819229 train acc 0.9560366104483752\n",
            "epoch 22 batch id 2441 loss 0.26913216710090637 train acc 0.9559862761163458\n",
            "epoch 22 batch id 2451 loss 0.022228360176086426 train acc 0.9559873521011832\n",
            "epoch 22 batch id 2461 loss 0.19932255148887634 train acc 0.9560011174319383\n",
            "epoch 22 batch id 2471 loss 0.30913686752319336 train acc 0.9559515378389316\n",
            "epoch 22 batch id 2481 loss 0.29591718316078186 train acc 0.9559527408303103\n",
            "epoch 22 batch id 2491 loss 0.18451060354709625 train acc 0.9559037535126456\n",
            "epoch 22 batch id 2501 loss 0.01180734857916832 train acc 0.9559301279488205\n",
            "epoch 22 batch id 2511 loss 0.042022306472063065 train acc 0.9559625149342891\n",
            "epoch 22 batch id 2521 loss 0.32013624906539917 train acc 0.955914071796906\n",
            "epoch 22 batch id 2531 loss 0.03199534863233566 train acc 0.95589070525484\n",
            "epoch 22 batch id 2541 loss 0.08437144011259079 train acc 0.9558859700905156\n",
            "epoch 22 batch id 2551 loss 0.19166041910648346 train acc 0.9557955213641709\n",
            "epoch 22 batch id 2561 loss 0.2331637740135193 train acc 0.9558033971105037\n",
            "epoch 22 batch id 2571 loss 0.18615692853927612 train acc 0.9558112115908207\n",
            "epoch 22 batch id 2581 loss 0.1026049256324768 train acc 0.9557947500968617\n",
            "epoch 22 batch id 2591 loss 0.019940931349992752 train acc 0.955814598610575\n",
            "epoch 22 batch id 2601 loss 0.1341855674982071 train acc 0.9558102652825836\n",
            "epoch 22 batch id 2611 loss 0.09041811525821686 train acc 0.9558358866334737\n",
            "epoch 22 batch id 2621 loss 0.14050957560539246 train acc 0.9558613124761541\n",
            "epoch 22 batch id 2631 loss 0.17183314263820648 train acc 0.9557974629418472\n",
            "epoch 22 batch id 2641 loss 0.11016840487718582 train acc 0.955799176448315\n",
            "epoch 22 batch id 2651 loss 0.005507055204361677 train acc 0.9558008770275368\n",
            "epoch 22 batch id 2661 loss 0.07454604655504227 train acc 0.9558143085306275\n",
            "epoch 22 batch id 2671 loss 0.23774734139442444 train acc 0.9557983901160614\n",
            "epoch 22 batch id 2681 loss 0.0854932963848114 train acc 0.9557767624020888\n",
            "epoch 22 batch id 2691 loss 0.11392497271299362 train acc 0.9557785209959123\n",
            "epoch 22 batch id 2701 loss 0.1534694880247116 train acc 0.9557918363569049\n",
            "epoch 22 batch id 2711 loss 0.005669264122843742 train acc 0.9557819992622648\n",
            "epoch 22 batch id 2721 loss 0.028704164549708366 train acc 0.9557722344726204\n",
            "epoch 22 batch id 2731 loss 0.03020612522959709 train acc 0.9557568198462102\n",
            "epoch 22 batch id 2741 loss 0.13103097677230835 train acc 0.9557700200656695\n",
            "epoch 22 batch id 2751 loss 0.2587570250034332 train acc 0.9557433660487096\n",
            "epoch 22 batch id 2761 loss 0.13274848461151123 train acc 0.9557848152843172\n",
            "epoch 22 batch id 2771 loss 0.17726024985313416 train acc 0.9557921328040418\n",
            "epoch 22 batch id 2781 loss 0.13818007707595825 train acc 0.9558443455591514\n",
            "epoch 22 batch id 2791 loss 0.17368969321250916 train acc 0.9558513973486206\n",
            "epoch 22 batch id 2801 loss 0.22603839635849 train acc 0.9558416636915388\n",
            "epoch 22 batch id 2811 loss 0.1152118593454361 train acc 0.9558709089292067\n",
            "epoch 22 batch id 2821 loss 0.07745824754238129 train acc 0.9558611751152074\n",
            "epoch 22 batch id 2831 loss 0.11363531649112701 train acc 0.9558901448251501\n",
            "epoch 22 batch id 2841 loss 0.0921269953250885 train acc 0.9559409098908835\n",
            "epoch 22 batch id 2851 loss 0.1370968371629715 train acc 0.9559200719045948\n",
            "epoch 22 batch id 2861 loss 0.07077187299728394 train acc 0.9558993795875568\n",
            "epoch 22 batch id 2871 loss 0.144254669547081 train acc 0.955867946708464\n",
            "epoch 22 batch id 2881 loss 0.09998739510774612 train acc 0.9558746962860118\n",
            "epoch 22 batch id 2891 loss 0.13498835265636444 train acc 0.9558543756485645\n",
            "epoch 22 batch id 2901 loss 0.2989659011363983 train acc 0.9558072647362978\n",
            "epoch 22 batch id 2911 loss 0.03712146356701851 train acc 0.9558570937822054\n",
            "epoch 22 batch id 2921 loss 0.0058203148655593395 train acc 0.9558156453269429\n",
            "epoch 22 batch id 2931 loss 0.07445112615823746 train acc 0.9557958034800409\n",
            "epoch 22 batch id 2941 loss 0.08123555034399033 train acc 0.9557601581094866\n",
            "epoch 22 batch id 2951 loss 0.06724002212285995 train acc 0.9558412402575398\n",
            "epoch 22 batch id 2961 loss 0.07476431876420975 train acc 0.9558795592705167\n",
            "epoch 22 batch id 2971 loss 0.08744940161705017 train acc 0.9558545102659037\n",
            "epoch 22 batch id 2981 loss 0.13200655579566956 train acc 0.9558663200268366\n",
            "epoch 22 batch id 2991 loss 0.08205138146877289 train acc 0.9558728268137746\n",
            "epoch 22 batch id 3001 loss 0.07110592722892761 train acc 0.95589491002999\n",
            "epoch 22 batch id 3011 loss 0.043638382107019424 train acc 0.95589608933909\n",
            "epoch 22 batch id 3021 loss 0.05825415253639221 train acc 0.9558558838133069\n",
            "epoch 22 batch id 3031 loss 0.2315138578414917 train acc 0.9558623391619927\n",
            "epoch 22 batch id 3041 loss 0.20719055831432343 train acc 0.9558173709306149\n",
            "epoch 22 batch id 3051 loss 0.15757247805595398 train acc 0.955844395280236\n",
            "epoch 22 batch id 3061 loss 0.1600329428911209 train acc 0.9558355112708266\n",
            "epoch 22 batch id 3071 loss 0.05744100734591484 train acc 0.9558114213611202\n",
            "epoch 22 batch id 3081 loss 0.3557218611240387 train acc 0.9558179162609542\n",
            "epoch 22 batch id 3091 loss 0.02569497749209404 train acc 0.9558193141378195\n",
            "epoch 22 batch id 3101 loss 0.12830068171024323 train acc 0.9558106256046437\n",
            "epoch 22 batch id 3111 loss 0.14744941890239716 train acc 0.9558271054323368\n",
            "epoch 22 batch id 3121 loss 0.12437436729669571 train acc 0.9558334668375521\n",
            "epoch 22 batch id 3131 loss 0.08382086455821991 train acc 0.9558647396997765\n",
            "epoch 22 batch id 3141 loss 0.18033623695373535 train acc 0.9558510426615727\n",
            "epoch 22 batch id 3151 loss 0.12474264204502106 train acc 0.955896937480165\n",
            "epoch 22 batch id 3161 loss 0.0793483555316925 train acc 0.9558881683011705\n",
            "epoch 22 batch id 3171 loss 0.051881466060876846 train acc 0.9559090192368338\n",
            "epoch 22 batch id 3181 loss 0.12114284187555313 train acc 0.9559051791889343\n",
            "epoch 22 batch id 3191 loss 0.19058090448379517 train acc 0.9559356392980257\n",
            "epoch 22 batch id 3201 loss 0.1543283462524414 train acc 0.955946383942518\n",
            "epoch 22 batch id 3211 loss 0.21877381205558777 train acc 0.9559473294923699\n",
            "epoch 22 batch id 3221 loss 0.11532676219940186 train acc 0.955890057435579\n",
            "epoch 22 batch id 3231 loss 0.14356599748134613 train acc 0.9558718276075518\n",
            "epoch 22 batch id 3241 loss 0.18251311779022217 train acc 0.9558922786177105\n",
            "epoch 22 batch id 3251 loss 0.14368517696857452 train acc 0.9558549292525377\n",
            "epoch 22 batch id 3261 loss 0.12603740394115448 train acc 0.9558321833793315\n",
            "epoch 22 batch id 3271 loss 0.10386292636394501 train acc 0.9558047997554264\n",
            "epoch 22 batch id 3281 loss 0.06995175033807755 train acc 0.9558252057299603\n",
            "epoch 22 batch id 3291 loss 0.14692926406860352 train acc 0.9558264965056213\n",
            "epoch 22 batch id 3301 loss 0.11217831820249557 train acc 0.9558372462890034\n",
            "epoch 22 batch id 3311 loss 0.13750572502613068 train acc 0.9558384929024464\n",
            "epoch 22 batch id 3321 loss 0.03772411867976189 train acc 0.9558538467329117\n",
            "epoch 22 batch id 3331 loss 0.21180568635463715 train acc 0.9557940558390874\n",
            "epoch 22 batch id 3341 loss 0.23090729117393494 train acc 0.9558000972762646\n",
            "epoch 22 batch id 3351 loss 0.03667039796710014 train acc 0.9557921142942405\n",
            "epoch 22 batch id 3361 loss 0.10890484601259232 train acc 0.9558120722999107\n",
            "epoch 22 batch id 3371 loss 0.1932521015405655 train acc 0.9557762904182735\n",
            "epoch 22 batch id 3381 loss 0.016400162130594254 train acc 0.9558192842354333\n",
            "epoch 22 batch id 3391 loss 0.1387869119644165 train acc 0.9558482011206134\n",
            "epoch 22 batch id 3401 loss 0.12880712747573853 train acc 0.9558631652455161\n",
            "epoch 22 batch id 3411 loss 0.08381976187229156 train acc 0.9559055262386397\n",
            "epoch 22 batch id 3421 loss 0.1031285747885704 train acc 0.9559248026892722\n",
            "epoch 22 batch id 3431 loss 0.2442207634449005 train acc 0.9559166423783153\n",
            "epoch 22 batch id 3441 loss 0.028205184265971184 train acc 0.9559266928218542\n",
            "epoch 22 batch id 3451 loss 0.10537606477737427 train acc 0.9559231019994204\n",
            "epoch 22 batch id 3461 loss 0.14865122735500336 train acc 0.9559421048829818\n",
            "epoch 22 batch id 3471 loss 0.09020516276359558 train acc 0.9559384903486027\n",
            "epoch 22 batch id 3481 loss 0.2010529637336731 train acc 0.9559214306233841\n",
            "epoch 22 batch id 3491 loss 0.06991217285394669 train acc 0.9559044686336293\n",
            "epoch 22 batch id 3501 loss 0.12455392628908157 train acc 0.9559099185946872\n",
            "epoch 22 batch id 3511 loss 0.07839912176132202 train acc 0.9558975363144403\n",
            "epoch 22 batch id 3521 loss 0.28224819898605347 train acc 0.9559118503266117\n",
            "epoch 22 batch id 3531 loss 0.3230438232421875 train acc 0.9559393585386576\n",
            "epoch 22 batch id 3541 loss 0.08632611483335495 train acc 0.9559446484044055\n",
            "epoch 22 batch id 3551 loss 0.1033216118812561 train acc 0.9559719093213179\n",
            "epoch 22 batch id 3561 loss 0.02856237068772316 train acc 0.9560297318169053\n",
            "epoch 22 batch id 3571 loss 0.21396364271640778 train acc 0.956012846541585\n",
            "epoch 22 batch id 3581 loss 0.3262174427509308 train acc 0.9560222354091036\n",
            "epoch 22 batch id 3591 loss 0.2428458034992218 train acc 0.9560141673628516\n",
            "epoch 22 batch id 3601 loss 0.35487955808639526 train acc 0.9560148222715912\n",
            "epoch 22 batch id 3611 loss 0.030311839655041695 train acc 0.9560284547216837\n",
            "epoch 22 batch id 3621 loss 0.0991634652018547 train acc 0.9560333816625242\n",
            "epoch 22 batch id 3631 loss 0.226966992020607 train acc 0.9560081589093914\n",
            "epoch 22 batch id 3641 loss 0.1061355248093605 train acc 0.956060319967042\n",
            "epoch 22 batch id 3651 loss 0.1682489961385727 train acc 0.9560351615995618\n",
            "epoch 22 batch id 3661 loss 0.06659021973609924 train acc 0.9560400163889647\n",
            "epoch 22 batch id 3671 loss 0.07738851010799408 train acc 0.956053357395805\n",
            "epoch 22 batch id 3681 loss 0.09997845441102982 train acc 0.9560581363759848\n",
            "epoch 22 batch id 3691 loss 0.04277799278497696 train acc 0.9560967556217828\n",
            "epoch 22 batch id 3701 loss 0.1517791897058487 train acc 0.9560845041880572\n",
            "epoch 22 batch id 3711 loss 0.0726415142416954 train acc 0.9561060024252223\n",
            "epoch 22 batch id 3721 loss 0.13924027979373932 train acc 0.9561189868314969\n",
            "epoch 22 batch id 3731 loss 0.02050381898880005 train acc 0.956165404717234\n",
            "epoch 22 batch id 3741 loss 0.12003545463085175 train acc 0.9561363940122962\n",
            "epoch 22 batch id 3751 loss 0.11005410552024841 train acc 0.9561283657691282\n",
            "epoch 22 batch id 3761 loss 0.11381358653306961 train acc 0.9561494615793672\n",
            "epoch 22 batch id 3771 loss 0.05526408180594444 train acc 0.9560875762397242\n",
            "epoch 22 batch id 3781 loss 0.1916884034872055 train acc 0.9560549457815393\n",
            "epoch 22 batch id 3791 loss 0.02109774947166443 train acc 0.9560348522817199\n",
            "epoch 22 batch id 3801 loss 0.10040383785963058 train acc 0.9560313075506446\n",
            "epoch 22 batch id 3811 loss 0.023204118013381958 train acc 0.9560728811335607\n",
            "epoch 22 batch id 3821 loss 0.16778667271137238 train acc 0.9560610769432086\n",
            "epoch 22 batch id 3831 loss 0.015158342197537422 train acc 0.9561023557817803\n",
            "epoch 22 batch id 3841 loss 0.05062149092555046 train acc 0.9560946042697215\n",
            "epoch 22 batch id 3851 loss 0.03196846321225166 train acc 0.9561396390547909\n",
            "epoch 22 batch id 3861 loss 0.08913257718086243 train acc 0.9561480186480187\n",
            "epoch 22 batch id 3871 loss 0.24642083048820496 train acc 0.9561523185223456\n",
            "epoch 22 batch id 3881 loss 0.12615682184696198 train acc 0.9561364661169801\n",
            "epoch 22 batch id 3891 loss 0.10323812812566757 train acc 0.9561046324852223\n",
            "epoch 22 batch id 3901 loss 0.04749288409948349 train acc 0.95607296206101\n",
            "epoch 22 batch id 3911 loss 0.10853590071201324 train acc 0.9560774098695985\n",
            "epoch 22 batch id 3921 loss 0.09173326939344406 train acc 0.9560738650854373\n",
            "epoch 22 batch id 3931 loss 0.17756296694278717 train acc 0.9560782879674383\n",
            "epoch 22 batch id 3941 loss 0.20046287775039673 train acc 0.9560430411063182\n",
            "epoch 22 batch id 3951 loss 0.07225443422794342 train acc 0.9560237914452038\n",
            "epoch 22 batch id 3961 loss 0.10668215900659561 train acc 0.9560125284019187\n",
            "epoch 22 batch id 3971 loss 0.09839537739753723 train acc 0.9560446046335935\n",
            "epoch 22 batch id 3981 loss 0.06459787487983704 train acc 0.9560294209997489\n",
            "epoch 22 batch id 3991 loss 0.23238228261470795 train acc 0.9560221435730394\n",
            "epoch 22 batch id 4001 loss 0.11543205380439758 train acc 0.9560109972506873\n",
            "epoch 22 batch id 4011 loss 0.1185031458735466 train acc 0.955972637746198\n",
            "epoch 22 batch id 4021 loss 0.05009262636303902 train acc 0.9559694416811738\n",
            "epoch 22 batch id 4031 loss 0.08287433534860611 train acc 0.956016652195485\n",
            "epoch 22 batch id 4041 loss 0.10811328887939453 train acc 0.9560133630289532\n",
            "epoch 22 batch id 4051 loss 0.08671939373016357 train acc 0.9559985188842262\n",
            "epoch 22 batch id 4061 loss 0.06764829158782959 train acc 0.9560376138882049\n",
            "epoch 22 batch id 4071 loss 0.19437290728092194 train acc 0.9560419737165315\n",
            "epoch 22 batch id 4081 loss 0.10917695611715317 train acc 0.9560577983337417\n",
            "epoch 22 batch id 4091 loss 0.10286515951156616 train acc 0.9560468100708873\n",
            "epoch 22 batch id 4101 loss 0.1238529235124588 train acc 0.9560434954889051\n",
            "epoch 22 batch id 4111 loss 0.058884721249341965 train acc 0.9560515993675505\n",
            "epoch 22 batch id 4121 loss 0.12228671461343765 train acc 0.9560407061392866\n",
            "epoch 22 batch id 4131 loss 0.1609123796224594 train acc 0.9560563422900025\n",
            "epoch 22 batch id 4141 loss 0.059641484171152115 train acc 0.9560530367061096\n",
            "epoch 22 batch id 4151 loss 0.19899477064609528 train acc 0.9560760961214165\n",
            "epoch 22 batch id 4161 loss 0.0409931018948555 train acc 0.9560577385243931\n",
            "epoch 22 batch id 4171 loss 0.09536951035261154 train acc 0.9560806760968593\n",
            "epoch 22 batch id 4181 loss 0.042830560356378555 train acc 0.9560885553695289\n",
            "epoch 22 batch id 4191 loss 0.19515849649906158 train acc 0.9560777559055118\n",
            "epoch 22 batch id 4201 loss 0.055706363171339035 train acc 0.9560930433230184\n",
            "epoch 22 batch id 4211 loss 0.15050196647644043 train acc 0.9560785739729281\n",
            "epoch 22 batch id 4221 loss 0.15803159773349762 train acc 0.9560493662639185\n",
            "epoch 22 batch id 4231 loss 0.09063128381967545 train acc 0.9560276825809502\n",
            "epoch 22 batch id 4241 loss 0.19538530707359314 train acc 0.9560392596085829\n",
            "epoch 22 batch id 4251 loss 0.20406536757946014 train acc 0.9560618089861209\n",
            "epoch 22 batch id 4261 loss 0.051047321408987045 train acc 0.9560915864820465\n",
            "epoch 22 batch id 4271 loss 0.10742999613285065 train acc 0.9561139077499414\n",
            "epoch 22 batch id 4281 loss 0.04115544259548187 train acc 0.9561361247372109\n",
            "epoch 22 batch id 4291 loss 0.07149156928062439 train acc 0.9561181834071312\n",
            "epoch 22 batch id 4301 loss 0.07204694300889969 train acc 0.9560930597535456\n",
            "epoch 22 batch id 4311 loss 0.06836623698472977 train acc 0.9561477905358385\n",
            "epoch 22 batch id 4321 loss 0.054328832775354385 train acc 0.9561407949548716\n",
            "epoch 22 batch id 4331 loss 0.16919489204883575 train acc 0.9561518702378203\n",
            "epoch 22 batch id 4341 loss 0.08274982869625092 train acc 0.9561484968901175\n",
            "epoch 22 batch id 4351 loss 0.08171490579843521 train acc 0.9561092277637325\n",
            "epoch 22 batch id 4361 loss 0.1298711746931076 train acc 0.9561059676679661\n",
            "epoch 22 batch id 4371 loss 0.0817059874534607 train acc 0.9561456188515214\n",
            "epoch 22 batch id 4381 loss 0.2502879202365875 train acc 0.9561137582743666\n",
            "epoch 22 batch id 4391 loss 0.03426673635840416 train acc 0.9561033933044865\n",
            "epoch 22 batch id 4401 loss 0.20878203213214874 train acc 0.9561072767552828\n",
            "epoch 22 batch id 4411 loss 0.1391395926475525 train acc 0.9561076003173884\n",
            "epoch 22 batch id 4421 loss 0.1167454868555069 train acc 0.9561149909522733\n",
            "epoch 22 batch id 4431 loss 0.15552780032157898 train acc 0.9561188219363574\n",
            "epoch 22 batch id 4441 loss 0.03491863235831261 train acc 0.9561155989641973\n",
            "epoch 22 batch id 4451 loss 0.14615896344184875 train acc 0.9561404740507751\n",
            "epoch 22 batch id 4461 loss 0.13941234350204468 train acc 0.9561442221475006\n",
            "epoch 22 batch id 4471 loss 0.15141117572784424 train acc 0.9561514482218743\n",
            "epoch 22 batch id 4481 loss 0.020933322608470917 train acc 0.9561377203749163\n",
            "epoch 22 batch id 4491 loss 0.14322654902935028 train acc 0.9561240536628813\n",
            "epoch 22 batch id 4501 loss 0.04704195261001587 train acc 0.9561173905798711\n",
            "epoch 22 batch id 4511 loss 0.1840791553258896 train acc 0.9561315395699401\n",
            "epoch 22 batch id 4521 loss 0.120355024933815 train acc 0.956107608936076\n",
            "epoch 22 batch id 4531 loss 0.08027071505784988 train acc 0.9560941293312735\n",
            "epoch 22 batch id 4541 loss 0.1306581348180771 train acc 0.9560738273508038\n",
            "epoch 22 batch id 4551 loss 0.23643772304058075 train acc 0.9560879477038013\n",
            "epoch 22 batch id 4561 loss 0.19409261643886566 train acc 0.9560814514360886\n",
            "epoch 22 batch id 4571 loss 0.20130500197410583 train acc 0.9560784018814263\n",
            "epoch 22 batch id 4581 loss 0.13896270096302032 train acc 0.9560685439860293\n",
            "epoch 22 batch id 4591 loss 0.07819482684135437 train acc 0.9560587290350686\n",
            "epoch 22 batch id 4601 loss 0.08777353167533875 train acc 0.9560285807433166\n",
            "epoch 22 batch id 4611 loss 0.08892347663640976 train acc 0.9560629472999349\n",
            "epoch 22 batch id 4621 loss 0.03562707453966141 train acc 0.9560836399047825\n",
            "epoch 22 batch id 4631 loss 0.08985145390033722 train acc 0.9560772511336645\n",
            "epoch 22 batch id 4641 loss 0.277943879365921 train acc 0.956080990088343\n",
            "epoch 22 batch id 4651 loss 0.19460070133209229 train acc 0.9560813534723716\n",
            "epoch 22 batch id 4661 loss 0.19301095604896545 train acc 0.9560984767217335\n",
            "epoch 22 batch id 4671 loss 0.059452999383211136 train acc 0.9561188717619353\n",
            "epoch 22 batch id 4681 loss 0.25372081995010376 train acc 0.9561258278145696\n",
            "epoch 22 batch id 4691 loss 0.04363195225596428 train acc 0.9561360850564912\n",
            "epoch 22 batch id 4701 loss 0.165598064661026 train acc 0.9561296798553499\n",
            "epoch 22 batch id 4711 loss 0.20875507593154907 train acc 0.9561266185523244\n",
            "epoch 22 batch id 4721 loss 0.1370316445827484 train acc 0.9561169508578691\n",
            "epoch 22 batch id 4731 loss 0.033789556473493576 train acc 0.9561436535616149\n",
            "epoch 22 batch id 4741 loss 0.13644610345363617 train acc 0.956157060746678\n",
            "epoch 22 batch id 4751 loss 0.16605162620544434 train acc 0.9561539675857714\n",
            "epoch 22 batch id 4761 loss 0.14192499220371246 train acc 0.9561443236714976\n",
            "epoch 22 batch id 4771 loss 0.0553317591547966 train acc 0.9561772951163278\n",
            "epoch 22 batch id 4781 loss 0.2239043116569519 train acc 0.9561545701736038\n",
            "epoch 22 batch id 4791 loss 0.25202882289886475 train acc 0.9561286787726988\n",
            "epoch 22 batch id 4801 loss 0.23576277494430542 train acc 0.9561256769423037\n",
            "epoch 22 batch id 4811 loss 0.06633487343788147 train acc 0.9561194398254002\n",
            "epoch 22 batch id 4821 loss 0.3101085126399994 train acc 0.9560905413814561\n",
            "epoch 22 batch id 4831 loss 0.15695765614509583 train acc 0.9561070430552681\n",
            "epoch 22 batch id 4841 loss 0.08725404739379883 train acc 0.9561137936376781\n",
            "epoch 22 batch id 4851 loss 0.06293230503797531 train acc 0.9561205163883735\n",
            "epoch 22 batch id 4861 loss 0.04857667163014412 train acc 0.9561368545566756\n",
            "epoch 22 batch id 4871 loss 0.1993957757949829 train acc 0.9561563334017655\n",
            "epoch 22 batch id 4881 loss 0.1194976270198822 train acc 0.9561661288670354\n",
            "epoch 22 batch id 4891 loss 0.051535263657569885 train acc 0.9561758842772439\n",
            "epoch 22 batch id 4901 loss 0.23907971382141113 train acc 0.9561696592532136\n",
            "epoch 22 batch id 4911 loss 0.2637709677219391 train acc 0.9561570963143963\n",
            "epoch 22 batch id 4921 loss 0.23605547845363617 train acc 0.9561509347693559\n",
            "epoch 22 batch id 4931 loss 0.29615890979766846 train acc 0.9561257858446562\n",
            "epoch 22 batch id 4941 loss 0.16332639753818512 train acc 0.956129199554746\n",
            "epoch 22 batch id 4951 loss 0.14039117097854614 train acc 0.9561136639062816\n",
            "epoch 22 train acc 0.9561226548315513\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "676825f0086744fab3520a984d4d1820",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 22 loss 0.5213685035705566 test acc 0.8268924120234605\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2c9834b830c402e85b899d8c39714f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 23 batch id 1 loss 0.07005057483911514 train acc 0.96875\n",
            "epoch 23 batch id 11 loss 0.10816562920808792 train acc 0.9616477272727273\n",
            "epoch 23 batch id 21 loss 0.09586917608976364 train acc 0.9650297619047619\n",
            "epoch 23 batch id 31 loss 0.13033147156238556 train acc 0.9596774193548387\n",
            "epoch 23 batch id 41 loss 0.2049945443868637 train acc 0.9603658536585366\n",
            "epoch 23 batch id 51 loss 0.21062539517879486 train acc 0.9586397058823529\n",
            "epoch 23 batch id 61 loss 0.050026893615722656 train acc 0.9574795081967213\n",
            "epoch 23 batch id 71 loss 0.23610851168632507 train acc 0.957306338028169\n",
            "epoch 23 batch id 81 loss 0.17263919115066528 train acc 0.9573688271604939\n",
            "epoch 23 batch id 91 loss 0.08026060461997986 train acc 0.9569024725274725\n",
            "epoch 23 batch id 101 loss 0.11837370693683624 train acc 0.9548267326732673\n",
            "epoch 23 batch id 111 loss 0.12923547625541687 train acc 0.9557995495495496\n",
            "epoch 23 batch id 121 loss 0.10732638090848923 train acc 0.9554493801652892\n",
            "epoch 23 batch id 131 loss 0.11677420139312744 train acc 0.9551526717557252\n",
            "epoch 23 batch id 141 loss 0.024373816326260567 train acc 0.9554521276595744\n",
            "epoch 23 batch id 151 loss 0.10252582281827927 train acc 0.9558153973509934\n",
            "epoch 23 batch id 161 loss 0.1012440025806427 train acc 0.9564246894409938\n",
            "epoch 23 batch id 171 loss 0.31301796436309814 train acc 0.9560489766081871\n",
            "epoch 23 batch id 181 loss 0.13991591334342957 train acc 0.9558011049723757\n",
            "epoch 23 batch id 191 loss 0.16899695992469788 train acc 0.9562336387434555\n",
            "epoch 23 batch id 201 loss 0.09892381727695465 train acc 0.9564676616915423\n",
            "epoch 23 batch id 211 loss 0.06700245290994644 train acc 0.9563092417061612\n",
            "epoch 23 batch id 221 loss 0.16789692640304565 train acc 0.9562358597285068\n",
            "epoch 23 batch id 231 loss 0.05879392847418785 train acc 0.956642316017316\n",
            "epoch 23 batch id 241 loss 0.3699146807193756 train acc 0.9563667012448133\n",
            "epoch 23 batch id 251 loss 0.19148311018943787 train acc 0.9566733067729084\n",
            "epoch 23 batch id 261 loss 0.09704777598381042 train acc 0.9567169540229885\n",
            "epoch 23 batch id 271 loss 0.04271368682384491 train acc 0.9574492619926199\n",
            "epoch 23 batch id 281 loss 0.1165803000330925 train acc 0.9577402135231317\n",
            "epoch 23 batch id 291 loss 0.16364195942878723 train acc 0.9579037800687286\n",
            "epoch 23 batch id 301 loss 0.19101743400096893 train acc 0.9579007475083057\n",
            "epoch 23 batch id 311 loss 0.3182750344276428 train acc 0.9576467041800643\n",
            "epoch 23 batch id 321 loss 0.22659990191459656 train acc 0.9576518691588785\n",
            "epoch 23 batch id 331 loss 0.1363144963979721 train acc 0.957892749244713\n",
            "epoch 23 batch id 341 loss 0.26011595129966736 train acc 0.9579820381231672\n",
            "epoch 23 batch id 351 loss 0.09014447778463364 train acc 0.9581997863247863\n",
            "epoch 23 batch id 361 loss 0.04412026330828667 train acc 0.9584920360110804\n",
            "epoch 23 batch id 371 loss 0.05595730245113373 train acc 0.9589791105121294\n",
            "epoch 23 batch id 381 loss 0.03471902012825012 train acc 0.9591535433070866\n",
            "epoch 23 batch id 391 loss 0.15389056503772736 train acc 0.959079283887468\n",
            "epoch 23 batch id 401 loss 0.12109129875898361 train acc 0.9590087281795511\n",
            "epoch 23 batch id 411 loss 0.06679299473762512 train acc 0.9593217761557178\n",
            "epoch 23 batch id 421 loss 0.198470339179039 train acc 0.9594714964370546\n",
            "epoch 23 batch id 431 loss 0.1211763322353363 train acc 0.9595417633410673\n",
            "epoch 23 batch id 441 loss 0.20686960220336914 train acc 0.9593962585034014\n",
            "epoch 23 batch id 451 loss 0.0676392912864685 train acc 0.9594997228381374\n",
            "epoch 23 batch id 461 loss 0.14206166565418243 train acc 0.959361442516269\n",
            "epoch 23 batch id 471 loss 0.08034562319517136 train acc 0.959593949044586\n",
            "epoch 23 batch id 481 loss 0.13048508763313293 train acc 0.9598817567567568\n",
            "epoch 23 batch id 491 loss 0.1210322305560112 train acc 0.9598396130346232\n",
            "epoch 23 batch id 501 loss 0.4470905661582947 train acc 0.9598615269461078\n",
            "epoch 23 batch id 511 loss 0.07888849079608917 train acc 0.9599131604696673\n",
            "epoch 23 batch id 521 loss 0.1548081338405609 train acc 0.9599328214971209\n",
            "epoch 23 batch id 531 loss 0.11838383227586746 train acc 0.9595397834274952\n",
            "epoch 23 batch id 541 loss 0.03813101351261139 train acc 0.959623382624769\n",
            "epoch 23 batch id 551 loss 0.050878770649433136 train acc 0.959703947368421\n",
            "epoch 23 batch id 561 loss 0.08132689446210861 train acc 0.9596145276292335\n",
            "epoch 23 batch id 571 loss 0.20992867648601532 train acc 0.9597197898423818\n",
            "epoch 23 batch id 581 loss 0.20255520939826965 train acc 0.9596600688468159\n",
            "epoch 23 batch id 591 loss 0.06708259135484695 train acc 0.9596552453468697\n",
            "epoch 23 batch id 601 loss 0.10455498099327087 train acc 0.9596245840266223\n",
            "epoch 23 batch id 611 loss 0.07767169177532196 train acc 0.9597739361702128\n",
            "epoch 23 batch id 621 loss 0.2403741478919983 train acc 0.9597675120772947\n",
            "epoch 23 batch id 631 loss 0.10463935136795044 train acc 0.9599098652931854\n",
            "epoch 23 batch id 641 loss 0.09956179559230804 train acc 0.9600965288611545\n",
            "epoch 23 batch id 651 loss 0.06596621870994568 train acc 0.9599654377880185\n",
            "epoch 23 batch id 661 loss 0.07661860436201096 train acc 0.9599565052950075\n",
            "epoch 23 batch id 671 loss 0.15280266106128693 train acc 0.9599944113263785\n",
            "epoch 23 batch id 681 loss 0.19911392033100128 train acc 0.9600770925110133\n",
            "epoch 23 batch id 691 loss 0.17554399371147156 train acc 0.9600669319826338\n",
            "epoch 23 batch id 701 loss 0.09430854022502899 train acc 0.9598341654778887\n",
            "epoch 23 batch id 711 loss 0.20793622732162476 train acc 0.9595859704641351\n",
            "epoch 23 batch id 721 loss 0.08444858342409134 train acc 0.9594313453536755\n",
            "epoch 23 batch id 731 loss 0.04341128095984459 train acc 0.9594733242134063\n",
            "epoch 23 batch id 741 loss 0.28223681449890137 train acc 0.9593876518218624\n",
            "epoch 23 batch id 751 loss 0.14626069366931915 train acc 0.9591170106524634\n",
            "epoch 23 batch id 761 loss 0.10291054844856262 train acc 0.9590382720105125\n",
            "epoch 23 batch id 771 loss 0.07276506721973419 train acc 0.9590426394293126\n",
            "epoch 23 batch id 781 loss 0.2114761471748352 train acc 0.9589868758002561\n",
            "epoch 23 batch id 791 loss 0.041044194251298904 train acc 0.959070796460177\n",
            "epoch 23 batch id 801 loss 0.050957076251506805 train acc 0.9590550873907615\n",
            "epoch 23 batch id 811 loss 0.14073991775512695 train acc 0.9591360974106042\n",
            "epoch 23 batch id 821 loss 0.035490646958351135 train acc 0.9592341656516443\n",
            "epoch 23 batch id 831 loss 0.20428992807865143 train acc 0.9592358604091457\n",
            "epoch 23 batch id 841 loss 0.0438803993165493 train acc 0.9593118311533888\n",
            "epoch 23 batch id 851 loss 0.2739504873752594 train acc 0.9591840481786134\n",
            "epoch 23 batch id 861 loss 0.06230916082859039 train acc 0.9591862659698026\n",
            "epoch 23 batch id 871 loss 0.011852779425680637 train acc 0.9593319460390356\n",
            "epoch 23 batch id 881 loss 0.05483381077647209 train acc 0.9593146992054483\n",
            "epoch 23 batch id 891 loss 0.06759888678789139 train acc 0.9594381313131313\n",
            "epoch 23 batch id 901 loss 0.17080570757389069 train acc 0.9594547724750278\n",
            "epoch 23 batch id 911 loss 0.05033518746495247 train acc 0.959436745334797\n",
            "epoch 23 batch id 921 loss 0.15472474694252014 train acc 0.9592664223669924\n",
            "epoch 23 batch id 931 loss 0.04114531725645065 train acc 0.9590829752953813\n",
            "epoch 23 batch id 941 loss 0.031736116856336594 train acc 0.9592189160467588\n",
            "epoch 23 batch id 951 loss 0.09783051162958145 train acc 0.9590562565720294\n",
            "epoch 23 batch id 961 loss 0.06213127449154854 train acc 0.9590107960457857\n",
            "epoch 23 batch id 971 loss 0.11785047501325607 train acc 0.958966271884655\n",
            "epoch 23 batch id 981 loss 0.025015415623784065 train acc 0.9591456422018348\n",
            "epoch 23 batch id 991 loss 0.4412928819656372 train acc 0.9591479566094854\n",
            "epoch 23 batch id 1001 loss 0.07537674158811569 train acc 0.9592126623376623\n",
            "epoch 23 batch id 1011 loss 0.13274440169334412 train acc 0.9592606330365975\n",
            "epoch 23 batch id 1021 loss 0.14729838073253632 train acc 0.9593688785504407\n",
            "epoch 23 batch id 1031 loss 0.11449918895959854 train acc 0.9592628516003879\n",
            "epoch 23 batch id 1041 loss 0.158401757478714 train acc 0.959323967339097\n",
            "epoch 23 batch id 1051 loss 0.05026815086603165 train acc 0.9592352521408183\n",
            "epoch 23 batch id 1061 loss 0.0862298533320427 train acc 0.9591629359095193\n",
            "epoch 23 batch id 1071 loss 0.016008693724870682 train acc 0.9592524509803921\n",
            "epoch 23 batch id 1081 loss 0.22837644815444946 train acc 0.9590512257169288\n",
            "epoch 23 batch id 1091 loss 0.22198712825775146 train acc 0.9589252978918423\n",
            "epoch 23 batch id 1101 loss 0.06902889162302017 train acc 0.9589435740236149\n",
            "epoch 23 batch id 1111 loss 0.3079407215118408 train acc 0.958778690369037\n",
            "epoch 23 batch id 1121 loss 0.20278829336166382 train acc 0.9588258251561106\n",
            "epoch 23 batch id 1131 loss 0.2590831518173218 train acc 0.9586925287356322\n",
            "epoch 23 batch id 1141 loss 0.13287045061588287 train acc 0.9587122042068361\n",
            "epoch 23 batch id 1151 loss 0.10429983586072922 train acc 0.9587179626411816\n",
            "epoch 23 batch id 1161 loss 0.08017492294311523 train acc 0.9586832472006891\n",
            "epoch 23 batch id 1171 loss 0.026797272264957428 train acc 0.9585957514944492\n",
            "epoch 23 batch id 1181 loss 0.07534492015838623 train acc 0.9585626587637596\n",
            "epoch 23 batch id 1191 loss 0.07944285869598389 train acc 0.9586088371116709\n",
            "epoch 23 batch id 1201 loss 0.09581956267356873 train acc 0.9587583263946711\n",
            "epoch 23 batch id 1211 loss 0.028793301433324814 train acc 0.9588150289017341\n",
            "epoch 23 batch id 1221 loss 0.1152103915810585 train acc 0.9587684275184275\n",
            "epoch 23 batch id 1231 loss 0.09626606106758118 train acc 0.9586845044679123\n",
            "epoch 23 batch id 1241 loss 0.0906222015619278 train acc 0.9586019339242546\n",
            "epoch 23 batch id 1251 loss 0.14190907776355743 train acc 0.9585706434852118\n",
            "epoch 23 batch id 1261 loss 0.10049448162317276 train acc 0.9585398493259318\n",
            "epoch 23 batch id 1271 loss 0.10307841747999191 train acc 0.9585710070810386\n",
            "epoch 23 batch id 1281 loss 0.10475430637598038 train acc 0.958711455893833\n",
            "epoch 23 batch id 1291 loss 0.2236875295639038 train acc 0.9587529047250194\n",
            "epoch 23 batch id 1301 loss 0.07540711760520935 train acc 0.9587576863950807\n",
            "epoch 23 batch id 1311 loss 0.08765186369419098 train acc 0.9587266399694889\n",
            "epoch 23 batch id 1321 loss 0.15221932530403137 train acc 0.9585896101438305\n",
            "epoch 23 batch id 1331 loss 0.14740020036697388 train acc 0.9585602930127723\n",
            "epoch 23 batch id 1341 loss 0.14517240226268768 train acc 0.9585430648769575\n",
            "epoch 23 batch id 1351 loss 0.016752667725086212 train acc 0.9586070503330866\n",
            "epoch 23 batch id 1361 loss 0.01572268269956112 train acc 0.9585552902277737\n",
            "epoch 23 batch id 1371 loss 0.06516411900520325 train acc 0.9585156819839533\n",
            "epoch 23 batch id 1381 loss 0.2154809832572937 train acc 0.9585105901520637\n",
            "epoch 23 batch id 1391 loss 0.01566014252603054 train acc 0.9585392703091301\n",
            "epoch 23 batch id 1401 loss 0.06159048154950142 train acc 0.9585675410421127\n",
            "epoch 23 batch id 1411 loss 0.10508996993303299 train acc 0.9585954110559887\n",
            "epoch 23 batch id 1421 loss 0.041828352957963943 train acc 0.9586668719211823\n",
            "epoch 23 batch id 1431 loss 0.05721437931060791 train acc 0.9586827393431167\n",
            "epoch 23 batch id 1441 loss 0.07342797517776489 train acc 0.9586550138792506\n",
            "epoch 23 batch id 1451 loss 0.1604277640581131 train acc 0.9587461233631978\n",
            "epoch 23 batch id 1461 loss 0.12809546291828156 train acc 0.958729038329911\n",
            "epoch 23 batch id 1471 loss 0.0624992698431015 train acc 0.958818405846363\n",
            "epoch 23 batch id 1481 loss 0.08730226755142212 train acc 0.9588116137744767\n",
            "epoch 23 batch id 1491 loss 0.19532698392868042 train acc 0.958867790073776\n",
            "epoch 23 batch id 1501 loss 0.09201223403215408 train acc 0.9589752664890073\n",
            "epoch 23 batch id 1511 loss 0.055860601365566254 train acc 0.9589882528127068\n",
            "epoch 23 batch id 1521 loss 0.04110481217503548 train acc 0.9589702498356345\n",
            "epoch 23 batch id 1531 loss 0.1079065278172493 train acc 0.9590035107772698\n",
            "epoch 23 batch id 1541 loss 0.06938892602920532 train acc 0.9590160609993511\n",
            "epoch 23 batch id 1551 loss 0.13083598017692566 train acc 0.9589277079303675\n",
            "epoch 23 batch id 1561 loss 0.13563567399978638 train acc 0.9589105541319667\n",
            "epoch 23 batch id 1571 loss 0.07482272386550903 train acc 0.9589930776575429\n",
            "epoch 23 batch id 1581 loss 0.054866254329681396 train acc 0.9589065464895635\n",
            "epoch 23 batch id 1591 loss 0.23037974536418915 train acc 0.958870207416719\n",
            "epoch 23 batch id 1601 loss 0.20627661049365997 train acc 0.9588733603997501\n",
            "epoch 23 batch id 1611 loss 0.13477301597595215 train acc 0.9588958721291123\n",
            "epoch 23 batch id 1621 loss 0.05447910726070404 train acc 0.9589470234423195\n",
            "epoch 23 batch id 1631 loss 0.06100315973162651 train acc 0.9589592274678111\n",
            "epoch 23 batch id 1641 loss 0.109927698969841 train acc 0.9590284125533212\n",
            "epoch 23 batch id 1651 loss 0.03514670580625534 train acc 0.9589926559660812\n",
            "epoch 23 batch id 1661 loss 0.10362444818019867 train acc 0.9590513997591812\n",
            "epoch 23 batch id 1671 loss 0.1567927598953247 train acc 0.9590439856373429\n",
            "epoch 23 batch id 1681 loss 0.17475761473178864 train acc 0.9590366597263533\n",
            "epoch 23 batch id 1691 loss 0.10044535994529724 train acc 0.9590479006505027\n",
            "epoch 23 batch id 1701 loss 0.0804944634437561 train acc 0.9590590094062317\n",
            "epoch 23 batch id 1711 loss 0.16724833846092224 train acc 0.9590882524839275\n",
            "epoch 23 batch id 1721 loss 0.19439426064491272 train acc 0.9591353137710633\n",
            "epoch 23 batch id 1731 loss 0.00720601063221693 train acc 0.959154751588677\n",
            "epoch 23 batch id 1741 loss 0.21224439144134521 train acc 0.9591649913842619\n",
            "epoch 23 batch id 1751 loss 0.04896874725818634 train acc 0.9591215733866362\n",
            "epoch 23 batch id 1761 loss 0.18344412744045258 train acc 0.9591407580919932\n",
            "epoch 23 batch id 1771 loss 0.16067953407764435 train acc 0.9591420807453416\n",
            "epoch 23 batch id 1781 loss 0.04934367910027504 train acc 0.9591872543514879\n",
            "epoch 23 batch id 1791 loss 0.20941083133220673 train acc 0.9590748883305416\n",
            "epoch 23 batch id 1801 loss 0.17622940242290497 train acc 0.959102581898945\n",
            "epoch 23 batch id 1811 loss 0.0954861268401146 train acc 0.959147225289895\n",
            "epoch 23 batch id 1821 loss 0.16647714376449585 train acc 0.9590798325096102\n",
            "epoch 23 batch id 1831 loss 0.24241071939468384 train acc 0.9590046422719826\n",
            "epoch 23 batch id 1841 loss 0.04212702810764313 train acc 0.9589727050516024\n",
            "epoch 23 batch id 1851 loss 0.14674265682697296 train acc 0.9590170853592652\n",
            "epoch 23 batch id 1861 loss 0.13776254653930664 train acc 0.959052592692101\n",
            "epoch 23 batch id 1871 loss 0.1492174118757248 train acc 0.9591628808123998\n",
            "epoch 23 batch id 1881 loss 0.061956800520420074 train acc 0.9591889287612971\n",
            "epoch 23 batch id 1891 loss 0.04999331384897232 train acc 0.9592973294553147\n",
            "epoch 23 batch id 1901 loss 0.27397701144218445 train acc 0.9592812993161494\n",
            "epoch 23 batch id 1911 loss 0.08508417755365372 train acc 0.959216378859236\n",
            "epoch 23 batch id 1921 loss 0.15793290734291077 train acc 0.9590870640291514\n",
            "epoch 23 batch id 1931 loss 0.14819347858428955 train acc 0.9590885551527706\n",
            "epoch 23 batch id 1941 loss 0.10143310576677322 train acc 0.9591463807315816\n",
            "epoch 23 batch id 1951 loss 0.13907334208488464 train acc 0.9591475525371604\n",
            "epoch 23 batch id 1961 loss 0.05209977179765701 train acc 0.959140744518103\n",
            "epoch 23 batch id 1971 loss 0.021490763872861862 train acc 0.9591498604769153\n",
            "epoch 23 batch id 1981 loss 0.1127280667424202 train acc 0.9591194472488642\n",
            "epoch 23 batch id 1991 loss 0.14068441092967987 train acc 0.9591207307885484\n",
            "epoch 23 batch id 2001 loss 0.16920305788516998 train acc 0.9591532358820589\n",
            "epoch 23 batch id 2011 loss 0.17132391035556793 train acc 0.9591310293386375\n",
            "epoch 23 batch id 2021 loss 0.22704970836639404 train acc 0.9591090425531915\n",
            "epoch 23 batch id 2031 loss 0.00534699484705925 train acc 0.9590949655342196\n",
            "epoch 23 batch id 2041 loss 0.020047185942530632 train acc 0.9591346153846154\n",
            "epoch 23 batch id 2051 loss 0.3261912763118744 train acc 0.9591053144807411\n",
            "epoch 23 batch id 2061 loss 0.07321509718894958 train acc 0.959114204269772\n",
            "epoch 23 batch id 2071 loss 0.1352205127477646 train acc 0.9590852848865282\n",
            "epoch 23 batch id 2081 loss 0.20459917187690735 train acc 0.9590566434406536\n",
            "epoch 23 batch id 2091 loss 0.08982247859239578 train acc 0.9590133309421329\n",
            "epoch 23 batch id 2101 loss 0.03589635714888573 train acc 0.9589778676820562\n",
            "epoch 23 batch id 2111 loss 0.05102866515517235 train acc 0.9590463642823307\n",
            "epoch 23 batch id 2121 loss 0.1744150072336197 train acc 0.9590552805280528\n",
            "epoch 23 batch id 2131 loss 0.13797356188297272 train acc 0.9591081065227592\n",
            "epoch 23 batch id 2141 loss 0.18685561418533325 train acc 0.9591166510976179\n",
            "epoch 23 batch id 2151 loss 0.1019417941570282 train acc 0.9591178521617852\n",
            "epoch 23 batch id 2161 loss 0.1157054603099823 train acc 0.9590467376214715\n",
            "epoch 23 batch id 2171 loss 0.020402658730745316 train acc 0.9591058268079227\n",
            "epoch 23 batch id 2181 loss 0.07473155856132507 train acc 0.9590927326914259\n",
            "epoch 23 batch id 2191 loss 0.06910307705402374 train acc 0.9591582040164308\n",
            "epoch 23 batch id 2201 loss 0.04579025134444237 train acc 0.9591520899591095\n",
            "epoch 23 batch id 2211 loss 0.10112147778272629 train acc 0.95911776345545\n",
            "epoch 23 batch id 2221 loss 0.05819055065512657 train acc 0.9591189216569113\n",
            "epoch 23 batch id 2231 loss 0.15075665712356567 train acc 0.9590990587180637\n",
            "epoch 23 batch id 2241 loss 0.157648965716362 train acc 0.9590793730477465\n",
            "epoch 23 batch id 2251 loss 0.042815737426280975 train acc 0.9590390382052422\n",
            "epoch 23 batch id 2261 loss 0.06936825811862946 train acc 0.9590405241043786\n",
            "epoch 23 batch id 2271 loss 0.16545599699020386 train acc 0.9590901585204755\n",
            "epoch 23 batch id 2281 loss 0.2019646167755127 train acc 0.9591119574747917\n",
            "epoch 23 batch id 2291 loss 0.09380660951137543 train acc 0.9591403862941946\n",
            "epoch 23 batch id 2301 loss 0.05632402375340462 train acc 0.9591210343328987\n",
            "epoch 23 batch id 2311 loss 0.1240101009607315 train acc 0.9590950887061878\n",
            "epoch 23 batch id 2321 loss 0.14140570163726807 train acc 0.9590895626884963\n",
            "epoch 23 batch id 2331 loss 0.09609147161245346 train acc 0.959030459030459\n",
            "epoch 23 batch id 2341 loss 0.0747728943824768 train acc 0.9590653032891927\n",
            "epoch 23 batch id 2351 loss 0.11343194544315338 train acc 0.9590134517226712\n",
            "epoch 23 batch id 2361 loss 0.18815742433071136 train acc 0.9590282189750106\n",
            "epoch 23 batch id 2371 loss 0.006095560267567635 train acc 0.9591021720792915\n",
            "epoch 23 batch id 2381 loss 0.05813472718000412 train acc 0.9591230050398992\n",
            "epoch 23 batch id 2391 loss 0.02800014615058899 train acc 0.9591436637390214\n",
            "epoch 23 batch id 2401 loss 0.10401740670204163 train acc 0.9591641503540191\n",
            "epoch 23 batch id 2411 loss 0.16774195432662964 train acc 0.9591066984653671\n",
            "epoch 23 batch id 2421 loss 0.05066584795713425 train acc 0.9591723461379595\n",
            "epoch 23 batch id 2431 loss 0.1619686633348465 train acc 0.9591667523652818\n",
            "epoch 23 batch id 2441 loss 0.10403869301080704 train acc 0.9591612044244162\n",
            "epoch 23 batch id 2451 loss 0.07077285647392273 train acc 0.9591047021623827\n",
            "epoch 23 batch id 2461 loss 0.3001105785369873 train acc 0.9591375457131247\n",
            "epoch 23 batch id 2471 loss 0.13822908699512482 train acc 0.9591385066774585\n",
            "epoch 23 batch id 2481 loss 0.1531953364610672 train acc 0.9592150342603789\n",
            "epoch 23 batch id 2491 loss 0.12667454779148102 train acc 0.9592031312725813\n",
            "epoch 23 batch id 2501 loss 0.011043742299079895 train acc 0.9591913234706118\n",
            "epoch 23 batch id 2511 loss 0.03364235535264015 train acc 0.9592667264038232\n",
            "epoch 23 batch id 2521 loss 0.2626192271709442 train acc 0.9592237703292344\n",
            "epoch 23 batch id 2531 loss 0.08327037841081619 train acc 0.9591996740418807\n",
            "epoch 23 batch id 2541 loss 0.06698517501354218 train acc 0.9591450216450217\n",
            "epoch 23 batch id 2551 loss 0.18321537971496582 train acc 0.9591030478243826\n",
            "epoch 23 batch id 2561 loss 0.1949886679649353 train acc 0.9590675029285436\n",
            "epoch 23 batch id 2571 loss 0.2473870813846588 train acc 0.9590747763516142\n",
            "epoch 23 batch id 2581 loss 0.06422939896583557 train acc 0.9591183165439752\n",
            "epoch 23 batch id 2591 loss 0.06395164877176285 train acc 0.959101215746816\n",
            "epoch 23 batch id 2601 loss 0.2024264633655548 train acc 0.959120290272972\n",
            "epoch 23 batch id 2611 loss 0.04274992644786835 train acc 0.9591571715817694\n",
            "epoch 23 batch id 2621 loss 0.18183200061321259 train acc 0.9591341568103777\n",
            "epoch 23 batch id 2631 loss 0.21733634173870087 train acc 0.9590756841505131\n",
            "epoch 23 batch id 2641 loss 0.05109667405486107 train acc 0.9590413195759182\n",
            "epoch 23 batch id 2651 loss 0.022433768957853317 train acc 0.9590425782723501\n",
            "epoch 23 batch id 2661 loss 0.08371076732873917 train acc 0.9590262119503946\n",
            "epoch 23 batch id 2671 loss 0.13710296154022217 train acc 0.9590275177836016\n",
            "epoch 23 batch id 2681 loss 0.08155804127454758 train acc 0.9589297370384186\n",
            "epoch 23 batch id 2691 loss 0.11877667158842087 train acc 0.9588907469342252\n",
            "epoch 23 batch id 2701 loss 0.060221198946237564 train acc 0.9589388189559422\n",
            "epoch 23 batch id 2711 loss 0.07404115051031113 train acc 0.9589346643305053\n",
            "epoch 23 batch id 2721 loss 0.08177658915519714 train acc 0.9589247978684308\n",
            "epoch 23 batch id 2731 loss 0.08780818432569504 train acc 0.9588863969242036\n",
            "epoch 23 batch id 2741 loss 0.17700155079364777 train acc 0.9589052809193724\n",
            "epoch 23 batch id 2751 loss 0.1046728864312172 train acc 0.9589240276263177\n",
            "epoch 23 batch id 2761 loss 0.02541285566985607 train acc 0.959010548714234\n",
            "epoch 23 batch id 2771 loss 0.07233475893735886 train acc 0.9590175027066041\n",
            "epoch 23 batch id 2781 loss 0.14444655179977417 train acc 0.9590637360661632\n",
            "epoch 23 batch id 2791 loss 0.14298798143863678 train acc 0.9590760480114654\n",
            "epoch 23 batch id 2801 loss 0.2832469642162323 train acc 0.9590659585862192\n",
            "epoch 23 batch id 2811 loss 0.09622067213058472 train acc 0.9590670579864817\n",
            "epoch 23 batch id 2821 loss 0.04712988808751106 train acc 0.9590847660404112\n",
            "epoch 23 batch id 2831 loss 0.08733363449573517 train acc 0.9590802719886966\n",
            "epoch 23 batch id 2841 loss 0.11687256395816803 train acc 0.959070309750088\n",
            "epoch 23 batch id 2851 loss 0.045728202909231186 train acc 0.9590823395299894\n",
            "epoch 23 batch id 2861 loss 0.07168804854154587 train acc 0.9590615169521146\n",
            "epoch 23 batch id 2871 loss 0.13119995594024658 train acc 0.9590789359108325\n",
            "epoch 23 batch id 2881 loss 0.07410943508148193 train acc 0.9590799635543215\n",
            "epoch 23 batch id 2891 loss 0.06197173148393631 train acc 0.9591134123140782\n",
            "epoch 23 batch id 2901 loss 0.16343680024147034 train acc 0.9590766115132713\n",
            "epoch 23 batch id 2911 loss 0.042154017835855484 train acc 0.959056166265888\n",
            "epoch 23 batch id 2921 loss 0.008058861829340458 train acc 0.9590412102019856\n",
            "epoch 23 batch id 2931 loss 0.025934606790542603 train acc 0.9589890395769362\n",
            "epoch 23 batch id 2941 loss 0.04923856258392334 train acc 0.9589531621897314\n",
            "epoch 23 batch id 2951 loss 0.0923568531870842 train acc 0.9589916553710607\n",
            "epoch 23 batch id 2961 loss 0.04626861959695816 train acc 0.9589982269503546\n",
            "epoch 23 batch id 2971 loss 0.07149200886487961 train acc 0.9589837176035005\n",
            "epoch 23 batch id 2981 loss 0.18503111600875854 train acc 0.9589640640724589\n",
            "epoch 23 batch id 2991 loss 0.07416164129972458 train acc 0.9589915580073554\n",
            "epoch 23 batch id 3001 loss 0.05413128063082695 train acc 0.9590032489170277\n",
            "epoch 23 batch id 3011 loss 0.26831209659576416 train acc 0.9590200514779144\n",
            "epoch 23 batch id 3021 loss 0.0735340341925621 train acc 0.9590005379013572\n",
            "epoch 23 batch id 3031 loss 0.21233581006526947 train acc 0.9589863081491257\n",
            "epoch 23 batch id 3041 loss 0.18278957903385162 train acc 0.9589413433081223\n",
            "epoch 23 batch id 3051 loss 0.28691521286964417 train acc 0.9589683710258932\n",
            "epoch 23 batch id 3061 loss 0.14978943765163422 train acc 0.958939072198628\n",
            "epoch 23 batch id 3071 loss 0.010661504231393337 train acc 0.9589557554542494\n",
            "epoch 23 batch id 3081 loss 0.30431869626045227 train acc 0.9589621876014282\n",
            "epoch 23 batch id 3091 loss 0.09097225964069366 train acc 0.958968578130055\n",
            "epoch 23 batch id 3101 loss 0.123243048787117 train acc 0.9589446952595937\n",
            "epoch 23 batch id 3111 loss 0.1257667988538742 train acc 0.9589460784313726\n",
            "epoch 23 batch id 3121 loss 0.13650934398174286 train acc 0.9589224206984941\n",
            "epoch 23 batch id 3131 loss 0.04781745746731758 train acc 0.958973770360907\n",
            "epoch 23 batch id 3141 loss 0.13942888379096985 train acc 0.9589650986946833\n",
            "epoch 23 batch id 3151 loss 0.07735258340835571 train acc 0.9589515233259283\n",
            "epoch 23 batch id 3161 loss 0.11349808424711227 train acc 0.9589578060740273\n",
            "epoch 23 batch id 3171 loss 0.044791750609874725 train acc 0.9589344843897823\n",
            "epoch 23 batch id 3181 loss 0.047431208193302155 train acc 0.9589456931782459\n",
            "epoch 23 batch id 3191 loss 0.050150927156209946 train acc 0.9589813146349107\n",
            "epoch 23 batch id 3201 loss 0.17491747438907623 train acc 0.9589483755076539\n",
            "epoch 23 batch id 3211 loss 0.24709579348564148 train acc 0.9589010432886951\n",
            "epoch 23 batch id 3221 loss 0.114138662815094 train acc 0.9588637069233158\n",
            "epoch 23 batch id 3231 loss 0.13857805728912354 train acc 0.9588411095636026\n",
            "epoch 23 batch id 3241 loss 0.11816634982824326 train acc 0.9588861462511571\n",
            "epoch 23 batch id 3251 loss 0.22663503885269165 train acc 0.9588828437403876\n",
            "epoch 23 batch id 3261 loss 0.12972518801689148 train acc 0.9588795614842073\n",
            "epoch 23 batch id 3271 loss 0.2692737877368927 train acc 0.9588333078569244\n",
            "epoch 23 batch id 3281 loss 0.1433079093694687 train acc 0.9588397211216093\n",
            "epoch 23 batch id 3291 loss 0.028049204498529434 train acc 0.9588650865998177\n",
            "epoch 23 batch id 3301 loss 0.14019475877285004 train acc 0.9588760981520751\n",
            "epoch 23 batch id 3311 loss 0.15175971388816833 train acc 0.9589106387798249\n",
            "epoch 23 batch id 3321 loss 0.10977711528539658 train acc 0.9589120370370371\n",
            "epoch 23 batch id 3331 loss 0.26478877663612366 train acc 0.9588477559291504\n",
            "epoch 23 batch id 3341 loss 0.13550297915935516 train acc 0.9588867479796468\n",
            "epoch 23 batch id 3351 loss 0.058197539299726486 train acc 0.9588788794389734\n",
            "epoch 23 batch id 3361 loss 0.10256906598806381 train acc 0.958922195775067\n",
            "epoch 23 batch id 3371 loss 0.11806024610996246 train acc 0.9589328092554138\n",
            "epoch 23 batch id 3381 loss 0.014064994640648365 train acc 0.9589618456078084\n",
            "epoch 23 batch id 3391 loss 0.10225501656532288 train acc 0.959004534060749\n",
            "epoch 23 batch id 3401 loss 0.28152936697006226 train acc 0.9590194060570421\n",
            "epoch 23 batch id 3411 loss 0.2594124674797058 train acc 0.9590341908531222\n",
            "epoch 23 batch id 3421 loss 0.07151250541210175 train acc 0.9590625913475592\n",
            "epoch 23 batch id 3431 loss 0.22234123945236206 train acc 0.9590361774992714\n",
            "epoch 23 batch id 3441 loss 0.08419719338417053 train acc 0.9590507846556233\n",
            "epoch 23 batch id 3451 loss 0.04307403042912483 train acc 0.9590471964647929\n",
            "epoch 23 batch id 3461 loss 0.059261422604322433 train acc 0.9590616873735914\n",
            "epoch 23 batch id 3471 loss 0.08388352394104004 train acc 0.9590850979544799\n",
            "epoch 23 batch id 3481 loss 0.2345132827758789 train acc 0.959081442114335\n",
            "epoch 23 batch id 3491 loss 0.03736116364598274 train acc 0.9590464766542538\n",
            "epoch 23 batch id 3501 loss 0.14676444232463837 train acc 0.9590697300771208\n",
            "epoch 23 batch id 3511 loss 0.0625266283750534 train acc 0.9590483480489889\n",
            "epoch 23 batch id 3521 loss 0.15923206508159637 train acc 0.9590759017324624\n",
            "epoch 23 batch id 3531 loss 0.13300064206123352 train acc 0.9590678986122911\n",
            "epoch 23 batch id 3541 loss 0.08139403164386749 train acc 0.9590511155040949\n",
            "epoch 23 batch id 3551 loss 0.06287769228219986 train acc 0.9590388270909603\n",
            "epoch 23 batch id 3561 loss 0.08402372896671295 train acc 0.959083649255827\n",
            "epoch 23 batch id 3571 loss 0.15994003415107727 train acc 0.9590932161859423\n",
            "epoch 23 batch id 3581 loss 0.284728080034256 train acc 0.9590852764590896\n",
            "epoch 23 batch id 3591 loss 0.056785471737384796 train acc 0.9590817321080479\n",
            "epoch 23 batch id 3601 loss 0.2333655208349228 train acc 0.9590521730074979\n",
            "epoch 23 batch id 3611 loss 0.005903781391680241 train acc 0.9590703752423152\n",
            "epoch 23 batch id 3621 loss 0.07130342721939087 train acc 0.9590625863021265\n",
            "epoch 23 batch id 3631 loss 0.16862866282463074 train acc 0.9590204144863674\n",
            "epoch 23 batch id 3641 loss 0.20748724043369293 train acc 0.9590385539686899\n",
            "epoch 23 batch id 3651 loss 0.20271116495132446 train acc 0.9589923993426458\n",
            "epoch 23 batch id 3661 loss 0.06272461265325546 train acc 0.9589763725757989\n",
            "epoch 23 batch id 3671 loss 0.08523567020893097 train acc 0.9590029964587305\n",
            "epoch 23 batch id 3681 loss 0.09522722661495209 train acc 0.9590252309155121\n",
            "epoch 23 batch id 3691 loss 0.08829399943351746 train acc 0.9590558114332159\n",
            "epoch 23 batch id 3701 loss 0.03827385976910591 train acc 0.9590608957038638\n",
            "epoch 23 batch id 3711 loss 0.1329905390739441 train acc 0.9590954257612503\n",
            "epoch 23 batch id 3721 loss 0.15748225152492523 train acc 0.9590793805428648\n",
            "epoch 23 batch id 3731 loss 0.18917657434940338 train acc 0.9590550455641919\n",
            "epoch 23 batch id 3741 loss 0.10617593675851822 train acc 0.959051724137931\n",
            "epoch 23 batch id 3751 loss 0.10289745032787323 train acc 0.959048420421221\n",
            "epoch 23 batch id 3761 loss 0.15766127407550812 train acc 0.9590202073916512\n",
            "epoch 23 batch id 3771 loss 0.05091811716556549 train acc 0.9590004309201803\n",
            "epoch 23 batch id 3781 loss 0.18943838775157928 train acc 0.9590055540862206\n",
            "epoch 23 batch id 3791 loss 0.03623479604721069 train acc 0.9590395014508045\n",
            "epoch 23 batch id 3801 loss 0.09025835990905762 train acc 0.9590650486714023\n",
            "epoch 23 batch id 3811 loss 0.10269998759031296 train acc 0.9590740619260036\n",
            "epoch 23 batch id 3821 loss 0.1361772119998932 train acc 0.959078938759487\n",
            "epoch 23 batch id 3831 loss 0.08297159522771835 train acc 0.95905931871574\n",
            "epoch 23 batch id 3841 loss 0.051215026527643204 train acc 0.9590438687841708\n",
            "epoch 23 batch id 3851 loss 0.05271254852414131 train acc 0.9590569008049857\n",
            "epoch 23 batch id 3861 loss 0.03885660693049431 train acc 0.9590415371665372\n",
            "epoch 23 batch id 3871 loss 0.16568614542484283 train acc 0.9590343257556188\n",
            "epoch 23 batch id 3881 loss 0.08298944681882858 train acc 0.9590352035557846\n",
            "epoch 23 batch id 3891 loss 0.14453233778476715 train acc 0.9589999357491648\n",
            "epoch 23 batch id 3901 loss 0.044145241379737854 train acc 0.9589688541399641\n",
            "epoch 23 batch id 3911 loss 0.034238167107105255 train acc 0.9589778828944004\n",
            "epoch 23 batch id 3921 loss 0.02317027933895588 train acc 0.9590067903596021\n",
            "epoch 23 batch id 3931 loss 0.22198070585727692 train acc 0.958991827779191\n",
            "epoch 23 batch id 3941 loss 0.16942507028579712 train acc 0.9589928000507485\n",
            "epoch 23 batch id 3951 loss 0.11755142360925674 train acc 0.9589739939255885\n",
            "epoch 23 batch id 3961 loss 0.11507181078195572 train acc 0.9589592274678111\n",
            "epoch 23 batch id 3971 loss 0.10098277032375336 train acc 0.9589484701586503\n",
            "epoch 23 batch id 3981 loss 0.04927949607372284 train acc 0.9589652411454408\n",
            "epoch 23 batch id 3991 loss 0.16958773136138916 train acc 0.958970182911551\n",
            "epoch 23 batch id 4001 loss 0.12289039045572281 train acc 0.958939952511872\n",
            "epoch 23 batch id 4011 loss 0.0780356302857399 train acc 0.9588787085514834\n",
            "epoch 23 batch id 4021 loss 0.04771466925740242 train acc 0.9588877144988809\n",
            "epoch 23 batch id 4031 loss 0.1929362714290619 train acc 0.9589160568097246\n",
            "epoch 23 batch id 4041 loss 0.08782574534416199 train acc 0.9589171925266023\n",
            "epoch 23 batch id 4051 loss 0.1200934424996376 train acc 0.9589298938533696\n",
            "epoch 23 batch id 4061 loss 0.23636487126350403 train acc 0.9589425326274317\n",
            "epoch 23 batch id 4071 loss 0.102389857172966 train acc 0.9589704618029968\n",
            "epoch 23 batch id 4081 loss 0.175750270485878 train acc 0.9589867679490321\n",
            "epoch 23 batch id 4091 loss 0.08696529269218445 train acc 0.9589953556587631\n",
            "epoch 23 batch id 4101 loss 0.055024851113557816 train acc 0.9590077115337723\n",
            "epoch 23 batch id 4111 loss 0.03205905109643936 train acc 0.9590048041838969\n",
            "epoch 23 batch id 4121 loss 0.09444823861122131 train acc 0.959009494054841\n",
            "epoch 23 batch id 4131 loss 0.10062666982412338 train acc 0.9590065964657468\n",
            "epoch 23 batch id 4141 loss 0.16931237280368805 train acc 0.9589961663849311\n",
            "epoch 23 batch id 4151 loss 0.06855618953704834 train acc 0.9590121356299687\n",
            "epoch 23 batch id 4161 loss 0.0883130133152008 train acc 0.9589867219418409\n",
            "epoch 23 batch id 4171 loss 0.2049969732761383 train acc 0.9589913989450971\n",
            "epoch 23 batch id 4181 loss 0.05596916005015373 train acc 0.9589736307103564\n",
            "epoch 23 batch id 4191 loss 0.21070623397827148 train acc 0.9589783166308757\n",
            "epoch 23 batch id 4201 loss 0.09241005033254623 train acc 0.9589941383004047\n",
            "epoch 23 batch id 4211 loss 0.24332162737846375 train acc 0.9589468059843268\n",
            "epoch 23 batch id 4221 loss 0.16344016790390015 train acc 0.9589404169628051\n",
            "epoch 23 batch id 4231 loss 0.20803429186344147 train acc 0.9589340581422832\n",
            "epoch 23 batch id 4241 loss 0.06631514430046082 train acc 0.9589682563074746\n",
            "epoch 23 batch id 4251 loss 0.14871206879615784 train acc 0.9589839155492825\n",
            "epoch 23 batch id 4261 loss 0.0529918372631073 train acc 0.959003168270359\n",
            "epoch 23 batch id 4271 loss 0.06470937281847 train acc 0.9590406228049637\n",
            "epoch 23 batch id 4281 loss 0.1334896832704544 train acc 0.9590596531184302\n",
            "epoch 23 batch id 4291 loss 0.01455689501017332 train acc 0.9590603880214402\n",
            "epoch 23 batch id 4301 loss 0.1876402348279953 train acc 0.9590175249941874\n",
            "epoch 23 batch id 4311 loss 0.1278829574584961 train acc 0.959032852006495\n",
            "epoch 23 batch id 4321 loss 0.062440596520900726 train acc 0.9590264117102523\n",
            "epoch 23 batch id 4331 loss 0.10989772528409958 train acc 0.959038039713692\n",
            "epoch 23 batch id 4341 loss 0.0932464525103569 train acc 0.9590568129463257\n",
            "epoch 23 batch id 4351 loss 0.17003291845321655 train acc 0.9590395886003218\n",
            "epoch 23 batch id 4361 loss 0.06095078960061073 train acc 0.9590511063976153\n",
            "epoch 23 batch id 4371 loss 0.11130881309509277 train acc 0.9590625714939374\n",
            "epoch 23 batch id 4381 loss 0.23421409726142883 train acc 0.9590383188769688\n",
            "epoch 23 batch id 4391 loss 0.08123424649238586 train acc 0.959060436119335\n",
            "epoch 23 batch id 4401 loss 0.07663388550281525 train acc 0.9590611508748011\n",
            "epoch 23 batch id 4411 loss 0.071533203125 train acc 0.9590512355474949\n",
            "epoch 23 batch id 4421 loss 0.17460322380065918 train acc 0.9590555021488351\n",
            "epoch 23 batch id 4431 loss 0.11465203762054443 train acc 0.9590562232001806\n",
            "epoch 23 batch id 4441 loss 0.04662591591477394 train acc 0.9590710144111687\n",
            "epoch 23 batch id 4451 loss 0.19965125620365143 train acc 0.9590681869242866\n",
            "epoch 23 batch id 4461 loss 0.0434001125395298 train acc 0.9590723772696704\n",
            "epoch 23 batch id 4471 loss 0.07626644521951675 train acc 0.9590730541265936\n",
            "epoch 23 batch id 4481 loss 0.15056687593460083 train acc 0.9590632671278733\n",
            "epoch 23 batch id 4491 loss 0.08834289014339447 train acc 0.9590361278111779\n",
            "epoch 23 batch id 4501 loss 0.01701822131872177 train acc 0.9590472950455454\n",
            "epoch 23 batch id 4511 loss 0.17751789093017578 train acc 0.958999528929284\n",
            "epoch 23 batch id 4521 loss 0.034160882234573364 train acc 0.9589865350586153\n",
            "epoch 23 batch id 4531 loss 0.10983867198228836 train acc 0.9589563562127565\n",
            "epoch 23 batch id 4541 loss 0.10137952864170074 train acc 0.9589676007487338\n",
            "epoch 23 batch id 4551 loss 0.1590225249528885 train acc 0.9589547626895187\n",
            "epoch 23 batch id 4561 loss 0.12151096016168594 train acc 0.958955684060513\n",
            "epoch 23 batch id 4571 loss 0.16170699894428253 train acc 0.958977111135419\n",
            "epoch 23 batch id 4581 loss 0.0779339149594307 train acc 0.9589472822527833\n",
            "epoch 23 batch id 4591 loss 0.07982797920703888 train acc 0.9589652308865171\n",
            "epoch 23 batch id 4601 loss 0.0237863902002573 train acc 0.9589593294935883\n",
            "epoch 23 batch id 4611 loss 0.10081057250499725 train acc 0.9589771741487747\n",
            "epoch 23 batch id 4621 loss 0.04333090782165527 train acc 0.9589949415710886\n",
            "epoch 23 batch id 4631 loss 0.09188491106033325 train acc 0.959019380263442\n",
            "epoch 23 batch id 4641 loss 0.11164002120494843 train acc 0.959036980176686\n",
            "epoch 23 batch id 4651 loss 0.3165731728076935 train acc 0.9590578639002365\n",
            "epoch 23 batch id 4661 loss 0.11700107902288437 train acc 0.9590551920188801\n",
            "epoch 23 batch id 4671 loss 0.01960578002035618 train acc 0.9590926728751873\n",
            "epoch 23 batch id 4681 loss 0.2806203067302704 train acc 0.9591066278572955\n",
            "epoch 23 batch id 4691 loss 0.11816180497407913 train acc 0.9590972074184608\n",
            "epoch 23 batch id 4701 loss 0.1872817724943161 train acc 0.9591110933843863\n",
            "epoch 23 batch id 4711 loss 0.1459735631942749 train acc 0.9591315538102314\n",
            "epoch 23 batch id 4721 loss 0.11410629749298096 train acc 0.9591055920355857\n",
            "epoch 23 batch id 4731 loss 0.0597333088517189 train acc 0.9590962534347918\n",
            "epoch 23 batch id 4741 loss 0.17244291305541992 train acc 0.9591001371018772\n",
            "epoch 23 batch id 4751 loss 0.23208265006542206 train acc 0.9591105819827405\n",
            "epoch 23 batch id 4761 loss 0.11350759863853455 train acc 0.9591144192396556\n",
            "epoch 23 batch id 4771 loss 0.15797732770442963 train acc 0.9591280653950953\n",
            "epoch 23 batch id 4781 loss 0.31975099444389343 train acc 0.959108973018197\n",
            "epoch 23 batch id 4791 loss 0.26447710394859314 train acc 0.9590997443122521\n",
            "epoch 23 batch id 4801 loss 0.08008716255426407 train acc 0.9591035721724641\n",
            "epoch 23 batch id 4811 loss 0.07776838541030884 train acc 0.9590976408231137\n",
            "epoch 23 batch id 4821 loss 0.16176386177539825 train acc 0.9590982161377307\n",
            "epoch 23 batch id 4831 loss 0.07047634571790695 train acc 0.9591278979507348\n",
            "epoch 23 batch id 4841 loss 0.15596164762973785 train acc 0.9591219531088618\n",
            "epoch 23 batch id 4851 loss 0.13821187615394592 train acc 0.9591321377035663\n",
            "epoch 23 batch id 4861 loss 0.050424642860889435 train acc 0.9591326373174244\n",
            "epoch 23 batch id 4871 loss 0.14258524775505066 train acc 0.959149173680969\n",
            "epoch 23 batch id 4881 loss 0.14009375870227814 train acc 0.9591368315918869\n",
            "epoch 23 batch id 4891 loss 0.08129199594259262 train acc 0.959124539971376\n",
            "epoch 23 batch id 4901 loss 0.012035350315272808 train acc 0.959125051009998\n",
            "epoch 23 batch id 4911 loss 0.187176913022995 train acc 0.9591191967012829\n",
            "epoch 23 batch id 4921 loss 0.196182519197464 train acc 0.9591101910180857\n",
            "epoch 23 batch id 4931 loss 0.10589379817247391 train acc 0.9590853782194281\n",
            "epoch 23 batch id 4941 loss 0.09492576122283936 train acc 0.9590828020643595\n",
            "epoch 23 batch id 4951 loss 0.22254323959350586 train acc 0.9590802363158958\n",
            "epoch 23 train acc 0.9590796188126982\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe0639b7018b4743a9d1f9c567014fb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 23 loss 0.7193415760993958 test acc 0.8327643878299121\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfa6d245458745f2b9120bf95018429d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 24 batch id 1 loss 0.11357512325048447 train acc 0.984375\n",
            "epoch 24 batch id 11 loss 0.07443581521511078 train acc 0.9545454545454546\n",
            "epoch 24 batch id 21 loss 0.06431832909584045 train acc 0.9598214285714286\n",
            "epoch 24 batch id 31 loss 0.06997425854206085 train acc 0.9556451612903226\n",
            "epoch 24 batch id 41 loss 0.04134758561849594 train acc 0.9576981707317073\n",
            "epoch 24 batch id 51 loss 0.19695381820201874 train acc 0.9564950980392157\n",
            "epoch 24 batch id 61 loss 0.09756364673376083 train acc 0.9541495901639344\n",
            "epoch 24 batch id 71 loss 0.19252505898475647 train acc 0.9557658450704225\n",
            "epoch 24 batch id 81 loss 0.09050536155700684 train acc 0.9562114197530864\n",
            "epoch 24 batch id 91 loss 0.18848249316215515 train acc 0.9562156593406593\n",
            "epoch 24 batch id 101 loss 0.07936515659093857 train acc 0.9551361386138614\n",
            "epoch 24 batch id 111 loss 0.008811648935079575 train acc 0.9550957207207207\n",
            "epoch 24 batch id 121 loss 0.2074483036994934 train acc 0.9559659090909091\n",
            "epoch 24 batch id 131 loss 0.14327245950698853 train acc 0.955868320610687\n",
            "epoch 24 batch id 141 loss 0.07067341357469559 train acc 0.9554521276595744\n",
            "epoch 24 batch id 151 loss 0.10844417661428452 train acc 0.9557119205298014\n",
            "epoch 24 batch id 161 loss 0.13033026456832886 train acc 0.9567158385093167\n",
            "epoch 24 batch id 171 loss 0.22821153700351715 train acc 0.9567799707602339\n",
            "epoch 24 batch id 181 loss 0.051712170243263245 train acc 0.9576139502762431\n",
            "epoch 24 batch id 191 loss 0.08108227699995041 train acc 0.9577879581151832\n",
            "epoch 24 batch id 201 loss 0.09628701210021973 train acc 0.9582555970149254\n",
            "epoch 24 batch id 211 loss 0.12325587868690491 train acc 0.9583086492890995\n",
            "epoch 24 batch id 221 loss 0.2773544490337372 train acc 0.957649886877828\n",
            "epoch 24 batch id 231 loss 0.1712922751903534 train acc 0.958400974025974\n",
            "epoch 24 batch id 241 loss 0.09374874085187912 train acc 0.9581172199170125\n",
            "epoch 24 batch id 251 loss 0.2146272361278534 train acc 0.9581673306772909\n",
            "epoch 24 batch id 261 loss 0.09593139588832855 train acc 0.9578544061302682\n",
            "epoch 24 batch id 271 loss 0.04326759651303291 train acc 0.9581411439114391\n",
            "epoch 24 batch id 281 loss 0.06634669005870819 train acc 0.958185053380783\n",
            "epoch 24 batch id 291 loss 0.11556396633386612 train acc 0.9587091924398625\n",
            "epoch 24 batch id 301 loss 0.13844993710517883 train acc 0.9588870431893688\n",
            "epoch 24 batch id 311 loss 0.05165256932377815 train acc 0.9588022508038585\n",
            "epoch 24 batch id 321 loss 0.20691554248332977 train acc 0.9586740654205608\n",
            "epoch 24 batch id 331 loss 0.09614642709493637 train acc 0.9588368580060423\n",
            "epoch 24 batch id 341 loss 0.2501659393310547 train acc 0.9587609970674487\n",
            "epoch 24 batch id 351 loss 0.045902833342552185 train acc 0.9588230056980057\n",
            "epoch 24 batch id 361 loss 0.11197572946548462 train acc 0.9588815789473685\n",
            "epoch 24 batch id 371 loss 0.10583067685365677 train acc 0.9591475741239892\n",
            "epoch 24 batch id 381 loss 0.01965429075062275 train acc 0.9594406167979003\n",
            "epoch 24 batch id 391 loss 0.18307416141033173 train acc 0.9596787084398977\n",
            "epoch 24 batch id 401 loss 0.009548541158437729 train acc 0.9599438902743143\n",
            "epoch 24 batch id 411 loss 0.07817798107862473 train acc 0.9601201338199513\n",
            "epoch 24 batch id 421 loss 0.1121269017457962 train acc 0.9603251187648456\n",
            "epoch 24 batch id 431 loss 0.22115911543369293 train acc 0.9602668213457076\n",
            "epoch 24 batch id 441 loss 0.1706308275461197 train acc 0.9601403061224489\n",
            "epoch 24 batch id 451 loss 0.17642778158187866 train acc 0.9601579822616408\n",
            "epoch 24 batch id 461 loss 0.1109752506017685 train acc 0.9599715292841648\n",
            "epoch 24 batch id 471 loss 0.04422420263290405 train acc 0.9602906050955414\n",
            "epoch 24 batch id 481 loss 0.34006720781326294 train acc 0.9604015072765073\n",
            "epoch 24 batch id 491 loss 0.03362433612346649 train acc 0.9605397148676171\n",
            "epoch 24 batch id 501 loss 0.24746935069561005 train acc 0.9606724051896207\n",
            "epoch 24 batch id 511 loss 0.12391161173582077 train acc 0.9606470156555773\n",
            "epoch 24 batch id 521 loss 0.132781982421875 train acc 0.9607725527831094\n",
            "epoch 24 batch id 531 loss 0.11388564109802246 train acc 0.9606285310734464\n",
            "epoch 24 batch id 541 loss 0.052161239087581635 train acc 0.9605187153419593\n",
            "epoch 24 batch id 551 loss 0.1289086639881134 train acc 0.9605830308529946\n",
            "epoch 24 batch id 561 loss 0.06658123433589935 train acc 0.9605336452762924\n",
            "epoch 24 batch id 571 loss 0.17245818674564362 train acc 0.9605680823117339\n",
            "epoch 24 batch id 581 loss 0.27339011430740356 train acc 0.9604399741824441\n",
            "epoch 24 batch id 591 loss 0.21257904171943665 train acc 0.9603690778341794\n",
            "epoch 24 batch id 601 loss 0.0754489079117775 train acc 0.96045653078203\n",
            "epoch 24 batch id 611 loss 0.13990414142608643 train acc 0.9606178396072013\n",
            "epoch 24 batch id 621 loss 0.2662210166454315 train acc 0.9607487922705314\n",
            "epoch 24 batch id 631 loss 0.14708766341209412 train acc 0.960627971473851\n",
            "epoch 24 batch id 641 loss 0.1557084321975708 train acc 0.9605352964118564\n",
            "epoch 24 batch id 651 loss 0.01854686439037323 train acc 0.9605414746543779\n",
            "epoch 24 batch id 661 loss 0.037249185144901276 train acc 0.9608074886535553\n",
            "epoch 24 batch id 671 loss 0.07164092361927032 train acc 0.9608792846497765\n",
            "epoch 24 batch id 681 loss 0.14401577413082123 train acc 0.9609489720998532\n",
            "epoch 24 batch id 691 loss 0.25360697507858276 train acc 0.9607226845151954\n",
            "epoch 24 batch id 701 loss 0.1794663667678833 train acc 0.9605028530670471\n",
            "epoch 24 batch id 711 loss 0.18742488324642181 train acc 0.9603551336146273\n",
            "epoch 24 batch id 721 loss 0.12275926023721695 train acc 0.9602765256588072\n",
            "epoch 24 batch id 731 loss 0.08321824669837952 train acc 0.9602641928864569\n",
            "epoch 24 batch id 741 loss 0.07465118914842606 train acc 0.9602943657219973\n",
            "epoch 24 batch id 751 loss 0.13940148055553436 train acc 0.9601364846870839\n",
            "epoch 24 batch id 761 loss 0.02589905634522438 train acc 0.9602291392904073\n",
            "epoch 24 batch id 771 loss 0.11025535315275192 train acc 0.9600356679636836\n",
            "epoch 24 batch id 781 loss 0.16432452201843262 train acc 0.9599871959026889\n",
            "epoch 24 batch id 791 loss 0.03318491205573082 train acc 0.9601967446270544\n",
            "epoch 24 batch id 801 loss 0.046993281692266464 train acc 0.9602450062421972\n",
            "epoch 24 batch id 811 loss 0.17889909446239471 train acc 0.9603884093711468\n",
            "epoch 24 batch id 821 loss 0.03983180969953537 train acc 0.960376065773447\n",
            "epoch 24 batch id 831 loss 0.16332289576530457 train acc 0.9603264139590855\n",
            "epoch 24 batch id 841 loss 0.02462790347635746 train acc 0.9605008917954816\n",
            "epoch 24 batch id 851 loss 0.13780270516872406 train acc 0.9604509400705052\n",
            "epoch 24 batch id 861 loss 0.18029488623142242 train acc 0.9605110336817654\n",
            "epoch 24 batch id 871 loss 0.042301781475543976 train acc 0.9605338691159586\n",
            "epoch 24 batch id 881 loss 0.1659764051437378 train acc 0.9605029795686719\n",
            "epoch 24 batch id 891 loss 0.04318467527627945 train acc 0.9605253928170595\n",
            "epoch 24 batch id 901 loss 0.13087502121925354 train acc 0.9605299667036626\n",
            "epoch 24 batch id 911 loss 0.07660951465368271 train acc 0.9605515916575192\n",
            "epoch 24 batch id 921 loss 0.1976141631603241 train acc 0.9604030944625407\n",
            "epoch 24 batch id 931 loss 0.138294979929924 train acc 0.9603417024704619\n",
            "epoch 24 batch id 941 loss 0.025670824572443962 train acc 0.9605638947927736\n",
            "epoch 24 batch id 951 loss 0.047467853873968124 train acc 0.960419952681388\n",
            "epoch 24 batch id 961 loss 0.11818710714578629 train acc 0.9603765608740895\n",
            "epoch 24 batch id 971 loss 0.160640150308609 train acc 0.9603018795056643\n",
            "epoch 24 batch id 981 loss 0.026627955958247185 train acc 0.9603561416921509\n",
            "epoch 24 batch id 991 loss 0.10263176262378693 train acc 0.9604881432896064\n",
            "epoch 24 batch id 1001 loss 0.10085532069206238 train acc 0.9605706793206793\n",
            "epoch 24 batch id 1011 loss 0.11136945337057114 train acc 0.9605433976261127\n",
            "epoch 24 batch id 1021 loss 0.16453120112419128 train acc 0.9605319539666993\n",
            "epoch 24 batch id 1031 loss 0.11002539843320847 train acc 0.9605965082444229\n",
            "epoch 24 batch id 1041 loss 0.09395948052406311 train acc 0.960494716618636\n",
            "epoch 24 batch id 1051 loss 0.08452732115983963 train acc 0.9603948620361561\n",
            "epoch 24 batch id 1061 loss 0.2073604166507721 train acc 0.9603557964184731\n",
            "epoch 24 batch id 1071 loss 0.0561312660574913 train acc 0.9604487628384687\n",
            "epoch 24 batch id 1081 loss 0.09795469045639038 train acc 0.9603810129509713\n",
            "epoch 24 batch id 1091 loss 0.2999829351902008 train acc 0.9603288267644363\n",
            "epoch 24 batch id 1101 loss 0.06805375963449478 train acc 0.9603201634877384\n",
            "epoch 24 batch id 1111 loss 0.1669721007347107 train acc 0.9602694644464447\n",
            "epoch 24 batch id 1121 loss 0.06166977062821388 train acc 0.9604287466547725\n",
            "epoch 24 batch id 1131 loss 0.11106184870004654 train acc 0.9602812776304156\n",
            "epoch 24 batch id 1141 loss 0.0710168406367302 train acc 0.9603828878177038\n",
            "epoch 24 batch id 1151 loss 0.09219908714294434 train acc 0.9603605560382277\n",
            "epoch 24 batch id 1161 loss 0.13026662170886993 train acc 0.9603520671834626\n",
            "epoch 24 batch id 1171 loss 0.09375890344381332 train acc 0.9603170367207515\n",
            "epoch 24 batch id 1181 loss 0.049189455807209015 train acc 0.9603884419983065\n",
            "epoch 24 batch id 1191 loss 0.06447799503803253 train acc 0.9603930520570949\n",
            "epoch 24 batch id 1201 loss 0.11369652301073074 train acc 0.9605797252289758\n",
            "epoch 24 batch id 1211 loss 0.010852954350411892 train acc 0.9607117052023122\n",
            "epoch 24 batch id 1221 loss 0.12052692472934723 train acc 0.9607391482391482\n",
            "epoch 24 batch id 1231 loss 0.07180139422416687 train acc 0.9606772948822095\n",
            "epoch 24 batch id 1241 loss 0.18129760026931763 train acc 0.9606668009669621\n",
            "epoch 24 batch id 1251 loss 0.1599145233631134 train acc 0.9606314948041567\n",
            "epoch 24 batch id 1261 loss 0.11012192070484161 train acc 0.9606463124504362\n",
            "epoch 24 batch id 1271 loss 0.15820284187793732 train acc 0.9606977773406766\n",
            "epoch 24 batch id 1281 loss 0.11408662050962448 train acc 0.9607118462138954\n",
            "epoch 24 batch id 1291 loss 0.08228781074285507 train acc 0.9607620061967467\n",
            "epoch 24 batch id 1301 loss 0.07520102709531784 train acc 0.9606432551883167\n",
            "epoch 24 batch id 1311 loss 0.0714912936091423 train acc 0.9607050915331807\n",
            "epoch 24 batch id 1321 loss 0.1228388249874115 train acc 0.9607541635124905\n",
            "epoch 24 batch id 1331 loss 0.15756691992282867 train acc 0.960673365890308\n",
            "epoch 24 batch id 1341 loss 0.19817224144935608 train acc 0.9606520320656227\n",
            "epoch 24 batch id 1351 loss 0.10040261596441269 train acc 0.9607004071058475\n",
            "epoch 24 batch id 1361 loss 0.08998911082744598 train acc 0.960702149155033\n",
            "epoch 24 batch id 1371 loss 0.03686593472957611 train acc 0.9607494529540481\n",
            "epoch 24 batch id 1381 loss 0.12750092148780823 train acc 0.9607508146270818\n",
            "epoch 24 batch id 1391 loss 0.04260987788438797 train acc 0.9607296908698778\n",
            "epoch 24 batch id 1401 loss 0.054607246071100235 train acc 0.9607757851534618\n",
            "epoch 24 batch id 1411 loss 0.08096151053905487 train acc 0.9608433734939759\n",
            "epoch 24 batch id 1421 loss 0.03735249489545822 train acc 0.960811048557354\n",
            "epoch 24 batch id 1431 loss 0.08889699727296829 train acc 0.960691823899371\n",
            "epoch 24 batch id 1441 loss 0.16729465126991272 train acc 0.9606284698126302\n",
            "epoch 24 batch id 1451 loss 0.05163427069783211 train acc 0.9606521364576155\n",
            "epoch 24 batch id 1461 loss 0.0652519166469574 train acc 0.9606433949349761\n",
            "epoch 24 batch id 1471 loss 0.159296914935112 train acc 0.9606453942895989\n",
            "epoch 24 batch id 1481 loss 0.10900961607694626 train acc 0.9605524139095206\n",
            "epoch 24 batch id 1491 loss 0.08679543435573578 train acc 0.9606073943661971\n",
            "epoch 24 batch id 1501 loss 0.207515150308609 train acc 0.9606095936042638\n",
            "epoch 24 batch id 1511 loss 0.04289288818836212 train acc 0.9606221045665122\n",
            "epoch 24 batch id 1521 loss 0.08895090967416763 train acc 0.9606652695595004\n",
            "epoch 24 batch id 1531 loss 0.0761549100279808 train acc 0.9606874591770085\n",
            "epoch 24 batch id 1541 loss 0.03415742143988609 train acc 0.9607499188838416\n",
            "epoch 24 batch id 1551 loss 0.07201080024242401 train acc 0.9606302385557705\n",
            "epoch 24 batch id 1561 loss 0.10124820470809937 train acc 0.9606021780909674\n",
            "epoch 24 batch id 1571 loss 0.13389995694160461 train acc 0.9606540420114577\n",
            "epoch 24 batch id 1581 loss 0.18383942544460297 train acc 0.960655834914611\n",
            "epoch 24 batch id 1591 loss 0.2556336522102356 train acc 0.9606870678818353\n",
            "epoch 24 batch id 1601 loss 0.1489463448524475 train acc 0.9607764678326046\n",
            "epoch 24 batch id 1611 loss 0.0656789019703865 train acc 0.9607095747982619\n",
            "epoch 24 batch id 1621 loss 0.10293218493461609 train acc 0.9606917026526836\n",
            "epoch 24 batch id 1631 loss 0.0017010994488373399 train acc 0.9607602697731453\n",
            "epoch 24 batch id 1641 loss 0.06535811722278595 train acc 0.9607042199878123\n",
            "epoch 24 batch id 1651 loss 0.04554377496242523 train acc 0.9607056329497274\n",
            "epoch 24 batch id 1661 loss 0.06553398072719574 train acc 0.9607258428657435\n",
            "epoch 24 batch id 1671 loss 0.15299955010414124 train acc 0.9606803560742071\n",
            "epoch 24 batch id 1681 loss 0.06960760056972504 train acc 0.9606447055324212\n",
            "epoch 24 batch id 1691 loss 0.0741015374660492 train acc 0.9606279568302779\n",
            "epoch 24 batch id 1701 loss 0.041944731026887894 train acc 0.9606940770135215\n",
            "epoch 24 batch id 1711 loss 0.12449084222316742 train acc 0.9606681034482759\n",
            "epoch 24 batch id 1721 loss 0.14434897899627686 train acc 0.9607423009877978\n",
            "epoch 24 batch id 1731 loss 0.019773006439208984 train acc 0.9608066146735991\n",
            "epoch 24 batch id 1741 loss 0.08557496219873428 train acc 0.9607176191843768\n",
            "epoch 24 batch id 1751 loss 0.07047725468873978 train acc 0.9607277984009137\n",
            "epoch 24 batch id 1761 loss 0.12471367418766022 train acc 0.9607289892106757\n",
            "epoch 24 batch id 1771 loss 0.017409512773156166 train acc 0.9607566346696782\n",
            "epoch 24 batch id 1781 loss 0.09752947092056274 train acc 0.9608102891633914\n",
            "epoch 24 batch id 1791 loss 0.14639736711978912 train acc 0.960802275265215\n",
            "epoch 24 batch id 1801 loss 0.11464264988899231 train acc 0.9608203775680177\n",
            "epoch 24 batch id 1811 loss 0.09959974884986877 train acc 0.9608555356156819\n",
            "epoch 24 batch id 1821 loss 0.23259992897510529 train acc 0.9608216639209226\n",
            "epoch 24 batch id 1831 loss 0.06821871548891068 train acc 0.960813762971054\n",
            "epoch 24 batch id 1841 loss 0.05410224199295044 train acc 0.960805947854427\n",
            "epoch 24 batch id 1851 loss 0.09401261806488037 train acc 0.9608404240950837\n",
            "epoch 24 batch id 1861 loss 0.049924228340387344 train acc 0.9608493417517464\n",
            "epoch 24 batch id 1871 loss 0.266849547624588 train acc 0.9608915686798504\n",
            "epoch 24 batch id 1881 loss 0.05635800585150719 train acc 0.9609001196172249\n",
            "epoch 24 batch id 1891 loss 0.07328197360038757 train acc 0.9609581570597567\n",
            "epoch 24 batch id 1901 loss 0.10085979104042053 train acc 0.9609744871120462\n",
            "epoch 24 batch id 1911 loss 0.0688643828034401 train acc 0.9609088827838828\n",
            "epoch 24 batch id 1921 loss 0.1384962946176529 train acc 0.960852095262884\n",
            "epoch 24 batch id 1931 loss 0.12036890536546707 train acc 0.9608120792335577\n",
            "epoch 24 batch id 1941 loss 0.029304208233952522 train acc 0.9608127253992788\n",
            "epoch 24 batch id 1951 loss 0.14904910326004028 train acc 0.960845399794977\n",
            "epoch 24 batch id 1961 loss 0.05177218094468117 train acc 0.960853837327894\n",
            "epoch 24 batch id 1971 loss 0.028082439675927162 train acc 0.9609176813800101\n",
            "epoch 24 batch id 1981 loss 0.0347074419260025 train acc 0.9609256688541141\n",
            "epoch 24 batch id 1991 loss 0.10388893634080887 train acc 0.9609335760924159\n",
            "epoch 24 batch id 2001 loss 0.16238708794116974 train acc 0.960933595702149\n",
            "epoch 24 batch id 2011 loss 0.11127471178770065 train acc 0.9609180755842864\n",
            "epoch 24 batch id 2021 loss 0.18905800580978394 train acc 0.9609027090549233\n",
            "epoch 24 batch id 2031 loss 0.007835174910724163 train acc 0.9608259478089611\n",
            "epoch 24 batch id 2041 loss 0.05035337433218956 train acc 0.9608035276825085\n",
            "epoch 24 batch id 2051 loss 0.3788716495037079 train acc 0.9607889444173574\n",
            "epoch 24 batch id 2061 loss 0.06550654768943787 train acc 0.960782083939835\n",
            "epoch 24 batch id 2071 loss 0.20022781193256378 train acc 0.9607526557218735\n",
            "epoch 24 batch id 2081 loss 0.12435076385736465 train acc 0.9607685607880827\n",
            "epoch 24 batch id 2091 loss 0.04974978044629097 train acc 0.9607469512195121\n",
            "epoch 24 batch id 2101 loss 0.05181271210312843 train acc 0.9608147905759162\n",
            "epoch 24 batch id 2111 loss 0.02464890480041504 train acc 0.9608227735670298\n",
            "epoch 24 batch id 2121 loss 0.036117903888225555 train acc 0.9608380480905233\n",
            "epoch 24 batch id 2131 loss 0.12587618827819824 train acc 0.9608311825434068\n",
            "epoch 24 batch id 2141 loss 0.13805678486824036 train acc 0.9607878911723494\n",
            "epoch 24 batch id 2151 loss 0.07963945716619492 train acc 0.9608394351464435\n",
            "epoch 24 batch id 2161 loss 0.2112368941307068 train acc 0.9608109671448404\n",
            "epoch 24 batch id 2171 loss 0.200723335146904 train acc 0.9607539728235837\n",
            "epoch 24 batch id 2181 loss 0.1251809448003769 train acc 0.9607619784502521\n",
            "epoch 24 batch id 2191 loss 0.14582662284374237 train acc 0.9607770424463715\n",
            "epoch 24 batch id 2201 loss 0.08120559900999069 train acc 0.9607706724216265\n",
            "epoch 24 batch id 2211 loss 0.10069186985492706 train acc 0.9607855608322027\n",
            "epoch 24 batch id 2221 loss 0.10569935292005539 train acc 0.9607721746960829\n",
            "epoch 24 batch id 2231 loss 0.2582891881465912 train acc 0.9607589085611833\n",
            "epoch 24 batch id 2241 loss 0.11514219641685486 train acc 0.960745760821062\n",
            "epoch 24 batch id 2251 loss 0.027805300429463387 train acc 0.9607535539760107\n",
            "epoch 24 batch id 2261 loss 0.05347049981355667 train acc 0.9607750995134896\n",
            "epoch 24 batch id 2271 loss 0.08514314144849777 train acc 0.9608170959929546\n",
            "epoch 24 batch id 2281 loss 0.12033544480800629 train acc 0.9608587242437527\n",
            "epoch 24 batch id 2291 loss 0.2093573957681656 train acc 0.9608999890877347\n",
            "epoch 24 batch id 2301 loss 0.04641343280673027 train acc 0.9609137331594959\n",
            "epoch 24 batch id 2311 loss 0.06850066781044006 train acc 0.9609205971440935\n",
            "epoch 24 batch id 2321 loss 0.17360810935497284 train acc 0.9609004739336493\n",
            "epoch 24 batch id 2331 loss 0.2009802907705307 train acc 0.9608738202488203\n",
            "epoch 24 batch id 2341 loss 0.16295939683914185 train acc 0.9609007902605724\n",
            "epoch 24 batch id 2351 loss 0.06237711012363434 train acc 0.9608544236495108\n",
            "epoch 24 batch id 2361 loss 0.20083510875701904 train acc 0.9608547755188479\n",
            "epoch 24 batch id 2371 loss 0.10849031060934067 train acc 0.9608880746520455\n",
            "epoch 24 batch id 2381 loss 0.07684847712516785 train acc 0.96087515749685\n",
            "epoch 24 batch id 2391 loss 0.16406255960464478 train acc 0.9608623483897951\n",
            "epoch 24 batch id 2401 loss 0.08349177241325378 train acc 0.9608561536859642\n",
            "epoch 24 batch id 2411 loss 0.06250443309545517 train acc 0.9608564910825383\n",
            "epoch 24 batch id 2421 loss 0.012794507667422295 train acc 0.9608890954151177\n",
            "epoch 24 batch id 2431 loss 0.17980436980724335 train acc 0.9609021493212669\n",
            "epoch 24 batch id 2441 loss 0.11557988822460175 train acc 0.9608446845555101\n",
            "epoch 24 batch id 2451 loss 0.013009628280997276 train acc 0.9608769379844961\n",
            "epoch 24 batch id 2461 loss 0.141032874584198 train acc 0.9608771840715157\n",
            "epoch 24 batch id 2471 loss 0.13734661042690277 train acc 0.9608521347632537\n",
            "epoch 24 batch id 2481 loss 0.25338730216026306 train acc 0.9608902660217654\n",
            "epoch 24 batch id 2491 loss 0.15921232104301453 train acc 0.9608340024086712\n",
            "epoch 24 batch id 2501 loss 0.020151620730757713 train acc 0.9608531587365055\n",
            "epoch 24 batch id 2511 loss 0.0426965169608593 train acc 0.9608348267622461\n",
            "epoch 24 batch id 2521 loss 0.21677741408348083 train acc 0.9607856505355018\n",
            "epoch 24 batch id 2531 loss 0.06958920508623123 train acc 0.9607985973923351\n",
            "epoch 24 batch id 2541 loss 0.05718399956822395 train acc 0.9607622491145218\n",
            "epoch 24 batch id 2551 loss 0.0735204815864563 train acc 0.9607445609564876\n",
            "epoch 24 batch id 2561 loss 0.061996981501579285 train acc 0.960720909800859\n",
            "epoch 24 batch id 2571 loss 0.1703442931175232 train acc 0.9607095974329055\n",
            "epoch 24 batch id 2581 loss 0.054968759417533875 train acc 0.9607286419992251\n",
            "epoch 24 batch id 2591 loss 0.11881999671459198 train acc 0.9606992956387496\n",
            "epoch 24 batch id 2601 loss 0.08130160719156265 train acc 0.9607182333717801\n",
            "epoch 24 batch id 2611 loss 0.057605352252721786 train acc 0.9607669475296822\n",
            "epoch 24 batch id 2621 loss 0.09836560487747192 train acc 0.9607497138496757\n",
            "epoch 24 batch id 2631 loss 0.14465859532356262 train acc 0.9607326111744584\n",
            "epoch 24 batch id 2641 loss 0.0509813129901886 train acc 0.9607511359333586\n",
            "epoch 24 batch id 2651 loss 0.17458310723304749 train acc 0.96066932289702\n",
            "epoch 24 batch id 2661 loss 0.053823843598365784 train acc 0.9606879462608042\n",
            "epoch 24 batch id 2671 loss 0.1485580950975418 train acc 0.9606947304380382\n",
            "epoch 24 batch id 2681 loss 0.06907915323972702 train acc 0.9606723237597912\n",
            "epoch 24 batch id 2691 loss 0.12673066556453705 train acc 0.9606152452619844\n",
            "epoch 24 batch id 2701 loss 0.07683712989091873 train acc 0.9606742873009997\n",
            "epoch 24 batch id 2711 loss 0.007952405139803886 train acc 0.960681021763187\n",
            "epoch 24 batch id 2721 loss 0.08302506804466248 train acc 0.96070493384785\n",
            "epoch 24 batch id 2731 loss 0.06971313059329987 train acc 0.9607286708165507\n",
            "epoch 24 batch id 2741 loss 0.06607916951179504 train acc 0.9607750364830354\n",
            "epoch 24 batch id 2751 loss 0.24430230259895325 train acc 0.9607585877862596\n",
            "epoch 24 batch id 2761 loss 0.14439702033996582 train acc 0.9607875316914162\n",
            "epoch 24 batch id 2771 loss 0.0052238223142921925 train acc 0.9608331829664382\n",
            "epoch 24 batch id 2781 loss 0.09095094352960587 train acc 0.960839176555196\n",
            "epoch 24 batch id 2791 loss 0.09940601140260696 train acc 0.9608339304908635\n",
            "epoch 24 batch id 2801 loss 0.2155950516462326 train acc 0.9608287218850411\n",
            "epoch 24 batch id 2811 loss 0.05867666006088257 train acc 0.9608791355389541\n",
            "epoch 24 batch id 2821 loss 0.07501353323459625 train acc 0.9609070365118753\n",
            "epoch 24 batch id 2831 loss 0.15681815147399902 train acc 0.9609016248675379\n",
            "epoch 24 batch id 2841 loss 0.27892324328422546 train acc 0.9608907514959522\n",
            "epoch 24 batch id 2851 loss 0.025451676920056343 train acc 0.9608909154682568\n",
            "epoch 24 batch id 2861 loss 0.07635750621557236 train acc 0.9608637714085984\n",
            "epoch 24 batch id 2871 loss 0.14034177362918854 train acc 0.9608857976314873\n",
            "epoch 24 batch id 2881 loss 0.15248428285121918 train acc 0.960853436306838\n",
            "epoch 24 batch id 2891 loss 0.04498393461108208 train acc 0.9608861553095814\n",
            "epoch 24 batch id 2901 loss 0.04131309688091278 train acc 0.9608594019303688\n",
            "epoch 24 batch id 2911 loss 0.03980793431401253 train acc 0.9608972432153899\n",
            "epoch 24 batch id 2921 loss 0.10589625686407089 train acc 0.9608759842519685\n",
            "epoch 24 batch id 2931 loss 0.01389426738023758 train acc 0.9608975179119754\n",
            "epoch 24 batch id 2941 loss 0.20444521307945251 train acc 0.9608232743964638\n",
            "epoch 24 batch id 2951 loss 0.11227153986692429 train acc 0.9608713148085395\n",
            "epoch 24 batch id 2961 loss 0.027428677305579185 train acc 0.9608557075312395\n",
            "epoch 24 batch id 2971 loss 0.14062893390655518 train acc 0.9608244278020869\n",
            "epoch 24 batch id 2981 loss 0.05949089303612709 train acc 0.9608195655820194\n",
            "epoch 24 batch id 2991 loss 0.06275445222854614 train acc 0.9608513039117352\n",
            "epoch 24 batch id 3001 loss 0.015066967345774174 train acc 0.9608255581472842\n",
            "epoch 24 batch id 3011 loss 0.05989539995789528 train acc 0.9608103620059781\n",
            "epoch 24 batch id 3021 loss 0.09674789756536484 train acc 0.9608159549817941\n",
            "epoch 24 batch id 3031 loss 0.18526127934455872 train acc 0.9608112009237876\n",
            "epoch 24 batch id 3041 loss 0.39116930961608887 train acc 0.9607653732324893\n",
            "epoch 24 batch id 3051 loss 0.04292949289083481 train acc 0.9607710586692888\n",
            "epoch 24 batch id 3061 loss 0.08938685804605484 train acc 0.9608022296635087\n",
            "epoch 24 batch id 3071 loss 0.1077590137720108 train acc 0.9607874063822859\n",
            "epoch 24 batch id 3081 loss 0.2220296561717987 train acc 0.9607980363518338\n",
            "epoch 24 batch id 3091 loss 0.022409766912460327 train acc 0.9608288175347783\n",
            "epoch 24 batch id 3101 loss 0.10535062849521637 train acc 0.9608090132215414\n",
            "epoch 24 batch id 3111 loss 0.08422782272100449 train acc 0.9607893362262938\n",
            "epoch 24 batch id 3121 loss 0.06242690607905388 train acc 0.9607998237744313\n",
            "epoch 24 batch id 3131 loss 0.0313941091299057 train acc 0.9608651389332482\n",
            "epoch 24 batch id 3141 loss 0.15103717148303986 train acc 0.9608653693091372\n",
            "epoch 24 batch id 3151 loss 0.043545421212911606 train acc 0.9608854331958109\n",
            "epoch 24 batch id 3161 loss 0.0828004702925682 train acc 0.960895484024043\n",
            "epoch 24 batch id 3171 loss 0.04445168748497963 train acc 0.9608956165247556\n",
            "epoch 24 batch id 3181 loss 0.0935036838054657 train acc 0.9608908362150267\n",
            "epoch 24 batch id 3191 loss 0.05215015634894371 train acc 0.960910568787214\n",
            "epoch 24 batch id 3201 loss 0.12819325923919678 train acc 0.96091553420806\n",
            "epoch 24 batch id 3211 loss 0.19239024817943573 train acc 0.960862075677359\n",
            "epoch 24 batch id 3221 loss 0.2633764147758484 train acc 0.9608380549518784\n",
            "epoch 24 batch id 3231 loss 0.05966067314147949 train acc 0.9608383627359951\n",
            "epoch 24 batch id 3241 loss 0.023398566991090775 train acc 0.960872415921012\n",
            "epoch 24 batch id 3251 loss 0.24916762113571167 train acc 0.9608581974776992\n",
            "epoch 24 batch id 3261 loss 0.10486005246639252 train acc 0.9608344832873352\n",
            "epoch 24 batch id 3271 loss 0.10303255915641785 train acc 0.9608109140935494\n",
            "epoch 24 batch id 3281 loss 0.0574195571243763 train acc 0.9608493980493752\n",
            "epoch 24 batch id 3291 loss 0.02071220614016056 train acc 0.9608496657550897\n",
            "epoch 24 batch id 3301 loss 0.02427535504102707 train acc 0.9608546652529536\n",
            "epoch 24 batch id 3311 loss 0.03248549997806549 train acc 0.9608879492600423\n",
            "epoch 24 batch id 3321 loss 0.07291857898235321 train acc 0.9609069180969587\n",
            "epoch 24 batch id 3331 loss 0.1771932989358902 train acc 0.96086010207145\n",
            "epoch 24 batch id 3341 loss 0.17344918847084045 train acc 0.9608837174498653\n",
            "epoch 24 batch id 3351 loss 0.02263723872601986 train acc 0.9608745523724261\n",
            "epoch 24 batch id 3361 loss 0.15580807626247406 train acc 0.9609165798869385\n",
            "epoch 24 batch id 3371 loss 0.1626577526330948 train acc 0.9609027365766835\n",
            "epoch 24 batch id 3381 loss 0.07602811604738235 train acc 0.9608982179828454\n",
            "epoch 24 batch id 3391 loss 0.1263839304447174 train acc 0.9609075493954585\n",
            "epoch 24 batch id 3401 loss 0.19880396127700806 train acc 0.9609030432225816\n",
            "epoch 24 batch id 3411 loss 0.06534267961978912 train acc 0.9609535326883611\n",
            "epoch 24 batch id 3421 loss 0.06783866882324219 train acc 0.9609991596024554\n",
            "epoch 24 batch id 3431 loss 0.27424484491348267 train acc 0.960962547362285\n",
            "epoch 24 batch id 3441 loss 0.04714222624897957 train acc 0.9610124237140366\n",
            "epoch 24 batch id 3451 loss 0.10398627817630768 train acc 0.9610122066067807\n",
            "epoch 24 batch id 3461 loss 0.013181126676499844 train acc 0.9610300491187518\n",
            "epoch 24 batch id 3471 loss 0.03696803003549576 train acc 0.9610657951598963\n",
            "epoch 24 batch id 3481 loss 0.21256934106349945 train acc 0.9610564492961793\n",
            "epoch 24 batch id 3491 loss 0.07819367200136185 train acc 0.9610740117444858\n",
            "epoch 24 batch id 3501 loss 0.06732963770627975 train acc 0.9610914738646101\n",
            "epoch 24 batch id 3511 loss 0.15941770374774933 train acc 0.9610821347194531\n",
            "epoch 24 batch id 3521 loss 0.18670645356178284 train acc 0.9611039122408407\n",
            "epoch 24 batch id 3531 loss 0.19542109966278076 train acc 0.9611034409515717\n",
            "epoch 24 batch id 3541 loss 0.06759434938430786 train acc 0.9611294478960746\n",
            "epoch 24 batch id 3551 loss 0.1412144899368286 train acc 0.9611157068431427\n",
            "epoch 24 batch id 3561 loss 0.018133897334337234 train acc 0.9611678601516428\n",
            "epoch 24 batch id 3571 loss 0.11716075986623764 train acc 0.9611847171660599\n",
            "epoch 24 batch id 3581 loss 0.11097588390111923 train acc 0.9612102066461882\n",
            "epoch 24 batch id 3591 loss 0.23584747314453125 train acc 0.9611920426065163\n",
            "epoch 24 batch id 3601 loss 0.15882405638694763 train acc 0.9611826575951125\n",
            "epoch 24 batch id 3611 loss 0.014411902986466885 train acc 0.9611949598449183\n",
            "epoch 24 batch id 3621 loss 0.05127917230129242 train acc 0.9611899337199669\n",
            "epoch 24 batch id 3631 loss 0.060378286987543106 train acc 0.9611806320572845\n",
            "epoch 24 batch id 3641 loss 0.04330213740468025 train acc 0.9612314611370503\n",
            "epoch 24 batch id 3651 loss 0.09574338793754578 train acc 0.9612178170364284\n",
            "epoch 24 batch id 3661 loss 0.10824801027774811 train acc 0.9612170513520896\n",
            "epoch 24 batch id 3671 loss 0.06035058945417404 train acc 0.9612460841732497\n",
            "epoch 24 batch id 3681 loss 0.11224032938480377 train acc 0.9612834487910894\n",
            "epoch 24 batch id 3691 loss 0.03687170892953873 train acc 0.9613163776754267\n",
            "epoch 24 batch id 3701 loss 0.12256839871406555 train acc 0.9613237976222643\n",
            "epoch 24 batch id 3711 loss 0.0728394016623497 train acc 0.9613690716787928\n",
            "epoch 24 batch id 3721 loss 0.06160162761807442 train acc 0.9613931066917495\n",
            "epoch 24 batch id 3731 loss 0.040256567299366 train acc 0.9613918855534709\n",
            "epoch 24 batch id 3741 loss 0.17389412224292755 train acc 0.9613781408714247\n",
            "epoch 24 batch id 3751 loss 0.049939244985580444 train acc 0.9613728005865103\n",
            "epoch 24 batch id 3761 loss 0.12016327679157257 train acc 0.9613799521403882\n",
            "epoch 24 batch id 3771 loss 0.06752955168485641 train acc 0.9613663484486874\n",
            "epoch 24 batch id 3781 loss 0.11473681032657623 train acc 0.9613817442475535\n",
            "epoch 24 batch id 3791 loss 0.01188727654516697 train acc 0.9614176668425217\n",
            "epoch 24 batch id 3801 loss 0.08505775779485703 train acc 0.9614122928176796\n",
            "epoch 24 batch id 3811 loss 0.12078393995761871 train acc 0.9614315468381003\n",
            "epoch 24 batch id 3821 loss 0.04025980457663536 train acc 0.961430253860246\n",
            "epoch 24 batch id 3831 loss 0.05504217743873596 train acc 0.9614493604802924\n",
            "epoch 24 batch id 3841 loss 0.11918827891349792 train acc 0.9614561637594377\n",
            "epoch 24 batch id 3851 loss 0.008764708414673805 train acc 0.9614751038691249\n",
            "epoch 24 batch id 3861 loss 0.11456356197595596 train acc 0.9614291958041958\n",
            "epoch 24 batch id 3871 loss 0.17565403878688812 train acc 0.9614198527512271\n",
            "epoch 24 batch id 3881 loss 0.10440171509981155 train acc 0.9614186098943571\n",
            "epoch 24 batch id 3891 loss 0.0605744868516922 train acc 0.9614254047802622\n",
            "epoch 24 batch id 3901 loss 0.04610639810562134 train acc 0.9614121379133556\n",
            "epoch 24 batch id 3911 loss 0.03415937349200249 train acc 0.9614348951674764\n",
            "epoch 24 batch id 3921 loss 0.04299737513065338 train acc 0.9614336266258607\n",
            "epoch 24 batch id 3931 loss 0.24124836921691895 train acc 0.9614204400915798\n",
            "epoch 24 batch id 3941 loss 0.06984476745128632 train acc 0.9614430030449125\n",
            "epoch 24 batch id 3951 loss 0.04930446296930313 train acc 0.961433814224247\n",
            "epoch 24 batch id 3961 loss 0.3377341032028198 train acc 0.9614365059328452\n",
            "epoch 24 batch id 3971 loss 0.035385970026254654 train acc 0.9614549231931503\n",
            "epoch 24 batch id 3981 loss 0.03193298354744911 train acc 0.9614536234614418\n",
            "epoch 24 batch id 3991 loss 0.1485380381345749 train acc 0.9614523302430469\n",
            "epoch 24 batch id 4001 loss 0.09571363776922226 train acc 0.9614432329417646\n",
            "epoch 24 batch id 4011 loss 0.11518342792987823 train acc 0.9614030167040638\n",
            "epoch 24 batch id 4021 loss 0.034445345401763916 train acc 0.9614407174832131\n",
            "epoch 24 batch id 4031 loss 0.03401842713356018 train acc 0.9614859836268916\n",
            "epoch 24 batch id 4041 loss 0.15367327630519867 train acc 0.9614846263301163\n",
            "epoch 24 batch id 4051 loss 0.1001412495970726 train acc 0.9615064181683535\n",
            "epoch 24 batch id 4061 loss 0.05908339470624924 train acc 0.9615165599606008\n",
            "epoch 24 batch id 4071 loss 0.06323549151420593 train acc 0.9615036231884058\n",
            "epoch 24 batch id 4081 loss 0.04562282934784889 train acc 0.9615290370007351\n",
            "epoch 24 batch id 4091 loss 0.04502633213996887 train acc 0.961531410413102\n",
            "epoch 24 batch id 4101 loss 0.1290096789598465 train acc 0.961545202389661\n",
            "epoch 24 batch id 4111 loss 0.0853973925113678 train acc 0.9615437241547069\n",
            "epoch 24 batch id 4121 loss 0.030717138200998306 train acc 0.9615536277602523\n",
            "epoch 24 batch id 4131 loss 0.09756054729223251 train acc 0.9615445715323166\n",
            "epoch 24 batch id 4141 loss 0.09548056870698929 train acc 0.961577064718667\n",
            "epoch 24 batch id 4151 loss 0.1164756491780281 train acc 0.9616056371958565\n",
            "epoch 24 batch id 4161 loss 0.18911175429821014 train acc 0.9615627253064167\n",
            "epoch 24 batch id 4171 loss 0.11487413942813873 train acc 0.9615762107408296\n",
            "epoch 24 batch id 4181 loss 0.08308436721563339 train acc 0.9615709459459459\n",
            "epoch 24 batch id 4191 loss 0.2913466989994049 train acc 0.9615731627296588\n",
            "epoch 24 batch id 4201 loss 0.05910416319966316 train acc 0.961590246369912\n",
            "epoch 24 batch id 4211 loss 0.13100384175777435 train acc 0.961555301591071\n",
            "epoch 24 batch id 4221 loss 0.15828734636306763 train acc 0.9615316275764036\n",
            "epoch 24 batch id 4231 loss 0.1251945048570633 train acc 0.9615228373906878\n",
            "epoch 24 batch id 4241 loss 0.07732876390218735 train acc 0.9615288257486442\n",
            "epoch 24 batch id 4251 loss 0.183590829372406 train acc 0.9615421371442013\n",
            "epoch 24 batch id 4261 loss 0.09942290931940079 train acc 0.9615590530391926\n",
            "epoch 24 batch id 4271 loss 0.09861468523740768 train acc 0.9615685729337392\n",
            "epoch 24 batch id 4281 loss 0.06323055177927017 train acc 0.9615853480495211\n",
            "epoch 24 batch id 4291 loss 0.03692610189318657 train acc 0.9616020449778606\n",
            "epoch 24 batch id 4301 loss 0.10438944399356842 train acc 0.961578702627296\n",
            "epoch 24 batch id 4311 loss 0.055744659155607224 train acc 0.9616098353050336\n",
            "epoch 24 batch id 4321 loss 0.048477139323949814 train acc 0.9616155114556816\n",
            "epoch 24 batch id 4331 loss 0.19022220373153687 train acc 0.9616283768182867\n",
            "epoch 24 batch id 4341 loss 0.09756921976804733 train acc 0.9616195865008063\n",
            "epoch 24 batch id 4351 loss 0.19991905987262726 train acc 0.9615605607906228\n",
            "epoch 24 batch id 4361 loss 0.14536409080028534 train acc 0.9615555491859665\n",
            "epoch 24 batch id 4371 loss 0.20191428065299988 train acc 0.9615720086936628\n",
            "epoch 24 batch id 4381 loss 0.1971631795167923 train acc 0.9615527276877425\n",
            "epoch 24 batch id 4391 loss 0.011573558673262596 train acc 0.9615370929173309\n",
            "epoch 24 batch id 4401 loss 0.15831664204597473 train acc 0.9615144285389684\n",
            "epoch 24 batch id 4411 loss 0.007651061285287142 train acc 0.9615379165722059\n",
            "epoch 24 batch id 4421 loss 0.08839088678359985 train acc 0.9615542298122597\n",
            "epoch 24 batch id 4431 loss 0.10181664675474167 train acc 0.9615739957120288\n",
            "epoch 24 batch id 4441 loss 0.025922730565071106 train acc 0.961569044134204\n",
            "epoch 24 batch id 4451 loss 0.14823195338249207 train acc 0.9615781565940238\n",
            "epoch 24 batch id 4461 loss 0.07213996350765228 train acc 0.9615697153104685\n",
            "epoch 24 batch id 4471 loss 0.041238658130168915 train acc 0.9615717960187877\n",
            "epoch 24 batch id 4481 loss 0.038950007408857346 train acc 0.9615773543851819\n",
            "epoch 24 batch id 4491 loss 0.08521023392677307 train acc 0.9615585337341349\n",
            "epoch 24 batch id 4501 loss 0.040091827511787415 train acc 0.9615467396134192\n",
            "epoch 24 batch id 4511 loss 0.24871043860912323 train acc 0.9615419252937264\n",
            "epoch 24 batch id 4521 loss 0.11647007614374161 train acc 0.9615129396151294\n",
            "epoch 24 batch id 4531 loss 0.19396695494651794 train acc 0.9614806334142574\n",
            "epoch 24 batch id 4541 loss 0.10209093987941742 train acc 0.9614794373486016\n",
            "epoch 24 batch id 4551 loss 0.11711213737726212 train acc 0.9614816798505823\n",
            "epoch 24 batch id 4561 loss 0.3187483549118042 train acc 0.9614736351677263\n",
            "epoch 24 batch id 4571 loss 0.1152832880616188 train acc 0.9615032268650185\n",
            "epoch 24 batch id 4581 loss 0.03410835936665535 train acc 0.9615054027504911\n",
            "epoch 24 batch id 4591 loss 0.058405160903930664 train acc 0.9615075691570464\n",
            "epoch 24 batch id 4601 loss 0.14051708579063416 train acc 0.9614893501412737\n",
            "epoch 24 batch id 4611 loss 0.054200176149606705 train acc 0.9615186510518325\n",
            "epoch 24 batch id 4621 loss 0.06893584877252579 train acc 0.9615376812378273\n",
            "epoch 24 batch id 4631 loss 0.1402636468410492 train acc 0.9615498812351544\n",
            "epoch 24 batch id 4641 loss 0.13380186259746552 train acc 0.9615855957767723\n",
            "epoch 24 batch id 4651 loss 0.31477782130241394 train acc 0.9615606858740056\n",
            "epoch 24 batch id 4661 loss 0.06182873249053955 train acc 0.9615861671315168\n",
            "epoch 24 batch id 4671 loss 0.03100287914276123 train acc 0.9615814333119247\n",
            "epoch 24 batch id 4681 loss 0.15804973244667053 train acc 0.9616067613757744\n",
            "epoch 24 batch id 4691 loss 0.020384101197123528 train acc 0.9616286506075463\n",
            "epoch 24 batch id 4701 loss 0.10100900381803513 train acc 0.9616305041480536\n",
            "epoch 24 batch id 4711 loss 0.08658644556999207 train acc 0.9616389832307366\n",
            "epoch 24 batch id 4721 loss 0.010733908973634243 train acc 0.9616110199110358\n",
            "epoch 24 batch id 4731 loss 0.05233178287744522 train acc 0.9616162016487001\n",
            "epoch 24 batch id 4741 loss 0.10949518531560898 train acc 0.9616180658089011\n",
            "epoch 24 batch id 4751 loss 0.15348957479000092 train acc 0.961626499684277\n",
            "epoch 24 batch id 4761 loss 0.07139168679714203 train acc 0.9616184887628649\n",
            "epoch 24 batch id 4771 loss 0.08767847716808319 train acc 0.9616203364074617\n",
            "epoch 24 batch id 4781 loss 0.21910415589809418 train acc 0.9616123718887262\n",
            "epoch 24 batch id 4791 loss 0.3243410587310791 train acc 0.9616077019411396\n",
            "epoch 24 batch id 4801 loss 0.15362203121185303 train acc 0.9616063059779213\n",
            "epoch 24 batch id 4811 loss 0.07928217947483063 train acc 0.9616179068800665\n",
            "epoch 24 batch id 4821 loss 0.26469436287879944 train acc 0.9615938083385189\n",
            "epoch 24 batch id 4831 loss 0.04812648892402649 train acc 0.9616118557234526\n",
            "epoch 24 batch id 4841 loss 0.20972344279289246 train acc 0.9616169179921504\n",
            "epoch 24 batch id 4851 loss 0.14363449811935425 train acc 0.9616058544629973\n",
            "epoch 24 batch id 4861 loss 0.09254195541143417 train acc 0.9616173369677021\n",
            "epoch 24 batch id 4871 loss 0.07307296246290207 train acc 0.9616544344077191\n",
            "epoch 24 batch id 4881 loss 0.060725536197423935 train acc 0.9616433620159803\n",
            "epoch 24 batch id 4891 loss 0.11238476634025574 train acc 0.9616355295440605\n",
            "epoch 24 batch id 4901 loss 0.04895690456032753 train acc 0.9616468577841257\n",
            "epoch 24 batch id 4911 loss 0.16658660769462585 train acc 0.9616517766239056\n",
            "epoch 24 batch id 4921 loss 0.2252788096666336 train acc 0.9616344492989229\n",
            "epoch 24 batch id 4931 loss 0.1985827535390854 train acc 0.96161402352464\n",
            "epoch 24 batch id 4941 loss 0.16067814826965332 train acc 0.9616126543209876\n",
            "epoch 24 batch id 4951 loss 0.1484111249446869 train acc 0.9616270702888305\n",
            "epoch 24 train acc 0.9616293877345169\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "387986cc3c294abca5fe7d4e950fc5e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 24 loss 1.2046148777008057 test acc 0.8341779692082111\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3962ddaae649489bb9c03c89a992f9db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 25 batch id 1 loss 0.10329972207546234 train acc 0.953125\n",
            "epoch 25 batch id 11 loss 0.08610204607248306 train acc 0.9630681818181818\n",
            "epoch 25 batch id 21 loss 0.013829285278916359 train acc 0.9650297619047619\n",
            "epoch 25 batch id 31 loss 0.11690449714660645 train acc 0.9616935483870968\n",
            "epoch 25 batch id 41 loss 0.019627101719379425 train acc 0.9664634146341463\n",
            "epoch 25 batch id 51 loss 0.09012921154499054 train acc 0.9659926470588235\n",
            "epoch 25 batch id 61 loss 0.035199474543333054 train acc 0.9641393442622951\n",
            "epoch 25 batch id 71 loss 0.11936142295598984 train acc 0.9643485915492958\n",
            "epoch 25 batch id 81 loss 0.09166649729013443 train acc 0.9631558641975309\n",
            "epoch 25 batch id 91 loss 0.20753920078277588 train acc 0.9625686813186813\n",
            "epoch 25 batch id 101 loss 0.17781732976436615 train acc 0.9605507425742574\n",
            "epoch 25 batch id 111 loss 0.01659189537167549 train acc 0.9610078828828829\n",
            "epoch 25 batch id 121 loss 0.19185122847557068 train acc 0.9620351239669421\n",
            "epoch 25 batch id 131 loss 0.03650802746415138 train acc 0.9625477099236641\n",
            "epoch 25 batch id 141 loss 0.021920476108789444 train acc 0.9619902482269503\n",
            "epoch 25 batch id 151 loss 0.06578875333070755 train acc 0.9620240066225165\n",
            "epoch 25 batch id 161 loss 0.1835046261548996 train acc 0.9623447204968945\n",
            "epoch 25 batch id 171 loss 0.24760548770427704 train acc 0.9620796783625731\n",
            "epoch 25 batch id 181 loss 0.09768939763307571 train acc 0.9614986187845304\n",
            "epoch 25 batch id 191 loss 0.10691861063241959 train acc 0.9613056282722513\n",
            "epoch 25 batch id 201 loss 0.22960850596427917 train acc 0.9614427860696517\n",
            "epoch 25 batch id 211 loss 0.044480204582214355 train acc 0.9614188388625592\n",
            "epoch 25 batch id 221 loss 0.07891927659511566 train acc 0.9615384615384616\n",
            "epoch 25 batch id 231 loss 0.05282604321837425 train acc 0.9623917748917749\n",
            "epoch 25 batch id 241 loss 0.155995711684227 train acc 0.961683091286307\n",
            "epoch 25 batch id 251 loss 0.20974037051200867 train acc 0.961902390438247\n",
            "epoch 25 batch id 261 loss 0.1423977017402649 train acc 0.9621048850574713\n",
            "epoch 25 batch id 271 loss 0.0567408949136734 train acc 0.9624077490774908\n",
            "epoch 25 batch id 281 loss 0.026046862825751305 train acc 0.9629670818505338\n",
            "epoch 25 batch id 291 loss 0.09605623036623001 train acc 0.9633805841924399\n",
            "epoch 25 batch id 301 loss 0.10228586196899414 train acc 0.9631955980066446\n",
            "epoch 25 batch id 311 loss 0.13760696351528168 train acc 0.9635249196141479\n",
            "epoch 25 batch id 321 loss 0.19423426687717438 train acc 0.9635416666666666\n",
            "epoch 25 batch id 331 loss 0.058522384613752365 train acc 0.9634629909365559\n",
            "epoch 25 batch id 341 loss 0.19969016313552856 train acc 0.9632514662756598\n",
            "epoch 25 batch id 351 loss 0.013229941949248314 train acc 0.96309650997151\n",
            "epoch 25 batch id 361 loss 0.17534732818603516 train acc 0.9633396814404432\n",
            "epoch 25 batch id 371 loss 0.2866474390029907 train acc 0.9636960916442049\n",
            "epoch 25 batch id 381 loss 0.014030066318809986 train acc 0.9637057086614174\n",
            "epoch 25 batch id 391 loss 0.2362462282180786 train acc 0.9634750639386189\n",
            "epoch 25 batch id 401 loss 0.0028470323886722326 train acc 0.9635286783042394\n",
            "epoch 25 batch id 411 loss 0.056717947125434875 train acc 0.963617700729927\n",
            "epoch 25 batch id 421 loss 0.10606537759304047 train acc 0.9636282660332541\n",
            "epoch 25 batch id 431 loss 0.1708969622850418 train acc 0.9635295823665894\n",
            "epoch 25 batch id 441 loss 0.1594536155462265 train acc 0.9635062358276644\n",
            "epoch 25 batch id 451 loss 0.06305799633264542 train acc 0.9638996674057649\n",
            "epoch 25 batch id 461 loss 0.013753239996731281 train acc 0.9640048806941431\n",
            "epoch 25 batch id 471 loss 0.07026244699954987 train acc 0.9640724522292994\n",
            "epoch 25 batch id 481 loss 0.11806970089673996 train acc 0.9640072765072765\n",
            "epoch 25 batch id 491 loss 0.06783919781446457 train acc 0.9641356924643585\n",
            "epoch 25 batch id 501 loss 0.15455156564712524 train acc 0.9641966067864272\n",
            "epoch 25 batch id 511 loss 0.16918373107910156 train acc 0.9639187866927593\n",
            "epoch 25 batch id 521 loss 0.1346941888332367 train acc 0.9640714971209213\n",
            "epoch 25 batch id 531 loss 0.07806320488452911 train acc 0.9639241996233522\n",
            "epoch 25 batch id 541 loss 0.037603091448545456 train acc 0.9638689926062847\n",
            "epoch 25 batch id 551 loss 0.06421743333339691 train acc 0.963929219600726\n",
            "epoch 25 batch id 561 loss 0.11850588768720627 train acc 0.9638201871657754\n",
            "epoch 25 batch id 571 loss 0.1799934208393097 train acc 0.9640707092819615\n",
            "epoch 25 batch id 581 loss 0.09095513820648193 train acc 0.9641243545611016\n",
            "epoch 25 batch id 591 loss 0.06903586536645889 train acc 0.9642026226734348\n",
            "epoch 25 batch id 601 loss 0.01718839257955551 train acc 0.9644082778702163\n",
            "epoch 25 batch id 611 loss 0.12961098551750183 train acc 0.9641468903436988\n",
            "epoch 25 batch id 621 loss 0.1791168451309204 train acc 0.9641958534621579\n",
            "epoch 25 batch id 631 loss 0.07015850394964218 train acc 0.9641194532488114\n",
            "epoch 25 batch id 641 loss 0.04073537141084671 train acc 0.9640941887675507\n",
            "epoch 25 batch id 651 loss 0.024072451516985893 train acc 0.9640456989247311\n",
            "epoch 25 batch id 661 loss 0.052687425166368484 train acc 0.9641168683812406\n",
            "epoch 25 batch id 671 loss 0.18517018854618073 train acc 0.9640694858420268\n",
            "epoch 25 batch id 681 loss 0.1118631586432457 train acc 0.9639776064610867\n",
            "epoch 25 batch id 691 loss 0.09552854299545288 train acc 0.9638657742402316\n",
            "epoch 25 batch id 701 loss 0.12178842723369598 train acc 0.963779422253923\n",
            "epoch 25 batch id 711 loss 0.18476559221744537 train acc 0.9638273558368495\n",
            "epoch 25 batch id 721 loss 0.14906077086925507 train acc 0.9637872746185853\n",
            "epoch 25 batch id 731 loss 0.01696976274251938 train acc 0.9637696648426812\n",
            "epoch 25 batch id 741 loss 0.018379777669906616 train acc 0.963879048582996\n",
            "epoch 25 batch id 751 loss 0.13035164773464203 train acc 0.9638606857523302\n",
            "epoch 25 batch id 761 loss 0.022799678146839142 train acc 0.9639044021024967\n",
            "epoch 25 batch id 771 loss 0.14872826635837555 train acc 0.9639064526588845\n",
            "epoch 25 batch id 781 loss 0.0628737211227417 train acc 0.9637884122919335\n",
            "epoch 25 batch id 791 loss 0.0367419570684433 train acc 0.9639301517067004\n",
            "epoch 25 batch id 801 loss 0.04134589061141014 train acc 0.9639122971285893\n",
            "epoch 25 batch id 811 loss 0.22691775858402252 train acc 0.9639526818742293\n",
            "epoch 25 batch id 821 loss 0.07200416922569275 train acc 0.964068209500609\n",
            "epoch 25 batch id 831 loss 0.23037776350975037 train acc 0.9640869434416366\n",
            "epoch 25 batch id 841 loss 0.010234640911221504 train acc 0.9643096016646849\n",
            "epoch 25 batch id 851 loss 0.1828846037387848 train acc 0.9640863689776733\n",
            "epoch 25 batch id 861 loss 0.07275272905826569 train acc 0.9641223867595818\n",
            "epoch 25 batch id 871 loss 0.026281291618943214 train acc 0.9641037600459242\n",
            "epoch 25 batch id 881 loss 0.04782387986779213 train acc 0.9640323496027242\n",
            "epoch 25 batch id 891 loss 0.02370944246649742 train acc 0.963980078563412\n",
            "epoch 25 batch id 901 loss 0.09653403609991074 train acc 0.9639463096559379\n",
            "epoch 25 batch id 911 loss 0.26250505447387695 train acc 0.9639475850713501\n",
            "epoch 25 batch id 921 loss 0.022503772750496864 train acc 0.9638979370249728\n",
            "epoch 25 batch id 931 loss 0.00541384331882 train acc 0.9638661385606875\n",
            "epoch 25 batch id 941 loss 0.04622824862599373 train acc 0.9640342720510096\n",
            "epoch 25 batch id 951 loss 0.15095211565494537 train acc 0.963820977917981\n",
            "epoch 25 batch id 961 loss 0.0483316145837307 train acc 0.9637259365244537\n",
            "epoch 25 batch id 971 loss 0.14248643815517426 train acc 0.9636650360453141\n",
            "epoch 25 batch id 981 loss 0.03829663246870041 train acc 0.963796508664628\n",
            "epoch 25 batch id 991 loss 0.19976605474948883 train acc 0.9637361251261353\n",
            "epoch 25 batch id 1001 loss 0.12712568044662476 train acc 0.9638018231768232\n",
            "epoch 25 batch id 1011 loss 0.08000720292329788 train acc 0.9637425816023739\n",
            "epoch 25 batch id 1021 loss 0.10618916153907776 train acc 0.9638375367286973\n",
            "epoch 25 batch id 1031 loss 0.067128024995327 train acc 0.9638397187196897\n",
            "epoch 25 batch id 1041 loss 0.13414551317691803 train acc 0.9638568683957733\n",
            "epoch 25 batch id 1051 loss 0.023621054366230965 train acc 0.9636655566127498\n",
            "epoch 25 batch id 1061 loss 0.09898750483989716 train acc 0.9636103911404336\n",
            "epoch 25 batch id 1071 loss 0.008977022022008896 train acc 0.9636729691876751\n",
            "epoch 25 batch id 1081 loss 0.17299142479896545 train acc 0.9635898473635522\n",
            "epoch 25 batch id 1091 loss 0.23259207606315613 train acc 0.963579857928506\n",
            "epoch 25 batch id 1101 loss 0.17577742040157318 train acc 0.9635416666666666\n",
            "epoch 25 batch id 1111 loss 0.08855153620243073 train acc 0.963490099009901\n",
            "epoch 25 batch id 1121 loss 0.06026682257652283 train acc 0.9635788358608386\n",
            "epoch 25 batch id 1131 loss 0.2579638659954071 train acc 0.9635692970822282\n",
            "epoch 25 batch id 1141 loss 0.06866873800754547 train acc 0.9636147020157756\n",
            "epoch 25 batch id 1151 loss 0.09840213507413864 train acc 0.9634692658557776\n",
            "epoch 25 batch id 1161 loss 0.08700913935899734 train acc 0.9633397932816538\n",
            "epoch 25 batch id 1171 loss 0.048308927565813065 train acc 0.9632258753202391\n",
            "epoch 25 batch id 1181 loss 0.18618248403072357 train acc 0.96320649872989\n",
            "epoch 25 batch id 1191 loss 0.03576193377375603 train acc 0.9633186397984886\n",
            "epoch 25 batch id 1201 loss 0.07491413503885269 train acc 0.9634679433805162\n",
            "epoch 25 batch id 1211 loss 0.03255850449204445 train acc 0.963576073492981\n",
            "epoch 25 batch id 1221 loss 0.15503336489200592 train acc 0.9635800573300574\n",
            "epoch 25 batch id 1231 loss 0.10125169903039932 train acc 0.9634316612510154\n",
            "epoch 25 batch id 1241 loss 0.08743669092655182 train acc 0.9634745165189363\n",
            "epoch 25 batch id 1251 loss 0.2306443601846695 train acc 0.963466726618705\n",
            "epoch 25 batch id 1261 loss 0.050333574414253235 train acc 0.9635334060269627\n",
            "epoch 25 batch id 1271 loss 0.08787935972213745 train acc 0.9635621557828482\n",
            "epoch 25 batch id 1281 loss 0.049426544457674026 train acc 0.9635904566744731\n",
            "epoch 25 batch id 1291 loss 0.07833883166313171 train acc 0.9635699070487994\n",
            "epoch 25 batch id 1301 loss 0.1524185836315155 train acc 0.9635256533435819\n",
            "epoch 25 batch id 1311 loss 0.19857898354530334 train acc 0.9635297482837528\n",
            "epoch 25 batch id 1321 loss 0.020709382370114326 train acc 0.9635929220287661\n",
            "epoch 25 batch id 1331 loss 0.08199232071638107 train acc 0.9635847107438017\n",
            "epoch 25 batch id 1341 loss 0.24514544010162354 train acc 0.9635649701715138\n",
            "epoch 25 batch id 1351 loss 0.18544192612171173 train acc 0.9636380458919319\n",
            "epoch 25 batch id 1361 loss 0.032782506197690964 train acc 0.9635493203526818\n",
            "epoch 25 batch id 1371 loss 0.03737277910113335 train acc 0.9635074762946754\n",
            "epoch 25 batch id 1381 loss 0.22535111010074615 train acc 0.9635341238233165\n",
            "epoch 25 batch id 1391 loss 0.022351941093802452 train acc 0.9635042235801582\n",
            "epoch 25 batch id 1401 loss 0.03255109861493111 train acc 0.9635193611705924\n",
            "epoch 25 batch id 1411 loss 0.037442635744810104 train acc 0.9635564316087881\n",
            "epoch 25 batch id 1421 loss 0.01523253321647644 train acc 0.9636039760731879\n",
            "epoch 25 batch id 1431 loss 0.02728107012808323 train acc 0.9636399371069182\n",
            "epoch 25 batch id 1441 loss 0.24510161578655243 train acc 0.9635344378903539\n",
            "epoch 25 batch id 1451 loss 0.02710031159222126 train acc 0.9635919193659546\n",
            "epoch 25 batch id 1461 loss 0.030856024473905563 train acc 0.9636272245037646\n",
            "epoch 25 batch id 1471 loss 0.026953445747494698 train acc 0.963693915703603\n",
            "epoch 25 batch id 1481 loss 0.16731855273246765 train acc 0.9636331026333559\n",
            "epoch 25 batch id 1491 loss 0.24070009589195251 train acc 0.9637093393695506\n",
            "epoch 25 batch id 1501 loss 0.19226981699466705 train acc 0.9636804630246503\n",
            "epoch 25 batch id 1511 loss 0.12309328466653824 train acc 0.9637346955658505\n",
            "epoch 25 batch id 1521 loss 0.030082600191235542 train acc 0.9638293063773833\n",
            "epoch 25 batch id 1531 loss 0.03910253569483757 train acc 0.9639430927498367\n",
            "epoch 25 batch id 1541 loss 0.16500365734100342 train acc 0.9639134490590525\n",
            "epoch 25 batch id 1551 loss 0.12479330599308014 train acc 0.9638035944551901\n",
            "epoch 25 batch id 1561 loss 0.15651914477348328 train acc 0.9637752242152466\n",
            "epoch 25 batch id 1571 loss 0.022106394171714783 train acc 0.9637869987269255\n",
            "epoch 25 batch id 1581 loss 0.10252795368432999 train acc 0.9638282732447818\n",
            "epoch 25 batch id 1591 loss 0.1447741985321045 train acc 0.9638788497800126\n",
            "epoch 25 batch id 1601 loss 0.09148760885000229 train acc 0.9640166302311055\n",
            "epoch 25 batch id 1611 loss 0.09947418421506882 train acc 0.9639490223463687\n",
            "epoch 25 batch id 1621 loss 0.04453732818365097 train acc 0.9639304441702653\n",
            "epoch 25 batch id 1631 loss 0.01283756922930479 train acc 0.9639599938687922\n",
            "epoch 25 batch id 1641 loss 0.0835517942905426 train acc 0.9639225319926874\n",
            "epoch 25 batch id 1651 loss 0.16270437836647034 train acc 0.9639139158086009\n",
            "epoch 25 batch id 1661 loss 0.16828393936157227 train acc 0.9639524382901866\n",
            "epoch 25 batch id 1671 loss 0.14443810284137726 train acc 0.963962447636146\n",
            "epoch 25 batch id 1681 loss 0.1282130926847458 train acc 0.9638886823319452\n",
            "epoch 25 batch id 1691 loss 0.11823654174804688 train acc 0.9639266706091071\n",
            "epoch 25 batch id 1701 loss 0.047937843948602676 train acc 0.9639550264550265\n",
            "epoch 25 batch id 1711 loss 0.07567714154720306 train acc 0.9639282583284629\n",
            "epoch 25 batch id 1721 loss 0.2097693234682083 train acc 0.96396535444509\n",
            "epoch 25 batch id 1731 loss 0.0011300209444016218 train acc 0.9640020219526285\n",
            "epoch 25 batch id 1741 loss 0.0784224271774292 train acc 0.9640113440551408\n",
            "epoch 25 batch id 1751 loss 0.057315923273563385 train acc 0.9639670188463735\n",
            "epoch 25 batch id 1761 loss 0.018938228487968445 train acc 0.964056289040318\n",
            "epoch 25 batch id 1771 loss 0.09769562631845474 train acc 0.9641092603049125\n",
            "epoch 25 batch id 1781 loss 0.14464884996414185 train acc 0.9641265440763616\n",
            "epoch 25 batch id 1791 loss 0.11300007253885269 train acc 0.9640912897822446\n",
            "epoch 25 batch id 1801 loss 0.02728164568543434 train acc 0.9641692115491394\n",
            "epoch 25 batch id 1811 loss 0.04369935765862465 train acc 0.9642117614577581\n",
            "epoch 25 batch id 1821 loss 0.10075374692678452 train acc 0.9641251372872048\n",
            "epoch 25 batch id 1831 loss 0.033708009868860245 train acc 0.9641589295466958\n",
            "epoch 25 batch id 1841 loss 0.012394621036946774 train acc 0.9641668929929387\n",
            "epoch 25 batch id 1851 loss 0.022406699135899544 train acc 0.9642169773095624\n",
            "epoch 25 batch id 1861 loss 0.02619478479027748 train acc 0.9641825631380978\n",
            "epoch 25 batch id 1871 loss 0.13756802678108215 train acc 0.9642403794762159\n",
            "epoch 25 batch id 1881 loss 0.01891370303928852 train acc 0.96424774056353\n",
            "epoch 25 batch id 1891 loss 0.14905798435211182 train acc 0.9643128635642517\n",
            "epoch 25 batch id 1901 loss 0.10473937541246414 train acc 0.9643362046291426\n",
            "epoch 25 batch id 1911 loss 0.05681109055876732 train acc 0.9642775379382522\n",
            "epoch 25 batch id 1921 loss 0.06918807327747345 train acc 0.9642438833940656\n",
            "epoch 25 batch id 1931 loss 0.05100613832473755 train acc 0.9642753107198343\n",
            "epoch 25 batch id 1941 loss 0.054955750703811646 train acc 0.9642742143225141\n",
            "epoch 25 batch id 1951 loss 0.22470861673355103 train acc 0.9642571117375704\n",
            "epoch 25 batch id 1961 loss 0.058369770646095276 train acc 0.9642959586945437\n",
            "epoch 25 batch id 1971 loss 0.011923694983124733 train acc 0.9642868467782851\n",
            "epoch 25 batch id 1981 loss 0.13038714230060577 train acc 0.9642699394245331\n",
            "epoch 25 batch id 1991 loss 0.16888166964054108 train acc 0.9642767453540935\n",
            "epoch 25 batch id 2001 loss 0.19422218203544617 train acc 0.9642678660669665\n",
            "epoch 25 batch id 2011 loss 0.20931705832481384 train acc 0.9642435355544505\n",
            "epoch 25 batch id 2021 loss 0.26623252034187317 train acc 0.9642735650667986\n",
            "epoch 25 batch id 2031 loss 0.0873175710439682 train acc 0.9642032865583456\n",
            "epoch 25 batch id 2041 loss 0.08754745870828629 train acc 0.964171974522293\n",
            "epoch 25 batch id 2051 loss 0.27591726183891296 train acc 0.9641485860555826\n",
            "epoch 25 batch id 2061 loss 0.05128402262926102 train acc 0.9641936559922368\n",
            "epoch 25 batch id 2071 loss 0.1131938248872757 train acc 0.9641402100434573\n",
            "epoch 25 batch id 2081 loss 0.17793715000152588 train acc 0.9640947861604997\n",
            "epoch 25 batch id 2091 loss 0.0730871930718422 train acc 0.9640722142515543\n",
            "epoch 25 batch id 2101 loss 0.022345663979649544 train acc 0.9641167896239886\n",
            "epoch 25 batch id 2111 loss 0.04926640912890434 train acc 0.9641017290383704\n",
            "epoch 25 batch id 2121 loss 0.10119296610355377 train acc 0.9640647100424328\n",
            "epoch 25 batch id 2131 loss 0.07489056885242462 train acc 0.9641160253402159\n",
            "epoch 25 batch id 2141 loss 0.10433892160654068 train acc 0.9640938813638487\n",
            "epoch 25 batch id 2151 loss 0.14441700279712677 train acc 0.9641445839144583\n",
            "epoch 25 batch id 2161 loss 0.20949313044548035 train acc 0.9641586649699213\n",
            "epoch 25 batch id 2171 loss 0.013007376343011856 train acc 0.9641942077383694\n",
            "epoch 25 batch id 2181 loss 0.08601981401443481 train acc 0.964157783127006\n",
            "epoch 25 batch id 2191 loss 0.02012144960463047 train acc 0.9641858740301232\n",
            "epoch 25 batch id 2201 loss 0.11107837408781052 train acc 0.9641924125397546\n",
            "epoch 25 batch id 2211 loss 0.006820362992584705 train acc 0.9641988919041158\n",
            "epoch 25 batch id 2221 loss 0.07480749487876892 train acc 0.9642053129221072\n",
            "epoch 25 batch id 2231 loss 0.08731009811162949 train acc 0.9641766584491259\n",
            "epoch 25 batch id 2241 loss 0.21786174178123474 train acc 0.9641343150379295\n",
            "epoch 25 batch id 2251 loss 0.053107865154743195 train acc 0.9641201132829853\n",
            "epoch 25 batch id 2261 loss 0.14225803315639496 train acc 0.9641060371517027\n",
            "epoch 25 batch id 2271 loss 0.07929793000221252 train acc 0.9641058454425363\n",
            "epoch 25 batch id 2281 loss 0.10260656476020813 train acc 0.9641536058746164\n",
            "epoch 25 batch id 2291 loss 0.13986368477344513 train acc 0.9641941292012222\n",
            "epoch 25 batch id 2301 loss 0.025018082931637764 train acc 0.9642139287266406\n",
            "epoch 25 batch id 2311 loss 0.1495385766029358 train acc 0.9642267957594115\n",
            "epoch 25 batch id 2321 loss 0.11873931437730789 train acc 0.964185695820767\n",
            "epoch 25 batch id 2331 loss 0.0637478157877922 train acc 0.9641583547833548\n",
            "epoch 25 batch id 2341 loss 0.03765193000435829 train acc 0.9641979923109782\n",
            "epoch 25 batch id 2351 loss 0.036749888211488724 train acc 0.9641841237771162\n",
            "epoch 25 batch id 2361 loss 0.1018732488155365 train acc 0.9641836086404066\n",
            "epoch 25 batch id 2371 loss 0.0028986353427171707 train acc 0.9642687684521299\n",
            "epoch 25 batch id 2381 loss 0.008327789604663849 train acc 0.9642810268794624\n",
            "epoch 25 batch id 2391 loss 0.0531296581029892 train acc 0.96426704307821\n",
            "epoch 25 batch id 2401 loss 0.14591670036315918 train acc 0.9642531757601\n",
            "epoch 25 batch id 2411 loss 0.03366352617740631 train acc 0.9642783077561178\n",
            "epoch 25 batch id 2421 loss 0.31572625041007996 train acc 0.9642903242461792\n",
            "epoch 25 batch id 2431 loss 0.06044634431600571 train acc 0.9643150966680378\n",
            "epoch 25 batch id 2441 loss 0.03402993455529213 train acc 0.96427565546907\n",
            "epoch 25 batch id 2451 loss 0.04600125923752785 train acc 0.9642429110567116\n",
            "epoch 25 batch id 2461 loss 0.17683856189250946 train acc 0.9642040837058107\n",
            "epoch 25 batch id 2471 loss 0.15009750425815582 train acc 0.964203510724403\n",
            "epoch 25 batch id 2481 loss 0.1877305805683136 train acc 0.9642596231358324\n",
            "epoch 25 batch id 2491 loss 0.11423184722661972 train acc 0.9642525592131674\n",
            "epoch 25 batch id 2501 loss 0.017704222351312637 train acc 0.9642517992802879\n",
            "epoch 25 batch id 2511 loss 0.02970259077847004 train acc 0.9642634906411788\n",
            "epoch 25 batch id 2521 loss 0.1810726821422577 train acc 0.9642317036890123\n",
            "epoch 25 batch id 2531 loss 0.03562411293387413 train acc 0.9643174634531806\n",
            "epoch 25 batch id 2541 loss 0.11097487807273865 train acc 0.9643287583628493\n",
            "epoch 25 batch id 2551 loss 0.06943437457084656 train acc 0.9643644649157194\n",
            "epoch 25 batch id 2561 loss 0.07260528951883316 train acc 0.9643754880905896\n",
            "epoch 25 batch id 2571 loss 0.13554507493972778 train acc 0.9643985803189421\n",
            "epoch 25 batch id 2581 loss 0.09718777239322662 train acc 0.964421493607129\n",
            "epoch 25 batch id 2591 loss 0.015230929479002953 train acc 0.9643959861057507\n",
            "epoch 25 batch id 2601 loss 0.23583979904651642 train acc 0.9644427623990772\n",
            "epoch 25 batch id 2611 loss 0.012297842651605606 train acc 0.9644891803906549\n",
            "epoch 25 batch id 2621 loss 0.26196885108947754 train acc 0.9644577451354445\n",
            "epoch 25 batch id 2631 loss 0.20539915561676025 train acc 0.9644324876472824\n",
            "epoch 25 batch id 2641 loss 0.11687567830085754 train acc 0.9644429193487315\n",
            "epoch 25 batch id 2651 loss 0.0008132835500873625 train acc 0.9644591663523199\n",
            "epoch 25 batch id 2661 loss 0.05038445442914963 train acc 0.9644635475385194\n",
            "epoch 25 batch id 2671 loss 0.13068035244941711 train acc 0.9644795956570573\n",
            "epoch 25 batch id 2681 loss 0.07441116124391556 train acc 0.9644663838120104\n",
            "epoch 25 batch id 2691 loss 0.10393182933330536 train acc 0.9644532701597919\n",
            "epoch 25 batch id 2701 loss 0.061098068952560425 train acc 0.9644807478711588\n",
            "epoch 25 batch id 2711 loss 0.004325487185269594 train acc 0.9644907322021394\n",
            "epoch 25 batch id 2721 loss 0.028482073917984962 train acc 0.9645063855200294\n",
            "epoch 25 batch id 2731 loss 0.0040104505605995655 train acc 0.9645390882460637\n",
            "epoch 25 batch id 2741 loss 0.12184074521064758 train acc 0.9645487504560379\n",
            "epoch 25 batch id 2751 loss 0.23096108436584473 train acc 0.9645356234096693\n",
            "epoch 25 batch id 2761 loss 0.052894480526447296 train acc 0.9645622057225642\n",
            "epoch 25 batch id 2771 loss 0.08240360766649246 train acc 0.9645660411403826\n",
            "epoch 25 batch id 2781 loss 0.11716251820325851 train acc 0.9646035598705501\n",
            "epoch 25 batch id 2791 loss 0.10980049520730972 train acc 0.9646128179863848\n",
            "epoch 25 batch id 2801 loss 0.15225355327129364 train acc 0.9646164316315602\n",
            "epoch 25 batch id 2811 loss 0.05618033558130264 train acc 0.9646311366061899\n",
            "epoch 25 batch id 2821 loss 0.03485257551074028 train acc 0.964645737327189\n",
            "epoch 25 batch id 2831 loss 0.03281440585851669 train acc 0.9646657541504768\n",
            "epoch 25 batch id 2841 loss 0.257800430059433 train acc 0.9646801302358324\n",
            "epoch 25 batch id 2851 loss 0.05780373141169548 train acc 0.964688924938618\n",
            "epoch 25 batch id 2861 loss 0.11570939421653748 train acc 0.9646921967843411\n",
            "epoch 25 batch id 2871 loss 0.13002195954322815 train acc 0.9647008881922675\n",
            "epoch 25 batch id 2881 loss 0.06833723187446594 train acc 0.9646878254078445\n",
            "epoch 25 batch id 2891 loss 0.02303154021501541 train acc 0.964691067104808\n",
            "epoch 25 batch id 2901 loss 0.19300611317157745 train acc 0.9646565839365736\n",
            "epoch 25 batch id 2911 loss 0.03315620496869087 train acc 0.9646652782548952\n",
            "epoch 25 batch id 2921 loss 0.01717677153646946 train acc 0.9646846114344403\n",
            "epoch 25 batch id 2931 loss 0.008792685344815254 train acc 0.9646824889116342\n",
            "epoch 25 batch id 2941 loss 0.1827363818883896 train acc 0.9646538167290037\n",
            "epoch 25 batch id 2951 loss 0.03754868358373642 train acc 0.9647312351745171\n",
            "epoch 25 batch id 2961 loss 0.07838955521583557 train acc 0.9647553613644039\n",
            "epoch 25 batch id 2971 loss 0.08741890639066696 train acc 0.9647530292830697\n",
            "epoch 25 batch id 2981 loss 0.11982209980487823 train acc 0.9647402297886615\n",
            "epoch 25 batch id 2991 loss 0.041802242398262024 train acc 0.9647484119023738\n",
            "epoch 25 batch id 3001 loss 0.011327289044857025 train acc 0.964751332889037\n",
            "epoch 25 batch id 3011 loss 0.10991925746202469 train acc 0.9647179093324477\n",
            "epoch 25 batch id 3021 loss 0.06561021506786346 train acc 0.9647105676928169\n",
            "epoch 25 batch id 3031 loss 0.12774375081062317 train acc 0.9647135846255361\n",
            "epoch 25 batch id 3041 loss 0.18630047142505646 train acc 0.9646703387043736\n",
            "epoch 25 batch id 3051 loss 0.07836000621318817 train acc 0.9647093166175025\n",
            "epoch 25 batch id 3061 loss 0.11395252496004105 train acc 0.9646459490362627\n",
            "epoch 25 batch id 3071 loss 0.14099964499473572 train acc 0.9646593129273853\n",
            "epoch 25 batch id 3081 loss 0.12210758775472641 train acc 0.9646675186627718\n",
            "epoch 25 batch id 3091 loss 0.06221926957368851 train acc 0.9646706163054027\n",
            "epoch 25 batch id 3101 loss 0.06635941565036774 train acc 0.964648500483715\n",
            "epoch 25 batch id 3111 loss 0.0850120335817337 train acc 0.9646516393442623\n",
            "epoch 25 batch id 3121 loss 0.08939272165298462 train acc 0.9646948093559756\n",
            "epoch 25 batch id 3131 loss 0.030399933457374573 train acc 0.9647476844458639\n",
            "epoch 25 batch id 3141 loss 0.11084045469760895 train acc 0.9647256049028972\n",
            "epoch 25 batch id 3151 loss 0.06444579362869263 train acc 0.9647284592192955\n",
            "epoch 25 batch id 3161 loss 0.0911313071846962 train acc 0.9647263524201202\n",
            "epoch 25 batch id 3171 loss 0.1504339873790741 train acc 0.9647193314411857\n",
            "epoch 25 batch id 3181 loss 0.11888014525175095 train acc 0.9647074426281044\n",
            "epoch 25 batch id 3191 loss 0.018638720735907555 train acc 0.9647152146662489\n",
            "epoch 25 batch id 3201 loss 0.13244681060314178 train acc 0.9646887691346454\n",
            "epoch 25 batch id 3211 loss 0.11261393874883652 train acc 0.9646332918094052\n",
            "epoch 25 batch id 3221 loss 0.23765508830547333 train acc 0.9645975628686744\n",
            "epoch 25 batch id 3231 loss 0.1259828358888626 train acc 0.9645813989476942\n",
            "epoch 25 batch id 3241 loss 0.045185793191194534 train acc 0.9645894399876581\n",
            "epoch 25 batch id 3251 loss 0.29513484239578247 train acc 0.9645685942786835\n",
            "epoch 25 batch id 3261 loss 0.12457538396120071 train acc 0.9645430849432689\n",
            "epoch 25 batch id 3271 loss 0.1907651722431183 train acc 0.9644747401406297\n",
            "epoch 25 batch id 3281 loss 0.22957688570022583 train acc 0.9644401478207864\n",
            "epoch 25 batch id 3291 loss 0.08297643065452576 train acc 0.9644532436949256\n",
            "epoch 25 batch id 3301 loss 0.053937721997499466 train acc 0.9644473265677067\n",
            "epoch 25 batch id 3311 loss 0.22156569361686707 train acc 0.9644461643008154\n",
            "epoch 25 batch id 3321 loss 0.10885689407587051 train acc 0.9644732384823849\n",
            "epoch 25 batch id 3331 loss 0.07678790390491486 train acc 0.9644344791353948\n",
            "epoch 25 batch id 3341 loss 0.10212313383817673 train acc 0.9644473959892248\n",
            "epoch 25 batch id 3351 loss 0.04003850743174553 train acc 0.9644229334527007\n",
            "epoch 25 batch id 3361 loss 0.09622776508331299 train acc 0.964421861053258\n",
            "epoch 25 batch id 3371 loss 0.10513550043106079 train acc 0.9644300652625334\n",
            "epoch 25 batch id 3381 loss 0.016307925805449486 train acc 0.9644520851818988\n",
            "epoch 25 batch id 3391 loss 0.3051873445510864 train acc 0.9644140740194633\n",
            "epoch 25 batch id 3401 loss 0.09479869157075882 train acc 0.9644038518082917\n",
            "epoch 25 batch id 3411 loss 0.13371488451957703 train acc 0.9644028510700674\n",
            "epoch 25 batch id 3421 loss 0.11130017042160034 train acc 0.9644155583162818\n",
            "epoch 25 batch id 3431 loss 0.271577388048172 train acc 0.9643780967647916\n",
            "epoch 25 batch id 3441 loss 0.162230983376503 train acc 0.9643817204301075\n",
            "epoch 25 batch id 3451 loss 0.08228994905948639 train acc 0.9643581570559259\n",
            "epoch 25 batch id 3461 loss 0.03276755288243294 train acc 0.9643753611672927\n",
            "epoch 25 batch id 3471 loss 0.12271323055028915 train acc 0.9643924661480842\n",
            "epoch 25 batch id 3481 loss 0.07859157025814056 train acc 0.9643825409365125\n",
            "epoch 25 batch id 3491 loss 0.06903082877397537 train acc 0.9643816241764538\n",
            "epoch 25 batch id 3501 loss 0.0721987783908844 train acc 0.9644253427592117\n",
            "epoch 25 batch id 3511 loss 0.021516844630241394 train acc 0.9644243093135859\n",
            "epoch 25 batch id 3521 loss 0.11990094184875488 train acc 0.9644365947174098\n",
            "epoch 25 batch id 3531 loss 0.061059437692165375 train acc 0.9644399603511753\n",
            "epoch 25 batch id 3541 loss 0.05755976587533951 train acc 0.9644388943801186\n",
            "epoch 25 batch id 3551 loss 0.09867752343416214 train acc 0.964415833568009\n",
            "epoch 25 batch id 3561 loss 0.029279667884111404 train acc 0.9644323925863522\n",
            "epoch 25 batch id 3571 loss 0.14028066396713257 train acc 0.9644357322878745\n",
            "epoch 25 batch id 3581 loss 0.25462862849235535 train acc 0.9644390533370567\n",
            "epoch 25 batch id 3591 loss 0.189839705824852 train acc 0.9644206001113896\n",
            "epoch 25 batch id 3601 loss 0.1504175215959549 train acc 0.9644239447375729\n",
            "epoch 25 batch id 3611 loss 0.007261972408741713 train acc 0.9644791955137081\n",
            "epoch 25 batch id 3621 loss 0.03885762393474579 train acc 0.9644694145263739\n",
            "epoch 25 batch id 3631 loss 0.05868127569556236 train acc 0.9644467777471771\n",
            "epoch 25 batch id 3641 loss 0.1385170966386795 train acc 0.9644628879428728\n",
            "epoch 25 batch id 3651 loss 0.0767841711640358 train acc 0.9644361133935908\n",
            "epoch 25 batch id 3661 loss 0.03736202046275139 train acc 0.9644265569516526\n",
            "epoch 25 batch id 3671 loss 0.09292197227478027 train acc 0.9644383342413512\n",
            "epoch 25 batch id 3681 loss 0.06925027072429657 train acc 0.9644542923118717\n",
            "epoch 25 batch id 3691 loss 0.07471897453069687 train acc 0.9644870969926849\n",
            "epoch 25 batch id 3701 loss 0.05479186773300171 train acc 0.9644817279113753\n",
            "epoch 25 batch id 3711 loss 0.07492600381374359 train acc 0.9645184923201293\n",
            "epoch 25 batch id 3721 loss 0.13298852741718292 train acc 0.9645340634238108\n",
            "epoch 25 batch id 3731 loss 0.04818755015730858 train acc 0.9645495510586974\n",
            "epoch 25 batch id 3741 loss 0.17616496980190277 train acc 0.964514835605453\n",
            "epoch 25 batch id 3751 loss 0.09603869915008545 train acc 0.964513629698747\n",
            "epoch 25 batch id 3761 loss 0.16689294576644897 train acc 0.9645082757245413\n",
            "epoch 25 batch id 3771 loss 0.05744105949997902 train acc 0.9644905197560328\n",
            "epoch 25 batch id 3781 loss 0.1139407753944397 train acc 0.9644893877281142\n",
            "epoch 25 batch id 3791 loss 0.10520143806934357 train acc 0.9644965048799788\n",
            "epoch 25 batch id 3801 loss 0.0545833595097065 train acc 0.9645035845830044\n",
            "epoch 25 batch id 3811 loss 0.13083994388580322 train acc 0.9645352269745474\n",
            "epoch 25 batch id 3821 loss 0.16830959916114807 train acc 0.9645176328186339\n",
            "epoch 25 batch id 3831 loss 0.1020500436425209 train acc 0.9645123662229182\n",
            "epoch 25 batch id 3841 loss 0.052789293229579926 train acc 0.9644949231970841\n",
            "epoch 25 batch id 3851 loss 0.0029790957923978567 train acc 0.9645140872500649\n",
            "epoch 25 batch id 3861 loss 0.022129889577627182 train acc 0.9645210113960114\n",
            "epoch 25 batch id 3871 loss 0.09201686829328537 train acc 0.964527899767502\n",
            "epoch 25 batch id 3881 loss 0.13533048331737518 train acc 0.9645468307137336\n",
            "epoch 25 batch id 3891 loss 0.012346707284450531 train acc 0.9645496016448214\n",
            "epoch 25 batch id 3901 loss 0.052963029593229294 train acc 0.9645123045372981\n",
            "epoch 25 batch id 3911 loss 0.03852459788322449 train acc 0.9644991690104833\n",
            "epoch 25 batch id 3921 loss 0.03142792731523514 train acc 0.9644900854373885\n",
            "epoch 25 batch id 3931 loss 0.0688663050532341 train acc 0.9644810480793691\n",
            "epoch 25 batch id 3941 loss 0.250349760055542 train acc 0.9644958449632073\n",
            "epoch 25 batch id 3951 loss 0.04190452769398689 train acc 0.9644789293849658\n",
            "epoch 25 batch id 3961 loss 0.060762032866477966 train acc 0.9644699886392325\n",
            "epoch 25 batch id 3971 loss 0.12524715065956116 train acc 0.9644728972550994\n",
            "epoch 25 batch id 3981 loss 0.05289854109287262 train acc 0.9644522418990203\n",
            "epoch 25 batch id 3991 loss 0.15864436328411102 train acc 0.9644630105236782\n",
            "epoch 25 batch id 4001 loss 0.09107424318790436 train acc 0.9644268620344914\n",
            "epoch 25 batch id 4011 loss 0.10554427653551102 train acc 0.9644103714784343\n",
            "epoch 25 batch id 4021 loss 0.23393072187900543 train acc 0.9643823053966675\n",
            "epoch 25 batch id 4031 loss 0.048766784369945526 train acc 0.9644163979161499\n",
            "epoch 25 batch id 4041 loss 0.05745532363653183 train acc 0.9644193887651571\n",
            "epoch 25 batch id 4051 loss 0.029970290139317513 train acc 0.9644069365588743\n",
            "epoch 25 batch id 4061 loss 0.09337443113327026 train acc 0.9644253262743167\n",
            "epoch 25 batch id 4071 loss 0.17625167965888977 train acc 0.964466654384672\n",
            "epoch 25 batch id 4081 loss 0.07339096814393997 train acc 0.964484807645185\n",
            "epoch 25 batch id 4091 loss 0.21729989349842072 train acc 0.9644455817648496\n",
            "epoch 25 batch id 4101 loss 0.1267811357975006 train acc 0.9644522677395757\n",
            "epoch 25 batch id 4111 loss 0.029436470940709114 train acc 0.9644247141814644\n",
            "epoch 25 batch id 4121 loss 0.09447716921567917 train acc 0.964442793011405\n",
            "epoch 25 batch id 4131 loss 0.1555986851453781 train acc 0.96442674291939\n",
            "epoch 25 batch id 4141 loss 0.12387512624263763 train acc 0.9644220900748611\n",
            "epoch 25 batch id 4151 loss 0.12689663469791412 train acc 0.9644325162611419\n",
            "epoch 25 batch id 4161 loss 0.1318996399641037 train acc 0.9644166065849555\n",
            "epoch 25 batch id 4171 loss 0.10055717080831528 train acc 0.9644082654039798\n",
            "epoch 25 batch id 4181 loss 0.0169582050293684 train acc 0.964411175556087\n",
            "epoch 25 batch id 4191 loss 0.1769867092370987 train acc 0.9644178000477213\n",
            "epoch 25 batch id 4201 loss 0.05227530747652054 train acc 0.9644429897643418\n",
            "epoch 25 batch id 4211 loss 0.08889231085777283 train acc 0.9644124020422702\n",
            "epoch 25 batch id 4221 loss 0.18909142911434174 train acc 0.9643856609808102\n",
            "epoch 25 batch id 4231 loss 0.05138575658202171 train acc 0.9644070550697235\n",
            "epoch 25 batch id 4241 loss 0.022996485233306885 train acc 0.964409926904032\n",
            "epoch 25 batch id 4251 loss 0.10329756140708923 train acc 0.9644458656786639\n",
            "epoch 25 batch id 4261 loss 0.014479240402579308 train acc 0.9644596338887585\n",
            "epoch 25 batch id 4271 loss 0.016634706407785416 train acc 0.9644952879887614\n",
            "epoch 25 batch id 4281 loss 0.10233009606599808 train acc 0.9645234758234057\n",
            "epoch 25 batch id 4291 loss 0.08721333742141724 train acc 0.9645260428804474\n",
            "epoch 25 batch id 4301 loss 0.07953832298517227 train acc 0.9645031678679377\n",
            "epoch 25 batch id 4311 loss 0.08144821226596832 train acc 0.9645420146137788\n",
            "epoch 25 batch id 4321 loss 0.06293056160211563 train acc 0.964522824577644\n",
            "epoch 25 batch id 4331 loss 0.1109594777226448 train acc 0.9645217617178481\n",
            "epoch 25 batch id 4341 loss 0.07448074966669083 train acc 0.9645423001612532\n",
            "epoch 25 batch id 4351 loss 0.11630083620548248 train acc 0.9645196506550219\n",
            "epoch 25 batch id 4361 loss 0.07717528939247131 train acc 0.9645293510662692\n",
            "epoch 25 batch id 4371 loss 0.07678335905075073 train acc 0.9645247083047358\n",
            "epoch 25 batch id 4381 loss 0.1771790087223053 train acc 0.9645165202008674\n",
            "epoch 25 batch id 4391 loss 0.013123567216098309 train acc 0.9645261614666363\n",
            "epoch 25 batch id 4401 loss 0.1735796183347702 train acc 0.9645286582594865\n",
            "epoch 25 batch id 4411 loss 0.06250908225774765 train acc 0.9645276014509182\n",
            "epoch 25 batch id 4421 loss 0.115781769156456 train acc 0.9645300836914725\n",
            "epoch 25 batch id 4431 loss 0.024399908259510994 train acc 0.9645184495599187\n",
            "epoch 25 batch id 4441 loss 0.014533476904034615 train acc 0.9645385329880658\n",
            "epoch 25 batch id 4451 loss 0.043191276490688324 train acc 0.964562036620984\n",
            "epoch 25 batch id 4461 loss 0.0677737295627594 train acc 0.9645609168347904\n",
            "epoch 25 batch id 4471 loss 0.09366865456104279 train acc 0.9645563073138\n",
            "epoch 25 batch id 4481 loss 0.08536671102046967 train acc 0.9645691530908279\n",
            "epoch 25 batch id 4491 loss 0.06257789582014084 train acc 0.9645749832999332\n",
            "epoch 25 batch id 4501 loss 0.084065280854702 train acc 0.9645773161519662\n",
            "epoch 25 batch id 4511 loss 0.29878750443458557 train acc 0.9645588561294614\n",
            "epoch 25 batch id 4521 loss 0.08605557680130005 train acc 0.9645577582393275\n",
            "epoch 25 batch id 4531 loss 0.006460514850914478 train acc 0.9645670105936879\n",
            "epoch 25 batch id 4541 loss 0.08433607965707779 train acc 0.964583103941863\n",
            "epoch 25 batch id 4551 loss 0.24575695395469666 train acc 0.9645956932542299\n",
            "epoch 25 batch id 4561 loss 0.10886090248823166 train acc 0.9645910984433238\n",
            "epoch 25 batch id 4571 loss 0.09849918633699417 train acc 0.9645967786042442\n",
            "epoch 25 batch id 4581 loss 0.09709859639406204 train acc 0.9645990231390527\n",
            "epoch 25 batch id 4591 loss 0.05372293293476105 train acc 0.9645944510999782\n",
            "epoch 25 batch id 4601 loss 0.1087273508310318 train acc 0.9645661269289285\n",
            "epoch 25 batch id 4611 loss 0.10097309201955795 train acc 0.9645684233355021\n",
            "epoch 25 batch id 4621 loss 0.0786735862493515 train acc 0.9645707098030729\n",
            "epoch 25 batch id 4631 loss 0.10004035383462906 train acc 0.9645662383934356\n",
            "epoch 25 batch id 4641 loss 0.16815900802612305 train acc 0.9645920868347339\n",
            "epoch 25 batch id 4651 loss 0.20513969659805298 train acc 0.9646043861535154\n",
            "epoch 25 batch id 4661 loss 0.05766298249363899 train acc 0.9646166326968462\n",
            "epoch 25 batch id 4671 loss 0.03780588507652283 train acc 0.964638862128024\n",
            "epoch 25 batch id 4681 loss 0.2372525930404663 train acc 0.9646543206579791\n",
            "epoch 25 batch id 4691 loss 0.01342082116752863 train acc 0.9646730441270518\n",
            "epoch 25 batch id 4701 loss 0.11879586428403854 train acc 0.9646817166560306\n",
            "epoch 25 batch id 4711 loss 0.045274097472429276 train acc 0.9646538686053916\n",
            "epoch 25 batch id 4721 loss 0.07411076128482819 train acc 0.96463606757043\n",
            "epoch 25 batch id 4731 loss 0.051925450563430786 train acc 0.9646216444726273\n",
            "epoch 25 batch id 4741 loss 0.05901484936475754 train acc 0.964613873655347\n",
            "epoch 25 batch id 4751 loss 0.16995824873447418 train acc 0.964616001894338\n",
            "epoch 25 batch id 4761 loss 0.10967016220092773 train acc 0.9646279668136946\n",
            "epoch 25 batch id 4771 loss 0.06151425465941429 train acc 0.9646431565709495\n",
            "epoch 25 batch id 4781 loss 0.1611717939376831 train acc 0.9646288694833717\n",
            "epoch 25 batch id 4791 loss 0.19279207289218903 train acc 0.9646015967438948\n",
            "epoch 25 batch id 4801 loss 0.09130360186100006 train acc 0.9645939647990002\n",
            "epoch 25 batch id 4811 loss 0.15353292226791382 train acc 0.96459610787778\n",
            "epoch 25 batch id 4821 loss 0.17649804055690765 train acc 0.9646014830947937\n",
            "epoch 25 batch id 4831 loss 0.03087654337286949 train acc 0.9646327106189195\n",
            "epoch 25 batch id 4841 loss 0.14089737832546234 train acc 0.9646186221854989\n",
            "epoch 25 batch id 4851 loss 0.056990575045347214 train acc 0.9646239177489178\n",
            "epoch 25 batch id 4861 loss 0.03221288323402405 train acc 0.964613119728451\n",
            "epoch 25 batch id 4871 loss 0.05582951009273529 train acc 0.9646280281256415\n",
            "epoch 25 batch id 4881 loss 0.190769761800766 train acc 0.9646268694939562\n",
            "epoch 25 batch id 4891 loss 0.055225711315870285 train acc 0.9646129370271928\n",
            "epoch 25 batch id 4901 loss 0.07356856018304825 train acc 0.9646277545398898\n",
            "epoch 25 batch id 4911 loss 0.20181116461753845 train acc 0.9646138770107922\n",
            "epoch 25 batch id 4921 loss 0.1837415099143982 train acc 0.9646000558829506\n",
            "epoch 25 batch id 4931 loss 0.30185389518737793 train acc 0.9645704471709592\n",
            "epoch 25 batch id 4941 loss 0.050696223974227905 train acc 0.9645630945152803\n",
            "epoch 25 batch id 4951 loss 0.13874156773090363 train acc 0.9645526156332054\n",
            "epoch 25 train acc 0.9645482398628202\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "884a1becfb5d4e44864bc6487d5a2036",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 25 loss 0.578264594078064 test acc 0.8349111070381232\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838fffc74c694ffca24c56aae8146671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 26 batch id 1 loss 0.08910667151212692 train acc 0.953125\n",
            "epoch 26 batch id 11 loss 0.037380870431661606 train acc 0.9673295454545454\n",
            "epoch 26 batch id 21 loss 0.01216110959649086 train acc 0.9672619047619048\n",
            "epoch 26 batch id 31 loss 0.09057959169149399 train acc 0.967741935483871\n",
            "epoch 26 batch id 41 loss 0.11352284997701645 train acc 0.9717987804878049\n",
            "epoch 26 batch id 51 loss 0.015593525022268295 train acc 0.9721200980392157\n",
            "epoch 26 batch id 61 loss 0.04929785430431366 train acc 0.9697745901639344\n",
            "epoch 26 batch id 71 loss 0.1704360842704773 train acc 0.96875\n",
            "epoch 26 batch id 81 loss 0.10251765698194504 train acc 0.9685570987654321\n",
            "epoch 26 batch id 91 loss 0.08686063438653946 train acc 0.9680631868131868\n",
            "epoch 26 batch id 101 loss 0.0975838452577591 train acc 0.9664294554455446\n",
            "epoch 26 batch id 111 loss 0.004129838664084673 train acc 0.9672015765765766\n",
            "epoch 26 batch id 121 loss 0.1799115389585495 train acc 0.9672004132231405\n",
            "epoch 26 batch id 131 loss 0.06616798788309097 train acc 0.967199427480916\n",
            "epoch 26 batch id 141 loss 0.04948475956916809 train acc 0.9673093971631206\n",
            "epoch 26 batch id 151 loss 0.04849967360496521 train acc 0.9674048013245033\n",
            "epoch 26 batch id 161 loss 0.06414108723402023 train acc 0.9672942546583851\n",
            "epoch 26 batch id 171 loss 0.22611983120441437 train acc 0.9665570175438597\n",
            "epoch 26 batch id 181 loss 0.03623609617352486 train acc 0.9670234806629834\n",
            "epoch 26 batch id 191 loss 0.122374527156353 train acc 0.9669502617801047\n",
            "epoch 26 batch id 201 loss 0.22051219642162323 train acc 0.9672730099502488\n",
            "epoch 26 batch id 211 loss 0.06525512784719467 train acc 0.9674911137440758\n",
            "epoch 26 batch id 221 loss 0.1960850954055786 train acc 0.9672652714932126\n",
            "epoch 26 batch id 231 loss 0.1680402308702469 train acc 0.9672619047619048\n",
            "epoch 26 batch id 241 loss 0.11892489343881607 train acc 0.9668698132780082\n",
            "epoch 26 batch id 251 loss 0.14193095266819 train acc 0.9669447211155379\n",
            "epoch 26 batch id 261 loss 0.13203898072242737 train acc 0.9668342911877394\n",
            "epoch 26 batch id 271 loss 0.05383594334125519 train acc 0.9673662361623616\n",
            "epoch 26 batch id 281 loss 0.014353320933878422 train acc 0.9673598754448398\n",
            "epoch 26 batch id 291 loss 0.0812663733959198 train acc 0.9677298109965635\n",
            "epoch 26 batch id 301 loss 0.09673614799976349 train acc 0.9673484219269103\n",
            "epoch 26 batch id 311 loss 0.10548597574234009 train acc 0.9673934887459807\n",
            "epoch 26 batch id 321 loss 0.25577861070632935 train acc 0.9672897196261683\n",
            "epoch 26 batch id 331 loss 0.024251580238342285 train acc 0.9674282477341389\n",
            "epoch 26 batch id 341 loss 0.20216673612594604 train acc 0.9675586510263929\n",
            "epoch 26 batch id 351 loss 0.11797899752855301 train acc 0.9673254985754985\n",
            "epoch 26 batch id 361 loss 0.059914279729127884 train acc 0.9673649584487535\n",
            "epoch 26 batch id 371 loss 0.10224684327840805 train acc 0.967739218328841\n",
            "epoch 26 batch id 381 loss 0.013758968561887741 train acc 0.9679297900262467\n",
            "epoch 26 batch id 391 loss 0.09724187850952148 train acc 0.9678708439897699\n",
            "epoch 26 batch id 401 loss 0.0007549416041001678 train acc 0.9677758728179551\n",
            "epoch 26 batch id 411 loss 0.051480744034051895 train acc 0.9678756082725061\n",
            "epoch 26 batch id 421 loss 0.1662941873073578 train acc 0.9680819477434679\n",
            "epoch 26 batch id 431 loss 0.12048090994358063 train acc 0.9679161832946636\n",
            "epoch 26 batch id 441 loss 0.23257321119308472 train acc 0.967687074829932\n",
            "epoch 26 batch id 451 loss 0.10817854106426239 train acc 0.9676759977827051\n",
            "epoch 26 batch id 461 loss 0.11916201561689377 train acc 0.9675976138828634\n",
            "epoch 26 batch id 471 loss 0.020313013345003128 train acc 0.9680201698513801\n",
            "epoch 26 batch id 481 loss 0.10694902390241623 train acc 0.9681652806652806\n",
            "epoch 26 batch id 491 loss 0.032694995403289795 train acc 0.9682726578411406\n",
            "epoch 26 batch id 501 loss 0.2646072506904602 train acc 0.9682198103792415\n",
            "epoch 26 batch id 511 loss 0.09807876497507095 train acc 0.9682301859099804\n",
            "epoch 26 batch id 521 loss 0.134237602353096 train acc 0.9683601247600768\n",
            "epoch 26 batch id 531 loss 0.03685808554291725 train acc 0.9684557438794726\n",
            "epoch 26 batch id 541 loss 0.05623525381088257 train acc 0.968259011090573\n",
            "epoch 26 batch id 551 loss 0.06290509551763535 train acc 0.9682395644283122\n",
            "epoch 26 batch id 561 loss 0.19254590570926666 train acc 0.9679979946524064\n",
            "epoch 26 batch id 571 loss 0.23223687708377838 train acc 0.9679564360770578\n",
            "epoch 26 batch id 581 loss 0.04691450670361519 train acc 0.9678356282271945\n",
            "epoch 26 batch id 591 loss 0.058784980326890945 train acc 0.9678246615905245\n",
            "epoch 26 batch id 601 loss 0.07136522978544235 train acc 0.9678920549084858\n",
            "epoch 26 batch id 611 loss 0.24578706920146942 train acc 0.9675480769230769\n",
            "epoch 26 batch id 621 loss 0.18694525957107544 train acc 0.9675674315619968\n",
            "epoch 26 batch id 631 loss 0.05642365664243698 train acc 0.9675118858954042\n",
            "epoch 26 batch id 641 loss 0.21040412783622742 train acc 0.967579953198128\n",
            "epoch 26 batch id 651 loss 0.022357499226927757 train acc 0.967357910906298\n",
            "epoch 26 batch id 661 loss 0.14800222218036652 train acc 0.9674498865355522\n",
            "epoch 26 batch id 671 loss 0.07273899763822556 train acc 0.9674459761549925\n",
            "epoch 26 batch id 681 loss 0.13342174887657166 train acc 0.9675339574155654\n",
            "epoch 26 batch id 691 loss 0.07556513696908951 train acc 0.9674837192474675\n",
            "epoch 26 batch id 701 loss 0.1614893674850464 train acc 0.9674572039942939\n",
            "epoch 26 batch id 711 loss 0.1553858071565628 train acc 0.9672995780590717\n",
            "epoch 26 batch id 721 loss 0.18146944046020508 train acc 0.9672763522884882\n",
            "epoch 26 batch id 731 loss 0.036295827478170395 train acc 0.9672537619699042\n",
            "epoch 26 batch id 741 loss 0.1074620857834816 train acc 0.9672528677462888\n",
            "epoch 26 batch id 751 loss 0.113357312977314 train acc 0.9671271637816246\n",
            "epoch 26 batch id 761 loss 0.05645928904414177 train acc 0.9670868922470434\n",
            "epoch 26 batch id 771 loss 0.04712403938174248 train acc 0.9670273994811932\n",
            "epoch 26 batch id 781 loss 0.08889446407556534 train acc 0.9669294174135723\n",
            "epoch 26 batch id 791 loss 0.08715599030256271 train acc 0.9668536662452591\n",
            "epoch 26 batch id 801 loss 0.010991029441356659 train acc 0.966916354556804\n",
            "epoch 26 batch id 811 loss 0.11496119946241379 train acc 0.966977496917386\n",
            "epoch 26 batch id 821 loss 0.027448005974292755 train acc 0.9671132764920828\n",
            "epoch 26 batch id 831 loss 0.20550864934921265 train acc 0.9670389590854392\n",
            "epoch 26 batch id 841 loss 0.04504714161157608 train acc 0.9670593043995244\n",
            "epoch 26 batch id 851 loss 0.06592030823230743 train acc 0.9670057285546416\n",
            "epoch 26 batch id 861 loss 0.09957022964954376 train acc 0.96693524970964\n",
            "epoch 26 batch id 871 loss 0.055558085441589355 train acc 0.9668484500574053\n",
            "epoch 26 batch id 881 loss 0.05787527561187744 train acc 0.9667813564131669\n",
            "epoch 26 batch id 891 loss 0.004728999454528093 train acc 0.9669262065095399\n",
            "epoch 26 batch id 901 loss 0.24181127548217773 train acc 0.9668250554938956\n",
            "epoch 26 batch id 911 loss 0.15764591097831726 train acc 0.9668976399560922\n",
            "epoch 26 batch id 921 loss 0.04940531775355339 train acc 0.966849891422367\n",
            "epoch 26 batch id 931 loss 0.015277721919119358 train acc 0.9667863856068744\n",
            "epoch 26 batch id 941 loss 0.030346648767590523 train acc 0.9668238575982997\n",
            "epoch 26 batch id 951 loss 0.03943513706326485 train acc 0.9667783911671924\n",
            "epoch 26 batch id 961 loss 0.11953852325677872 train acc 0.9666688345473465\n",
            "epoch 26 batch id 971 loss 0.13852961361408234 train acc 0.9666580844490217\n",
            "epoch 26 batch id 981 loss 0.02395167574286461 train acc 0.9667590468909276\n",
            "epoch 26 batch id 991 loss 0.19465292990207672 train acc 0.9668422048435923\n",
            "epoch 26 batch id 1001 loss 0.18330292403697968 train acc 0.9668144355644356\n",
            "epoch 26 batch id 1011 loss 0.02052658423781395 train acc 0.9668026706231454\n",
            "epoch 26 batch id 1021 loss 0.029250310733914375 train acc 0.9668523506366308\n",
            "epoch 26 batch id 1031 loss 0.060883499681949615 train acc 0.9668707565470417\n",
            "epoch 26 batch id 1041 loss 0.16614505648612976 train acc 0.9667987512007685\n",
            "epoch 26 batch id 1051 loss 0.04495112970471382 train acc 0.9667429828734538\n",
            "epoch 26 batch id 1061 loss 0.025119712576270103 train acc 0.9667177191328935\n",
            "epoch 26 batch id 1071 loss 0.006001913920044899 train acc 0.9666929271708683\n",
            "epoch 26 batch id 1081 loss 0.26058244705200195 train acc 0.9665963228492137\n",
            "epoch 26 batch id 1091 loss 0.33688056468963623 train acc 0.966487167736022\n",
            "epoch 26 batch id 1101 loss 0.05697429180145264 train acc 0.9664793369663942\n",
            "epoch 26 batch id 1111 loss 0.09745976328849792 train acc 0.9664575832583259\n",
            "epoch 26 batch id 1121 loss 0.09169220924377441 train acc 0.9665756021409456\n",
            "epoch 26 batch id 1131 loss 0.22266793251037598 train acc 0.9666086427939876\n",
            "epoch 26 batch id 1141 loss 0.05693012848496437 train acc 0.9666547984224365\n",
            "epoch 26 batch id 1151 loss 0.12044421583414078 train acc 0.9666187011294527\n",
            "epoch 26 batch id 1161 loss 0.15738171339035034 train acc 0.9665966838931955\n",
            "epoch 26 batch id 1171 loss 0.11812880635261536 train acc 0.9665350128095644\n",
            "epoch 26 batch id 1181 loss 0.06380347907543182 train acc 0.9665405376799323\n",
            "epoch 26 batch id 1191 loss 0.025061873719096184 train acc 0.9665984466834593\n",
            "epoch 26 batch id 1201 loss 0.17001882195472717 train acc 0.9667204412989175\n",
            "epoch 26 batch id 1211 loss 0.06228490546345711 train acc 0.9668017134599505\n",
            "epoch 26 batch id 1221 loss 0.20036444067955017 train acc 0.9668560606060606\n",
            "epoch 26 batch id 1231 loss 0.03997525945305824 train acc 0.9668079813160032\n",
            "epoch 26 batch id 1241 loss 0.04517995938658714 train acc 0.9667984488315874\n",
            "epoch 26 batch id 1251 loss 0.12296333909034729 train acc 0.9667640887290168\n",
            "epoch 26 batch id 1261 loss 0.10040117055177689 train acc 0.9667922283901665\n",
            "epoch 26 batch id 1271 loss 0.07719463109970093 train acc 0.9668199252557041\n",
            "epoch 26 batch id 1281 loss 0.01257246918976307 train acc 0.9669325722092116\n",
            "epoch 26 batch id 1291 loss 0.17013126611709595 train acc 0.9669829589465531\n",
            "epoch 26 batch id 1301 loss 0.005822901148349047 train acc 0.9670205611068409\n",
            "epoch 26 batch id 1311 loss 0.12775330245494843 train acc 0.9670099160945843\n",
            "epoch 26 batch id 1321 loss 0.028246847912669182 train acc 0.9669402914458743\n",
            "epoch 26 batch id 1331 loss 0.2287214696407318 train acc 0.9668951915852743\n",
            "epoch 26 batch id 1341 loss 0.2206503450870514 train acc 0.9668158090976883\n",
            "epoch 26 batch id 1351 loss 0.0454353392124176 train acc 0.966795429311621\n",
            "epoch 26 batch id 1361 loss 0.06270720809698105 train acc 0.9667523879500367\n",
            "epoch 26 batch id 1371 loss 0.025372181087732315 train acc 0.9667555616338439\n",
            "epoch 26 batch id 1381 loss 0.09928229451179504 train acc 0.9667247465604635\n",
            "epoch 26 batch id 1391 loss 0.06936836987733841 train acc 0.9666831416247305\n",
            "epoch 26 batch id 1401 loss 0.01777452789247036 train acc 0.9667648108493933\n",
            "epoch 26 batch id 1411 loss 0.02659601904451847 train acc 0.9667567328136074\n",
            "epoch 26 batch id 1421 loss 0.11010456085205078 train acc 0.9667157811400422\n",
            "epoch 26 batch id 1431 loss 0.023721707984805107 train acc 0.9666754018169113\n",
            "epoch 26 batch id 1441 loss 0.05622635781764984 train acc 0.9666572692574601\n",
            "epoch 26 batch id 1451 loss 0.03422406315803528 train acc 0.9668009131633356\n",
            "epoch 26 batch id 1461 loss 0.0131244957447052 train acc 0.9668035592060232\n",
            "epoch 26 batch id 1471 loss 0.05201202630996704 train acc 0.9668699014276003\n",
            "epoch 26 batch id 1481 loss 0.07544512301683426 train acc 0.9668509453072248\n",
            "epoch 26 batch id 1491 loss 0.1720762848854065 train acc 0.9668427230046949\n",
            "epoch 26 batch id 1501 loss 0.06819215416908264 train acc 0.9668554297135243\n",
            "epoch 26 batch id 1511 loss 0.03731490299105644 train acc 0.9669093315684977\n",
            "epoch 26 batch id 1521 loss 0.015203109942376614 train acc 0.9670036160420776\n",
            "epoch 26 batch id 1531 loss 0.026052355766296387 train acc 0.9670150228608753\n",
            "epoch 26 batch id 1541 loss 0.04399656131863594 train acc 0.9670465606748865\n",
            "epoch 26 batch id 1551 loss 0.04581104964017868 train acc 0.9669467279174726\n",
            "epoch 26 batch id 1561 loss 0.20331822335720062 train acc 0.9669382607303011\n",
            "epoch 26 batch id 1571 loss 0.026051990687847137 train acc 0.9669497931253979\n",
            "epoch 26 batch id 1581 loss 0.06335300207138062 train acc 0.9669315306767868\n",
            "epoch 26 batch id 1591 loss 0.19189737737178802 train acc 0.9669527812696417\n",
            "epoch 26 batch id 1601 loss 0.26352307200431824 train acc 0.9669444878201124\n",
            "epoch 26 batch id 1611 loss 0.14104115962982178 train acc 0.966907200496586\n",
            "epoch 26 batch id 1621 loss 0.09199202060699463 train acc 0.9669474861196792\n",
            "epoch 26 batch id 1631 loss 0.00145414297003299 train acc 0.9669776977314531\n",
            "epoch 26 batch id 1641 loss 0.09504120796918869 train acc 0.9669599329677027\n",
            "epoch 26 batch id 1651 loss 0.11237121373414993 train acc 0.9669518473652332\n",
            "epoch 26 batch id 1661 loss 0.17435304820537567 train acc 0.9670379289584587\n",
            "epoch 26 batch id 1671 loss 0.20445643365383148 train acc 0.966973369239976\n",
            "epoch 26 batch id 1681 loss 0.08170782774686813 train acc 0.9669374628197501\n",
            "epoch 26 batch id 1691 loss 0.05899606645107269 train acc 0.9669389414547604\n",
            "epoch 26 batch id 1701 loss 0.05070855841040611 train acc 0.9669771457965902\n",
            "epoch 26 batch id 1711 loss 0.11480296403169632 train acc 0.966969243132671\n",
            "epoch 26 batch id 1721 loss 0.030310798436403275 train acc 0.9669886693782684\n",
            "epoch 26 batch id 1731 loss 0.004311076831072569 train acc 0.9669988445984979\n",
            "epoch 26 batch id 1741 loss 0.12259290367364883 train acc 0.9670537765651924\n",
            "epoch 26 batch id 1751 loss 0.06022806093096733 train acc 0.9671259280411194\n",
            "epoch 26 batch id 1761 loss 0.01962098851799965 train acc 0.9671795144804088\n",
            "epoch 26 batch id 1771 loss 0.009692012332379818 train acc 0.9672148503670243\n",
            "epoch 26 batch id 1781 loss 0.09359467029571533 train acc 0.9672497894441325\n",
            "epoch 26 batch id 1791 loss 0.06687481701374054 train acc 0.9672058207705193\n",
            "epoch 26 batch id 1801 loss 0.03908172994852066 train acc 0.9672317462520822\n",
            "epoch 26 batch id 1811 loss 0.07768168300390244 train acc 0.9672487575924903\n",
            "epoch 26 batch id 1821 loss 0.17463907599449158 train acc 0.9672226798462383\n",
            "epoch 26 batch id 1831 loss 0.06308367848396301 train acc 0.9672139541234298\n",
            "epoch 26 batch id 1841 loss 0.10562507063150406 train acc 0.9671543997827268\n",
            "epoch 26 batch id 1851 loss 0.09049756824970245 train acc 0.9671714613722312\n",
            "epoch 26 batch id 1861 loss 0.04964069277048111 train acc 0.967171547555078\n",
            "epoch 26 batch id 1871 loss 0.13616058230400085 train acc 0.9672801977552111\n",
            "epoch 26 batch id 1881 loss 0.0644368976354599 train acc 0.9672547846889952\n",
            "epoch 26 batch id 1891 loss 0.10171809792518616 train acc 0.96727921734532\n",
            "epoch 26 batch id 1901 loss 0.08776181191205978 train acc 0.9673198316675434\n",
            "epoch 26 batch id 1911 loss 0.08444032818078995 train acc 0.9673027864992151\n",
            "epoch 26 batch id 1921 loss 0.029721833765506744 train acc 0.9672777850078085\n",
            "epoch 26 batch id 1931 loss 0.03977115452289581 train acc 0.9672935007767995\n",
            "epoch 26 batch id 1941 loss 0.3091415762901306 train acc 0.9673090546110252\n",
            "epoch 26 batch id 1951 loss 0.18751227855682373 train acc 0.9672924141465915\n",
            "epoch 26 batch id 1961 loss 0.039994992315769196 train acc 0.9673556221315656\n",
            "epoch 26 batch id 1971 loss 0.017623478546738625 train acc 0.96740233384069\n",
            "epoch 26 batch id 1981 loss 0.04405262693762779 train acc 0.9674091367995962\n",
            "epoch 26 batch id 1991 loss 0.1049853265285492 train acc 0.9674237192365646\n",
            "epoch 26 batch id 2001 loss 0.15038439631462097 train acc 0.9673756871564217\n",
            "epoch 26 batch id 2011 loss 0.10892582684755325 train acc 0.9673514420686226\n",
            "epoch 26 batch id 2021 loss 0.05937495827674866 train acc 0.9673506308758041\n",
            "epoch 26 batch id 2031 loss 0.015551933087408543 train acc 0.9673190546528804\n",
            "epoch 26 batch id 2041 loss 0.1064925268292427 train acc 0.9672724767270946\n",
            "epoch 26 batch id 2051 loss 0.18040740489959717 train acc 0.9672872988785958\n",
            "epoch 26 batch id 2061 loss 0.017569083720445633 train acc 0.9673095584667637\n",
            "epoch 26 batch id 2071 loss 0.11181550472974777 train acc 0.9672863351038146\n",
            "epoch 26 batch id 2081 loss 0.12203828245401382 train acc 0.9672933685728016\n",
            "epoch 26 batch id 2091 loss 0.10004759579896927 train acc 0.9672480272596844\n",
            "epoch 26 batch id 2101 loss 0.0624779537320137 train acc 0.9672477391718229\n",
            "epoch 26 batch id 2111 loss 0.02094249427318573 train acc 0.967299265750829\n",
            "epoch 26 batch id 2121 loss 0.05768023058772087 train acc 0.9673061056105611\n",
            "epoch 26 batch id 2131 loss 0.018119582906365395 train acc 0.9673202135147818\n",
            "epoch 26 batch id 2141 loss 0.07941834628582001 train acc 0.9673049976646427\n",
            "epoch 26 batch id 2151 loss 0.0490766316652298 train acc 0.9673407717340772\n",
            "epoch 26 batch id 2161 loss 0.1762450635433197 train acc 0.9673039102267469\n",
            "epoch 26 batch id 2171 loss 0.04941819608211517 train acc 0.9673249654537079\n",
            "epoch 26 batch id 2181 loss 0.07487703114748001 train acc 0.9672741861531408\n",
            "epoch 26 batch id 2191 loss 0.0467490553855896 train acc 0.9672951848471018\n",
            "epoch 26 batch id 2201 loss 0.04047137871384621 train acc 0.9672662994093594\n",
            "epoch 26 batch id 2211 loss 0.026696406304836273 train acc 0.9672376752600633\n",
            "epoch 26 batch id 2221 loss 0.16441957652568817 train acc 0.9672163439891941\n",
            "epoch 26 batch id 2231 loss 0.027911607176065445 train acc 0.9672232182877634\n",
            "epoch 26 batch id 2241 loss 0.2918558418750763 train acc 0.9672091142347167\n",
            "epoch 26 batch id 2251 loss 0.05431276932358742 train acc 0.9671604286983563\n",
            "epoch 26 batch id 2261 loss 0.01730233244597912 train acc 0.9671881910659\n",
            "epoch 26 batch id 2271 loss 0.030572205781936646 train acc 0.9672088287098195\n",
            "epoch 26 batch id 2281 loss 0.09494838118553162 train acc 0.9672018851380974\n",
            "epoch 26 batch id 2291 loss 0.24821217358112335 train acc 0.9672154626800524\n",
            "epoch 26 batch id 2301 loss 0.04235256463289261 train acc 0.9672425032594524\n",
            "epoch 26 batch id 2311 loss 0.10629744827747345 train acc 0.967262548680225\n",
            "epoch 26 batch id 2321 loss 0.07846542447805405 train acc 0.9672151012494614\n",
            "epoch 26 batch id 2331 loss 0.11266666650772095 train acc 0.9671479515229515\n",
            "epoch 26 batch id 2341 loss 0.029128964990377426 train acc 0.9671814929517301\n",
            "epoch 26 batch id 2351 loss 0.09186568856239319 train acc 0.9671549340706083\n",
            "epoch 26 batch id 2361 loss 0.13594774901866913 train acc 0.9671815438373571\n",
            "epoch 26 batch id 2371 loss 0.029298676177859306 train acc 0.9672079291438211\n",
            "epoch 26 batch id 2381 loss 0.15893104672431946 train acc 0.9672012809743805\n",
            "epoch 26 batch id 2391 loss 0.03263501450419426 train acc 0.9672142931827687\n",
            "epoch 26 batch id 2401 loss 0.09169658273458481 train acc 0.9671816430653895\n",
            "epoch 26 batch id 2411 loss 0.07723967730998993 train acc 0.9672140709249274\n",
            "epoch 26 batch id 2421 loss 0.07023544609546661 train acc 0.9672462308963239\n",
            "epoch 26 batch id 2431 loss 0.11610007286071777 train acc 0.967226707116413\n",
            "epoch 26 batch id 2441 loss 0.04258919879794121 train acc 0.9672393486276116\n",
            "epoch 26 batch id 2451 loss 0.13443000614643097 train acc 0.9672200122399021\n",
            "epoch 26 batch id 2461 loss 0.08927100896835327 train acc 0.9672389272653393\n",
            "epoch 26 batch id 2471 loss 0.10174395143985748 train acc 0.9672323957911777\n",
            "epoch 26 batch id 2481 loss 0.0929691419005394 train acc 0.9673014913341395\n",
            "epoch 26 batch id 2491 loss 0.12109248340129852 train acc 0.9673135788839823\n",
            "epoch 26 batch id 2501 loss 0.04079343378543854 train acc 0.9673005797680928\n",
            "epoch 26 batch id 2511 loss 0.038698431104421616 train acc 0.967368578255675\n",
            "epoch 26 batch id 2521 loss 0.08232352137565613 train acc 0.9672686929789766\n",
            "epoch 26 batch id 2531 loss 0.036924805492162704 train acc 0.9673177597787436\n",
            "epoch 26 batch id 2541 loss 0.0337940976023674 train acc 0.9673110979929161\n",
            "epoch 26 batch id 2551 loss 0.06154246628284454 train acc 0.9673044884359074\n",
            "epoch 26 batch id 2561 loss 0.03980472683906555 train acc 0.9673162338930106\n",
            "epoch 26 batch id 2571 loss 0.15019004046916962 train acc 0.9673096557759626\n",
            "epoch 26 batch id 2581 loss 0.11861510574817657 train acc 0.967303128632313\n",
            "epoch 26 batch id 2591 loss 0.006161781027913094 train acc 0.9672785604013894\n",
            "epoch 26 batch id 2601 loss 0.05405513942241669 train acc 0.9672902249134948\n",
            "epoch 26 batch id 2611 loss 0.09301422536373138 train acc 0.9672958157793948\n",
            "epoch 26 batch id 2621 loss 0.07972501963376999 train acc 0.9672596337275849\n",
            "epoch 26 batch id 2631 loss 0.1441802829504013 train acc 0.9671999714937286\n",
            "epoch 26 batch id 2641 loss 0.14615541696548462 train acc 0.967158510034078\n",
            "epoch 26 batch id 2651 loss 0.023782439529895782 train acc 0.9671998774047529\n",
            "epoch 26 batch id 2661 loss 0.05230625718832016 train acc 0.9672057027433296\n",
            "epoch 26 batch id 2671 loss 0.06132402643561363 train acc 0.9672524335454886\n",
            "epoch 26 batch id 2681 loss 0.0528378039598465 train acc 0.9672580193957478\n",
            "epoch 26 batch id 2691 loss 0.143984854221344 train acc 0.9672287253808993\n",
            "epoch 26 batch id 2701 loss 0.06684667617082596 train acc 0.9672459274342836\n",
            "epoch 26 batch id 2711 loss 0.10058384388685226 train acc 0.9672399483585393\n",
            "epoch 26 batch id 2721 loss 0.08713304251432419 train acc 0.9671880742374127\n",
            "epoch 26 batch id 2731 loss 0.011898688040673733 train acc 0.9671937934822409\n",
            "epoch 26 batch id 2741 loss 0.2087739259004593 train acc 0.967176669098869\n",
            "epoch 26 batch id 2751 loss 0.04609153792262077 train acc 0.9671483097055616\n",
            "epoch 26 batch id 2761 loss 0.03252159804105759 train acc 0.9671767475552336\n",
            "epoch 26 batch id 2771 loss 0.02856042981147766 train acc 0.9671542313244316\n",
            "epoch 26 batch id 2781 loss 0.1208745688199997 train acc 0.9671655879180151\n",
            "epoch 26 batch id 2791 loss 0.06443104892969131 train acc 0.9671600680759584\n",
            "epoch 26 batch id 2801 loss 0.17416439950466156 train acc 0.9671434309175294\n",
            "epoch 26 batch id 2811 loss 0.02636786177754402 train acc 0.9671713802917111\n",
            "epoch 26 batch id 2821 loss 0.02461814694106579 train acc 0.9671769762495569\n",
            "epoch 26 batch id 2831 loss 0.02973100356757641 train acc 0.9671825326739668\n",
            "epoch 26 batch id 2841 loss 0.147344708442688 train acc 0.9671990496304118\n",
            "epoch 26 batch id 2851 loss 0.02111649326980114 train acc 0.9672209312521922\n",
            "epoch 26 batch id 2861 loss 0.07017162442207336 train acc 0.9672044302691367\n",
            "epoch 26 batch id 2871 loss 0.17609497904777527 train acc 0.9672152560083594\n",
            "epoch 26 batch id 2881 loss 0.07638642191886902 train acc 0.9672260065949323\n",
            "epoch 26 batch id 2891 loss 0.003959112800657749 train acc 0.9672366828087167\n",
            "epoch 26 batch id 2901 loss 0.053841058164834976 train acc 0.9672580575663564\n",
            "epoch 26 batch id 2911 loss 0.06820248067378998 train acc 0.9672792854689111\n",
            "epoch 26 batch id 2921 loss 0.0026838781777769327 train acc 0.9672575744608011\n",
            "epoch 26 batch id 2931 loss 0.12051364779472351 train acc 0.9672679972705561\n",
            "epoch 26 batch id 2941 loss 0.08871957659721375 train acc 0.9672517851071064\n",
            "epoch 26 batch id 2951 loss 0.099706269800663 train acc 0.9672939257878685\n",
            "epoch 26 batch id 2961 loss 0.19394712150096893 train acc 0.9672988432961838\n",
            "epoch 26 batch id 2971 loss 0.05292866379022598 train acc 0.967266913497139\n",
            "epoch 26 batch id 2981 loss 0.13073749840259552 train acc 0.967240439449849\n",
            "epoch 26 batch id 2991 loss 0.10686934739351273 train acc 0.9672716064861251\n",
            "epoch 26 batch id 3001 loss 0.004869238473474979 train acc 0.967307772409197\n",
            "epoch 26 batch id 3011 loss 0.2656676769256592 train acc 0.9672451012952508\n",
            "epoch 26 batch id 3021 loss 0.0465003065764904 train acc 0.967234566368752\n",
            "epoch 26 batch id 3031 loss 0.26538360118865967 train acc 0.9671983256351039\n",
            "epoch 26 batch id 3041 loss 0.1071472018957138 train acc 0.9671674613613943\n",
            "epoch 26 batch id 3051 loss 0.03965824097394943 train acc 0.967177769583743\n",
            "epoch 26 batch id 3061 loss 0.11029866337776184 train acc 0.967157383208102\n",
            "epoch 26 batch id 3071 loss 0.023598171770572662 train acc 0.9672032725496581\n",
            "epoch 26 batch id 3081 loss 0.1434733122587204 train acc 0.9672032213567023\n",
            "epoch 26 batch id 3091 loss 0.04785628616809845 train acc 0.9671981154966031\n",
            "epoch 26 batch id 3101 loss 0.12794113159179688 train acc 0.9672031199613028\n",
            "epoch 26 batch id 3111 loss 0.06581251323223114 train acc 0.9672080922532947\n",
            "epoch 26 batch id 3121 loss 0.05803144723176956 train acc 0.9671729814162128\n",
            "epoch 26 batch id 3131 loss 0.032406073063611984 train acc 0.9671730277866496\n",
            "epoch 26 batch id 3141 loss 0.11553236842155457 train acc 0.9671780483922318\n",
            "epoch 26 batch id 3151 loss 0.07644802331924438 train acc 0.967202872104094\n",
            "epoch 26 batch id 3161 loss 0.050808124244213104 train acc 0.9672077665295793\n",
            "epoch 26 batch id 3171 loss 0.06141292303800583 train acc 0.9671781378114159\n",
            "epoch 26 batch id 3181 loss 0.11258076131343842 train acc 0.9671683432882742\n",
            "epoch 26 batch id 3191 loss 0.02344631776213646 train acc 0.9671928862425572\n",
            "epoch 26 batch id 3201 loss 0.19108761847019196 train acc 0.967139175257732\n",
            "epoch 26 batch id 3211 loss 0.22229717671871185 train acc 0.9671101292432264\n",
            "epoch 26 batch id 3221 loss 0.2217947244644165 train acc 0.9670861145606955\n",
            "epoch 26 batch id 3231 loss 0.04469454288482666 train acc 0.9670574125657692\n",
            "epoch 26 batch id 3241 loss 0.04076186195015907 train acc 0.9670529929034248\n",
            "epoch 26 batch id 3251 loss 0.1186433807015419 train acc 0.9670534066441095\n",
            "epoch 26 batch id 3261 loss 0.18896706402301788 train acc 0.9670250689972401\n",
            "epoch 26 batch id 3271 loss 0.05679910257458687 train acc 0.9669969046163253\n",
            "epoch 26 batch id 3281 loss 0.21010032296180725 train acc 0.9669546251142944\n",
            "epoch 26 batch id 3291 loss 0.03904186561703682 train acc 0.9669600805226375\n",
            "epoch 26 batch id 3301 loss 0.08235982060432434 train acc 0.9669749697061496\n",
            "epoch 26 batch id 3311 loss 0.07256503403186798 train acc 0.966985049833887\n",
            "epoch 26 batch id 3321 loss 0.023595189675688744 train acc 0.9669715447154471\n",
            "epoch 26 batch id 3331 loss 0.0750807449221611 train acc 0.9669159036325428\n",
            "epoch 26 batch id 3341 loss 0.07099214196205139 train acc 0.9669307467824004\n",
            "epoch 26 batch id 3351 loss 0.12321984767913818 train acc 0.9668895478961504\n",
            "epoch 26 batch id 3361 loss 0.10821495950222015 train acc 0.9668904343945255\n",
            "epoch 26 batch id 3371 loss 0.11635853350162506 train acc 0.9668727751409077\n",
            "epoch 26 batch id 3381 loss 0.003002055222168565 train acc 0.9669060559006211\n",
            "epoch 26 batch id 3391 loss 0.07578083127737045 train acc 0.9669114936596874\n",
            "epoch 26 batch id 3401 loss 0.2651592195034027 train acc 0.966926087915319\n",
            "epoch 26 batch id 3411 loss 0.17252206802368164 train acc 0.966922273526825\n",
            "epoch 26 batch id 3421 loss 0.07539871335029602 train acc 0.9669550204618532\n",
            "epoch 26 batch id 3431 loss 0.24605856835842133 train acc 0.9669283736519965\n",
            "epoch 26 batch id 3441 loss 0.011919540353119373 train acc 0.9669518308631212\n",
            "epoch 26 batch id 3451 loss 0.03238222748041153 train acc 0.9669479860909881\n",
            "epoch 26 batch id 3461 loss 0.0250454880297184 train acc 0.9669622219011846\n",
            "epoch 26 batch id 3471 loss 0.06376080960035324 train acc 0.9669628709305675\n",
            "epoch 26 batch id 3481 loss 0.07939492166042328 train acc 0.966945561620224\n",
            "epoch 26 batch id 3491 loss 0.15099437534809113 train acc 0.9669328272701232\n",
            "epoch 26 batch id 3501 loss 0.043966781347990036 train acc 0.9669603327620679\n",
            "epoch 26 batch id 3511 loss 0.034043241292238235 train acc 0.9669698803759612\n",
            "epoch 26 batch id 3521 loss 0.13425886631011963 train acc 0.9669882490769668\n",
            "epoch 26 batch id 3531 loss 0.12453117221593857 train acc 0.9669976635514018\n",
            "epoch 26 batch id 3541 loss 0.0679168626666069 train acc 0.9669937870658006\n",
            "epoch 26 batch id 3551 loss 0.033187467604875565 train acc 0.9669855322444382\n",
            "epoch 26 batch id 3561 loss 0.019339175894856453 train acc 0.9670168140971637\n",
            "epoch 26 batch id 3571 loss 0.1319986879825592 train acc 0.967034794175301\n",
            "epoch 26 batch id 3581 loss 0.23779137432575226 train acc 0.9670264939960905\n",
            "epoch 26 batch id 3591 loss 0.1376027911901474 train acc 0.9670399958228906\n",
            "epoch 26 batch id 3601 loss 0.16010470688343048 train acc 0.9670447445154123\n",
            "epoch 26 batch id 3611 loss 0.020995501428842545 train acc 0.9670624480753254\n",
            "epoch 26 batch id 3621 loss 0.08035814017057419 train acc 0.96706279342723\n",
            "epoch 26 batch id 3631 loss 0.09209568798542023 train acc 0.967024407876618\n",
            "epoch 26 batch id 3641 loss 0.15740160644054413 train acc 0.9670162730019225\n",
            "epoch 26 batch id 3651 loss 0.17287319898605347 train acc 0.9669996233908518\n",
            "epoch 26 batch id 3661 loss 0.028869586065411568 train acc 0.9670086724938541\n",
            "epoch 26 batch id 3671 loss 0.06384015083312988 train acc 0.9670346976300735\n",
            "epoch 26 batch id 3681 loss 0.06495848298072815 train acc 0.9670266232002174\n",
            "epoch 26 batch id 3691 loss 0.11123023927211761 train acc 0.9670566919534002\n",
            "epoch 26 batch id 3701 loss 0.0556572824716568 train acc 0.9670148270737638\n",
            "epoch 26 batch id 3711 loss 0.17203395068645477 train acc 0.9670026610078146\n",
            "epoch 26 batch id 3721 loss 0.11889539659023285 train acc 0.9669821620532115\n",
            "epoch 26 batch id 3731 loss 0.05538509413599968 train acc 0.9670036518359689\n",
            "epoch 26 batch id 3741 loss 0.06653472781181335 train acc 0.9669999665864742\n",
            "epoch 26 batch id 3751 loss 0.09332963824272156 train acc 0.967025459877366\n",
            "epoch 26 batch id 3761 loss 0.17381958663463593 train acc 0.9670051183195959\n",
            "epoch 26 batch id 3771 loss 0.0675397515296936 train acc 0.9669931715725272\n",
            "epoch 26 batch id 3781 loss 0.10678184777498245 train acc 0.9669812880190426\n",
            "epoch 26 batch id 3791 loss 0.00813375785946846 train acc 0.9670065615932472\n",
            "epoch 26 batch id 3801 loss 0.05524053797125816 train acc 0.9670193699026572\n",
            "epoch 26 batch id 3811 loss 0.06890184432268143 train acc 0.9670485108895303\n",
            "epoch 26 batch id 3821 loss 0.19124047458171844 train acc 0.9670488746401465\n",
            "epoch 26 batch id 3831 loss 0.09501681476831436 train acc 0.9670329222135213\n",
            "epoch 26 batch id 3841 loss 0.035654135048389435 train acc 0.9670170528508201\n",
            "epoch 26 batch id 3851 loss 0.03419586271047592 train acc 0.9670134380680343\n",
            "epoch 26 batch id 3861 loss 0.034733328968286514 train acc 0.967009842009842\n",
            "epoch 26 batch id 3871 loss 0.14644616842269897 train acc 0.966998191681736\n",
            "epoch 26 batch id 3881 loss 0.23537962138652802 train acc 0.9669866013913939\n",
            "epoch 26 batch id 3891 loss 0.027393415570259094 train acc 0.9669790863531226\n",
            "epoch 26 batch id 3901 loss 0.0755385085940361 train acc 0.9669595936939246\n",
            "epoch 26 batch id 3911 loss 0.04877808317542076 train acc 0.966972161851189\n",
            "epoch 26 batch id 3921 loss 0.02678995206952095 train acc 0.9669607561846467\n",
            "epoch 26 batch id 3931 loss 0.08591321855783463 train acc 0.966953383363012\n",
            "epoch 26 batch id 3941 loss 0.04439188912510872 train acc 0.9669856952550114\n",
            "epoch 26 batch id 3951 loss 0.025464026257395744 train acc 0.9669822513287776\n",
            "epoch 26 batch id 3961 loss 0.19313198328018188 train acc 0.9669748800807877\n",
            "epoch 26 batch id 3971 loss 0.042960941791534424 train acc 0.966987219843868\n",
            "epoch 26 batch id 3981 loss 0.03020688332617283 train acc 0.9669798731474504\n",
            "epoch 26 batch id 3991 loss 0.1762421578168869 train acc 0.9669803933851165\n",
            "epoch 26 batch id 4001 loss 0.09100840240716934 train acc 0.9669691952011997\n",
            "epoch 26 batch id 4011 loss 0.01712447591125965 train acc 0.9669502617801047\n",
            "epoch 26 batch id 4021 loss 0.15497887134552002 train acc 0.9669508517781646\n",
            "epoch 26 batch id 4031 loss 0.06388739496469498 train acc 0.9669863247333168\n",
            "epoch 26 batch id 4041 loss 0.20466867089271545 train acc 0.9669945558030191\n",
            "epoch 26 batch id 4051 loss 0.02606971748173237 train acc 0.9669757467292026\n",
            "epoch 26 batch id 4061 loss 0.037650216370821 train acc 0.9669878108840188\n",
            "epoch 26 batch id 4071 loss 0.07251416146755219 train acc 0.9669959776467698\n",
            "epoch 26 batch id 4081 loss 0.09340714663267136 train acc 0.9670232479784366\n",
            "epoch 26 batch id 4091 loss 0.1691916286945343 train acc 0.9670121913957468\n",
            "epoch 26 batch id 4101 loss 0.07771715521812439 train acc 0.9670011887344551\n",
            "epoch 26 batch id 4111 loss 0.04855756089091301 train acc 0.9670092434930674\n",
            "epoch 26 batch id 4121 loss 0.15120305120944977 train acc 0.9670096760495025\n",
            "epoch 26 batch id 4131 loss 0.1020253449678421 train acc 0.9670025417574437\n",
            "epoch 26 batch id 4141 loss 0.030628560110926628 train acc 0.9670180813813088\n",
            "epoch 26 batch id 4151 loss 0.07898754626512527 train acc 0.967041074439894\n",
            "epoch 26 batch id 4161 loss 0.0671166330575943 train acc 0.967022650805095\n",
            "epoch 26 batch id 4171 loss 0.14777444303035736 train acc 0.9670005694078159\n",
            "epoch 26 batch id 4181 loss 0.006168989464640617 train acc 0.9670047536474528\n",
            "epoch 26 batch id 4191 loss 0.23532643914222717 train acc 0.9669977332378907\n",
            "epoch 26 batch id 4201 loss 0.06404406577348709 train acc 0.9670390978338491\n",
            "epoch 26 batch id 4211 loss 0.08468740433454514 train acc 0.9670431607694134\n",
            "epoch 26 batch id 4221 loss 0.0869208425283432 train acc 0.9670360992655769\n",
            "epoch 26 batch id 4231 loss 0.04670966416597366 train acc 0.9670290711415741\n",
            "epoch 26 batch id 4241 loss 0.03134285658597946 train acc 0.9670552346144777\n",
            "epoch 26 batch id 4251 loss 0.20390132069587708 train acc 0.9670628969654199\n",
            "epoch 26 batch id 4261 loss 0.045342348515987396 train acc 0.9670741903309082\n",
            "epoch 26 batch id 4271 loss 0.07535870373249054 train acc 0.9671110395691875\n",
            "epoch 26 batch id 4281 loss 0.05593099072575569 train acc 0.9671294674141556\n",
            "epoch 26 batch id 4291 loss 0.14603187143802643 train acc 0.9670931892332789\n",
            "epoch 26 batch id 4301 loss 0.06428282707929611 train acc 0.9670897756335736\n",
            "epoch 26 batch id 4311 loss 0.07304718345403671 train acc 0.9671153734632336\n",
            "epoch 26 batch id 4321 loss 0.07473447173833847 train acc 0.9670938440175885\n",
            "epoch 26 batch id 4331 loss 0.12805764377117157 train acc 0.9670543754329254\n",
            "epoch 26 batch id 4341 loss 0.034769006073474884 train acc 0.9670654803040774\n",
            "epoch 26 batch id 4351 loss 0.04934125021100044 train acc 0.9670693518731326\n",
            "epoch 26 batch id 4361 loss 0.0962287038564682 train acc 0.9670982859435909\n",
            "epoch 26 batch id 4371 loss 0.16281689703464508 train acc 0.9671235129261039\n",
            "epoch 26 batch id 4381 loss 0.1293937712907791 train acc 0.9671200924446474\n",
            "epoch 26 batch id 4391 loss 0.06663818657398224 train acc 0.9671202459576407\n",
            "epoch 26 batch id 4401 loss 0.18245632946491241 train acc 0.9671239491024767\n",
            "epoch 26 batch id 4411 loss 0.046742819249629974 train acc 0.9670957549308546\n",
            "epoch 26 batch id 4421 loss 0.09114965051412582 train acc 0.9670924281836688\n",
            "epoch 26 batch id 4431 loss 0.09676553308963776 train acc 0.9670926427443015\n",
            "epoch 26 batch id 4441 loss 0.04508792608976364 train acc 0.9671034113938303\n",
            "epoch 26 batch id 4451 loss 0.006033042445778847 train acc 0.9671246629970793\n",
            "epoch 26 batch id 4461 loss 0.006634776014834642 train acc 0.9671318090114324\n",
            "epoch 26 batch id 4471 loss 0.13484442234039307 train acc 0.9671109651084768\n",
            "epoch 26 batch id 4481 loss 0.023354968056082726 train acc 0.9671111359071636\n",
            "epoch 26 batch id 4491 loss 0.12408233433961868 train acc 0.9671078267646404\n",
            "epoch 26 batch id 4501 loss 0.0516977496445179 train acc 0.9670871750722062\n",
            "epoch 26 batch id 4511 loss 0.10469169169664383 train acc 0.9670631511859898\n",
            "epoch 26 batch id 4521 loss 0.10044901072978973 train acc 0.9670634262331342\n",
            "epoch 26 batch id 4531 loss 0.0032967510633170605 train acc 0.9670809423968219\n",
            "epoch 26 batch id 4541 loss 0.14097127318382263 train acc 0.9670811770535125\n",
            "epoch 26 batch id 4551 loss 0.26578816771507263 train acc 0.967064244122171\n",
            "epoch 26 batch id 4561 loss 0.21618983149528503 train acc 0.9670542370094277\n",
            "epoch 26 batch id 4571 loss 0.06806465238332748 train acc 0.9670750382848392\n",
            "epoch 26 batch id 4581 loss 0.10649373382329941 train acc 0.967088927090155\n",
            "epoch 26 batch id 4591 loss 0.24069510400295258 train acc 0.9670857384012198\n",
            "epoch 26 batch id 4601 loss 0.13652601838111877 train acc 0.9670859595740057\n",
            "epoch 26 batch id 4611 loss 0.04522702842950821 train acc 0.9671132888744307\n",
            "epoch 26 batch id 4621 loss 0.03290709853172302 train acc 0.9671033055615668\n",
            "epoch 26 batch id 4631 loss 0.10877575725317001 train acc 0.9671102353703304\n",
            "epoch 26 batch id 4641 loss 0.13206122815608978 train acc 0.9671137685843568\n",
            "epoch 26 batch id 4651 loss 0.17140738666057587 train acc 0.9671139271124489\n",
            "epoch 26 batch id 4661 loss 0.08365311473608017 train acc 0.9671341986698133\n",
            "epoch 26 batch id 4671 loss 0.12643446028232574 train acc 0.9671309676728752\n",
            "epoch 26 batch id 4681 loss 0.2885819971561432 train acc 0.9671143986327708\n",
            "epoch 26 batch id 4691 loss 0.15462726354599 train acc 0.9670945693881902\n",
            "epoch 26 batch id 4701 loss 0.12391523271799088 train acc 0.9671147096362476\n",
            "epoch 26 batch id 4711 loss 0.19068102538585663 train acc 0.9671115474421567\n",
            "epoch 26 batch id 4721 loss 0.06034307926893234 train acc 0.967108398644355\n",
            "epoch 26 batch id 4731 loss 0.0378863625228405 train acc 0.9670986577890509\n",
            "epoch 26 batch id 4741 loss 0.20955771207809448 train acc 0.96708566230753\n",
            "epoch 26 batch id 4751 loss 0.1518973857164383 train acc 0.9670760103136182\n",
            "epoch 26 batch id 4761 loss 0.1568225920200348 train acc 0.9670499894980046\n",
            "epoch 26 batch id 4771 loss 0.11700529605150223 train acc 0.9670339027457556\n",
            "epoch 26 batch id 4781 loss 0.24407516419887543 train acc 0.9669950062748379\n",
            "epoch 26 batch id 4791 loss 0.156516894698143 train acc 0.966988885410144\n",
            "epoch 26 batch id 4801 loss 0.06919532269239426 train acc 0.9669892991043533\n",
            "epoch 26 batch id 4811 loss 0.042112085968256 train acc 0.9669572334234047\n",
            "epoch 26 batch id 4821 loss 0.13634514808654785 train acc 0.9669415059116366\n",
            "epoch 26 batch id 4831 loss 0.026231655851006508 train acc 0.9669646553508591\n",
            "epoch 26 batch id 4841 loss 0.02364075742661953 train acc 0.9669618880396612\n",
            "epoch 26 batch id 4851 loss 0.18914209306240082 train acc 0.9669655741084312\n",
            "epoch 26 batch id 4861 loss 0.27808451652526855 train acc 0.9669499588562024\n",
            "epoch 26 batch id 4871 loss 0.02281319722533226 train acc 0.9669664853212893\n",
            "epoch 26 batch id 4881 loss 0.0627962201833725 train acc 0.9669829440688383\n",
            "epoch 26 batch id 4891 loss 0.17855204641819 train acc 0.9669418319362094\n",
            "epoch 26 batch id 4901 loss 0.02736339345574379 train acc 0.9669455213221791\n",
            "epoch 26 batch id 4911 loss 0.10235455632209778 train acc 0.9669301058847485\n",
            "epoch 26 batch id 4921 loss 0.1431688666343689 train acc 0.9669242786019102\n",
            "epoch 26 batch id 4931 loss 0.2421814501285553 train acc 0.9668962938552018\n",
            "epoch 26 batch id 4941 loss 0.053853146731853485 train acc 0.9668873962760575\n",
            "epoch 26 batch id 4951 loss 0.19549517333507538 train acc 0.9668659109270854\n",
            "epoch 26 train acc 0.9668618872301795\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c46faca4009422788f6ae1be4c29f77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 26 loss 1.116703987121582 test acc 0.8380842192082111\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11df5adf0546452eaf771cf78fdc5a4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 27 batch id 1 loss 0.051748231053352356 train acc 0.96875\n",
            "epoch 27 batch id 11 loss 0.016184231266379356 train acc 0.9701704545454546\n",
            "epoch 27 batch id 21 loss 0.024327313527464867 train acc 0.9694940476190477\n",
            "epoch 27 batch id 31 loss 0.18317018449306488 train acc 0.9672379032258065\n",
            "epoch 27 batch id 41 loss 0.02787160873413086 train acc 0.96875\n",
            "epoch 27 batch id 51 loss 0.02194705791771412 train acc 0.9699754901960784\n",
            "epoch 27 batch id 61 loss 0.03683336451649666 train acc 0.9669569672131147\n",
            "epoch 27 batch id 71 loss 0.18393971025943756 train acc 0.9658890845070423\n",
            "epoch 27 batch id 81 loss 0.1249697282910347 train acc 0.9662422839506173\n",
            "epoch 27 batch id 91 loss 0.03651723638176918 train acc 0.9677197802197802\n",
            "epoch 27 batch id 101 loss 0.05794212594628334 train acc 0.9661200495049505\n",
            "epoch 27 batch id 111 loss 0.01231735572218895 train acc 0.9662162162162162\n",
            "epoch 27 batch id 121 loss 0.03802305459976196 train acc 0.9670712809917356\n",
            "epoch 27 batch id 131 loss 0.0733073502779007 train acc 0.9670801526717557\n",
            "epoch 27 batch id 141 loss 0.03649062290787697 train acc 0.9675310283687943\n",
            "epoch 27 batch id 151 loss 0.046951454132795334 train acc 0.9683360927152318\n",
            "epoch 27 batch id 161 loss 0.056403208523988724 train acc 0.969041149068323\n",
            "epoch 27 batch id 171 loss 0.06367336958646774 train acc 0.9688413742690059\n",
            "epoch 27 batch id 181 loss 0.07310442626476288 train acc 0.9690953038674033\n",
            "epoch 27 batch id 191 loss 0.143234521150589 train acc 0.96842277486911\n",
            "epoch 27 batch id 201 loss 0.12283966690301895 train acc 0.9689054726368159\n",
            "epoch 27 batch id 211 loss 0.020077873021364212 train acc 0.9686759478672986\n",
            "epoch 27 batch id 221 loss 0.15480151772499084 train acc 0.9679722850678733\n",
            "epoch 27 batch id 231 loss 0.04379872977733612 train acc 0.9686147186147186\n",
            "epoch 27 batch id 241 loss 0.22680874168872833 train acc 0.9684906639004149\n",
            "epoch 27 batch id 251 loss 0.1775897890329361 train acc 0.9683764940239044\n",
            "epoch 27 batch id 261 loss 0.06600771844387054 train acc 0.9685105363984674\n",
            "epoch 27 batch id 271 loss 0.0054044900462031364 train acc 0.9688076568265682\n",
            "epoch 27 batch id 281 loss 0.014457556419074535 train acc 0.9690836298932385\n",
            "epoch 27 batch id 291 loss 0.06942437589168549 train acc 0.9693406357388317\n",
            "epoch 27 batch id 301 loss 0.2073276937007904 train acc 0.9691133720930233\n",
            "epoch 27 batch id 311 loss 0.13015025854110718 train acc 0.9692021704180064\n",
            "epoch 27 batch id 321 loss 0.11769984662532806 train acc 0.9692854361370716\n",
            "epoch 27 batch id 331 loss 0.0092757074162364 train acc 0.9692692598187311\n",
            "epoch 27 batch id 341 loss 0.15232029557228088 train acc 0.9693456744868035\n",
            "epoch 27 batch id 351 loss 0.015473445877432823 train acc 0.9691951566951567\n",
            "epoch 27 batch id 361 loss 0.07821798324584961 train acc 0.969226108033241\n",
            "epoch 27 batch id 371 loss 0.10179741680622101 train acc 0.9696344339622641\n",
            "epoch 27 batch id 381 loss 0.020002448931336403 train acc 0.9699393044619422\n",
            "epoch 27 batch id 391 loss 0.07108639180660248 train acc 0.969988810741688\n",
            "epoch 27 batch id 401 loss 0.008704788982868195 train acc 0.9701527431421446\n",
            "epoch 27 batch id 411 loss 0.042035460472106934 train acc 0.9699665450121655\n",
            "epoch 27 batch id 421 loss 0.09931136667728424 train acc 0.9699005344418052\n",
            "epoch 27 batch id 431 loss 0.21240457892417908 train acc 0.9698375870069605\n",
            "epoch 27 batch id 441 loss 0.12750251591205597 train acc 0.969812925170068\n",
            "epoch 27 batch id 451 loss 0.028938790783286095 train acc 0.9699279379157428\n",
            "epoch 27 batch id 461 loss 0.013258739374577999 train acc 0.9696990238611713\n",
            "epoch 27 batch id 471 loss 0.01482327003031969 train acc 0.9700106157112527\n",
            "epoch 27 batch id 481 loss 0.09587882459163666 train acc 0.9699194386694386\n",
            "epoch 27 batch id 491 loss 0.01887679286301136 train acc 0.9698319755600815\n",
            "epoch 27 batch id 501 loss 0.12580925226211548 train acc 0.9697791916167665\n",
            "epoch 27 batch id 511 loss 0.12073733657598495 train acc 0.9697284735812133\n",
            "epoch 27 batch id 521 loss 0.12492471933364868 train acc 0.9699196257197696\n",
            "epoch 27 batch id 531 loss 0.1281449794769287 train acc 0.9696916195856874\n",
            "epoch 27 batch id 541 loss 0.0693531483411789 train acc 0.9695875693160814\n",
            "epoch 27 batch id 551 loss 0.0698821023106575 train acc 0.9697141560798548\n",
            "epoch 27 batch id 561 loss 0.07291947305202484 train acc 0.9697248217468806\n",
            "epoch 27 batch id 571 loss 0.13285335898399353 train acc 0.969871935201401\n",
            "epoch 27 batch id 581 loss 0.25268521904945374 train acc 0.9697719449225474\n",
            "epoch 27 batch id 591 loss 0.035488445311784744 train acc 0.9698339678510999\n",
            "epoch 27 batch id 601 loss 0.033739492297172546 train acc 0.9698419301164726\n",
            "epoch 27 batch id 611 loss 0.28325068950653076 train acc 0.9696450490998363\n",
            "epoch 27 batch id 621 loss 0.24505257606506348 train acc 0.9697061191626409\n",
            "epoch 27 batch id 631 loss 0.13636326789855957 train acc 0.9695423930269413\n",
            "epoch 27 batch id 641 loss 0.14476783573627472 train acc 0.9696275351014041\n",
            "epoch 27 batch id 651 loss 0.028137197718024254 train acc 0.9696860599078341\n",
            "epoch 27 batch id 661 loss 0.02430843561887741 train acc 0.969766452344932\n",
            "epoch 27 batch id 671 loss 0.037710726261138916 train acc 0.9697047317436661\n",
            "epoch 27 batch id 681 loss 0.20803804695606232 train acc 0.9694612701908958\n",
            "epoch 27 batch id 691 loss 0.04895102605223656 train acc 0.9693379160636758\n",
            "epoch 27 batch id 701 loss 0.09092241525650024 train acc 0.9693741084165478\n",
            "epoch 27 batch id 711 loss 0.13488958775997162 train acc 0.9693433544303798\n",
            "epoch 27 batch id 721 loss 0.16734591126441956 train acc 0.9692050970873787\n",
            "epoch 27 batch id 731 loss 0.017790302634239197 train acc 0.9692843707250342\n",
            "epoch 27 batch id 741 loss 0.12504124641418457 train acc 0.9692349865047234\n",
            "epoch 27 batch id 751 loss 0.29712334275245667 train acc 0.9689580559254327\n",
            "epoch 27 batch id 761 loss 0.13249844312667847 train acc 0.9689142575558476\n",
            "epoch 27 batch id 771 loss 0.09368026256561279 train acc 0.9688107976653697\n",
            "epoch 27 batch id 781 loss 0.2134561389684677 train acc 0.9687299935979513\n",
            "epoch 27 batch id 791 loss 0.09186402708292007 train acc 0.9688092604298356\n",
            "epoch 27 batch id 801 loss 0.009187768213450909 train acc 0.9689450686641697\n",
            "epoch 27 batch id 811 loss 0.1359250694513321 train acc 0.9688848643649816\n",
            "epoch 27 batch id 821 loss 0.012165620923042297 train acc 0.9689212850182704\n",
            "epoch 27 batch id 831 loss 0.14339719712734222 train acc 0.9688440132370638\n",
            "epoch 27 batch id 841 loss 0.006260490044951439 train acc 0.9688614744351962\n",
            "epoch 27 batch id 851 loss 0.06930834800004959 train acc 0.968933607520564\n",
            "epoch 27 batch id 861 loss 0.03775334358215332 train acc 0.9688588850174216\n",
            "epoch 27 batch id 871 loss 0.0507592149078846 train acc 0.9688935132032147\n",
            "epoch 27 batch id 881 loss 0.07305895537137985 train acc 0.96875\n",
            "epoch 27 batch id 891 loss 0.05342132970690727 train acc 0.9687324635241302\n",
            "epoch 27 batch id 901 loss 0.07242333889007568 train acc 0.9687326581576027\n",
            "epoch 27 batch id 911 loss 0.13694711029529572 train acc 0.96875\n",
            "epoch 27 batch id 921 loss 0.08854153752326965 train acc 0.9686651737242128\n",
            "epoch 27 batch id 931 loss 0.1013757660984993 train acc 0.9685150375939849\n",
            "epoch 27 batch id 941 loss 0.028483537957072258 train acc 0.9686171625929861\n",
            "epoch 27 batch id 951 loss 0.12382959574460983 train acc 0.9685528391167192\n",
            "epoch 27 batch id 961 loss 0.08074503391981125 train acc 0.968636186264308\n",
            "epoch 27 batch id 971 loss 0.09175102412700653 train acc 0.9686856333676622\n",
            "epoch 27 batch id 981 loss 0.02232350781559944 train acc 0.9687340723751274\n",
            "epoch 27 batch id 991 loss 0.10805179178714752 train acc 0.968765766902119\n",
            "epoch 27 batch id 1001 loss 0.0744624063372612 train acc 0.9688436563436563\n",
            "epoch 27 batch id 1011 loss 0.007712920196354389 train acc 0.9688427299703264\n",
            "epoch 27 batch id 1021 loss 0.12502849102020264 train acc 0.9689183398628796\n",
            "epoch 27 batch id 1031 loss 0.008547254838049412 train acc 0.9689470174587779\n",
            "epoch 27 batch id 1041 loss 0.04382384940981865 train acc 0.9690051633045149\n",
            "epoch 27 batch id 1051 loss 0.04278899356722832 train acc 0.9688689343482397\n",
            "epoch 27 batch id 1061 loss 0.11885858327150345 train acc 0.9687794533459001\n",
            "epoch 27 batch id 1071 loss 0.0023802665527909994 train acc 0.9688229458450047\n",
            "epoch 27 batch id 1081 loss 0.10925943404436111 train acc 0.9686632747456059\n",
            "epoch 27 batch id 1091 loss 0.2869615852832794 train acc 0.9686354262144822\n",
            "epoch 27 batch id 1101 loss 0.1237163320183754 train acc 0.9686932334241598\n",
            "epoch 27 batch id 1111 loss 0.15315358340740204 train acc 0.9686093609360936\n",
            "epoch 27 batch id 1121 loss 0.05478249862790108 train acc 0.9687639384478145\n",
            "epoch 27 batch id 1131 loss 0.07150270789861679 train acc 0.968694739168877\n",
            "epoch 27 batch id 1141 loss 0.15597335994243622 train acc 0.9686541411042945\n",
            "epoch 27 batch id 1151 loss 0.0749482586979866 train acc 0.9686278236316247\n",
            "epoch 27 batch id 1161 loss 0.04814792796969414 train acc 0.9686827088716624\n",
            "epoch 27 batch id 1171 loss 0.032814379781484604 train acc 0.9686699402220325\n",
            "epoch 27 batch id 1181 loss 0.08302102237939835 train acc 0.9686573878069432\n",
            "epoch 27 batch id 1191 loss 0.011532148346304893 train acc 0.9687237615449202\n",
            "epoch 27 batch id 1201 loss 0.09055529534816742 train acc 0.96875\n",
            "epoch 27 batch id 1211 loss 0.0011228594230487943 train acc 0.96875\n",
            "epoch 27 batch id 1221 loss 0.09475623816251755 train acc 0.9688139844389845\n",
            "epoch 27 batch id 1231 loss 0.2705300748348236 train acc 0.968661149471974\n",
            "epoch 27 batch id 1241 loss 0.02553478442132473 train acc 0.9686492747784046\n",
            "epoch 27 batch id 1251 loss 0.10294504463672638 train acc 0.9686001199040767\n",
            "epoch 27 batch id 1261 loss 0.05719292163848877 train acc 0.9686260904044409\n",
            "epoch 27 batch id 1271 loss 0.11101973801851273 train acc 0.9687008261211645\n",
            "epoch 27 batch id 1281 loss 0.025774089619517326 train acc 0.9687987900078064\n",
            "epoch 27 batch id 1291 loss 0.2936217188835144 train acc 0.9688347211463981\n",
            "epoch 27 batch id 1301 loss 0.04998617619276047 train acc 0.9687860299769409\n",
            "epoch 27 batch id 1311 loss 0.18650485575199127 train acc 0.9687023264683448\n",
            "epoch 27 batch id 1321 loss 0.02698165364563465 train acc 0.9687381718395155\n",
            "epoch 27 batch id 1331 loss 0.06817370653152466 train acc 0.9687147821187078\n",
            "epoch 27 batch id 1341 loss 0.12396172434091568 train acc 0.9687383482475764\n",
            "epoch 27 batch id 1351 loss 0.008119132369756699 train acc 0.9687153034789046\n",
            "epoch 27 batch id 1361 loss 0.1429765373468399 train acc 0.968646675238795\n",
            "epoch 27 batch id 1371 loss 0.017483800649642944 train acc 0.9686702224653537\n",
            "epoch 27 batch id 1381 loss 0.05725619196891785 train acc 0.9687726285300506\n",
            "epoch 27 batch id 1391 loss 0.017809061333537102 train acc 0.9688061646297628\n",
            "epoch 27 batch id 1401 loss 0.23279410600662231 train acc 0.9688057637401856\n",
            "epoch 27 batch id 1411 loss 0.02482886053621769 train acc 0.9689161055988661\n",
            "epoch 27 batch id 1421 loss 0.011101871728897095 train acc 0.9689479239971851\n",
            "epoch 27 batch id 1431 loss 0.10895396023988724 train acc 0.9689356219426974\n",
            "epoch 27 batch id 1441 loss 0.0494326576590538 train acc 0.9688692748091603\n",
            "epoch 27 batch id 1451 loss 0.05590193346142769 train acc 0.9689007580978636\n",
            "epoch 27 batch id 1461 loss 0.013773960992693901 train acc 0.968889031485284\n",
            "epoch 27 batch id 1471 loss 0.02812633104622364 train acc 0.9689624405166554\n",
            "epoch 27 batch id 1481 loss 0.04195760190486908 train acc 0.968876603646185\n",
            "epoch 27 batch id 1491 loss 0.09325949102640152 train acc 0.9688862340710932\n",
            "epoch 27 batch id 1501 loss 0.0858350396156311 train acc 0.9688749167221852\n",
            "epoch 27 batch id 1511 loss 0.032352447509765625 train acc 0.9688430675049636\n",
            "epoch 27 batch id 1521 loss 0.031555723398923874 train acc 0.9688527284681131\n",
            "epoch 27 batch id 1531 loss 0.02762344479560852 train acc 0.9688724689745265\n",
            "epoch 27 batch id 1541 loss 0.07283183932304382 train acc 0.968810837118754\n",
            "epoch 27 batch id 1551 loss 0.05783357098698616 train acc 0.9688507414571245\n",
            "epoch 27 batch id 1561 loss 0.09383920580148697 train acc 0.9688100576553491\n",
            "epoch 27 batch id 1571 loss 0.06466148793697357 train acc 0.9688494589433482\n",
            "epoch 27 batch id 1581 loss 0.04613466188311577 train acc 0.9688587128399747\n",
            "epoch 27 batch id 1591 loss 0.19544783234596252 train acc 0.9689169547454431\n",
            "epoch 27 batch id 1601 loss 0.1340595930814743 train acc 0.9689354309806371\n",
            "epoch 27 batch id 1611 loss 0.1116909310221672 train acc 0.9688760862818125\n",
            "epoch 27 batch id 1621 loss 0.04839928075671196 train acc 0.968894586674892\n",
            "epoch 27 batch id 1631 loss 0.027383198961615562 train acc 0.9688937001839363\n",
            "epoch 27 batch id 1641 loss 0.08178144693374634 train acc 0.9689023461304083\n",
            "epoch 27 batch id 1651 loss 0.05007835477590561 train acc 0.9688919594185342\n",
            "epoch 27 batch id 1661 loss 0.06847357749938965 train acc 0.9689005117399158\n",
            "epoch 27 batch id 1671 loss 0.10570722073316574 train acc 0.9689183123877917\n",
            "epoch 27 batch id 1681 loss 0.12434067577123642 train acc 0.968852245687091\n",
            "epoch 27 batch id 1691 loss 0.0966455265879631 train acc 0.9688424009461857\n",
            "epoch 27 batch id 1701 loss 0.055414870381355286 train acc 0.9688234861845973\n",
            "epoch 27 batch id 1711 loss 0.07811596989631653 train acc 0.9688321887784921\n",
            "epoch 27 batch id 1721 loss 0.0782056376338005 train acc 0.9688861853573504\n",
            "epoch 27 batch id 1731 loss 0.0007405115757137537 train acc 0.9689666377816292\n",
            "epoch 27 batch id 1741 loss 0.0792410597205162 train acc 0.9689743681792073\n",
            "epoch 27 batch id 1751 loss 0.07545100897550583 train acc 0.9689730868075386\n",
            "epoch 27 batch id 1761 loss 0.10452486574649811 train acc 0.96893632879046\n",
            "epoch 27 batch id 1771 loss 0.04531920328736305 train acc 0.968944099378882\n",
            "epoch 27 batch id 1781 loss 0.14159338176250458 train acc 0.9689605558674902\n",
            "epoch 27 batch id 1791 loss 0.1846599578857422 train acc 0.9689070351758794\n",
            "epoch 27 batch id 1801 loss 0.050193049013614655 train acc 0.9689148389783454\n",
            "epoch 27 batch id 1811 loss 0.040393032133579254 train acc 0.9689139287686361\n",
            "epoch 27 batch id 1821 loss 0.0921713337302208 train acc 0.9688872872048325\n",
            "epoch 27 batch id 1831 loss 0.05004280060529709 train acc 0.9688694702348444\n",
            "epoch 27 batch id 1841 loss 0.010511551983654499 train acc 0.9688094106463878\n",
            "epoch 27 batch id 1851 loss 0.053012676537036896 train acc 0.968792206915181\n",
            "epoch 27 batch id 1861 loss 0.042139098048210144 train acc 0.9687667920472864\n",
            "epoch 27 batch id 1871 loss 0.08465670794248581 train acc 0.9688752672367718\n",
            "epoch 27 batch id 1881 loss 0.06701715290546417 train acc 0.9689078282828283\n",
            "epoch 27 batch id 1891 loss 0.15033838152885437 train acc 0.9689317821258593\n",
            "epoch 27 batch id 1901 loss 0.2109031230211258 train acc 0.9689226065228826\n",
            "epoch 27 batch id 1911 loss 0.0704759731888771 train acc 0.9688971742543171\n",
            "epoch 27 batch id 1921 loss 0.10559671372175217 train acc 0.9688638729828214\n",
            "epoch 27 batch id 1931 loss 0.07224975526332855 train acc 0.9688794665976178\n",
            "epoch 27 batch id 1941 loss 0.033345453441143036 train acc 0.9688948995363215\n",
            "epoch 27 batch id 1951 loss 0.12478774040937424 train acc 0.9689261916965659\n",
            "epoch 27 batch id 1961 loss 0.05407160520553589 train acc 0.9689731004589495\n",
            "epoch 27 batch id 1971 loss 0.014524606987833977 train acc 0.9689878234398782\n",
            "epoch 27 batch id 1981 loss 0.03545834496617317 train acc 0.9689708480565371\n",
            "epoch 27 batch id 1991 loss 0.1305873692035675 train acc 0.968961891009543\n",
            "epoch 27 batch id 2001 loss 0.03396248072385788 train acc 0.9690233008495752\n",
            "epoch 27 batch id 2011 loss 0.08994632214307785 train acc 0.9689986325211337\n",
            "epoch 27 batch id 2021 loss 0.07549218088388443 train acc 0.9690437902028699\n",
            "epoch 27 batch id 2031 loss 0.0020852130837738514 train acc 0.969034650418513\n",
            "epoch 27 batch id 2041 loss 0.025491809472441673 train acc 0.9690256001959824\n",
            "epoch 27 batch id 2051 loss 0.30696502327919006 train acc 0.9690090199902487\n",
            "epoch 27 batch id 2061 loss 0.20472218096256256 train acc 0.9690305070354197\n",
            "epoch 27 batch id 2071 loss 0.06379670649766922 train acc 0.9690517865765331\n",
            "epoch 27 batch id 2081 loss 0.05773930996656418 train acc 0.9690578447861605\n",
            "epoch 27 batch id 2091 loss 0.16089746356010437 train acc 0.9690040650406504\n",
            "epoch 27 batch id 2101 loss 0.009805661626160145 train acc 0.9690326035221323\n",
            "epoch 27 batch id 2111 loss 0.03137538954615593 train acc 0.9690164613927049\n",
            "epoch 27 batch id 2121 loss 0.01795177161693573 train acc 0.9690152050919377\n",
            "epoch 27 batch id 2131 loss 0.09374614804983139 train acc 0.9691019474425152\n",
            "epoch 27 batch id 2141 loss 0.12162777036428452 train acc 0.9691148995796357\n",
            "epoch 27 batch id 2151 loss 0.10408701747655869 train acc 0.9691713156671315\n",
            "epoch 27 batch id 2161 loss 0.13969510793685913 train acc 0.9691115224433133\n",
            "epoch 27 batch id 2171 loss 0.14473988115787506 train acc 0.9691170543528328\n",
            "epoch 27 batch id 2181 loss 0.08719702064990997 train acc 0.9691368638239339\n",
            "epoch 27 batch id 2191 loss 0.02987385168671608 train acc 0.969170755362848\n",
            "epoch 27 batch id 2201 loss 0.040763843804597855 train acc 0.9691546456156293\n",
            "epoch 27 batch id 2211 loss 0.01356418989598751 train acc 0.9691245477159657\n",
            "epoch 27 batch id 2221 loss 0.04279445856809616 train acc 0.9691158262044124\n",
            "epoch 27 batch id 2231 loss 0.045761436223983765 train acc 0.9691211900493052\n",
            "epoch 27 batch id 2241 loss 0.17406301200389862 train acc 0.9691125613565372\n",
            "epoch 27 batch id 2251 loss 0.1323557049036026 train acc 0.9690762438916037\n",
            "epoch 27 batch id 2261 loss 0.1486259549856186 train acc 0.9690955329500222\n",
            "epoch 27 batch id 2271 loss 0.14763541519641876 train acc 0.9691352928225452\n",
            "epoch 27 batch id 2281 loss 0.11352664977312088 train acc 0.9691199035510741\n",
            "epoch 27 batch id 2291 loss 0.0805949717760086 train acc 0.9691387494543867\n",
            "epoch 27 batch id 2301 loss 0.06642083823680878 train acc 0.9691438504997827\n",
            "epoch 27 batch id 2311 loss 0.25135505199432373 train acc 0.9691624296841195\n",
            "epoch 27 batch id 2321 loss 0.11279069632291794 train acc 0.9691404566996984\n",
            "epoch 27 batch id 2331 loss 0.11155916005373001 train acc 0.9691253753753754\n",
            "epoch 27 batch id 2341 loss 0.08411086350679398 train acc 0.9691237718923537\n",
            "epoch 27 batch id 2351 loss 0.16261960566043854 train acc 0.9690623670778392\n",
            "epoch 27 batch id 2361 loss 0.18887482583522797 train acc 0.9690742799661161\n",
            "epoch 27 batch id 2371 loss 0.019242778420448303 train acc 0.9691256326444538\n",
            "epoch 27 batch id 2381 loss 0.0010451101697981358 train acc 0.9691634292314154\n",
            "epoch 27 batch id 2391 loss 0.021295739337801933 train acc 0.969142095357591\n",
            "epoch 27 batch id 2401 loss 0.03766448423266411 train acc 0.9691534777176176\n",
            "epoch 27 batch id 2411 loss 0.04483823478221893 train acc 0.9691388428038158\n",
            "epoch 27 batch id 2421 loss 0.0380517803132534 train acc 0.9691307827344072\n",
            "epoch 27 batch id 2431 loss 0.05619475618004799 train acc 0.9691292163718634\n",
            "epoch 27 batch id 2441 loss 0.04055375978350639 train acc 0.969127662843097\n",
            "epoch 27 batch id 2451 loss 0.032429151237010956 train acc 0.9691133720930233\n",
            "epoch 27 batch id 2461 loss 0.23442094027996063 train acc 0.969099197480699\n",
            "epoch 27 batch id 2471 loss 0.07962695509195328 train acc 0.9690724908943747\n",
            "epoch 27 batch id 2481 loss 0.1150105744600296 train acc 0.9690963825070537\n",
            "epoch 27 batch id 2491 loss 0.14639228582382202 train acc 0.969076174227218\n",
            "epoch 27 batch id 2501 loss 0.018349746242165565 train acc 0.9691061075569772\n",
            "epoch 27 batch id 2511 loss 0.015159694477915764 train acc 0.969123357228196\n",
            "epoch 27 batch id 2521 loss 0.057063933461904526 train acc 0.9690784906783022\n",
            "epoch 27 batch id 2531 loss 0.1496698409318924 train acc 0.9691080600553141\n",
            "epoch 27 batch id 2541 loss 0.07678195834159851 train acc 0.9691066509248327\n",
            "epoch 27 batch id 2551 loss 0.08490343391895294 train acc 0.9691175029400235\n",
            "epoch 27 batch id 2561 loss 0.22949396073818207 train acc 0.9690916634127295\n",
            "epoch 27 batch id 2571 loss 0.2074429988861084 train acc 0.9691146441073513\n",
            "epoch 27 batch id 2581 loss 0.09086191654205322 train acc 0.9691132313056955\n",
            "epoch 27 batch id 2591 loss 0.0821649581193924 train acc 0.9691057989193361\n",
            "epoch 27 batch id 2601 loss 0.1475568562746048 train acc 0.9691164455978469\n",
            "epoch 27 batch id 2611 loss 0.04416094720363617 train acc 0.9691629165070854\n",
            "epoch 27 batch id 2621 loss 0.06237812712788582 train acc 0.9691374952308279\n",
            "epoch 27 batch id 2631 loss 0.25930356979370117 train acc 0.969088511972634\n",
            "epoch 27 batch id 2641 loss 0.10300547629594803 train acc 0.9690458159787959\n",
            "epoch 27 batch id 2651 loss 0.003563871141523123 train acc 0.9690211241041117\n",
            "epoch 27 batch id 2661 loss 0.06741896271705627 train acc 0.9690201052236002\n",
            "epoch 27 batch id 2671 loss 0.15646584331989288 train acc 0.9690366435791838\n",
            "epoch 27 batch id 2681 loss 0.1399262249469757 train acc 0.9689772939201791\n",
            "epoch 27 batch id 2691 loss 0.03539891541004181 train acc 0.9689532237086584\n",
            "epoch 27 batch id 2701 loss 0.140663743019104 train acc 0.9689582562014069\n",
            "epoch 27 batch id 2711 loss 0.003938901703804731 train acc 0.9689401973441535\n",
            "epoch 27 batch id 2721 loss 0.038073986768722534 train acc 0.9689509830944506\n",
            "epoch 27 batch id 2731 loss 0.06599946320056915 train acc 0.9689674112046869\n",
            "epoch 27 batch id 2741 loss 0.1313648670911789 train acc 0.9689894199197373\n",
            "epoch 27 batch id 2751 loss 0.11818588525056839 train acc 0.968965830607052\n",
            "epoch 27 batch id 2761 loss 0.02619030885398388 train acc 0.9689820264396958\n",
            "epoch 27 batch id 2771 loss 0.046442899852991104 train acc 0.9690150216528329\n",
            "epoch 27 batch id 2781 loss 0.059040870517492294 train acc 0.9690477795756922\n",
            "epoch 27 batch id 2791 loss 0.10181935876607895 train acc 0.9690299175922609\n",
            "epoch 27 batch id 2801 loss 0.11659948527812958 train acc 0.9689787129596573\n",
            "epoch 27 batch id 2811 loss 0.08798617869615555 train acc 0.9690112504446816\n",
            "epoch 27 batch id 2821 loss 0.10072403401136398 train acc 0.9690214019851117\n",
            "epoch 27 batch id 2831 loss 0.05628030374646187 train acc 0.9690204433062523\n",
            "epoch 27 batch id 2841 loss 0.13991442322731018 train acc 0.9690084917282646\n",
            "epoch 27 batch id 2851 loss 0.004067958332598209 train acc 0.9690240266573132\n",
            "epoch 27 batch id 2861 loss 0.08244160562753677 train acc 0.9690121461027613\n",
            "epoch 27 batch id 2871 loss 0.18026673793792725 train acc 0.9689840212469523\n",
            "epoch 27 batch id 2881 loss 0.04477770999073982 train acc 0.9690049028115237\n",
            "epoch 27 batch id 2891 loss 0.09528084099292755 train acc 0.9689824022829471\n",
            "epoch 27 batch id 2901 loss 0.0473509356379509 train acc 0.9689869872457774\n",
            "epoch 27 batch id 2911 loss 0.021154936403036118 train acc 0.9689861731363792\n",
            "epoch 27 batch id 2921 loss 0.002547062700614333 train acc 0.968958618623759\n",
            "epoch 27 batch id 2931 loss 0.05929093062877655 train acc 0.9689205902422381\n",
            "epoch 27 batch id 2941 loss 0.015642806887626648 train acc 0.9689040717443047\n",
            "epoch 27 batch id 2951 loss 0.024238748475909233 train acc 0.9689564977973568\n",
            "epoch 27 batch id 2961 loss 0.025398435071110725 train acc 0.9689821850726106\n",
            "epoch 27 batch id 2971 loss 0.06276904791593552 train acc 0.9689761443958264\n",
            "epoch 27 batch id 2981 loss 0.10939464718103409 train acc 0.9689334535390809\n",
            "epoch 27 batch id 2991 loss 0.10279859602451324 train acc 0.9689798562353727\n",
            "epoch 27 batch id 3001 loss 0.04523783549666405 train acc 0.9689895034988337\n",
            "epoch 27 batch id 3011 loss 0.12952420115470886 train acc 0.96898351876453\n",
            "epoch 27 batch id 3021 loss 0.10508643090724945 train acc 0.9689568851373718\n",
            "epoch 27 batch id 3031 loss 0.2639927864074707 train acc 0.9689562025734081\n",
            "epoch 27 batch id 3041 loss 0.16851305961608887 train acc 0.9689195577112791\n",
            "epoch 27 batch id 3051 loss 0.24526958167552948 train acc 0.9689343657817109\n",
            "epoch 27 batch id 3061 loss 0.0992911234498024 train acc 0.9689133453119896\n",
            "epoch 27 batch id 3071 loss 0.019138043746352196 train acc 0.9689331650928037\n",
            "epoch 27 batch id 3081 loss 0.09489333629608154 train acc 0.9689173563777994\n",
            "epoch 27 batch id 3091 loss 0.07405013591051102 train acc 0.9689168149466192\n",
            "epoch 27 batch id 3101 loss 0.12104599922895432 train acc 0.9689011609158336\n",
            "epoch 27 batch id 3111 loss 0.10628817975521088 train acc 0.9689207650273224\n",
            "epoch 27 batch id 3121 loss 0.031749218702316284 train acc 0.9689652755527075\n",
            "epoch 27 batch id 3131 loss 0.004808512516319752 train acc 0.9689795592462472\n",
            "epoch 27 batch id 3141 loss 0.059081386774778366 train acc 0.9689838029290035\n",
            "epoch 27 batch id 3151 loss 0.11153241991996765 train acc 0.968978102189781\n",
            "epoch 27 batch id 3161 loss 0.23300740122795105 train acc 0.9689427791838026\n",
            "epoch 27 batch id 3171 loss 0.02366800606250763 train acc 0.9689520261747083\n",
            "epoch 27 batch id 3181 loss 0.09958098828792572 train acc 0.9689612150267212\n",
            "epoch 27 batch id 3191 loss 0.026685869321227074 train acc 0.9689850360388593\n",
            "epoch 27 batch id 3201 loss 0.2619067132472992 train acc 0.968930607622618\n",
            "epoch 27 batch id 3211 loss 0.1314823180437088 train acc 0.9689105808159452\n",
            "epoch 27 batch id 3221 loss 0.06446575373411179 train acc 0.9688809764048432\n",
            "epoch 27 batch id 3231 loss 0.061086662113666534 train acc 0.9688563912101517\n",
            "epoch 27 batch id 3241 loss 0.0803556889295578 train acc 0.9688705260721999\n",
            "epoch 27 batch id 3251 loss 0.14662156999111176 train acc 0.9688557366964011\n",
            "epoch 27 batch id 3261 loss 0.0909293070435524 train acc 0.968821872125115\n",
            "epoch 27 batch id 3271 loss 0.1287008672952652 train acc 0.9688120987465607\n",
            "epoch 27 batch id 3281 loss 0.18290960788726807 train acc 0.968778573605608\n",
            "epoch 27 batch id 3291 loss 0.014787936583161354 train acc 0.96875\n",
            "epoch 27 batch id 3301 loss 0.05621863529086113 train acc 0.9687594668282339\n",
            "epoch 27 batch id 3311 loss 0.07558142393827438 train acc 0.9687735955904561\n",
            "epoch 27 batch id 3321 loss 0.1554504632949829 train acc 0.9687782294489612\n",
            "epoch 27 batch id 3331 loss 0.04306845739483833 train acc 0.9687453092164515\n",
            "epoch 27 batch id 3341 loss 0.06634407490491867 train acc 0.96876403023047\n",
            "epoch 27 batch id 3351 loss 0.15028145909309387 train acc 0.9687779767233662\n",
            "epoch 27 batch id 3361 loss 0.07422402501106262 train acc 0.9687871913121094\n",
            "epoch 27 batch id 3371 loss 0.12203039228916168 train acc 0.9688102566004153\n",
            "epoch 27 batch id 3381 loss 0.0031959055922925472 train acc 0.9688378068618752\n",
            "epoch 27 batch id 3391 loss 0.08549832552671432 train acc 0.9688467634915954\n",
            "epoch 27 batch id 3401 loss 0.18717226386070251 train acc 0.9688189135548368\n",
            "epoch 27 batch id 3411 loss 0.11752703040838242 train acc 0.9688324538258575\n",
            "epoch 27 batch id 3421 loss 0.029757840558886528 train acc 0.9688596170710319\n",
            "epoch 27 batch id 3431 loss 0.1483885645866394 train acc 0.9688775138443603\n",
            "epoch 27 batch id 3441 loss 0.03973837196826935 train acc 0.9688816841034583\n",
            "epoch 27 batch id 3451 loss 0.04525556042790413 train acc 0.9688586641553173\n",
            "epoch 27 batch id 3461 loss 0.021855009719729424 train acc 0.968858350187807\n",
            "epoch 27 batch id 3471 loss 0.054609425365924835 train acc 0.9688625396139441\n",
            "epoch 27 batch id 3481 loss 0.2540547847747803 train acc 0.9688487503590922\n",
            "epoch 27 batch id 3491 loss 0.002704183105379343 train acc 0.9688350401031223\n",
            "epoch 27 batch id 3501 loss 0.058881018310785294 train acc 0.9688838903170522\n",
            "epoch 27 batch id 3511 loss 0.02923404984176159 train acc 0.9688924095699231\n",
            "epoch 27 batch id 3521 loss 0.09830446541309357 train acc 0.9689097557512071\n",
            "epoch 27 batch id 3531 loss 0.16273389756679535 train acc 0.9689181534975928\n",
            "epoch 27 batch id 3541 loss 0.07668493688106537 train acc 0.9689309164077944\n",
            "epoch 27 batch id 3551 loss 0.026439841836690903 train acc 0.9689260067586596\n",
            "epoch 27 batch id 3561 loss 0.006679378915578127 train acc 0.9689430637461387\n",
            "epoch 27 batch id 3571 loss 0.18524886667728424 train acc 0.9689293965275834\n",
            "epoch 27 batch id 3581 loss 0.06609667837619781 train acc 0.9689463487852555\n",
            "epoch 27 batch id 3591 loss 0.13626861572265625 train acc 0.9689675577833473\n",
            "epoch 27 batch id 3601 loss 0.04080312326550484 train acc 0.9689712926964732\n",
            "epoch 27 batch id 3611 loss 0.10908211767673492 train acc 0.9689836610357242\n",
            "epoch 27 batch id 3621 loss 0.07966986298561096 train acc 0.9689614402098867\n",
            "epoch 27 batch id 3631 loss 0.08228516578674316 train acc 0.9689651611126412\n",
            "epoch 27 batch id 3641 loss 0.12102910131216049 train acc 0.9689731529799506\n",
            "epoch 27 batch id 3651 loss 0.15599581599235535 train acc 0.9689511435223227\n",
            "epoch 27 batch id 3661 loss 0.05273984372615814 train acc 0.9689633979786944\n",
            "epoch 27 batch id 3671 loss 0.05320005491375923 train acc 0.9689883546717516\n",
            "epoch 27 batch id 3681 loss 0.17803941667079926 train acc 0.9690004414561261\n",
            "epoch 27 batch id 3691 loss 0.057852089405059814 train acc 0.969020929287456\n",
            "epoch 27 batch id 3701 loss 0.032351914793252945 train acc 0.9690075317481762\n",
            "epoch 27 batch id 3711 loss 0.03891231119632721 train acc 0.9690026273241714\n",
            "epoch 27 batch id 3721 loss 0.06798814982175827 train acc 0.9690103466809997\n",
            "epoch 27 batch id 3731 loss 0.023742547258734703 train acc 0.9690264004288395\n",
            "epoch 27 batch id 3741 loss 0.10529390722513199 train acc 0.9690006014434643\n",
            "epoch 27 batch id 3751 loss 0.04052311182022095 train acc 0.9689957677952546\n",
            "epoch 27 batch id 3761 loss 0.21273988485336304 train acc 0.9689701874501463\n",
            "epoch 27 batch id 3771 loss 0.02795843407511711 train acc 0.9689488862370724\n",
            "epoch 27 batch id 3781 loss 0.14602185785770416 train acc 0.9689524927267918\n",
            "epoch 27 batch id 3791 loss 0.0006958040175959468 train acc 0.9689766882089158\n",
            "epoch 27 batch id 3801 loss 0.057138629257678986 train acc 0.9689843133385951\n",
            "epoch 27 batch id 3811 loss 0.019275614991784096 train acc 0.9690205982681711\n",
            "epoch 27 batch id 3821 loss 0.07074303925037384 train acc 0.9690321578120911\n",
            "epoch 27 batch id 3831 loss 0.006663732696324587 train acc 0.9690314212999217\n",
            "epoch 27 batch id 3841 loss 0.006444241385906935 train acc 0.9690388245248633\n",
            "epoch 27 batch id 3851 loss 0.01380468625575304 train acc 0.9690299597507142\n",
            "epoch 27 batch id 3861 loss 0.033532578498125076 train acc 0.9690211408961409\n",
            "epoch 27 batch id 3871 loss 0.23674380779266357 train acc 0.9690204404546628\n",
            "epoch 27 batch id 3881 loss 0.22558915615081787 train acc 0.9690157175985571\n",
            "epoch 27 batch id 3891 loss 0.009520496241748333 train acc 0.9690230660498587\n",
            "epoch 27 batch id 3901 loss 0.030483994632959366 train acc 0.9690063445270444\n",
            "epoch 27 batch id 3911 loss 0.040518030524253845 train acc 0.9690016939401688\n",
            "epoch 27 batch id 3921 loss 0.014969362877309322 train acc 0.9690050369803621\n",
            "epoch 27 batch id 3931 loss 0.05040635168552399 train acc 0.9689805393029763\n",
            "epoch 27 batch id 3941 loss 0.06817345321178436 train acc 0.9690077074346612\n",
            "epoch 27 batch id 3951 loss 0.015146726742386818 train acc 0.9690070551759048\n",
            "epoch 27 batch id 3961 loss 0.02446264959871769 train acc 0.9690024614996213\n",
            "epoch 27 batch id 3971 loss 0.13824306428432465 train acc 0.968978217073785\n",
            "epoch 27 batch id 3981 loss 0.014229672029614449 train acc 0.9689619442351168\n",
            "epoch 27 batch id 3991 loss 0.20571596920490265 train acc 0.9689614131796542\n",
            "epoch 27 batch id 4001 loss 0.09445828944444656 train acc 0.9689413584103974\n",
            "epoch 27 batch id 4011 loss 0.01865459978580475 train acc 0.968929194714535\n",
            "epoch 27 batch id 4021 loss 0.05296383425593376 train acc 0.9688976622730664\n",
            "epoch 27 batch id 4031 loss 0.0422496572136879 train acc 0.9689321818407343\n",
            "epoch 27 batch id 4041 loss 0.04354788735508919 train acc 0.9689394642415243\n",
            "epoch 27 batch id 4051 loss 0.031208734959363937 train acc 0.9689467106887188\n",
            "epoch 27 batch id 4061 loss 0.03101167641580105 train acc 0.9689616165968973\n",
            "epoch 27 batch id 4071 loss 0.15951618552207947 train acc 0.9689649349054287\n",
            "epoch 27 batch id 4081 loss 0.013557647354900837 train acc 0.9689912092624356\n",
            "epoch 27 batch id 4091 loss 0.10766886919736862 train acc 0.9689677034954779\n",
            "epoch 27 batch id 4101 loss 0.09552566707134247 train acc 0.9689862228724702\n",
            "epoch 27 batch id 4111 loss 0.05923919752240181 train acc 0.9689780467039649\n",
            "epoch 27 batch id 4121 loss 0.04336976632475853 train acc 0.9689850764377578\n",
            "epoch 27 batch id 4131 loss 0.15718156099319458 train acc 0.9689693778746067\n",
            "epoch 27 batch id 4141 loss 0.08173807710409164 train acc 0.9689877143202125\n",
            "epoch 27 batch id 4151 loss 0.08583700656890869 train acc 0.9689909058058299\n",
            "epoch 27 batch id 4161 loss 0.07808994501829147 train acc 0.968960285988945\n",
            "epoch 27 batch id 4171 loss 0.24725547432899475 train acc 0.9689560357228483\n",
            "epoch 27 batch id 4181 loss 0.016615256667137146 train acc 0.9689592800765368\n",
            "epoch 27 batch id 4191 loss 0.12021088600158691 train acc 0.9689401395848246\n",
            "epoch 27 batch id 4201 loss 0.0856088399887085 train acc 0.9689731611521066\n",
            "epoch 27 batch id 4211 loss 0.15829573571681976 train acc 0.9689503680835906\n",
            "epoch 27 batch id 4221 loss 0.14691489934921265 train acc 0.968927683013504\n",
            "epoch 27 batch id 4231 loss 0.13113032281398773 train acc 0.9689383419995273\n",
            "epoch 27 batch id 4241 loss 0.08171585202217102 train acc 0.9689563192643245\n",
            "epoch 27 batch id 4251 loss 0.09169533848762512 train acc 0.9689631851329099\n",
            "epoch 27 batch id 4261 loss 0.008045729249715805 train acc 0.9689810197136822\n",
            "epoch 27 batch id 4271 loss 0.00874846987426281 train acc 0.9689951123858581\n",
            "epoch 27 batch id 4281 loss 0.1005028486251831 train acc 0.9690091392198085\n",
            "epoch 27 batch id 4291 loss 0.11895793676376343 train acc 0.9689976112794221\n",
            "epoch 27 batch id 4301 loss 0.09336009621620178 train acc 0.9689788711927458\n",
            "epoch 27 batch id 4311 loss 0.046524155884981155 train acc 0.9689964625376942\n",
            "epoch 27 batch id 4321 loss 0.04281441494822502 train acc 0.9689886600323999\n",
            "epoch 27 batch id 4331 loss 0.12150254100561142 train acc 0.9689772858462249\n",
            "epoch 27 batch id 4341 loss 0.048727624118328094 train acc 0.9689731628656991\n",
            "epoch 27 batch id 4351 loss 0.13313287496566772 train acc 0.9689511031946679\n",
            "epoch 27 batch id 4361 loss 0.07172232866287231 train acc 0.9689613907360697\n",
            "epoch 27 batch id 4371 loss 0.08897241950035095 train acc 0.9689787805994051\n",
            "epoch 27 batch id 4381 loss 0.15657450258731842 train acc 0.9689425930152933\n",
            "epoch 27 batch id 4391 loss 0.05216825753450394 train acc 0.968963504896379\n",
            "epoch 27 batch id 4401 loss 0.08168227225542068 train acc 0.9689488184503522\n",
            "epoch 27 batch id 4411 loss 0.00687780324369669 train acc 0.9689589945590569\n",
            "epoch 27 batch id 4421 loss 0.08212505280971527 train acc 0.9689514532911107\n",
            "epoch 27 batch id 4431 loss 0.05567228049039841 train acc 0.9689580512299707\n",
            "epoch 27 batch id 4441 loss 0.05898255854845047 train acc 0.9689646194550777\n",
            "epoch 27 batch id 4451 loss 0.008930276148021221 train acc 0.9689746686137947\n",
            "epoch 27 batch id 4461 loss 0.06836432218551636 train acc 0.9689671598296347\n",
            "epoch 27 batch id 4471 loss 0.10212602466344833 train acc 0.9689666741221203\n",
            "epoch 27 batch id 4481 loss 0.024082908406853676 train acc 0.9689871122517295\n",
            "epoch 27 batch id 4491 loss 0.12186861783266068 train acc 0.968983105099087\n",
            "epoch 27 batch id 4501 loss 0.06191930174827576 train acc 0.9689860586536325\n",
            "epoch 27 batch id 4511 loss 0.19822123646736145 train acc 0.9689543615606295\n",
            "epoch 27 batch id 4521 loss 0.048188190907239914 train acc 0.9689400851581509\n",
            "epoch 27 batch id 4531 loss 0.12517274916172028 train acc 0.9689431141028471\n",
            "epoch 27 batch id 4541 loss 0.08817160874605179 train acc 0.9689358070909492\n",
            "epoch 27 batch id 4551 loss 0.059894584119319916 train acc 0.968945698747528\n",
            "epoch 27 batch id 4561 loss 0.10567659139633179 train acc 0.9689281407586056\n",
            "epoch 27 batch id 4571 loss 0.047457948327064514 train acc 0.9689448424852329\n",
            "epoch 27 batch id 4581 loss 0.13552682101726532 train acc 0.9689444171578258\n",
            "epoch 27 batch id 4591 loss 0.05367157980799675 train acc 0.9689371868873884\n",
            "epoch 27 batch id 4601 loss 0.025523526594042778 train acc 0.9689367800478157\n",
            "epoch 27 batch id 4611 loss 0.0716155543923378 train acc 0.9689499295163739\n",
            "epoch 27 batch id 4621 loss 0.0156706515699625 train acc 0.9689562594676477\n",
            "epoch 27 batch id 4631 loss 0.07917758822441101 train acc 0.968969310084215\n",
            "epoch 27 batch id 4641 loss 0.12022995203733444 train acc 0.9689856711915535\n",
            "epoch 27 batch id 4651 loss 0.17959731817245483 train acc 0.9689851644807568\n",
            "epoch 27 batch id 4661 loss 0.06151854246854782 train acc 0.9690148305084746\n",
            "epoch 27 batch id 4671 loss 0.011365460231900215 train acc 0.969034334189681\n",
            "epoch 27 batch id 4681 loss 0.19912032783031464 train acc 0.9690270508438368\n",
            "epoch 27 batch id 4691 loss 0.010505816899240017 train acc 0.96902979108932\n",
            "epoch 27 batch id 4701 loss 0.10561147332191467 train acc 0.9690291959157626\n",
            "epoch 27 batch id 4711 loss 0.1748141050338745 train acc 0.969028603268945\n",
            "epoch 27 batch id 4721 loss 0.05650913715362549 train acc 0.9690048453717433\n",
            "epoch 27 batch id 4731 loss 0.06856802850961685 train acc 0.969007609384908\n",
            "epoch 27 batch id 4741 loss 0.060003072023391724 train acc 0.969007066019827\n",
            "epoch 27 batch id 4751 loss 0.1632630079984665 train acc 0.9689867922542622\n",
            "epoch 27 batch id 4761 loss 0.08623102307319641 train acc 0.9689895767695862\n",
            "epoch 27 batch id 4771 loss 0.11563471704721451 train acc 0.9689988996017607\n",
            "epoch 27 batch id 4781 loss 0.10605616867542267 train acc 0.9689983790002091\n",
            "epoch 27 batch id 4791 loss 0.2861577272415161 train acc 0.9689685086620747\n",
            "epoch 27 batch id 4801 loss 0.10956314951181412 train acc 0.9689615444699021\n",
            "epoch 27 batch id 4811 loss 0.008589270524680614 train acc 0.9689805913531491\n",
            "epoch 27 batch id 4821 loss 0.1383999139070511 train acc 0.9689833540759178\n",
            "epoch 27 batch id 4831 loss 0.08656300604343414 train acc 0.9689828710411923\n",
            "epoch 27 batch id 4841 loss 0.041602253913879395 train acc 0.9689856176409832\n",
            "epoch 27 batch id 4851 loss 0.08017385751008987 train acc 0.9689980158730159\n",
            "epoch 27 batch id 4861 loss 0.22565652430057526 train acc 0.9689878625797161\n",
            "epoch 27 batch id 4871 loss 0.09814723581075668 train acc 0.9690034130568672\n",
            "epoch 27 batch id 4881 loss 0.36315664649009705 train acc 0.9689932903093629\n",
            "epoch 27 batch id 4891 loss 0.13366270065307617 train acc 0.9689800143120016\n",
            "epoch 27 batch id 4901 loss 0.00405671214684844 train acc 0.9689699806162008\n",
            "epoch 27 batch id 4911 loss 0.1125398501753807 train acc 0.9689631694155977\n",
            "epoch 27 batch id 4921 loss 0.18731842935085297 train acc 0.9689627362324731\n",
            "epoch 27 batch id 4931 loss 0.15149816870689392 train acc 0.9689306175218009\n",
            "epoch 27 batch id 4941 loss 0.07046862691640854 train acc 0.9689081157660393\n",
            "epoch 27 batch id 4951 loss 0.1636040210723877 train acc 0.9689046404766714\n",
            "epoch 27 train acc 0.9689139096227557\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad607554035242d9842be0830b09044b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 27 loss 1.1305696964263916 test acc 0.8401129490469208\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3881430808904e82b4dba9e9cf5e1b11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 28 batch id 1 loss 0.03161221742630005 train acc 0.984375\n",
            "epoch 28 batch id 11 loss 0.04334741458296776 train acc 0.9801136363636364\n",
            "epoch 28 batch id 21 loss 0.08136703073978424 train acc 0.9799107142857143\n",
            "epoch 28 batch id 31 loss 0.08565108478069305 train acc 0.9722782258064516\n",
            "epoch 28 batch id 41 loss 0.029638148844242096 train acc 0.9725609756097561\n",
            "epoch 28 batch id 51 loss 0.0960630401968956 train acc 0.9736519607843137\n",
            "epoch 28 batch id 61 loss 0.05320138484239578 train acc 0.9725922131147541\n",
            "epoch 28 batch id 71 loss 0.2917992174625397 train acc 0.9716109154929577\n",
            "epoch 28 batch id 81 loss 0.21252086758613586 train acc 0.9712577160493827\n",
            "epoch 28 batch id 91 loss 0.1126992478966713 train acc 0.9702953296703297\n",
            "epoch 28 batch id 101 loss 0.11645626276731491 train acc 0.9701423267326733\n",
            "epoch 28 batch id 111 loss 0.010392289608716965 train acc 0.9694538288288288\n",
            "epoch 28 batch id 121 loss 0.13642561435699463 train acc 0.9699121900826446\n",
            "epoch 28 batch id 131 loss 0.09564360231161118 train acc 0.9698234732824428\n",
            "epoch 28 batch id 141 loss 0.04208684340119362 train acc 0.9699689716312057\n",
            "epoch 28 batch id 151 loss 0.06700357794761658 train acc 0.9703021523178808\n",
            "epoch 28 batch id 161 loss 0.10090412944555283 train acc 0.9702057453416149\n",
            "epoch 28 batch id 171 loss 0.044342417269945145 train acc 0.9702119883040936\n",
            "epoch 28 batch id 181 loss 0.16785147786140442 train acc 0.9703038674033149\n",
            "epoch 28 batch id 191 loss 0.11218347400426865 train acc 0.9699770942408377\n",
            "epoch 28 batch id 201 loss 0.039643511176109314 train acc 0.9703824626865671\n",
            "epoch 28 batch id 211 loss 0.13113529980182648 train acc 0.9709715639810427\n",
            "epoch 28 batch id 221 loss 0.15575699508190155 train acc 0.9706589366515838\n",
            "epoch 28 batch id 231 loss 0.040905971080064774 train acc 0.9706439393939394\n",
            "epoch 28 batch id 241 loss 0.18692006170749664 train acc 0.9706301867219918\n",
            "epoch 28 batch id 251 loss 0.11059985309839249 train acc 0.97074203187251\n",
            "epoch 28 batch id 261 loss 0.07611308246850967 train acc 0.9710847701149425\n",
            "epoch 28 batch id 271 loss 0.09163479506969452 train acc 0.971344557195572\n",
            "epoch 28 batch id 281 loss 0.016420017927885056 train acc 0.9715858540925267\n",
            "epoch 28 batch id 291 loss 0.12031565606594086 train acc 0.9717031786941581\n",
            "epoch 28 batch id 301 loss 0.11207646131515503 train acc 0.9708783222591362\n",
            "epoch 28 batch id 311 loss 0.048063915222883224 train acc 0.971161575562701\n",
            "epoch 28 batch id 321 loss 0.1467822939157486 train acc 0.9712811526479751\n",
            "epoch 28 batch id 331 loss 0.009896565228700638 train acc 0.9711574773413897\n",
            "epoch 28 batch id 341 loss 0.1942540407180786 train acc 0.9712701612903226\n",
            "epoch 28 batch id 351 loss 0.037244975566864014 train acc 0.9713764245014245\n",
            "epoch 28 batch id 361 loss 0.04791863635182381 train acc 0.9714335180055401\n",
            "epoch 28 batch id 371 loss 0.10193139314651489 train acc 0.9716138814016172\n",
            "epoch 28 batch id 381 loss 0.04150966554880142 train acc 0.9716207349081365\n",
            "epoch 28 batch id 391 loss 0.1766369789838791 train acc 0.9718270460358056\n",
            "epoch 28 batch id 401 loss 0.0008258044254034758 train acc 0.9719061720698254\n",
            "epoch 28 batch id 411 loss 0.09909668564796448 train acc 0.9720194647201946\n",
            "epoch 28 batch id 421 loss 0.012328558601439 train acc 0.972164489311164\n",
            "epoch 28 batch id 431 loss 0.05081270635128021 train acc 0.9722302784222738\n",
            "epoch 28 batch id 441 loss 0.12753349542617798 train acc 0.9720450680272109\n",
            "epoch 28 batch id 451 loss 0.029857002198696136 train acc 0.9721798780487805\n",
            "epoch 28 batch id 461 loss 0.023942602798342705 train acc 0.971902114967462\n",
            "epoch 28 batch id 471 loss 0.007393623236566782 train acc 0.9720674097664543\n",
            "epoch 28 batch id 481 loss 0.07925113290548325 train acc 0.9722258316008316\n",
            "epoch 28 batch id 491 loss 0.02058262564241886 train acc 0.9721232179226069\n",
            "epoch 28 batch id 501 loss 0.23993828892707825 train acc 0.9720870758483033\n",
            "epoch 28 batch id 511 loss 0.1159805953502655 train acc 0.9719911937377691\n",
            "epoch 28 batch id 521 loss 0.12200285494327545 train acc 0.9720489443378119\n",
            "epoch 28 batch id 531 loss 0.13833482563495636 train acc 0.9717514124293786\n",
            "epoch 28 batch id 541 loss 0.025764165446162224 train acc 0.971753696857671\n",
            "epoch 28 batch id 551 loss 0.04964131489396095 train acc 0.9718976860254084\n",
            "epoch 28 batch id 561 loss 0.23857836425304413 train acc 0.9715630570409982\n",
            "epoch 28 batch id 571 loss 0.042209040373563766 train acc 0.9716779772329247\n",
            "epoch 28 batch id 581 loss 0.07218801230192184 train acc 0.971520008605852\n",
            "epoch 28 batch id 591 loss 0.06219029426574707 train acc 0.9716582064297801\n",
            "epoch 28 batch id 601 loss 0.029339788481593132 train acc 0.9717138103161398\n",
            "epoch 28 batch id 611 loss 0.10250544548034668 train acc 0.9717420212765957\n",
            "epoch 28 batch id 621 loss 0.17307372391223907 train acc 0.9717693236714976\n",
            "epoch 28 batch id 631 loss 0.046576887369155884 train acc 0.9718700475435816\n",
            "epoch 28 batch id 641 loss 0.26060613989830017 train acc 0.9719188767550702\n",
            "epoch 28 batch id 651 loss 0.14250260591506958 train acc 0.9718942012288786\n",
            "epoch 28 batch id 661 loss 0.013230610638856888 train acc 0.9720593797276853\n",
            "epoch 28 batch id 671 loss 0.013650993816554546 train acc 0.9720799180327869\n",
            "epoch 28 batch id 681 loss 0.09423640370368958 train acc 0.9720539647577092\n",
            "epoch 28 batch id 691 loss 0.09649986773729324 train acc 0.9719157018813314\n",
            "epoch 28 batch id 701 loss 0.10044167190790176 train acc 0.9718482524964337\n",
            "epoch 28 batch id 711 loss 0.23204968869686127 train acc 0.9717387482419128\n",
            "epoch 28 batch id 721 loss 0.0670953169465065 train acc 0.9716972954230236\n",
            "epoch 28 batch id 731 loss 0.15103687345981598 train acc 0.9717424760601915\n",
            "epoch 28 batch id 741 loss 0.010305005125701427 train acc 0.9717442645074224\n",
            "epoch 28 batch id 751 loss 0.22684136033058167 train acc 0.9716211717709721\n",
            "epoch 28 batch id 761 loss 0.01704367995262146 train acc 0.9716655716162943\n",
            "epoch 28 batch id 771 loss 0.1038348525762558 train acc 0.9715061608300908\n",
            "epoch 28 batch id 781 loss 0.03522665426135063 train acc 0.9714908770806658\n",
            "epoch 28 batch id 791 loss 0.14931637048721313 train acc 0.9714957332490518\n",
            "epoch 28 batch id 801 loss 0.009975684806704521 train acc 0.971539481897628\n",
            "epoch 28 batch id 811 loss 0.13536293804645538 train acc 0.971562885326757\n",
            "epoch 28 batch id 821 loss 0.020334698259830475 train acc 0.9716808769792935\n",
            "epoch 28 batch id 831 loss 0.23673957586288452 train acc 0.9716080024067388\n",
            "epoch 28 batch id 841 loss 0.0681995302438736 train acc 0.9716111771700356\n",
            "epoch 28 batch id 851 loss 0.08974970132112503 train acc 0.971522473560517\n",
            "epoch 28 batch id 861 loss 0.059550195932388306 train acc 0.9715084204413472\n",
            "epoch 28 batch id 871 loss 0.04035123810172081 train acc 0.9714588117106774\n",
            "epoch 28 batch id 881 loss 0.13073056936264038 train acc 0.9714990068104427\n",
            "epoch 28 batch id 891 loss 0.015545227564871311 train acc 0.9714856902356902\n",
            "epoch 28 batch id 901 loss 0.12600786983966827 train acc 0.9714206437291898\n",
            "epoch 28 batch id 911 loss 0.06395013630390167 train acc 0.9713570252469813\n",
            "epoch 28 batch id 921 loss 0.004004784394055605 train acc 0.9712269272529859\n",
            "epoch 28 batch id 931 loss 0.030373843386769295 train acc 0.971233888292159\n",
            "epoch 28 batch id 941 loss 0.02913285791873932 train acc 0.9712573060573858\n",
            "epoch 28 batch id 951 loss 0.05192175880074501 train acc 0.971181650893796\n",
            "epoch 28 batch id 961 loss 0.04694975167512894 train acc 0.9712539021852237\n",
            "epoch 28 batch id 971 loss 0.07224863767623901 train acc 0.9710832904222451\n",
            "epoch 28 batch id 981 loss 0.07542356103658676 train acc 0.9711391437308868\n",
            "epoch 28 batch id 991 loss 0.11770935356616974 train acc 0.9711150353178607\n",
            "epoch 28 batch id 1001 loss 0.10895125567913055 train acc 0.9711226273726273\n",
            "epoch 28 batch id 1011 loss 0.02723110280930996 train acc 0.9710991592482691\n",
            "epoch 28 batch id 1021 loss 0.07075434178113937 train acc 0.9712291870714985\n",
            "epoch 28 batch id 1031 loss 0.018179072067141533 train acc 0.9712506062075654\n",
            "epoch 28 batch id 1041 loss 0.13725942373275757 train acc 0.9711965658021133\n",
            "epoch 28 batch id 1051 loss 0.03298252075910568 train acc 0.9711584205518554\n",
            "epoch 28 batch id 1061 loss 0.05907226353883743 train acc 0.9710915409990575\n",
            "epoch 28 batch id 1071 loss 0.0032335002906620502 train acc 0.9710988562091504\n",
            "epoch 28 batch id 1081 loss 0.08947043120861053 train acc 0.9710915818686402\n",
            "epoch 28 batch id 1091 loss 0.3173631429672241 train acc 0.9710557974335472\n",
            "epoch 28 batch id 1101 loss 0.06204912066459656 train acc 0.9710632379654859\n",
            "epoch 28 batch id 1111 loss 0.1569211781024933 train acc 0.9709861611161116\n",
            "epoch 28 batch id 1121 loss 0.07927402853965759 train acc 0.9710777207850134\n",
            "epoch 28 batch id 1131 loss 0.048433560878038406 train acc 0.9710018788682582\n",
            "epoch 28 batch id 1141 loss 0.0996367558836937 train acc 0.9710506134969326\n",
            "epoch 28 batch id 1151 loss 0.036602724343538284 train acc 0.9710306255430061\n",
            "epoch 28 batch id 1161 loss 0.08285831660032272 train acc 0.9709840654608096\n",
            "epoch 28 batch id 1171 loss 0.0284479521214962 train acc 0.9709383005977796\n",
            "epoch 28 batch id 1181 loss 0.11303924769163132 train acc 0.9709197713801863\n",
            "epoch 28 batch id 1191 loss 0.018392611294984818 train acc 0.9710196263643996\n",
            "epoch 28 batch id 1201 loss 0.08269640803337097 train acc 0.971156848459617\n",
            "epoch 28 batch id 1211 loss 0.03700703755021095 train acc 0.9712530966143683\n",
            "epoch 28 batch id 1221 loss 0.12534824013710022 train acc 0.9712325962325963\n",
            "epoch 28 batch id 1231 loss 0.22956117987632751 train acc 0.9711235783915516\n",
            "epoch 28 batch id 1241 loss 0.06727679818868637 train acc 0.9711296333601934\n",
            "epoch 28 batch id 1251 loss 0.1142481341958046 train acc 0.9710856314948042\n",
            "epoch 28 batch id 1261 loss 0.053276125341653824 train acc 0.9711538461538461\n",
            "epoch 28 batch id 1271 loss 0.08924455940723419 train acc 0.9711349331235248\n",
            "epoch 28 batch id 1281 loss 0.023888295516371727 train acc 0.9712504879000781\n",
            "epoch 28 batch id 1291 loss 0.12220148742198944 train acc 0.9712916343919442\n",
            "epoch 28 batch id 1301 loss 0.008546079508960247 train acc 0.971272098385857\n",
            "epoch 28 batch id 1311 loss 0.06329366564750671 train acc 0.9712886155606407\n",
            "epoch 28 batch id 1321 loss 0.05508292838931084 train acc 0.9711866010598031\n",
            "epoch 28 batch id 1331 loss 0.07409096509218216 train acc 0.9710978587528174\n",
            "epoch 28 batch id 1341 loss 0.3907106816768646 train acc 0.9710453952274422\n",
            "epoch 28 batch id 1351 loss 0.051115259528160095 train acc 0.9710631014063656\n",
            "epoch 28 batch id 1361 loss 0.07756934314966202 train acc 0.9710116642174872\n",
            "epoch 28 batch id 1371 loss 0.09196241945028305 train acc 0.970972374179431\n",
            "epoch 28 batch id 1381 loss 0.07486915588378906 train acc 0.9710241672700941\n",
            "epoch 28 batch id 1391 loss 0.008521596901118755 train acc 0.9709853522645578\n",
            "epoch 28 batch id 1401 loss 0.011331891641020775 train acc 0.9709805496074233\n",
            "epoch 28 batch id 1411 loss 0.056794390082359314 train acc 0.9709868887313962\n",
            "epoch 28 batch id 1421 loss 0.010018310509622097 train acc 0.9710261259676284\n",
            "epoch 28 batch id 1431 loss 0.06016658991575241 train acc 0.9710320580013976\n",
            "epoch 28 batch id 1441 loss 0.08605650067329407 train acc 0.971027064538515\n",
            "epoch 28 batch id 1451 loss 0.11474593728780746 train acc 0.9710544452101999\n",
            "epoch 28 batch id 1461 loss 0.14273293316364288 train acc 0.9710386721423683\n",
            "epoch 28 batch id 1471 loss 0.07764693349599838 train acc 0.9710549796057104\n",
            "epoch 28 batch id 1481 loss 0.1299397051334381 train acc 0.9710183153274814\n",
            "epoch 28 batch id 1491 loss 0.17500266432762146 train acc 0.9710240610328639\n",
            "epoch 28 batch id 1501 loss 0.19591309130191803 train acc 0.97102973017988\n",
            "epoch 28 batch id 1511 loss 0.08979638665914536 train acc 0.971014642620781\n",
            "epoch 28 batch id 1521 loss 0.04950230196118355 train acc 0.9709894806048652\n",
            "epoch 28 batch id 1531 loss 0.025136791169643402 train acc 0.9710360875244938\n",
            "epoch 28 batch id 1541 loss 0.0355757400393486 train acc 0.9710009733939\n",
            "epoch 28 batch id 1551 loss 0.04493427649140358 train acc 0.9709663120567376\n",
            "epoch 28 batch id 1561 loss 0.05159667879343033 train acc 0.9709521140294682\n",
            "epoch 28 batch id 1571 loss 0.02153072878718376 train acc 0.970948042647995\n",
            "epoch 28 batch id 1581 loss 0.10193293541669846 train acc 0.9708946078431373\n",
            "epoch 28 batch id 1591 loss 0.1006605252623558 train acc 0.9708614864864865\n",
            "epoch 28 batch id 1601 loss 0.10475259274244308 train acc 0.9709166146158651\n",
            "epoch 28 batch id 1611 loss 0.08507908880710602 train acc 0.9708449720670391\n",
            "epoch 28 batch id 1621 loss 0.03704788535833359 train acc 0.9708127698951264\n",
            "epoch 28 batch id 1631 loss 0.07071761041879654 train acc 0.9707905426118946\n",
            "epoch 28 batch id 1641 loss 0.12718921899795532 train acc 0.9707590645947592\n",
            "epoch 28 batch id 1651 loss 0.08115372806787491 train acc 0.9706806480920654\n",
            "epoch 28 batch id 1661 loss 0.04773787409067154 train acc 0.9707254665863937\n",
            "epoch 28 batch id 1671 loss 0.18638674914836884 train acc 0.9707042938360263\n",
            "epoch 28 batch id 1681 loss 0.08604824542999268 train acc 0.9707019631171921\n",
            "epoch 28 batch id 1691 loss 0.28813430666923523 train acc 0.9707181401537551\n",
            "epoch 28 batch id 1701 loss 0.10926400870084763 train acc 0.970734126984127\n",
            "epoch 28 batch id 1711 loss 0.11537431180477142 train acc 0.9707133985973115\n",
            "epoch 28 batch id 1721 loss 0.03812471777200699 train acc 0.9707383062173155\n",
            "epoch 28 batch id 1731 loss 0.004353294149041176 train acc 0.9707629260543039\n",
            "epoch 28 batch id 1741 loss 0.024701684713363647 train acc 0.9707782883400344\n",
            "epoch 28 batch id 1751 loss 0.04268939793109894 train acc 0.97077562821245\n",
            "epoch 28 batch id 1761 loss 0.03067532368004322 train acc 0.9707907438955139\n",
            "epoch 28 batch id 1771 loss 0.049044396728277206 train acc 0.9707792207792207\n",
            "epoch 28 batch id 1781 loss 0.002014850964769721 train acc 0.9708116928691746\n",
            "epoch 28 batch id 1791 loss 0.05799292027950287 train acc 0.9708089056393077\n",
            "epoch 28 batch id 1801 loss 0.04093584045767784 train acc 0.970832176568573\n",
            "epoch 28 batch id 1811 loss 0.04812982305884361 train acc 0.9708379348426284\n",
            "epoch 28 batch id 1821 loss 0.09939010441303253 train acc 0.9708007276221856\n",
            "epoch 28 batch id 1831 loss 0.07462853193283081 train acc 0.9708065947569634\n",
            "epoch 28 batch id 1841 loss 0.03848687559366226 train acc 0.9707445002715915\n",
            "epoch 28 batch id 1851 loss 0.01886257529258728 train acc 0.9708181388438681\n",
            "epoch 28 batch id 1861 loss 0.10365195572376251 train acc 0.9707986297689414\n",
            "epoch 28 batch id 1871 loss 0.09027878195047379 train acc 0.970862840726884\n",
            "epoch 28 batch id 1881 loss 0.09354042261838913 train acc 0.970826687931951\n",
            "epoch 28 batch id 1891 loss 0.07718339562416077 train acc 0.9708900713907985\n",
            "epoch 28 batch id 1901 loss 0.11919490247964859 train acc 0.9709199105733824\n",
            "epoch 28 batch id 1911 loss 0.046166885644197464 train acc 0.970867673992674\n",
            "epoch 28 batch id 1921 loss 0.08237943053245544 train acc 0.9707997136907861\n",
            "epoch 28 batch id 1931 loss 0.05245746672153473 train acc 0.9707810072501295\n",
            "epoch 28 batch id 1941 loss 0.11781619489192963 train acc 0.9707463936115405\n",
            "epoch 28 batch id 1951 loss 0.06030498817563057 train acc 0.9707842132239877\n",
            "epoch 28 batch id 1961 loss 0.042698703706264496 train acc 0.9708296149923509\n",
            "epoch 28 batch id 1971 loss 0.049186140298843384 train acc 0.9708587011669203\n",
            "epoch 28 batch id 1981 loss 0.07781491428613663 train acc 0.9708007319535588\n",
            "epoch 28 batch id 1991 loss 0.17743055522441864 train acc 0.9707668884982421\n",
            "epoch 28 batch id 2001 loss 0.09372278302907944 train acc 0.9707099575212393\n",
            "epoch 28 batch id 2011 loss 0.23398596048355103 train acc 0.9706613625062158\n",
            "epoch 28 batch id 2021 loss 0.07290399074554443 train acc 0.9707137555665513\n",
            "epoch 28 batch id 2031 loss 0.07187842577695847 train acc 0.9706810068931561\n",
            "epoch 28 batch id 2041 loss 0.10605933517217636 train acc 0.970610301322881\n",
            "epoch 28 batch id 2051 loss 0.16548819839954376 train acc 0.9706164675767918\n",
            "epoch 28 batch id 2061 loss 0.025111055001616478 train acc 0.9706832241630277\n",
            "epoch 28 batch id 2071 loss 0.07844021171331406 train acc 0.9707191574118783\n",
            "epoch 28 batch id 2081 loss 0.08971061557531357 train acc 0.9707472369053339\n",
            "epoch 28 batch id 2091 loss 0.042246002703905106 train acc 0.9707825203252033\n",
            "epoch 28 batch id 2101 loss 0.01562581956386566 train acc 0.9708100309376487\n",
            "epoch 28 batch id 2111 loss 0.018639547750353813 train acc 0.9708224774988157\n",
            "epoch 28 batch id 2121 loss 0.10058137029409409 train acc 0.9708274398868458\n",
            "epoch 28 batch id 2131 loss 0.08990231901407242 train acc 0.9708690168934773\n",
            "epoch 28 batch id 2141 loss 0.1382981687784195 train acc 0.9708883115366651\n",
            "epoch 28 batch id 2151 loss 0.026408148929476738 train acc 0.9709437470943747\n",
            "epoch 28 batch id 2161 loss 0.2177145630121231 train acc 0.9709408260064785\n",
            "epoch 28 batch id 2171 loss 0.012500343844294548 train acc 0.9710027061262091\n",
            "epoch 28 batch id 2181 loss 0.2407909780740738 train acc 0.9709995414947272\n",
            "epoch 28 batch id 2191 loss 0.13688886165618896 train acc 0.9710035371976267\n",
            "epoch 28 batch id 2201 loss 0.10653183609247208 train acc 0.9710429918218991\n",
            "epoch 28 batch id 2211 loss 0.025399263948202133 train acc 0.9710396879240163\n",
            "epoch 28 batch id 2221 loss 0.09282192587852478 train acc 0.9710223435389465\n",
            "epoch 28 batch id 2231 loss 0.022375907748937607 train acc 0.9710471761541909\n",
            "epoch 28 batch id 2241 loss 0.1779450625181198 train acc 0.9710369254796966\n",
            "epoch 28 batch id 2251 loss 0.006304178386926651 train acc 0.9709781763660595\n",
            "epoch 28 batch id 2261 loss 0.009986666962504387 train acc 0.9709959641751438\n",
            "epoch 28 batch id 2271 loss 0.09822742640972137 train acc 0.9710067151034787\n",
            "epoch 28 batch id 2281 loss 0.04485171660780907 train acc 0.9710310718982902\n",
            "epoch 28 batch id 2291 loss 0.03037591278553009 train acc 0.9710756765604539\n",
            "epoch 28 batch id 2301 loss 0.057029400020837784 train acc 0.9711063124728379\n",
            "epoch 28 batch id 2311 loss 0.11240670084953308 train acc 0.9711502055387278\n",
            "epoch 28 batch id 2321 loss 0.19371558725833893 train acc 0.9711264002585093\n",
            "epoch 28 batch id 2331 loss 0.13658645749092102 train acc 0.9710759867009867\n",
            "epoch 28 batch id 2341 loss 0.15864752233028412 train acc 0.9710260038445109\n",
            "epoch 28 batch id 2351 loss 0.09027186781167984 train acc 0.971009676733305\n",
            "epoch 28 batch id 2361 loss 0.14006450772285461 train acc 0.9710067238458281\n",
            "epoch 28 batch id 2371 loss 0.004856818355619907 train acc 0.9710499261914803\n",
            "epoch 28 batch id 2381 loss 0.01604508049786091 train acc 0.9711058903821923\n",
            "epoch 28 batch id 2391 loss 0.043663814663887024 train acc 0.9710764324550397\n",
            "epoch 28 batch id 2401 loss 0.1050184965133667 train acc 0.9710407122032486\n",
            "epoch 28 batch id 2411 loss 0.0445476770401001 train acc 0.9710376918291166\n",
            "epoch 28 batch id 2421 loss 0.015106127597391605 train acc 0.9710734200743495\n",
            "epoch 28 batch id 2431 loss 0.06678256392478943 train acc 0.9711152817770465\n",
            "epoch 28 batch id 2441 loss 0.046326227486133575 train acc 0.9711311962310528\n",
            "epoch 28 batch id 2451 loss 0.013796106912195683 train acc 0.9711278559771522\n",
            "epoch 28 batch id 2461 loss 0.10520996153354645 train acc 0.9711245428687525\n",
            "epoch 28 batch id 2471 loss 0.15190812945365906 train acc 0.9710833164710644\n",
            "epoch 28 batch id 2481 loss 0.10397858172655106 train acc 0.9711368903667876\n",
            "epoch 28 batch id 2491 loss 0.11228148639202118 train acc 0.9711273083099157\n",
            "epoch 28 batch id 2501 loss 0.005397044587880373 train acc 0.9711490403838464\n",
            "epoch 28 batch id 2511 loss 0.021255476400256157 train acc 0.9711954898446834\n",
            "epoch 28 batch id 2521 loss 0.12946133315563202 train acc 0.9711362058706863\n",
            "epoch 28 batch id 2531 loss 0.02574307844042778 train acc 0.9711391248518372\n",
            "epoch 28 batch id 2541 loss 0.031797971576452255 train acc 0.9711358717040536\n",
            "epoch 28 batch id 2551 loss 0.040682777762413025 train acc 0.971157144257154\n",
            "epoch 28 batch id 2561 loss 0.025735830888152122 train acc 0.9711599472862164\n",
            "epoch 28 batch id 2571 loss 0.11484900861978531 train acc 0.9711444963049397\n",
            "epoch 28 batch id 2581 loss 0.06466032564640045 train acc 0.9711231111972104\n",
            "epoch 28 batch id 2591 loss 0.02719268947839737 train acc 0.9711561655731378\n",
            "epoch 28 batch id 2601 loss 0.15687806904315948 train acc 0.9711589292579778\n",
            "epoch 28 batch id 2611 loss 0.07023097574710846 train acc 0.9711676560704711\n",
            "epoch 28 batch id 2621 loss 0.11459245532751083 train acc 0.9711703548264021\n",
            "epoch 28 batch id 2631 loss 0.13114266097545624 train acc 0.971149277841125\n",
            "epoch 28 batch id 2641 loss 0.052438680082559586 train acc 0.971134276789095\n",
            "epoch 28 batch id 2651 loss 0.1276053488254547 train acc 0.9711252829121086\n",
            "epoch 28 batch id 2661 loss 0.08082041144371033 train acc 0.9711281003382187\n",
            "epoch 28 batch id 2671 loss 0.015253104269504547 train acc 0.9711484462748035\n",
            "epoch 28 batch id 2681 loss 0.03142736852169037 train acc 0.9711220160387914\n",
            "epoch 28 batch id 2691 loss 0.05006486549973488 train acc 0.9711190078037905\n",
            "epoch 28 batch id 2701 loss 0.08769001066684723 train acc 0.9711218067382451\n",
            "epoch 28 batch id 2711 loss 0.012519912794232368 train acc 0.9711245850239764\n",
            "epoch 28 batch id 2721 loss 0.014660140499472618 train acc 0.9711273428886439\n",
            "epoch 28 batch id 2731 loss 0.02516520768404007 train acc 0.9711186378615891\n",
            "epoch 28 batch id 2741 loss 0.12115879356861115 train acc 0.9710529916089019\n",
            "epoch 28 batch id 2751 loss 0.0904981940984726 train acc 0.9710048618684115\n",
            "epoch 28 batch id 2761 loss 0.011226042173802853 train acc 0.9710080134009417\n",
            "epoch 28 batch id 2771 loss 0.04182210564613342 train acc 0.9709998646697943\n",
            "epoch 28 batch id 2781 loss 0.07455330342054367 train acc 0.9710311039194535\n",
            "epoch 28 batch id 2791 loss 0.11558468639850616 train acc 0.9710285292010032\n",
            "epoch 28 batch id 2801 loss 0.16787812113761902 train acc 0.9710371295965726\n",
            "epoch 28 batch id 2811 loss 0.03328672796487808 train acc 0.9710345517609391\n",
            "epoch 28 batch id 2821 loss 0.14481908082962036 train acc 0.9710319922013471\n",
            "epoch 28 batch id 2831 loss 0.0782458707690239 train acc 0.9710184122218297\n",
            "epoch 28 batch id 2841 loss 0.12236670404672623 train acc 0.9710434266103485\n",
            "epoch 28 batch id 2851 loss 0.01895749755203724 train acc 0.9710682655208699\n",
            "epoch 28 batch id 2861 loss 0.0642145648598671 train acc 0.9710656239077246\n",
            "epoch 28 batch id 2871 loss 0.12190006673336029 train acc 0.9710847701149425\n",
            "epoch 28 batch id 2881 loss 0.07833400368690491 train acc 0.9710875130163138\n",
            "epoch 28 batch id 2891 loss 0.01202690601348877 train acc 0.9711388792805258\n",
            "epoch 28 batch id 2901 loss 0.021500326693058014 train acc 0.9711575749741469\n",
            "epoch 28 batch id 2911 loss 0.023876268416643143 train acc 0.9711761422191687\n",
            "epoch 28 batch id 2921 loss 0.005204925779253244 train acc 0.9711464395754879\n",
            "epoch 28 batch id 2931 loss 0.028115712106227875 train acc 0.971138263391334\n",
            "epoch 28 batch id 2941 loss 0.004578457213938236 train acc 0.971151394083645\n",
            "epoch 28 batch id 2951 loss 0.05670403689146042 train acc 0.9711697305997967\n",
            "epoch 28 batch id 2961 loss 0.06866534799337387 train acc 0.9711984971293482\n",
            "epoch 28 batch id 2971 loss 0.09565450251102448 train acc 0.9712218108381017\n",
            "epoch 28 batch id 2981 loss 0.02338135987520218 train acc 0.9711715867158671\n",
            "epoch 28 batch id 2991 loss 0.1294722706079483 train acc 0.9711687144767637\n",
            "epoch 28 batch id 3001 loss 0.0052719926461577415 train acc 0.9711658613795402\n",
            "epoch 28 batch id 3011 loss 0.22730356454849243 train acc 0.9711059448688143\n",
            "epoch 28 batch id 3021 loss 0.15415094792842865 train acc 0.9711136626944721\n",
            "epoch 28 batch id 3031 loss 0.15780368447303772 train acc 0.9711264846585286\n",
            "epoch 28 batch id 3041 loss 0.08309128135442734 train acc 0.9711186698454456\n",
            "epoch 28 batch id 3051 loss 0.12631887197494507 train acc 0.9711057849885284\n",
            "epoch 28 batch id 3061 loss 0.08708837628364563 train acc 0.97109298431885\n",
            "epoch 28 batch id 3071 loss 0.0040852464735507965 train acc 0.9710700911755129\n",
            "epoch 28 batch id 3081 loss 0.11630620807409286 train acc 0.9710574894514767\n",
            "epoch 28 batch id 3091 loss 0.04347292706370354 train acc 0.9710399142672275\n",
            "epoch 28 batch id 3101 loss 0.0769088864326477 train acc 0.971017413737504\n",
            "epoch 28 batch id 3111 loss 0.06311607360839844 train acc 0.97101012536162\n",
            "epoch 28 batch id 3121 loss 0.07132890820503235 train acc 0.9710229093239346\n",
            "epoch 28 batch id 3131 loss 0.03473538160324097 train acc 0.9710256307888854\n",
            "epoch 28 batch id 3141 loss 0.12968562543392181 train acc 0.9710432585163961\n",
            "epoch 28 batch id 3151 loss 0.038350436836481094 train acc 0.9710607743573468\n",
            "epoch 28 batch id 3161 loss 0.09102459996938705 train acc 0.9710485210376463\n",
            "epoch 28 batch id 3171 loss 0.024516865611076355 train acc 0.9710708372753075\n",
            "epoch 28 batch id 3181 loss 0.06372387707233429 train acc 0.9710537173844703\n",
            "epoch 28 batch id 3191 loss 0.02054973505437374 train acc 0.9710905672203071\n",
            "epoch 28 batch id 3201 loss 0.08316734433174133 train acc 0.9710832552327398\n",
            "epoch 28 batch id 3211 loss 0.17095109820365906 train acc 0.9710711227032077\n",
            "epoch 28 batch id 3221 loss 0.19654147326946259 train acc 0.9710445125737349\n",
            "epoch 28 batch id 3231 loss 0.07736191153526306 train acc 0.9710180671618693\n",
            "epoch 28 batch id 3241 loss 0.04600166156888008 train acc 0.9710351743289108\n",
            "epoch 28 batch id 3251 loss 0.10376318544149399 train acc 0.9710233389726238\n",
            "epoch 28 batch id 3261 loss 0.09524746239185333 train acc 0.9709828273535726\n",
            "epoch 28 batch id 3271 loss 0.08036275207996368 train acc 0.9709521170895751\n",
            "epoch 28 batch id 3281 loss 0.05690845102071762 train acc 0.9709501676318195\n",
            "epoch 28 batch id 3291 loss 0.008011164143681526 train acc 0.9709482300212702\n",
            "epoch 28 batch id 3301 loss 0.12877240777015686 train acc 0.9709605043926083\n",
            "epoch 28 batch id 3311 loss 0.06738526374101639 train acc 0.9709585472666868\n",
            "epoch 28 batch id 3321 loss 0.043200455605983734 train acc 0.970994241192412\n",
            "epoch 28 batch id 3331 loss 0.19295914471149445 train acc 0.9709546682677874\n",
            "epoch 28 batch id 3341 loss 0.25142666697502136 train acc 0.9709667764142472\n",
            "epoch 28 batch id 3351 loss 0.18514752388000488 train acc 0.9709368472097881\n",
            "epoch 28 batch id 3361 loss 0.08231751620769501 train acc 0.9709489363284737\n",
            "epoch 28 batch id 3371 loss 0.1169549822807312 train acc 0.9709285078611688\n",
            "epoch 28 batch id 3381 loss 0.0033046251628547907 train acc 0.9709359287193138\n",
            "epoch 28 batch id 3391 loss 0.0746229961514473 train acc 0.9709479135948098\n",
            "epoch 28 batch id 3401 loss 0.09832670539617538 train acc 0.9709690164657454\n",
            "epoch 28 batch id 3411 loss 0.08969704806804657 train acc 0.9709945763705659\n",
            "epoch 28 batch id 3421 loss 0.0887523740530014 train acc 0.9710291216018708\n",
            "epoch 28 batch id 3431 loss 0.1906319260597229 train acc 0.9710224788691344\n",
            "epoch 28 batch id 3441 loss 0.14382684230804443 train acc 0.9710385789014822\n",
            "epoch 28 batch id 3451 loss 0.10142384469509125 train acc 0.9710002535496958\n",
            "epoch 28 batch id 3461 loss 0.0574483722448349 train acc 0.970998266396995\n",
            "epoch 28 batch id 3471 loss 0.05280165746808052 train acc 0.9709827859406511\n",
            "epoch 28 batch id 3481 loss 0.07935746014118195 train acc 0.9709539284688308\n",
            "epoch 28 batch id 3491 loss 0.020630424842238426 train acc 0.970956566886279\n",
            "epoch 28 batch id 3501 loss 0.10341885685920715 train acc 0.9709815052842045\n",
            "epoch 28 batch id 3511 loss 0.1163080707192421 train acc 0.9709484477356879\n",
            "epoch 28 batch id 3521 loss 0.11129943281412125 train acc 0.9709777051973871\n",
            "epoch 28 batch id 3531 loss 0.19340473413467407 train acc 0.9709979467572926\n",
            "epoch 28 batch id 3541 loss 0.08097571134567261 train acc 0.9709783606325897\n",
            "epoch 28 batch id 3551 loss 0.07312119007110596 train acc 0.9709456843142776\n",
            "epoch 28 batch id 3561 loss 0.013198159635066986 train acc 0.9709790087054199\n",
            "epoch 28 batch id 3571 loss 0.07607077807188034 train acc 0.9709815177821338\n",
            "epoch 28 batch id 3581 loss 0.11753547191619873 train acc 0.9710058293772689\n",
            "epoch 28 batch id 3591 loss 0.07517694681882858 train acc 0.9710300055694793\n",
            "epoch 28 batch id 3601 loss 0.09125621616840363 train acc 0.9710497084143294\n",
            "epoch 28 batch id 3611 loss 0.06215400993824005 train acc 0.9710779562448075\n",
            "epoch 28 batch id 3621 loss 0.04799751564860344 train acc 0.971067212096106\n",
            "epoch 28 batch id 3631 loss 0.04082730412483215 train acc 0.9710694367942716\n",
            "epoch 28 batch id 3641 loss 0.03744680806994438 train acc 0.9711145633067838\n",
            "epoch 28 batch id 3651 loss 0.07836855947971344 train acc 0.9710952478772938\n",
            "epoch 28 batch id 3661 loss 0.051486484706401825 train acc 0.9711101816443595\n",
            "epoch 28 batch id 3671 loss 0.06146279722452164 train acc 0.9711165213838191\n",
            "epoch 28 batch id 3681 loss 0.0736163854598999 train acc 0.9711440505297474\n",
            "epoch 28 batch id 3691 loss 0.015479340218007565 train acc 0.9711502641560553\n",
            "epoch 28 batch id 3701 loss 0.029530467465519905 train acc 0.9711311132126452\n",
            "epoch 28 batch id 3711 loss 0.11771012842655182 train acc 0.9711457491242252\n",
            "epoch 28 batch id 3721 loss 0.10436923801898956 train acc 0.9711309123891427\n",
            "epoch 28 batch id 3731 loss 0.034932542592287064 train acc 0.9711370946127044\n",
            "epoch 28 batch id 3741 loss 0.05092358961701393 train acc 0.9711307137129109\n",
            "epoch 28 batch id 3751 loss 0.21614040434360504 train acc 0.9711035390562517\n",
            "epoch 28 batch id 3761 loss 0.08366703987121582 train acc 0.971080663387397\n",
            "epoch 28 batch id 3771 loss 0.020573485642671585 train acc 0.9710620525059666\n",
            "epoch 28 batch id 3781 loss 0.1738819181919098 train acc 0.9710435400687649\n",
            "epoch 28 batch id 3791 loss 0.0006507853977382183 train acc 0.9710539765233448\n",
            "epoch 28 batch id 3801 loss 0.048535704612731934 train acc 0.9710520257826888\n",
            "epoch 28 batch id 3811 loss 0.013813522644340992 train acc 0.9710664851744949\n",
            "epoch 28 batch id 3821 loss 0.13586066663265228 train acc 0.971076779638838\n",
            "epoch 28 batch id 3831 loss 0.02512747049331665 train acc 0.971074784651527\n",
            "epoch 28 batch id 3841 loss 0.0009943584445863962 train acc 0.9710483923457432\n",
            "epoch 28 batch id 3851 loss 0.010921109467744827 train acc 0.9710343092703194\n",
            "epoch 28 batch id 3861 loss 0.051452845335006714 train acc 0.9710041116291116\n",
            "epoch 28 batch id 3871 loss 0.11952836066484451 train acc 0.9709700335830534\n",
            "epoch 28 batch id 3881 loss 0.1414605975151062 train acc 0.9709683393455295\n",
            "epoch 28 batch id 3891 loss 0.012755845673382282 train acc 0.970978700848111\n",
            "epoch 28 batch id 3901 loss 0.026133593171834946 train acc 0.9709649769289925\n",
            "epoch 28 batch id 3911 loss 0.05130092799663544 train acc 0.9709313474814626\n",
            "epoch 28 batch id 3921 loss 0.07227328419685364 train acc 0.9708939046161693\n",
            "epoch 28 batch id 3931 loss 0.08601461350917816 train acc 0.9708685766980412\n",
            "epoch 28 batch id 3941 loss 0.09050905704498291 train acc 0.9708869893428064\n",
            "epoch 28 batch id 3951 loss 0.04645261541008949 train acc 0.9708815806125032\n",
            "epoch 28 batch id 3961 loss 0.05634899064898491 train acc 0.9708959227467812\n",
            "epoch 28 batch id 3971 loss 0.056755706667900085 train acc 0.9708826492067489\n",
            "epoch 28 batch id 3981 loss 0.022451722994446754 train acc 0.9708615925646822\n",
            "epoch 28 batch id 3991 loss 0.13573211431503296 train acc 0.9708641317965422\n",
            "epoch 28 batch id 4001 loss 0.07861804217100143 train acc 0.9708666583354162\n",
            "epoch 28 batch id 4011 loss 0.09239450842142105 train acc 0.9708457990526054\n",
            "epoch 28 batch id 4021 loss 0.0684371292591095 train acc 0.9708483586172594\n",
            "epoch 28 batch id 4031 loss 0.030358320102095604 train acc 0.9708702865293972\n",
            "epoch 28 batch id 4041 loss 0.05980827659368515 train acc 0.9708921059143776\n",
            "epoch 28 batch id 4051 loss 0.030413322150707245 train acc 0.9708983892865959\n",
            "epoch 28 batch id 4061 loss 0.13151133060455322 train acc 0.9708969465648855\n",
            "epoch 28 batch id 4071 loss 0.084018275141716 train acc 0.9709108634242201\n",
            "epoch 28 batch id 4081 loss 0.0687042698264122 train acc 0.9709438556726293\n",
            "epoch 28 batch id 4091 loss 0.04530393332242966 train acc 0.9709117575164996\n",
            "epoch 28 batch id 4101 loss 0.08524533361196518 train acc 0.9709141063155328\n",
            "epoch 28 batch id 4111 loss 0.016624409705400467 train acc 0.9708974397956701\n",
            "epoch 28 batch id 4121 loss 0.03510074317455292 train acc 0.9708732710507159\n",
            "epoch 28 batch id 4131 loss 0.1403336524963379 train acc 0.9708756959573953\n",
            "epoch 28 batch id 4141 loss 0.1928805410861969 train acc 0.9708781091523786\n",
            "epoch 28 batch id 4151 loss 0.08415455371141434 train acc 0.9708880390267406\n",
            "epoch 28 batch id 4161 loss 0.20307138562202454 train acc 0.970864125210286\n",
            "epoch 28 batch id 4171 loss 0.18710988759994507 train acc 0.9708740409973627\n",
            "epoch 28 batch id 4181 loss 0.00310110067948699 train acc 0.9708839093518297\n",
            "epoch 28 batch id 4191 loss 0.13376617431640625 train acc 0.9708788177046052\n",
            "epoch 28 batch id 4201 loss 0.11703192442655563 train acc 0.9708886277076887\n",
            "epoch 28 batch id 4211 loss 0.056209418922662735 train acc 0.9708983911184992\n",
            "epoch 28 batch id 4221 loss 0.09767750650644302 train acc 0.9708784944325989\n",
            "epoch 28 batch id 4231 loss 0.05138709396123886 train acc 0.9708771567005436\n",
            "epoch 28 batch id 4241 loss 0.10249050706624985 train acc 0.9708942466399434\n",
            "epoch 28 batch id 4251 loss 0.2633860409259796 train acc 0.970903904963538\n",
            "epoch 28 batch id 4261 loss 0.0029535230714827776 train acc 0.9709208519126965\n",
            "epoch 28 batch id 4271 loss 0.0602540448307991 train acc 0.9709340611098104\n",
            "epoch 28 batch id 4281 loss 0.19504059851169586 train acc 0.9709326092034571\n",
            "epoch 28 batch id 4291 loss 0.016582855954766273 train acc 0.970916598694943\n",
            "epoch 28 batch id 4301 loss 0.06759774684906006 train acc 0.9709042955126714\n",
            "epoch 28 batch id 4311 loss 0.05365755036473274 train acc 0.9709282938993273\n",
            "epoch 28 batch id 4321 loss 0.21216411888599396 train acc 0.97090878847489\n",
            "epoch 28 batch id 4331 loss 0.062004655599594116 train acc 0.9708929808358346\n",
            "epoch 28 batch id 4341 loss 0.07086100429296494 train acc 0.9708772460262612\n",
            "epoch 28 batch id 4351 loss 0.04003160446882248 train acc 0.9708400367731556\n",
            "epoch 28 batch id 4361 loss 0.13419602811336517 train acc 0.9708495757853703\n",
            "epoch 28 batch id 4371 loss 0.05340512841939926 train acc 0.9708590711507664\n",
            "epoch 28 batch id 4381 loss 0.13300012052059174 train acc 0.9708506904816252\n",
            "epoch 28 batch id 4391 loss 0.0028559789061546326 train acc 0.970849464814393\n",
            "epoch 28 batch id 4401 loss 0.03643092140555382 train acc 0.9708588957055214\n",
            "epoch 28 batch id 4411 loss 0.01591077633202076 train acc 0.970861199274541\n",
            "epoch 28 batch id 4421 loss 0.022785348817706108 train acc 0.9708634924225288\n",
            "epoch 28 batch id 4431 loss 0.04628589749336243 train acc 0.970851670051907\n",
            "epoch 28 batch id 4441 loss 0.06104356423020363 train acc 0.9708504559783833\n",
            "epoch 28 batch id 4451 loss 0.12758955359458923 train acc 0.970831695124691\n",
            "epoch 28 batch id 4461 loss 0.03910825401544571 train acc 0.9708305312710155\n",
            "epoch 28 batch id 4471 loss 0.11121980845928192 train acc 0.9708118989040483\n",
            "epoch 28 batch id 4481 loss 0.016412951052188873 train acc 0.9708107844231199\n",
            "epoch 28 batch id 4491 loss 0.06223467364907265 train acc 0.9707957581830328\n",
            "epoch 28 batch id 4501 loss 0.1176222488284111 train acc 0.9707946845145523\n",
            "epoch 28 batch id 4511 loss 0.2422504425048828 train acc 0.9707659055641764\n",
            "epoch 28 batch id 4521 loss 0.06796484440565109 train acc 0.9707614465826144\n",
            "epoch 28 batch id 4531 loss 0.0017871734453365207 train acc 0.9707535588170382\n",
            "epoch 28 batch id 4541 loss 0.10207615047693253 train acc 0.9707560284078397\n",
            "epoch 28 batch id 4551 loss 0.15618769824504852 train acc 0.9707481872116018\n",
            "epoch 28 batch id 4561 loss 0.11343500763177872 train acc 0.9707266772637579\n",
            "epoch 28 batch id 4571 loss 0.029018376022577286 train acc 0.9707394443229053\n",
            "epoch 28 batch id 4581 loss 0.03280556574463844 train acc 0.9707385123335516\n",
            "epoch 28 batch id 4591 loss 0.1027812734246254 train acc 0.9707409878022217\n",
            "epoch 28 batch id 4601 loss 0.13009460270404816 train acc 0.9707400565094545\n",
            "epoch 28 batch id 4611 loss 0.11627324670553207 train acc 0.9707492951637389\n",
            "epoch 28 batch id 4621 loss 0.095009446144104 train acc 0.9707517312270071\n",
            "epoch 28 batch id 4631 loss 0.076530821621418 train acc 0.9707541567695962\n",
            "epoch 28 batch id 4641 loss 0.10848665982484818 train acc 0.9707633053221288\n",
            "epoch 28 batch id 4651 loss 0.30152544379234314 train acc 0.9707589765641798\n",
            "epoch 28 batch id 4661 loss 0.09861205518245697 train acc 0.970771427805192\n",
            "epoch 28 batch id 4671 loss 0.14328138530254364 train acc 0.9707804806251338\n",
            "epoch 28 batch id 4681 loss 0.3303039073944092 train acc 0.970772804956206\n",
            "epoch 28 batch id 4691 loss 0.020778678357601166 train acc 0.9707618311660626\n",
            "epoch 28 batch id 4701 loss 0.05488342419266701 train acc 0.9707774941501808\n",
            "epoch 28 batch id 4711 loss 0.03474896401166916 train acc 0.9707765071110168\n",
            "epoch 28 batch id 4721 loss 0.035321835428476334 train acc 0.9707655952128786\n",
            "epoch 28 batch id 4731 loss 0.1457008719444275 train acc 0.9707745455506236\n",
            "epoch 28 batch id 4741 loss 0.18021944165229797 train acc 0.9707669795401814\n",
            "epoch 28 batch id 4751 loss 0.19661086797714233 train acc 0.9707528678173016\n",
            "epoch 28 batch id 4761 loss 0.09372519701719284 train acc 0.9707552247427012\n",
            "epoch 28 batch id 4771 loss 0.08169042319059372 train acc 0.9707673967721652\n",
            "epoch 28 batch id 4781 loss 0.15696927905082703 train acc 0.9707435682911525\n",
            "epoch 28 batch id 4791 loss 0.18094003200531006 train acc 0.970713316635358\n",
            "epoch 28 batch id 4801 loss 0.08949729055166245 train acc 0.9706994636534055\n",
            "epoch 28 batch id 4811 loss 0.018192972987890244 train acc 0.97071814591561\n",
            "epoch 28 batch id 4821 loss 0.13423438370227814 train acc 0.970720545529973\n",
            "epoch 28 batch id 4831 loss 0.02614324539899826 train acc 0.9707391068101843\n",
            "epoch 28 batch id 4841 loss 0.12142936885356903 train acc 0.9707349979343111\n",
            "epoch 28 batch id 4851 loss 0.10110155493021011 train acc 0.9707405689548547\n",
            "epoch 28 batch id 4861 loss 0.02117103897035122 train acc 0.9707493314132895\n",
            "epoch 28 batch id 4871 loss 0.07171888649463654 train acc 0.9707484346130159\n",
            "epoch 28 batch id 4881 loss 0.07246264815330505 train acc 0.970779553370211\n",
            "epoch 28 batch id 4891 loss 0.020080888643860817 train acc 0.9707658198732365\n",
            "epoch 28 batch id 4901 loss 0.021804487332701683 train acc 0.9707712711691492\n",
            "epoch 28 batch id 4911 loss 0.2095438838005066 train acc 0.9707576104663002\n",
            "epoch 28 batch id 4921 loss 0.23551617562770844 train acc 0.9707408301158301\n",
            "epoch 28 batch id 4931 loss 0.09687099605798721 train acc 0.970730455282904\n",
            "epoch 28 batch id 4941 loss 0.04838995262980461 train acc 0.9707137978142076\n",
            "epoch 28 batch id 4951 loss 0.12559986114501953 train acc 0.9707129872752979\n",
            "epoch 28 train acc 0.9707200675811983\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "092291718c924cdebf0c8882bfd6c0a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 28 loss 1.967054009437561 test acc 0.8399766312316715\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78e3c14135284eb6881f733e9d7adf6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 29 batch id 1 loss 0.07550250738859177 train acc 0.96875\n",
            "epoch 29 batch id 11 loss 0.02290601097047329 train acc 0.9801136363636364\n",
            "epoch 29 batch id 21 loss 0.026713285595178604 train acc 0.9754464285714286\n",
            "epoch 29 batch id 31 loss 0.08166692405939102 train acc 0.9737903225806451\n",
            "epoch 29 batch id 41 loss 0.085990771651268 train acc 0.975609756097561\n",
            "epoch 29 batch id 51 loss 0.014414872042834759 train acc 0.9767156862745098\n",
            "epoch 29 batch id 61 loss 0.09220323711633682 train acc 0.9736168032786885\n",
            "epoch 29 batch id 71 loss 0.1369486153125763 train acc 0.9727112676056338\n",
            "epoch 29 batch id 81 loss 0.174981489777565 train acc 0.9722222222222222\n",
            "epoch 29 batch id 91 loss 0.1428389996290207 train acc 0.9720123626373627\n",
            "epoch 29 batch id 101 loss 0.030252499505877495 train acc 0.9710705445544554\n",
            "epoch 29 batch id 111 loss 0.054520998150110245 train acc 0.9710022522522522\n",
            "epoch 29 batch id 121 loss 0.10311815142631531 train acc 0.971849173553719\n",
            "epoch 29 batch id 131 loss 0.08010341972112656 train acc 0.9717318702290076\n",
            "epoch 29 batch id 141 loss 0.019074222072958946 train acc 0.9716312056737588\n",
            "epoch 29 batch id 151 loss 0.1362457573413849 train acc 0.9717508278145696\n",
            "epoch 29 batch id 161 loss 0.03598823398351669 train acc 0.9721467391304348\n",
            "epoch 29 batch id 171 loss 0.09175105392932892 train acc 0.971765350877193\n",
            "epoch 29 batch id 181 loss 0.10793022811412811 train acc 0.972289364640884\n",
            "epoch 29 batch id 191 loss 0.07233920693397522 train acc 0.972267670157068\n",
            "epoch 29 batch id 201 loss 0.19920800626277924 train acc 0.972714552238806\n",
            "epoch 29 batch id 211 loss 0.019374925643205643 train acc 0.9730450236966824\n",
            "epoch 29 batch id 221 loss 0.09238316863775253 train acc 0.9727092760180995\n",
            "epoch 29 batch id 231 loss 0.1310386061668396 train acc 0.973146645021645\n",
            "epoch 29 batch id 241 loss 0.20886355638504028 train acc 0.9729642116182573\n",
            "epoch 29 batch id 251 loss 0.2388278990983963 train acc 0.9727340637450199\n",
            "epoch 29 batch id 261 loss 0.047602828592061996 train acc 0.9730004789272031\n",
            "epoch 29 batch id 271 loss 0.0771104171872139 train acc 0.9729589483394834\n",
            "epoch 29 batch id 281 loss 0.026957811787724495 train acc 0.973142793594306\n",
            "epoch 29 batch id 291 loss 0.09010954946279526 train acc 0.9736361683848798\n",
            "epoch 29 batch id 301 loss 0.24967224895954132 train acc 0.9733700166112956\n",
            "epoch 29 batch id 311 loss 0.022577056661248207 train acc 0.9736233922829582\n",
            "epoch 29 batch id 321 loss 0.17116636037826538 train acc 0.9734715732087228\n",
            "epoch 29 batch id 331 loss 0.006253779865801334 train acc 0.9737537764350453\n",
            "epoch 29 batch id 341 loss 0.2557753920555115 train acc 0.9739736070381232\n",
            "epoch 29 batch id 351 loss 0.08686350286006927 train acc 0.9739583333333334\n",
            "epoch 29 batch id 361 loss 0.051363687962293625 train acc 0.9740737534626038\n",
            "epoch 29 batch id 371 loss 0.03949489817023277 train acc 0.9741829514824798\n",
            "epoch 29 batch id 381 loss 0.01935143768787384 train acc 0.97436843832021\n",
            "epoch 29 batch id 391 loss 0.17522522807121277 train acc 0.9744645140664961\n",
            "epoch 29 batch id 401 loss 0.01745041087269783 train acc 0.9743220074812967\n",
            "epoch 29 batch id 411 loss 0.11678920686244965 train acc 0.9744145377128953\n",
            "epoch 29 batch id 421 loss 0.08694436401128769 train acc 0.9744655581947743\n",
            "epoch 29 batch id 431 loss 0.05920000001788139 train acc 0.9741879350348028\n",
            "epoch 29 batch id 441 loss 0.19230367243289948 train acc 0.9738520408163265\n",
            "epoch 29 batch id 451 loss 0.01889895275235176 train acc 0.9739121396895787\n",
            "epoch 29 batch id 461 loss 0.006702000740915537 train acc 0.9734273318872018\n",
            "epoch 29 batch id 471 loss 0.06878387928009033 train acc 0.9735270700636943\n",
            "epoch 29 batch id 481 loss 0.10511365532875061 train acc 0.9735576923076923\n",
            "epoch 29 batch id 491 loss 0.019220056012272835 train acc 0.973587067209776\n",
            "epoch 29 batch id 501 loss 0.15325291454792023 train acc 0.9733033932135728\n",
            "epoch 29 batch id 511 loss 0.08966778218746185 train acc 0.9732142857142857\n",
            "epoch 29 batch id 521 loss 0.08873152732849121 train acc 0.9730686180422264\n",
            "epoch 29 batch id 531 loss 0.08948182314634323 train acc 0.9727813088512242\n",
            "epoch 29 batch id 541 loss 0.0258223544806242 train acc 0.9727645563770795\n",
            "epoch 29 batch id 551 loss 0.08683482557535172 train acc 0.9727484119782214\n",
            "epoch 29 batch id 561 loss 0.1218840628862381 train acc 0.9727885472370766\n",
            "epoch 29 batch id 571 loss 0.07348138093948364 train acc 0.9728820052539404\n",
            "epoch 29 batch id 581 loss 0.08131740987300873 train acc 0.9728377796901894\n",
            "epoch 29 batch id 591 loss 0.02912776917219162 train acc 0.9729272419627749\n",
            "epoch 29 batch id 601 loss 0.050720490515232086 train acc 0.9729877287853578\n",
            "epoch 29 batch id 611 loss 0.08792665600776672 train acc 0.9727905073649754\n",
            "epoch 29 batch id 621 loss 0.3457338213920593 train acc 0.9728512479871175\n",
            "epoch 29 batch id 631 loss 0.054294586181640625 train acc 0.972959587955626\n",
            "epoch 29 batch id 641 loss 0.01381533034145832 train acc 0.9730401716068643\n",
            "epoch 29 batch id 651 loss 0.1253894567489624 train acc 0.972926267281106\n",
            "epoch 29 batch id 661 loss 0.055363476276397705 train acc 0.9730758320726173\n",
            "epoch 29 batch id 671 loss 0.13975319266319275 train acc 0.9729880774962743\n",
            "epoch 29 batch id 681 loss 0.07711321115493774 train acc 0.9729946769456681\n",
            "epoch 29 batch id 691 loss 0.2556263208389282 train acc 0.9728654124457308\n",
            "epoch 29 batch id 701 loss 0.05350480228662491 train acc 0.9727844151212554\n",
            "epoch 29 batch id 711 loss 0.1422528624534607 train acc 0.9725738396624473\n",
            "epoch 29 batch id 721 loss 0.05921727791428566 train acc 0.9723474341192788\n",
            "epoch 29 batch id 731 loss 0.02728915587067604 train acc 0.9723837209302325\n",
            "epoch 29 batch id 741 loss 0.03540090098977089 train acc 0.9724612010796221\n",
            "epoch 29 batch id 751 loss 0.05043119937181473 train acc 0.9723493675099867\n",
            "epoch 29 batch id 761 loss 0.05827299505472183 train acc 0.9722815374507228\n",
            "epoch 29 batch id 771 loss 0.14581996202468872 train acc 0.9722965304798963\n",
            "epoch 29 batch id 781 loss 0.061757076531648636 train acc 0.9722311139564661\n",
            "epoch 29 batch id 791 loss 0.09129935503005981 train acc 0.9722858723135271\n",
            "epoch 29 batch id 801 loss 0.04327220097184181 train acc 0.9723002496878901\n",
            "epoch 29 batch id 811 loss 0.10317578911781311 train acc 0.9724106041923551\n",
            "epoch 29 batch id 821 loss 0.016305387020111084 train acc 0.9724802070645554\n",
            "epoch 29 batch id 831 loss 0.20627357065677643 train acc 0.9724165162454874\n",
            "epoch 29 batch id 841 loss 0.008683660067617893 train acc 0.9724100772889417\n",
            "epoch 29 batch id 851 loss 0.1321113407611847 train acc 0.972293625146886\n",
            "epoch 29 batch id 861 loss 0.05728478729724884 train acc 0.9723976480836237\n",
            "epoch 29 batch id 871 loss 0.042924169450998306 train acc 0.9724634041331802\n",
            "epoch 29 batch id 881 loss 0.12756523489952087 train acc 0.9724035187287173\n",
            "epoch 29 batch id 891 loss 0.13378114998340607 train acc 0.9724151234567902\n",
            "epoch 29 batch id 901 loss 0.051591865718364716 train acc 0.9723917869034406\n",
            "epoch 29 batch id 911 loss 0.093899205327034 train acc 0.9724032656421515\n",
            "epoch 29 batch id 921 loss 0.02644486352801323 train acc 0.9723805646036916\n",
            "epoch 29 batch id 931 loss 0.007963914424180984 train acc 0.9724087003222341\n",
            "epoch 29 batch id 941 loss 0.01619681529700756 train acc 0.9724860520722636\n",
            "epoch 29 batch id 951 loss 0.13410720229148865 train acc 0.9723317560462671\n",
            "epoch 29 batch id 961 loss 0.08537853509187698 train acc 0.9722619667013528\n",
            "epoch 29 batch id 971 loss 0.15723064541816711 train acc 0.972225798146241\n",
            "epoch 29 batch id 981 loss 0.11140795797109604 train acc 0.9722540774719673\n",
            "epoch 29 batch id 991 loss 0.19759449362754822 train acc 0.972266019172553\n",
            "epoch 29 batch id 1001 loss 0.08124010264873505 train acc 0.9722933316683317\n",
            "epoch 29 batch id 1011 loss 0.1671646386384964 train acc 0.9722428288822947\n",
            "epoch 29 batch id 1021 loss 0.1064620316028595 train acc 0.9723310479921645\n",
            "epoch 29 batch id 1031 loss 0.00756194768473506 train acc 0.9723872453928225\n",
            "epoch 29 batch id 1041 loss 0.021864864975214005 train acc 0.972412343900096\n",
            "epoch 29 batch id 1051 loss 0.01720440573990345 train acc 0.9722882968601332\n",
            "epoch 29 batch id 1061 loss 0.015222219750285149 train acc 0.9722254948162111\n",
            "epoch 29 batch id 1071 loss 0.0063758487813174725 train acc 0.9722222222222222\n",
            "epoch 29 batch id 1081 loss 0.05666031688451767 train acc 0.9721901017576319\n",
            "epoch 29 batch id 1091 loss 0.07906273007392883 train acc 0.9722301787351054\n",
            "epoch 29 batch id 1101 loss 0.1043597012758255 train acc 0.972255336058129\n",
            "epoch 29 batch id 1111 loss 0.0897248238325119 train acc 0.9722659765976598\n",
            "epoch 29 batch id 1121 loss 0.021252868697047234 train acc 0.9723600579839429\n",
            "epoch 29 batch id 1131 loss 0.034486234188079834 train acc 0.9723143236074271\n",
            "epoch 29 batch id 1141 loss 0.08852512389421463 train acc 0.9722967791411042\n",
            "epoch 29 batch id 1151 loss 0.0711197480559349 train acc 0.9721980886185926\n",
            "epoch 29 batch id 1161 loss 0.01951269619166851 train acc 0.9722625968992248\n",
            "epoch 29 batch id 1171 loss 0.04484504833817482 train acc 0.9722726302305722\n",
            "epoch 29 batch id 1181 loss 0.008660445921123028 train acc 0.9723221845893311\n",
            "epoch 29 batch id 1191 loss 0.014218367636203766 train acc 0.9723577875734677\n",
            "epoch 29 batch id 1201 loss 0.14776332676410675 train acc 0.9724188176519567\n",
            "epoch 29 batch id 1211 loss 0.03564776852726936 train acc 0.972440132122213\n",
            "epoch 29 batch id 1221 loss 0.08821268379688263 train acc 0.9724866912366913\n",
            "epoch 29 batch id 1231 loss 0.05854819715023041 train acc 0.972405564581641\n",
            "epoch 29 batch id 1241 loss 0.023632999509572983 train acc 0.9724012892828364\n",
            "epoch 29 batch id 1251 loss 0.1029537096619606 train acc 0.972384592326139\n",
            "epoch 29 batch id 1261 loss 0.06738925725221634 train acc 0.9723805511498811\n",
            "epoch 29 batch id 1271 loss 0.06095387414097786 train acc 0.9724380409126672\n",
            "epoch 29 batch id 1281 loss 0.0019869531970471144 train acc 0.9725434231069477\n",
            "epoch 29 batch id 1291 loss 0.10283088684082031 train acc 0.9725382455460883\n",
            "epoch 29 batch id 1301 loss 0.011112161912024021 train acc 0.9724971176018448\n",
            "epoch 29 batch id 1311 loss 0.08804946392774582 train acc 0.9724923722349351\n",
            "epoch 29 batch id 1321 loss 0.13374102115631104 train acc 0.9723930734292203\n",
            "epoch 29 batch id 1331 loss 0.11891735345125198 train acc 0.9723187453042825\n",
            "epoch 29 batch id 1341 loss 0.14401809871196747 train acc 0.9722921327367636\n",
            "epoch 29 batch id 1351 loss 0.0808684229850769 train acc 0.972323741672835\n",
            "epoch 29 batch id 1361 loss 0.03705950081348419 train acc 0.9721826781778105\n",
            "epoch 29 batch id 1371 loss 0.05593913048505783 train acc 0.9721690371991247\n",
            "epoch 29 batch id 1381 loss 0.07878554612398148 train acc 0.9720763939174512\n",
            "epoch 29 batch id 1391 loss 0.12266431748867035 train acc 0.9720075485262402\n",
            "epoch 29 batch id 1401 loss 0.013922839425504208 train acc 0.9719954496788008\n",
            "epoch 29 batch id 1411 loss 0.08638063073158264 train acc 0.9719724486180015\n",
            "epoch 29 batch id 1421 loss 0.011947471648454666 train acc 0.9720047501759325\n",
            "epoch 29 batch id 1431 loss 0.037740807980298996 train acc 0.9720584381551363\n",
            "epoch 29 batch id 1441 loss 0.031580790877342224 train acc 0.9719921061762665\n",
            "epoch 29 batch id 1451 loss 0.026897527277469635 train acc 0.9720343728463129\n",
            "epoch 29 batch id 1461 loss 0.09910490363836288 train acc 0.9720011978097194\n",
            "epoch 29 batch id 1471 loss 0.06174471601843834 train acc 0.9720959381373215\n",
            "epoch 29 batch id 1481 loss 0.04719052091240883 train acc 0.9720838960162053\n",
            "epoch 29 batch id 1491 loss 0.13946786522865295 train acc 0.9720824949698189\n",
            "epoch 29 batch id 1501 loss 0.06735663115978241 train acc 0.9721019320453032\n",
            "epoch 29 batch id 1511 loss 0.058680277317762375 train acc 0.9721728160158836\n",
            "epoch 29 batch id 1521 loss 0.017225656658411026 train acc 0.9721605851413544\n",
            "epoch 29 batch id 1531 loss 0.031208768486976624 train acc 0.9721791312867407\n",
            "epoch 29 batch id 1541 loss 0.010585951618850231 train acc 0.9721974367293965\n",
            "epoch 29 batch id 1551 loss 0.10793375223875046 train acc 0.9722054319793682\n",
            "epoch 29 batch id 1561 loss 0.050894174724817276 train acc 0.9721933055733504\n",
            "epoch 29 batch id 1571 loss 0.0125525938346982 train acc 0.9722111712285169\n",
            "epoch 29 batch id 1581 loss 0.11662032455205917 train acc 0.9722288108791903\n",
            "epoch 29 batch id 1591 loss 0.127573162317276 train acc 0.9722462287869265\n",
            "epoch 29 batch id 1601 loss 0.061889342963695526 train acc 0.9723610243597751\n",
            "epoch 29 batch id 1611 loss 0.08553633093833923 train acc 0.9723289106145251\n",
            "epoch 29 batch id 1621 loss 0.057122521102428436 train acc 0.9723550277606415\n",
            "epoch 29 batch id 1631 loss 0.04148242250084877 train acc 0.9723904046597179\n",
            "epoch 29 batch id 1641 loss 0.12014645338058472 train acc 0.9723682205971969\n",
            "epoch 29 batch id 1651 loss 0.054757196456193924 train acc 0.9723179133858267\n",
            "epoch 29 batch id 1661 loss 0.05594377592206001 train acc 0.9723434677904876\n",
            "epoch 29 batch id 1671 loss 0.06188434734940529 train acc 0.972378067025733\n",
            "epoch 29 batch id 1681 loss 0.16089899837970734 train acc 0.9723007138607972\n",
            "epoch 29 batch id 1691 loss 0.06963211297988892 train acc 0.9723166765227675\n",
            "epoch 29 batch id 1701 loss 0.09593699872493744 train acc 0.9722681510875956\n",
            "epoch 29 batch id 1711 loss 0.058736030012369156 train acc 0.9722841174751607\n",
            "epoch 29 batch id 1721 loss 0.03360533341765404 train acc 0.9722998983149331\n",
            "epoch 29 batch id 1731 loss 0.0019896295852959156 train acc 0.972333549971115\n",
            "epoch 29 batch id 1741 loss 0.09574304521083832 train acc 0.9723039919586445\n",
            "epoch 29 batch id 1751 loss 0.06140388175845146 train acc 0.9722836950314107\n",
            "epoch 29 batch id 1761 loss 0.03698941320180893 train acc 0.9722991198182851\n",
            "epoch 29 batch id 1771 loss 0.001494240015745163 train acc 0.9723143704121965\n",
            "epoch 29 batch id 1781 loss 0.03051980771124363 train acc 0.9723557692307693\n",
            "epoch 29 batch id 1791 loss 0.09304212033748627 train acc 0.9723792573981016\n",
            "epoch 29 batch id 1801 loss 0.04137369617819786 train acc 0.9723938089950028\n",
            "epoch 29 batch id 1811 loss 0.020627668127417564 train acc 0.972416827719492\n",
            "epoch 29 batch id 1821 loss 0.1353917270898819 train acc 0.9723795304777595\n",
            "epoch 29 batch id 1831 loss 0.13950979709625244 train acc 0.97235970780994\n",
            "epoch 29 batch id 1841 loss 0.004367075860500336 train acc 0.9723316132536665\n",
            "epoch 29 batch id 1851 loss 0.04242435097694397 train acc 0.9723797947055646\n",
            "epoch 29 batch id 1861 loss 0.047224320471286774 train acc 0.9723518941429339\n",
            "epoch 29 batch id 1871 loss 0.09192826598882675 train acc 0.9724161544628541\n",
            "epoch 29 batch id 1881 loss 0.0499187670648098 train acc 0.9723966640085061\n",
            "epoch 29 batch id 1891 loss 0.08215948194265366 train acc 0.9724517451084083\n",
            "epoch 29 batch id 1901 loss 0.12432096153497696 train acc 0.9724898079957917\n",
            "epoch 29 batch id 1911 loss 0.03226526826620102 train acc 0.9724865907901622\n",
            "epoch 29 batch id 1921 loss 0.28960877656936646 train acc 0.9724427381572098\n",
            "epoch 29 batch id 1931 loss 0.0923171266913414 train acc 0.9724236147074055\n",
            "epoch 29 batch id 1941 loss 0.019070448353886604 train acc 0.9723724884080371\n",
            "epoch 29 batch id 1951 loss 0.20432382822036743 train acc 0.972385955920041\n",
            "epoch 29 batch id 1961 loss 0.055013202130794525 train acc 0.972431157572667\n",
            "epoch 29 batch id 1971 loss 0.008825218304991722 train acc 0.9724759005580923\n",
            "epoch 29 batch id 1981 loss 0.12213576585054398 train acc 0.9724886420999496\n",
            "epoch 29 batch id 1991 loss 0.12353236228227615 train acc 0.9724777122049222\n",
            "epoch 29 batch id 2001 loss 0.04684950038790703 train acc 0.9725215517241379\n",
            "epoch 29 batch id 2011 loss 0.07292799651622772 train acc 0.9725183366484336\n",
            "epoch 29 batch id 2021 loss 0.07889515906572342 train acc 0.9725383473527957\n",
            "epoch 29 batch id 2031 loss 0.04786119982600212 train acc 0.972535081240768\n",
            "epoch 29 batch id 2041 loss 0.15543362498283386 train acc 0.9725088804507594\n",
            "epoch 29 batch id 2051 loss 0.16431334614753723 train acc 0.9724600804485617\n",
            "epoch 29 batch id 2061 loss 0.0508236363530159 train acc 0.9724799854439592\n",
            "epoch 29 batch id 2071 loss 0.15097838640213013 train acc 0.9724544302269436\n",
            "epoch 29 batch id 2081 loss 0.15463152527809143 train acc 0.9723990869774147\n",
            "epoch 29 batch id 2091 loss 0.0687219575047493 train acc 0.9723741630798661\n",
            "epoch 29 batch id 2101 loss 0.008976360782980919 train acc 0.9724312827225131\n",
            "epoch 29 batch id 2111 loss 0.01933917962014675 train acc 0.972421245855045\n",
            "epoch 29 batch id 2121 loss 0.07500235736370087 train acc 0.9724260372465818\n",
            "epoch 29 batch id 2131 loss 0.028195464983582497 train acc 0.972504106053496\n",
            "epoch 29 batch id 2141 loss 0.09232611954212189 train acc 0.9725522536198038\n",
            "epoch 29 batch id 2151 loss 0.20156896114349365 train acc 0.9725926894467689\n",
            "epoch 29 batch id 2161 loss 0.17560429871082306 train acc 0.9725676770013882\n",
            "epoch 29 batch id 2171 loss 0.032415516674518585 train acc 0.9726004721326578\n",
            "epoch 29 batch id 2181 loss 0.11395005136728287 train acc 0.9725828175149014\n",
            "epoch 29 batch id 2191 loss 0.03662214055657387 train acc 0.9725653240529438\n",
            "epoch 29 batch id 2201 loss 0.030631933361291885 train acc 0.9725479895502045\n",
            "epoch 29 batch id 2211 loss 0.023872030898928642 train acc 0.9725449457259159\n",
            "epoch 29 batch id 2221 loss 0.04576457664370537 train acc 0.9725137888338586\n",
            "epoch 29 batch id 2231 loss 0.014352631755173206 train acc 0.97251792917974\n",
            "epoch 29 batch id 2241 loss 0.12037356942892075 train acc 0.9725220325747435\n",
            "epoch 29 batch id 2251 loss 0.018834954127669334 train acc 0.9724636272767658\n",
            "epoch 29 batch id 2261 loss 0.0030236286111176014 train acc 0.9724679345422379\n",
            "epoch 29 batch id 2271 loss 0.016391679644584656 train acc 0.972506605019815\n",
            "epoch 29 batch id 2281 loss 0.030564825981855392 train acc 0.972503836036826\n",
            "epoch 29 batch id 2291 loss 0.027473920956254005 train acc 0.9725420122217372\n",
            "epoch 29 batch id 2301 loss 0.0217864029109478 train acc 0.9725730660582356\n",
            "epoch 29 batch id 2311 loss 0.15964971482753754 train acc 0.9726308957161403\n",
            "epoch 29 batch id 2321 loss 0.108034148812294 train acc 0.9725939788884101\n",
            "epoch 29 batch id 2331 loss 0.10209008306264877 train acc 0.9725573788073788\n",
            "epoch 29 batch id 2341 loss 0.1624876856803894 train acc 0.9725678129004699\n",
            "epoch 29 batch id 2351 loss 0.050440460443496704 train acc 0.972571512122501\n",
            "epoch 29 batch id 2361 loss 0.10567659139633179 train acc 0.9726016518424396\n",
            "epoch 29 batch id 2371 loss 0.00215561268851161 train acc 0.9726776676507802\n",
            "epoch 29 batch id 2381 loss 0.02110612764954567 train acc 0.9727005459890802\n",
            "epoch 29 batch id 2391 loss 0.05457548797130585 train acc 0.9727166980342953\n",
            "epoch 29 batch id 2401 loss 0.02454119175672531 train acc 0.9727587463556852\n",
            "epoch 29 batch id 2411 loss 0.02897173911333084 train acc 0.9727810037328909\n",
            "epoch 29 batch id 2421 loss 0.03185896947979927 train acc 0.9728159851301115\n",
            "epoch 29 batch id 2431 loss 0.11323065310716629 train acc 0.9727864047716989\n",
            "epoch 29 batch id 2441 loss 0.0030740948859602213 train acc 0.972795473166735\n",
            "epoch 29 batch id 2451 loss 0.00170817319303751 train acc 0.9727598429212566\n",
            "epoch 29 batch id 2461 loss 0.1084260642528534 train acc 0.9727562474603819\n",
            "epoch 29 batch id 2471 loss 0.1331997960805893 train acc 0.9727716511533792\n",
            "epoch 29 batch id 2481 loss 0.19290024042129517 train acc 0.9728247178557033\n",
            "epoch 29 batch id 2491 loss 0.15091325342655182 train acc 0.9728146326776395\n",
            "epoch 29 batch id 2501 loss 0.019900964573025703 train acc 0.9728483606557377\n",
            "epoch 29 batch id 2511 loss 0.1340627819299698 train acc 0.9728507068896853\n",
            "epoch 29 batch id 2521 loss 0.04435015842318535 train acc 0.9728344406981356\n",
            "epoch 29 batch id 2531 loss 0.0748530924320221 train acc 0.9728306499407349\n",
            "epoch 29 batch id 2541 loss 0.02195798046886921 train acc 0.9728391873278237\n",
            "epoch 29 batch id 2551 loss 0.0837910920381546 train acc 0.9728047824382595\n",
            "epoch 29 batch id 2561 loss 0.027418024837970734 train acc 0.9728438598203827\n",
            "epoch 29 batch id 2571 loss 0.13247281312942505 train acc 0.9728400914041229\n",
            "epoch 29 batch id 2581 loss 0.051590729504823685 train acc 0.9728000290585045\n",
            "epoch 29 batch id 2591 loss 0.03985633701086044 train acc 0.9727783674257043\n",
            "epoch 29 batch id 2601 loss 0.055830784142017365 train acc 0.972822952710496\n",
            "epoch 29 batch id 2611 loss 0.03263760358095169 train acc 0.9728612121792417\n",
            "epoch 29 batch id 2621 loss 0.1295790821313858 train acc 0.972875333842045\n",
            "epoch 29 batch id 2631 loss 0.08285786956548691 train acc 0.9728596541239073\n",
            "epoch 29 batch id 2641 loss 0.025826405733823776 train acc 0.972873674744415\n",
            "epoch 29 batch id 2651 loss 0.0017937098164111376 train acc 0.9728463315729913\n",
            "epoch 29 batch id 2661 loss 0.03645949810743332 train acc 0.9728661687335588\n",
            "epoch 29 batch id 2671 loss 0.048738256096839905 train acc 0.9728800074878323\n",
            "epoch 29 batch id 2681 loss 0.08285925537347794 train acc 0.9728529466616934\n",
            "epoch 29 batch id 2691 loss 0.015933029353618622 train acc 0.9728609253065775\n",
            "epoch 29 batch id 2701 loss 0.07052880525588989 train acc 0.9728804146612365\n",
            "epoch 29 batch id 2711 loss 0.08454223722219467 train acc 0.9728594153448912\n",
            "epoch 29 batch id 2721 loss 0.015461443923413754 train acc 0.9728328280044102\n",
            "epoch 29 batch id 2731 loss 0.0110057033598423 train acc 0.9728579274990846\n",
            "epoch 29 batch id 2741 loss 0.16355274617671967 train acc 0.9728543414812112\n",
            "epoch 29 batch id 2751 loss 0.10988479852676392 train acc 0.9728451017811705\n",
            "epoch 29 batch id 2761 loss 0.011888675391674042 train acc 0.9728812024628758\n",
            "epoch 29 batch id 2771 loss 0.008243667893111706 train acc 0.9729114038253338\n",
            "epoch 29 batch id 2781 loss 0.03699547424912453 train acc 0.972924532542251\n",
            "epoch 29 batch id 2791 loss 0.049848075956106186 train acc 0.9729543622357578\n",
            "epoch 29 batch id 2801 loss 0.1678815484046936 train acc 0.9729505087468762\n",
            "epoch 29 batch id 2811 loss 0.08469710499048233 train acc 0.9729855923159018\n",
            "epoch 29 batch id 2821 loss 0.02721063233911991 train acc 0.9730093495214462\n",
            "epoch 29 batch id 2831 loss 0.02313074842095375 train acc 0.9730108618862593\n",
            "epoch 29 batch id 2841 loss 0.1565023958683014 train acc 0.9729903643083422\n",
            "epoch 29 batch id 2851 loss 0.010719262063503265 train acc 0.9729426078568923\n",
            "epoch 29 batch id 2861 loss 0.06250126659870148 train acc 0.9729224921356169\n",
            "epoch 29 batch id 2871 loss 0.17033210396766663 train acc 0.9729406130268199\n",
            "epoch 29 batch id 2881 loss 0.04810383915901184 train acc 0.9729423377299549\n",
            "epoch 29 batch id 2891 loss 0.1012241467833519 train acc 0.9729278363887928\n",
            "epoch 29 batch id 2901 loss 0.01640307530760765 train acc 0.9729511375387797\n",
            "epoch 29 batch id 2911 loss 0.022811299189925194 train acc 0.9729528083132944\n",
            "epoch 29 batch id 2921 loss 0.016436712816357613 train acc 0.9729063248887367\n",
            "epoch 29 batch id 2931 loss 0.06849286705255508 train acc 0.9728921443193449\n",
            "epoch 29 batch id 2941 loss 0.03017924353480339 train acc 0.9728993114586875\n",
            "epoch 29 batch id 2951 loss 0.043832797557115555 train acc 0.9729487885462555\n",
            "epoch 29 batch id 2961 loss 0.04661146551370621 train acc 0.9729451621073961\n",
            "epoch 29 batch id 2971 loss 0.027421291917562485 train acc 0.972920523392797\n",
            "epoch 29 batch id 2981 loss 0.07682756334543228 train acc 0.9729327406910433\n",
            "epoch 29 batch id 2991 loss 0.1387331187725067 train acc 0.9729553243062521\n",
            "epoch 29 batch id 3001 loss 0.012466183863580227 train acc 0.972982964011996\n",
            "epoch 29 batch id 3011 loss 0.03390829637646675 train acc 0.9729689056791764\n",
            "epoch 29 batch id 3021 loss 0.056316182017326355 train acc 0.9729756289308176\n",
            "epoch 29 batch id 3031 loss 0.19990046322345734 train acc 0.9729668426261959\n",
            "epoch 29 batch id 3041 loss 0.1605207324028015 train acc 0.9729324235448865\n",
            "epoch 29 batch id 3051 loss 0.10008043050765991 train acc 0.9729289577187807\n",
            "epoch 29 batch id 3061 loss 0.08970583975315094 train acc 0.9728999918327343\n",
            "epoch 29 batch id 3071 loss 0.0034930368419736624 train acc 0.9729373575382612\n",
            "epoch 29 batch id 3081 loss 0.24849064648151398 train acc 0.9729389808503732\n",
            "epoch 29 batch id 3091 loss 0.12880858778953552 train acc 0.97294059365901\n",
            "epoch 29 batch id 3101 loss 0.07366613298654556 train acc 0.9729472347629797\n",
            "epoch 29 batch id 3111 loss 0.08855997025966644 train acc 0.9729739231758278\n",
            "epoch 29 batch id 3121 loss 0.08386357128620148 train acc 0.9729653957065043\n",
            "epoch 29 batch id 3131 loss 0.0904422327876091 train acc 0.9729918556371766\n",
            "epoch 29 batch id 3141 loss 0.07209727168083191 train acc 0.9729982489652976\n",
            "epoch 29 batch id 3151 loss 0.16563823819160461 train acc 0.9729897254839733\n",
            "epoch 29 batch id 3161 loss 0.025980934500694275 train acc 0.9729911420436571\n",
            "epoch 29 batch id 3171 loss 0.02382035367190838 train acc 0.9729925496688742\n",
            "epoch 29 batch id 3181 loss 0.022998910397291183 train acc 0.9729693885570575\n",
            "epoch 29 batch id 3191 loss 0.02232927829027176 train acc 0.9730002350360388\n",
            "epoch 29 batch id 3201 loss 0.11151528358459473 train acc 0.9729918384879725\n",
            "epoch 29 batch id 3211 loss 0.08393814414739609 train acc 0.9729445655559016\n",
            "epoch 29 batch id 3221 loss 0.11477849632501602 train acc 0.9729024371313256\n",
            "epoch 29 batch id 3231 loss 0.09123463183641434 train acc 0.9728992571959145\n",
            "epoch 29 batch id 3241 loss 0.036565039306879044 train acc 0.9728960968836778\n",
            "epoch 29 batch id 3251 loss 0.14877165853977203 train acc 0.9728881498000616\n",
            "epoch 29 batch id 3261 loss 0.12246212363243103 train acc 0.9728419196565471\n",
            "epoch 29 batch id 3271 loss 0.0544637106359005 train acc 0.9728246331397127\n",
            "epoch 29 batch id 3281 loss 0.0025702165439724922 train acc 0.9728217387991466\n",
            "epoch 29 batch id 3291 loss 0.02658558264374733 train acc 0.9728046186569432\n",
            "epoch 29 batch id 3301 loss 0.04351978003978729 train acc 0.9727970690699788\n",
            "epoch 29 batch id 3311 loss 0.035722918808460236 train acc 0.9727990033222591\n",
            "epoch 29 batch id 3321 loss 0.025033414363861084 train acc 0.9728056308340861\n",
            "epoch 29 batch id 3331 loss 0.07137086987495422 train acc 0.9727840738516962\n",
            "epoch 29 batch id 3341 loss 0.0939616784453392 train acc 0.9727953831188267\n",
            "epoch 29 batch id 3351 loss 0.07429154217243195 train acc 0.9727739853774993\n",
            "epoch 29 batch id 3361 loss 0.13804051280021667 train acc 0.9728038530199346\n",
            "epoch 29 batch id 3371 loss 0.18952465057373047 train acc 0.9727918273509344\n",
            "epoch 29 batch id 3381 loss 0.03001711517572403 train acc 0.9728029798876072\n",
            "epoch 29 batch id 3391 loss 0.08140496164560318 train acc 0.9728140666470068\n",
            "epoch 29 batch id 3401 loss 0.2730218768119812 train acc 0.9727745516024698\n",
            "epoch 29 batch id 3411 loss 0.21078063547611237 train acc 0.9727856566989153\n",
            "epoch 29 batch id 3421 loss 0.02869882434606552 train acc 0.9727966968722596\n",
            "epoch 29 batch id 3431 loss 0.23422302305698395 train acc 0.9727393617021277\n",
            "epoch 29 batch id 3441 loss 0.07740171998739243 train acc 0.97275047224644\n",
            "epoch 29 batch id 3451 loss 0.054578084498643875 train acc 0.9727524630541872\n",
            "epoch 29 batch id 3461 loss 0.010716521181166172 train acc 0.9727544423577\n",
            "epoch 29 batch id 3471 loss 0.07822783291339874 train acc 0.9727834197637568\n",
            "epoch 29 batch id 3481 loss 0.10550093650817871 train acc 0.9727583668486067\n",
            "epoch 29 batch id 3491 loss 0.008206894621253014 train acc 0.9727603122314523\n",
            "epoch 29 batch id 3501 loss 0.19226424396038055 train acc 0.972775635532705\n",
            "epoch 29 batch id 3511 loss 0.05753670260310173 train acc 0.9727686200512674\n",
            "epoch 29 batch id 3521 loss 0.09599540382623672 train acc 0.9727971456972451\n",
            "epoch 29 batch id 3531 loss 0.05675377696752548 train acc 0.9728033843103937\n",
            "epoch 29 batch id 3541 loss 0.23190538585186005 train acc 0.9727742869245976\n",
            "epoch 29 batch id 3551 loss 0.0396658256649971 train acc 0.9727453534215714\n",
            "epoch 29 batch id 3561 loss 0.017695443704724312 train acc 0.972747297107554\n",
            "epoch 29 batch id 3571 loss 0.06544661521911621 train acc 0.9727492299075889\n",
            "epoch 29 batch id 3581 loss 0.14170396327972412 train acc 0.9727467886065345\n",
            "epoch 29 batch id 3591 loss 0.1115075945854187 train acc 0.9727313074352548\n",
            "epoch 29 batch id 3601 loss 0.06663838028907776 train acc 0.9727506248264371\n",
            "epoch 29 batch id 3611 loss 0.0015823444118723273 train acc 0.9727741622819164\n",
            "epoch 29 batch id 3621 loss 0.0601271353662014 train acc 0.9727457884562276\n",
            "epoch 29 batch id 3631 loss 0.07396011054515839 train acc 0.9727261773616084\n",
            "epoch 29 batch id 3641 loss 0.15844109654426575 train acc 0.972736713814886\n",
            "epoch 29 batch id 3651 loss 0.08653350919485092 train acc 0.9727043960558751\n",
            "epoch 29 batch id 3661 loss 0.047588903456926346 train acc 0.9726935946462715\n",
            "epoch 29 batch id 3671 loss 0.0711192786693573 train acc 0.9727083900844457\n",
            "epoch 29 batch id 3681 loss 0.10776335000991821 train acc 0.9727400842162456\n",
            "epoch 29 batch id 3691 loss 0.16939841210842133 train acc 0.9727208073692766\n",
            "epoch 29 batch id 3701 loss 0.06483221054077148 train acc 0.9727016346933262\n",
            "epoch 29 batch id 3711 loss 0.0711299404501915 train acc 0.9727246699002964\n",
            "epoch 29 batch id 3721 loss 0.19116859138011932 train acc 0.9727475812953507\n",
            "epoch 29 batch id 3731 loss 0.020786825567483902 train acc 0.9727787456445993\n",
            "epoch 29 batch id 3741 loss 0.11119996756315231 train acc 0.9727470930232558\n",
            "epoch 29 batch id 3751 loss 0.06456837058067322 train acc 0.9727614302852573\n",
            "epoch 29 batch id 3761 loss 0.08280471712350845 train acc 0.9727341465035895\n",
            "epoch 29 batch id 3771 loss 0.03642101213335991 train acc 0.9727318682047202\n",
            "epoch 29 batch id 3781 loss 0.07995504885911942 train acc 0.9727461319756678\n",
            "epoch 29 batch id 3791 loss 0.0012786427978426218 train acc 0.972747955684516\n",
            "epoch 29 batch id 3801 loss 0.06412380188703537 train acc 0.9727127729544857\n",
            "epoch 29 batch id 3811 loss 0.028893671929836273 train acc 0.9727433744424036\n",
            "epoch 29 batch id 3821 loss 0.1417357474565506 train acc 0.9727451910494634\n",
            "epoch 29 batch id 3831 loss 0.008532133884727955 train acc 0.972755155311929\n",
            "epoch 29 batch id 3841 loss 0.001844744198024273 train acc 0.9727609997396511\n",
            "epoch 29 batch id 3851 loss 0.004882571753114462 train acc 0.9727627564269021\n",
            "epoch 29 batch id 3861 loss 0.04296776279807091 train acc 0.9727725977725977\n",
            "epoch 29 batch id 3871 loss 0.19665776193141937 train acc 0.9727622061482821\n",
            "epoch 29 batch id 3881 loss 0.15142850577831268 train acc 0.9727518680752384\n",
            "epoch 29 batch id 3891 loss 0.02938361093401909 train acc 0.9727335517861733\n",
            "epoch 29 batch id 3901 loss 0.009671129286289215 train acc 0.9727153294027172\n",
            "epoch 29 batch id 3911 loss 0.01989874616265297 train acc 0.9726812196369216\n",
            "epoch 29 batch id 3921 loss 0.012759020552039146 train acc 0.9726831484315226\n",
            "epoch 29 batch id 3931 loss 0.0678054690361023 train acc 0.9726612185194607\n",
            "epoch 29 batch id 3941 loss 0.040712106972932816 train acc 0.9726750824663791\n",
            "epoch 29 batch id 3951 loss 0.0470784567296505 train acc 0.9726493292837256\n",
            "epoch 29 batch id 3961 loss 0.019660357385873795 train acc 0.9726631532441302\n",
            "epoch 29 batch id 3971 loss 0.04667522758245468 train acc 0.9726611684714178\n",
            "epoch 29 batch id 3981 loss 0.02936248853802681 train acc 0.9726513438834463\n",
            "epoch 29 batch id 3991 loss 0.3001972436904907 train acc 0.9726337384114258\n",
            "epoch 29 batch id 4001 loss 0.1015213131904602 train acc 0.9726279367658085\n",
            "epoch 29 batch id 4011 loss 0.09908031672239304 train acc 0.9725754176015956\n",
            "epoch 29 batch id 4021 loss 0.025730501860380173 train acc 0.9725853332504352\n",
            "epoch 29 batch id 4031 loss 0.0422385074198246 train acc 0.9726145807491937\n",
            "epoch 29 batch id 4041 loss 0.13165515661239624 train acc 0.9726088839396189\n",
            "epoch 29 batch id 4051 loss 0.079620361328125 train acc 0.9726147864724759\n",
            "epoch 29 batch id 4061 loss 0.03950761631131172 train acc 0.9726398978084215\n",
            "epoch 29 batch id 4071 loss 0.1281219869852066 train acc 0.9726533714075166\n",
            "epoch 29 batch id 4081 loss 0.041885197162628174 train acc 0.9726552928203872\n",
            "epoch 29 batch id 4091 loss 0.03582319617271423 train acc 0.9726763016377414\n",
            "epoch 29 batch id 4101 loss 0.017267175018787384 train acc 0.9726743477200682\n",
            "epoch 29 batch id 4111 loss 0.0200871080160141 train acc 0.9726724033081975\n",
            "epoch 29 batch id 4121 loss 0.050019945949316025 train acc 0.9726628852220335\n",
            "epoch 29 batch id 4131 loss 0.08529776334762573 train acc 0.9726345013313967\n",
            "epoch 29 batch id 4141 loss 0.02052230015397072 train acc 0.9726364404733157\n",
            "epoch 29 batch id 4151 loss 0.15925131738185883 train acc 0.9726233136593592\n",
            "epoch 29 batch id 4161 loss 0.02686011791229248 train acc 0.9725952295121365\n",
            "epoch 29 batch id 4171 loss 0.12140201032161713 train acc 0.972586010549029\n",
            "epoch 29 batch id 4181 loss 0.007822437211871147 train acc 0.9725880471179144\n",
            "epoch 29 batch id 4191 loss 0.11266572773456573 train acc 0.9725788892865664\n",
            "epoch 29 batch id 4201 loss 0.044511813670396805 train acc 0.9726032492263746\n",
            "epoch 29 batch id 4211 loss 0.07454173266887665 train acc 0.9725978093089528\n",
            "epoch 29 batch id 4221 loss 0.18368121981620789 train acc 0.9725701847903341\n",
            "epoch 29 batch id 4231 loss 0.11405999958515167 train acc 0.972550076813992\n",
            "epoch 29 batch id 4241 loss 0.08285194635391235 train acc 0.9725705906625796\n",
            "epoch 29 batch id 4251 loss 0.10468320548534393 train acc 0.9725946836038579\n",
            "epoch 29 batch id 4261 loss 0.05751366540789604 train acc 0.9726039955409528\n",
            "epoch 29 batch id 4271 loss 0.013138639740645885 train acc 0.9726315558417232\n",
            "epoch 29 batch id 4281 loss 0.2178364098072052 train acc 0.9726407381452932\n",
            "epoch 29 batch id 4291 loss 0.02262684516608715 train acc 0.972631670939175\n",
            "epoch 29 batch id 4301 loss 0.0834842100739479 train acc 0.9725972157637759\n",
            "epoch 29 batch id 4311 loss 0.13098429143428802 train acc 0.9726064138250986\n",
            "epoch 29 batch id 4321 loss 0.04788538068532944 train acc 0.9725866408238834\n",
            "epoch 29 batch id 4331 loss 0.05511649698019028 train acc 0.9725922131147541\n",
            "epoch 29 batch id 4341 loss 0.06275222450494766 train acc 0.9725869615296014\n",
            "epoch 29 batch id 4351 loss 0.07863509654998779 train acc 0.9725745518271661\n",
            "epoch 29 batch id 4361 loss 0.10459958761930466 train acc 0.9725801135060765\n",
            "epoch 29 batch id 4371 loss 0.06221585348248482 train acc 0.972589224433768\n",
            "epoch 29 batch id 4381 loss 0.22134055197238922 train acc 0.9725590618580233\n",
            "epoch 29 batch id 4391 loss 0.03671199828386307 train acc 0.97250412776133\n",
            "epoch 29 batch id 4401 loss 0.06708094477653503 train acc 0.9725097989093388\n",
            "epoch 29 batch id 4411 loss 0.11865202337503433 train acc 0.9725225289050102\n",
            "epoch 29 batch id 4421 loss 0.0655025988817215 train acc 0.9725139957023298\n",
            "epoch 29 batch id 4431 loss 0.02017829567193985 train acc 0.9725019747235387\n",
            "epoch 29 batch id 4441 loss 0.015381800010800362 train acc 0.9725005629362756\n",
            "epoch 29 batch id 4451 loss 0.0032852666918188334 train acc 0.972516709728151\n",
            "epoch 29 batch id 4461 loss 0.036791179329156876 train acc 0.9724977583501457\n",
            "epoch 29 batch id 4471 loss 0.10294953733682632 train acc 0.9724719022590025\n",
            "epoch 29 batch id 4481 loss 0.10226061940193176 train acc 0.9724670832403481\n",
            "epoch 29 batch id 4491 loss 0.034228041768074036 train acc 0.9724692440436429\n",
            "epoch 29 batch id 4501 loss 0.05273609235882759 train acc 0.9724609808931348\n",
            "epoch 29 batch id 4511 loss 0.09607424587011337 train acc 0.9724458268676568\n",
            "epoch 29 batch id 4521 loss 0.06311487406492233 train acc 0.9724514764432648\n",
            "epoch 29 batch id 4531 loss 0.0034954247530549765 train acc 0.972457101081439\n",
            "epoch 29 batch id 4541 loss 0.07904253900051117 train acc 0.9724764644351465\n",
            "epoch 29 batch id 4551 loss 0.15090933442115784 train acc 0.972485442759833\n",
            "epoch 29 batch id 4561 loss 0.06362807750701904 train acc 0.9724738270116202\n",
            "epoch 29 batch id 4571 loss 0.17612630128860474 train acc 0.9724759352439292\n",
            "epoch 29 batch id 4581 loss 0.029388371855020523 train acc 0.9724746234446627\n",
            "epoch 29 batch id 4591 loss 0.10230045765638351 train acc 0.9724631071661948\n",
            "epoch 29 batch id 4601 loss 0.024909861385822296 train acc 0.9724550369484894\n",
            "epoch 29 batch id 4611 loss 0.13725923001766205 train acc 0.9724605562784645\n",
            "epoch 29 batch id 4621 loss 0.011505880393087864 train acc 0.9724491452066653\n",
            "epoch 29 batch id 4631 loss 0.08833662420511246 train acc 0.9724614014251781\n",
            "epoch 29 batch id 4641 loss 0.14831477403640747 train acc 0.9724534044386985\n",
            "epoch 29 batch id 4651 loss 0.17511992156505585 train acc 0.9724622393033756\n",
            "epoch 29 batch id 4661 loss 0.060824934393167496 train acc 0.9724777408281484\n",
            "epoch 29 batch id 4671 loss 0.016595685854554176 train acc 0.9724764504388782\n",
            "epoch 29 batch id 4681 loss 0.16487707197666168 train acc 0.97247182760094\n",
            "epoch 29 batch id 4691 loss 0.004563991446048021 train acc 0.9724772170112982\n",
            "epoch 29 batch id 4701 loss 0.04140043258666992 train acc 0.9724859072537758\n",
            "epoch 29 batch id 4711 loss 0.08320260047912598 train acc 0.9725045107195924\n",
            "epoch 29 batch id 4721 loss 0.004819072317332029 train acc 0.9724899385723363\n",
            "epoch 29 batch id 4731 loss 0.05653255060315132 train acc 0.9724820333967449\n",
            "epoch 29 batch id 4741 loss 0.07367997616529465 train acc 0.9724741615692892\n",
            "epoch 29 batch id 4751 loss 0.16819927096366882 train acc 0.972486055567249\n",
            "epoch 29 batch id 4761 loss 0.1351660192012787 train acc 0.9724946177273682\n",
            "epoch 29 batch id 4771 loss 0.09249993413686752 train acc 0.9724965940054496\n",
            "epoch 29 batch id 4781 loss 0.12736064195632935 train acc 0.9724691487136582\n",
            "epoch 29 batch id 4791 loss 0.1262412965297699 train acc 0.9724581246086412\n",
            "epoch 29 batch id 4801 loss 0.07572805136442184 train acc 0.9724406373672152\n",
            "epoch 29 batch id 4811 loss 0.00881562102586031 train acc 0.97243296611931\n",
            "epoch 29 batch id 4821 loss 0.11347038298845291 train acc 0.9724544959551961\n",
            "epoch 29 batch id 4831 loss 0.024695035070180893 train acc 0.9724694680190437\n",
            "epoch 29 batch id 4841 loss 0.07639626413583755 train acc 0.972468240033051\n",
            "epoch 29 batch id 4851 loss 0.04210643842816353 train acc 0.9724637961245104\n",
            "epoch 29 batch id 4861 loss 0.00366029841825366 train acc 0.9724722279366386\n",
            "epoch 29 batch id 4871 loss 0.018689004704356194 train acc 0.9724934561691645\n",
            "epoch 29 batch id 4881 loss 0.02741457149386406 train acc 0.9725113962302807\n",
            "epoch 29 batch id 4891 loss 0.05458594486117363 train acc 0.9724973164996933\n",
            "epoch 29 batch id 4901 loss 0.06288987398147583 train acc 0.9725056110997755\n",
            "epoch 29 batch id 4911 loss 0.15961776673793793 train acc 0.9725106902871106\n",
            "epoch 29 batch id 4921 loss 0.23263949155807495 train acc 0.9725125736638894\n",
            "epoch 29 batch id 4931 loss 0.04187721759080887 train acc 0.972514449401744\n",
            "epoch 29 batch id 4941 loss 0.06589346379041672 train acc 0.9725131552317344\n",
            "epoch 29 batch id 4951 loss 0.18954335153102875 train acc 0.9724960866491618\n",
            "epoch 29 train acc 0.972496423790049\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5c297b6a3a04ea8964aa74351defd84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 29 loss 0.6540246605873108 test acc 0.8400980571847508\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af328ad0224c4ca79dde5549fac24f03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 30 batch id 1 loss 0.075991190969944 train acc 0.953125\n",
            "epoch 30 batch id 11 loss 0.01738031581044197 train acc 0.9801136363636364\n",
            "epoch 30 batch id 21 loss 0.009119562804698944 train acc 0.9799107142857143\n",
            "epoch 30 batch id 31 loss 0.2860282361507416 train acc 0.9707661290322581\n",
            "epoch 30 batch id 41 loss 0.1360214352607727 train acc 0.9710365853658537\n",
            "epoch 30 batch id 51 loss 0.060928087681531906 train acc 0.9712009803921569\n",
            "epoch 30 batch id 61 loss 0.029178673401474953 train acc 0.970030737704918\n",
            "epoch 30 batch id 71 loss 0.2045947164297104 train acc 0.9716109154929577\n",
            "epoch 30 batch id 81 loss 0.07661851495504379 train acc 0.9731867283950617\n",
            "epoch 30 batch id 91 loss 0.08372776955366135 train acc 0.9728708791208791\n",
            "epoch 30 batch id 101 loss 0.06356626003980637 train acc 0.9718440594059405\n",
            "epoch 30 batch id 111 loss 0.0007170484750531614 train acc 0.9722691441441441\n",
            "epoch 30 batch id 121 loss 0.21801090240478516 train acc 0.9728822314049587\n",
            "epoch 30 batch id 131 loss 0.17173756659030914 train acc 0.9732824427480916\n",
            "epoch 30 batch id 141 loss 0.03837305307388306 train acc 0.973404255319149\n",
            "epoch 30 batch id 151 loss 0.09104087948799133 train acc 0.9731995033112583\n",
            "epoch 30 batch id 161 loss 0.019186176359653473 train acc 0.9736024844720497\n",
            "epoch 30 batch id 171 loss 0.07151836901903152 train acc 0.9732273391812866\n",
            "epoch 30 batch id 181 loss 0.05446242541074753 train acc 0.9734116022099447\n",
            "epoch 30 batch id 191 loss 0.06689612567424774 train acc 0.9734129581151832\n",
            "epoch 30 batch id 201 loss 0.08873553574085236 train acc 0.9734141791044776\n",
            "epoch 30 batch id 211 loss 0.030949056148529053 train acc 0.9737114928909952\n",
            "epoch 30 batch id 221 loss 0.18541544675827026 train acc 0.9734869909502263\n",
            "epoch 30 batch id 231 loss 0.1246769055724144 train acc 0.9736201298701299\n",
            "epoch 30 batch id 241 loss 0.13946960866451263 train acc 0.973871887966805\n",
            "epoch 30 batch id 251 loss 0.0812496542930603 train acc 0.9743525896414342\n",
            "epoch 30 batch id 261 loss 0.11966941505670547 train acc 0.9744372605363985\n",
            "epoch 30 batch id 271 loss 0.01743929088115692 train acc 0.9746886531365314\n",
            "epoch 30 batch id 281 loss 0.108242928981781 train acc 0.974644128113879\n",
            "epoch 30 batch id 291 loss 0.08642670512199402 train acc 0.9750859106529209\n",
            "epoch 30 batch id 301 loss 0.0937226340174675 train acc 0.9748235049833887\n",
            "epoch 30 batch id 311 loss 0.1272173821926117 train acc 0.9749296623794212\n",
            "epoch 30 batch id 321 loss 0.32487064599990845 train acc 0.9745911214953271\n",
            "epoch 30 batch id 331 loss 0.003461136482656002 train acc 0.9747922960725075\n",
            "epoch 30 batch id 341 loss 0.22719240188598633 train acc 0.9747067448680352\n",
            "epoch 30 batch id 351 loss 0.02849227376282215 train acc 0.9749821937321937\n",
            "epoch 30 batch id 361 loss 0.025191020220518112 train acc 0.9749826869806094\n",
            "epoch 30 batch id 371 loss 0.08561717718839645 train acc 0.9751937331536388\n",
            "epoch 30 batch id 381 loss 0.05556783080101013 train acc 0.9754757217847769\n",
            "epoch 30 batch id 391 loss 0.08449069410562515 train acc 0.9756234015345269\n",
            "epoch 30 batch id 401 loss 0.0041131507605314255 train acc 0.975607855361596\n",
            "epoch 30 batch id 411 loss 0.012192219495773315 train acc 0.9755550486618005\n",
            "epoch 30 batch id 421 loss 0.1334630399942398 train acc 0.975541864608076\n",
            "epoch 30 batch id 431 loss 0.3088371157646179 train acc 0.9753480278422274\n",
            "epoch 30 batch id 441 loss 0.14673125743865967 train acc 0.9750921201814059\n",
            "epoch 30 batch id 451 loss 0.014721200801432133 train acc 0.9751940133037694\n",
            "epoch 30 batch id 461 loss 0.006631436757743359 train acc 0.9750542299349241\n",
            "epoch 30 batch id 471 loss 0.13540169596672058 train acc 0.9750862526539278\n",
            "epoch 30 batch id 481 loss 0.07990876585245132 train acc 0.9752468814968815\n",
            "epoch 30 batch id 491 loss 0.007464520633220673 train acc 0.9751463849287169\n",
            "epoch 30 batch id 501 loss 0.1992470622062683 train acc 0.9750499001996008\n",
            "epoch 30 batch id 511 loss 0.10088098049163818 train acc 0.974957191780822\n",
            "epoch 30 batch id 521 loss 0.09323933720588684 train acc 0.9749280230326296\n",
            "epoch 30 batch id 531 loss 0.04531550034880638 train acc 0.9747822504708098\n",
            "epoch 30 batch id 541 loss 0.09291575849056244 train acc 0.9746129852125693\n",
            "epoch 30 batch id 551 loss 0.2071569412946701 train acc 0.9745349364791288\n",
            "epoch 30 batch id 561 loss 0.03427012264728546 train acc 0.9743482620320856\n",
            "epoch 30 batch id 571 loss 0.14794804155826569 train acc 0.9743870402802102\n",
            "epoch 30 batch id 581 loss 0.1693934053182602 train acc 0.9743975903614458\n",
            "epoch 30 batch id 591 loss 0.03025943785905838 train acc 0.9744870981387479\n",
            "epoch 30 batch id 601 loss 0.017224228009581566 train acc 0.9744176372712147\n",
            "epoch 30 batch id 611 loss 0.07120270282030106 train acc 0.974376022913257\n",
            "epoch 30 batch id 621 loss 0.1616172194480896 train acc 0.974411231884058\n",
            "epoch 30 batch id 631 loss 0.1172538697719574 train acc 0.9744700871632329\n",
            "epoch 30 batch id 641 loss 0.034496743232011795 train acc 0.9745514820592823\n",
            "epoch 30 batch id 651 loss 0.042323872447013855 train acc 0.9744623655913979\n",
            "epoch 30 batch id 661 loss 0.007215437013655901 train acc 0.974588691376702\n",
            "epoch 30 batch id 671 loss 0.042860761284828186 train acc 0.9745482488822653\n",
            "epoch 30 batch id 681 loss 0.20132480561733246 train acc 0.9745778267254038\n",
            "epoch 30 batch id 691 loss 0.09384056180715561 train acc 0.9746065484804631\n",
            "epoch 30 batch id 701 loss 0.09566996246576309 train acc 0.9744561340941512\n",
            "epoch 30 batch id 711 loss 0.1589561253786087 train acc 0.9744198312236287\n",
            "epoch 30 batch id 721 loss 0.04415173456072807 train acc 0.9744062066574203\n",
            "epoch 30 batch id 731 loss 0.016064513474702835 train acc 0.9744143296853626\n",
            "epoch 30 batch id 741 loss 0.029030106961727142 train acc 0.974422233468286\n",
            "epoch 30 batch id 751 loss 0.10618341714143753 train acc 0.9742218708388815\n",
            "epoch 30 batch id 761 loss 0.03246050700545311 train acc 0.9743142247043364\n",
            "epoch 30 batch id 771 loss 0.19233666360378265 train acc 0.9741812581063554\n",
            "epoch 30 batch id 781 loss 0.07971258461475372 train acc 0.974151728553137\n",
            "epoch 30 batch id 791 loss 0.12081757932901382 train acc 0.9741229456384324\n",
            "epoch 30 batch id 801 loss 0.1040155291557312 train acc 0.9740558676654182\n",
            "epoch 30 batch id 811 loss 0.19365328550338745 train acc 0.9739519112207151\n",
            "epoch 30 batch id 821 loss 0.021586347371339798 train acc 0.9739837088915956\n",
            "epoch 30 batch id 831 loss 0.10164475440979004 train acc 0.9740147412755716\n",
            "epoch 30 batch id 841 loss 0.003446731250733137 train acc 0.9739707193816884\n",
            "epoch 30 batch id 851 loss 0.15198619663715363 train acc 0.9738542890716804\n",
            "epoch 30 batch id 861 loss 0.054341886192560196 train acc 0.9738675958188153\n",
            "epoch 30 batch id 871 loss 0.01823912002146244 train acc 0.9739344144661309\n",
            "epoch 30 batch id 881 loss 0.024457475170493126 train acc 0.9739642451759364\n",
            "epoch 30 batch id 891 loss 0.005944315809756517 train acc 0.9740109427609428\n",
            "epoch 30 batch id 901 loss 0.09747189283370972 train acc 0.9740045782463929\n",
            "epoch 30 batch id 911 loss 0.04110769182443619 train acc 0.9739468990120747\n",
            "epoch 30 batch id 921 loss 0.08632563799619675 train acc 0.9738735070575462\n",
            "epoch 30 batch id 931 loss 0.0215253047645092 train acc 0.9738688238453276\n",
            "epoch 30 batch id 941 loss 0.033870745450258255 train acc 0.9738476354941552\n",
            "epoch 30 batch id 951 loss 0.05925693362951279 train acc 0.9738104626708728\n",
            "epoch 30 batch id 961 loss 0.04898535832762718 train acc 0.973806581685744\n",
            "epoch 30 batch id 971 loss 0.1798781454563141 train acc 0.9737866889804325\n",
            "epoch 30 batch id 981 loss 0.03207770362496376 train acc 0.9738309123343527\n",
            "epoch 30 batch id 991 loss 0.13638071715831757 train acc 0.973826942482341\n",
            "epoch 30 batch id 1001 loss 0.0081991171464324 train acc 0.9739479270729271\n",
            "epoch 30 batch id 1011 loss 0.21213866770267487 train acc 0.9738656033630069\n",
            "epoch 30 batch id 1021 loss 0.03785158693790436 train acc 0.9739685357492655\n",
            "epoch 30 batch id 1031 loss 0.0122370645403862 train acc 0.9739936954413191\n",
            "epoch 30 batch id 1041 loss 0.16119959950447083 train acc 0.9740634005763689\n",
            "epoch 30 batch id 1051 loss 0.04235079139471054 train acc 0.9740128449096099\n",
            "epoch 30 batch id 1061 loss 0.0369359590113163 train acc 0.9740368755890669\n",
            "epoch 30 batch id 1071 loss 0.018307073041796684 train acc 0.9739729225023342\n",
            "epoch 30 batch id 1081 loss 0.04080720618367195 train acc 0.9739968778908418\n",
            "epoch 30 batch id 1091 loss 0.053686775267124176 train acc 0.9740060724106324\n",
            "epoch 30 batch id 1101 loss 0.16976295411586761 train acc 0.9739867166212534\n",
            "epoch 30 batch id 1111 loss 0.07220195233821869 train acc 0.9739255175517552\n",
            "epoch 30 batch id 1121 loss 0.042118050158023834 train acc 0.9739351025869759\n",
            "epoch 30 batch id 1131 loss 0.06125948578119278 train acc 0.9738478116710876\n",
            "epoch 30 batch id 1141 loss 0.1222190111875534 train acc 0.9738579097283085\n",
            "epoch 30 batch id 1151 loss 0.11725664883852005 train acc 0.9736642050390965\n",
            "epoch 30 batch id 1161 loss 0.04775669798254967 train acc 0.9736757105943152\n",
            "epoch 30 batch id 1171 loss 0.12545408308506012 train acc 0.9735936165670367\n",
            "epoch 30 batch id 1181 loss 0.12479391694068909 train acc 0.973645215918713\n",
            "epoch 30 batch id 1191 loss 0.04284517839550972 train acc 0.9737090680100756\n",
            "epoch 30 batch id 1201 loss 0.014276274479925632 train acc 0.9737718567860116\n",
            "epoch 30 batch id 1211 loss 0.0008034727652557194 train acc 0.973820706028076\n",
            "epoch 30 batch id 1221 loss 0.16417662799358368 train acc 0.9738303644553644\n",
            "epoch 30 batch id 1231 loss 0.3157247006893158 train acc 0.9737129366368806\n",
            "epoch 30 batch id 1241 loss 0.017888737842440605 train acc 0.9737107171635777\n",
            "epoch 30 batch id 1251 loss 0.11551317572593689 train acc 0.9737210231814548\n",
            "epoch 30 batch id 1261 loss 0.02321329154074192 train acc 0.9737807295796986\n",
            "epoch 30 batch id 1271 loss 0.1638580560684204 train acc 0.9737288552321007\n",
            "epoch 30 batch id 1281 loss 0.012403994798660278 train acc 0.9737387782982045\n",
            "epoch 30 batch id 1291 loss 0.3506842851638794 train acc 0.9737727536793184\n",
            "epoch 30 batch id 1301 loss 0.12614750862121582 train acc 0.973734146810146\n",
            "epoch 30 batch id 1311 loss 0.16945619881153107 train acc 0.9737318840579711\n",
            "epoch 30 batch id 1321 loss 0.021874969825148582 train acc 0.9737059992429977\n",
            "epoch 30 batch id 1331 loss 0.048226695507764816 train acc 0.9737157212622088\n",
            "epoch 30 batch id 1341 loss 0.25986483693122864 train acc 0.9736786912751678\n",
            "epoch 30 batch id 1351 loss 0.10814827680587769 train acc 0.9737116025166543\n",
            "epoch 30 batch id 1361 loss 0.13136731088161469 train acc 0.9735603416605437\n",
            "epoch 30 batch id 1371 loss 0.026437513530254364 train acc 0.9735936360320934\n",
            "epoch 30 batch id 1381 loss 0.04230290278792381 train acc 0.9736264482259233\n",
            "epoch 30 batch id 1391 loss 0.044205084443092346 train acc 0.9735689252336449\n",
            "epoch 30 batch id 1401 loss 0.025766383856534958 train acc 0.9735345289079229\n",
            "epoch 30 batch id 1411 loss 0.016446871683001518 train acc 0.9735892097802976\n",
            "epoch 30 batch id 1421 loss 0.16601717472076416 train acc 0.9735441590429276\n",
            "epoch 30 batch id 1431 loss 0.017839297652244568 train acc 0.9735761705101328\n",
            "epoch 30 batch id 1441 loss 0.037240736186504364 train acc 0.9735752081887578\n",
            "epoch 30 batch id 1451 loss 0.017428865656256676 train acc 0.9736173328738801\n",
            "epoch 30 batch id 1461 loss 0.013681678101420403 train acc 0.9736054072553045\n",
            "epoch 30 batch id 1471 loss 0.018723949790000916 train acc 0.9736892420122366\n",
            "epoch 30 batch id 1481 loss 0.05979996919631958 train acc 0.9736242403781229\n",
            "epoch 30 batch id 1491 loss 0.12827129662036896 train acc 0.9736439470154259\n",
            "epoch 30 batch id 1501 loss 0.056425437331199646 train acc 0.9737258494337109\n",
            "epoch 30 batch id 1511 loss 0.02873307652771473 train acc 0.9737549636002647\n",
            "epoch 30 batch id 1521 loss 0.13876254856586456 train acc 0.9738350591715976\n",
            "epoch 30 batch id 1531 loss 0.025214780122041702 train acc 0.9738426681907251\n",
            "epoch 30 batch id 1541 loss 0.018550531938672066 train acc 0.9737994808565866\n",
            "epoch 30 batch id 1551 loss 0.07273335754871368 train acc 0.9737971470019342\n",
            "epoch 30 batch id 1561 loss 0.07015033811330795 train acc 0.9737848334401025\n",
            "epoch 30 batch id 1571 loss 0.012627281248569489 train acc 0.9737826225334182\n",
            "epoch 30 batch id 1581 loss 0.028575889766216278 train acc 0.9737507906388362\n",
            "epoch 30 batch id 1591 loss 0.16954685747623444 train acc 0.9737782840980516\n",
            "epoch 30 batch id 1601 loss 0.062061820179224014 train acc 0.9738151936289818\n",
            "epoch 30 batch id 1611 loss 0.09281419962644577 train acc 0.9736964618249534\n",
            "epoch 30 batch id 1621 loss 0.12938736379146576 train acc 0.9736852251696484\n",
            "epoch 30 batch id 1631 loss 0.05185496062040329 train acc 0.9736549662783568\n",
            "epoch 30 batch id 1641 loss 0.10240450501441956 train acc 0.9736726843388178\n",
            "epoch 30 batch id 1651 loss 0.04660167545080185 train acc 0.9735671562689279\n",
            "epoch 30 batch id 1661 loss 0.060905907303094864 train acc 0.9736134105960265\n",
            "epoch 30 batch id 1671 loss 0.13074526190757751 train acc 0.9735843058049073\n",
            "epoch 30 batch id 1681 loss 0.16947579383850098 train acc 0.9735555472932778\n",
            "epoch 30 batch id 1691 loss 0.12194007635116577 train acc 0.9735733293908929\n",
            "epoch 30 batch id 1701 loss 0.04405456408858299 train acc 0.9736092739564962\n",
            "epoch 30 batch id 1711 loss 0.08036903291940689 train acc 0.9735717416715371\n",
            "epoch 30 batch id 1721 loss 0.013688500970602036 train acc 0.9736072777454968\n",
            "epoch 30 batch id 1731 loss 0.006319609936326742 train acc 0.973606296938186\n",
            "epoch 30 batch id 1741 loss 0.01226806640625 train acc 0.9736143021252154\n",
            "epoch 30 batch id 1751 loss 0.053422972559928894 train acc 0.9736132924043404\n",
            "epoch 30 batch id 1761 loss 0.01928284950554371 train acc 0.9736122941510506\n",
            "epoch 30 batch id 1771 loss 0.04694492369890213 train acc 0.97363777526821\n",
            "epoch 30 batch id 1781 loss 0.001939863432198763 train acc 0.9737156092083099\n",
            "epoch 30 batch id 1791 loss 0.09415075182914734 train acc 0.9737315047459519\n",
            "epoch 30 batch id 1801 loss 0.05923478677868843 train acc 0.97376457523598\n",
            "epoch 30 batch id 1811 loss 0.04804065823554993 train acc 0.9737627691882937\n",
            "epoch 30 batch id 1821 loss 0.08265124261379242 train acc 0.9737352416254805\n",
            "epoch 30 batch id 1831 loss 0.02343093603849411 train acc 0.9737592162752594\n",
            "epoch 30 batch id 1841 loss 0.05265859514474869 train acc 0.9736810836501901\n",
            "epoch 30 batch id 1851 loss 0.03874531388282776 train acc 0.9737135332252836\n",
            "epoch 30 batch id 1861 loss 0.16658063232898712 train acc 0.9737120499731328\n",
            "epoch 30 batch id 1871 loss 0.1418880820274353 train acc 0.9737606894708712\n",
            "epoch 30 batch id 1881 loss 0.02798561379313469 train acc 0.9737755847953217\n",
            "epoch 30 batch id 1891 loss 0.07578253000974655 train acc 0.9737985854045479\n",
            "epoch 30 batch id 1901 loss 0.09597817808389664 train acc 0.9738295633876907\n",
            "epoch 30 batch id 1911 loss 0.04004485905170441 train acc 0.9738356881214024\n",
            "epoch 30 batch id 1921 loss 0.004601931665092707 train acc 0.9737929463820927\n",
            "epoch 30 batch id 1931 loss 0.017834190279245377 train acc 0.9737830139823925\n",
            "epoch 30 batch id 1941 loss 0.006841663271188736 train acc 0.9737973338485317\n",
            "epoch 30 batch id 1951 loss 0.13842280209064484 train acc 0.9738115069195284\n",
            "epoch 30 batch id 1961 loss 0.04180475324392319 train acc 0.9737936639469659\n",
            "epoch 30 batch id 1971 loss 0.015975570306181908 train acc 0.973871131405378\n",
            "epoch 30 batch id 1981 loss 0.07370121777057648 train acc 0.9738926047450782\n",
            "epoch 30 batch id 1991 loss 0.11483315378427505 train acc 0.9738589276745354\n",
            "epoch 30 batch id 2001 loss 0.17727863788604736 train acc 0.9738412043978011\n",
            "epoch 30 batch id 2011 loss 0.07003608345985413 train acc 0.9738469666832422\n",
            "epoch 30 batch id 2021 loss 0.15666915476322174 train acc 0.9738217466600693\n",
            "epoch 30 batch id 2031 loss 0.001104659284465015 train acc 0.9737967749876908\n",
            "epoch 30 batch id 2041 loss 0.00776945473626256 train acc 0.9737720480156786\n",
            "epoch 30 batch id 2051 loss 0.24065889418125153 train acc 0.9737399439297904\n",
            "epoch 30 batch id 2061 loss 0.01456949021667242 train acc 0.9737688015526443\n",
            "epoch 30 batch id 2071 loss 0.07988182455301285 train acc 0.9737521125060358\n",
            "epoch 30 batch id 2081 loss 0.07437869906425476 train acc 0.9737731259010092\n",
            "epoch 30 batch id 2091 loss 0.038432519882917404 train acc 0.9737416307986609\n",
            "epoch 30 batch id 2101 loss 0.010717346332967281 train acc 0.9738071156592099\n",
            "epoch 30 batch id 2111 loss 0.01735389418900013 train acc 0.9738423732828043\n",
            "epoch 30 batch id 2121 loss 0.04483087360858917 train acc 0.9739214992927864\n",
            "epoch 30 batch id 2131 loss 0.014432480558753014 train acc 0.9739705537306429\n",
            "epoch 30 batch id 2141 loss 0.17429481446743011 train acc 0.9739461700140122\n",
            "epoch 30 batch id 2151 loss 0.17110514640808105 train acc 0.9739946536494654\n",
            "epoch 30 batch id 2161 loss 0.12732098996639252 train acc 0.9739631536325775\n",
            "epoch 30 batch id 2171 loss 0.043582733720541 train acc 0.9739391409488715\n",
            "epoch 30 batch id 2181 loss 0.09224799275398254 train acc 0.973944005043558\n",
            "epoch 30 batch id 2191 loss 0.1484941840171814 train acc 0.9739773505248744\n",
            "epoch 30 batch id 2201 loss 0.06284890323877335 train acc 0.9739819968196275\n",
            "epoch 30 batch id 2211 loss 0.04147610813379288 train acc 0.9739724672094076\n",
            "epoch 30 batch id 2221 loss 0.0680055022239685 train acc 0.9739278478162989\n",
            "epoch 30 batch id 2231 loss 0.012693597003817558 train acc 0.9739256499327655\n",
            "epoch 30 batch id 2241 loss 0.257571816444397 train acc 0.9739234716644355\n",
            "epoch 30 batch id 2251 loss 0.148231640458107 train acc 0.9739004886717014\n",
            "epoch 30 batch id 2261 loss 0.002668630564585328 train acc 0.9739329942503318\n",
            "epoch 30 batch id 2271 loss 0.009889122098684311 train acc 0.9739789740202554\n",
            "epoch 30 batch id 2281 loss 0.035111647099256516 train acc 0.9739766001753617\n",
            "epoch 30 batch id 2291 loss 0.02423286810517311 train acc 0.9739878873854212\n",
            "epoch 30 batch id 2301 loss 0.09457007795572281 train acc 0.9740194480660582\n",
            "epoch 30 batch id 2311 loss 0.06302113085985184 train acc 0.9740439744699264\n",
            "epoch 30 batch id 2321 loss 0.15044556558132172 train acc 0.9739942373976734\n",
            "epoch 30 batch id 2331 loss 0.26680490374565125 train acc 0.9739382239382239\n",
            "epoch 30 batch id 2341 loss 0.006903316825628281 train acc 0.9739694574967962\n",
            "epoch 30 batch id 2351 loss 0.13231083750724792 train acc 0.9739605487026797\n",
            "epoch 30 batch id 2361 loss 0.13052622973918915 train acc 0.973984805167302\n",
            "epoch 30 batch id 2371 loss 0.12230130285024643 train acc 0.9740022669759595\n",
            "epoch 30 batch id 2381 loss 0.0019162208773195744 train acc 0.9740392692146157\n",
            "epoch 30 batch id 2391 loss 0.08112744241952896 train acc 0.9740498222501046\n",
            "epoch 30 batch id 2401 loss 0.021716516464948654 train acc 0.9740407642648896\n",
            "epoch 30 batch id 2411 loss 0.0330607146024704 train acc 0.9740317814184986\n",
            "epoch 30 batch id 2421 loss 0.147541806101799 train acc 0.974035780669145\n",
            "epoch 30 batch id 2431 loss 0.13052232563495636 train acc 0.9740011826408885\n",
            "epoch 30 batch id 2441 loss 0.001937494263984263 train acc 0.974005274477673\n",
            "epoch 30 batch id 2451 loss 0.0025428342632949352 train acc 0.9740029579763362\n",
            "epoch 30 batch id 2461 loss 0.10580499470233917 train acc 0.9740006603006908\n",
            "epoch 30 batch id 2471 loss 0.18299119174480438 train acc 0.973979411169567\n",
            "epoch 30 batch id 2481 loss 0.14627841114997864 train acc 0.9740213119709794\n",
            "epoch 30 batch id 2491 loss 0.07180267572402954 train acc 0.9740315134484143\n",
            "epoch 30 batch id 2501 loss 0.05002262070775032 train acc 0.9739979008396641\n",
            "epoch 30 batch id 2511 loss 0.08221610635519028 train acc 0.9740330047789725\n",
            "epoch 30 batch id 2521 loss 0.0708003044128418 train acc 0.9739686632288774\n",
            "epoch 30 batch id 2531 loss 0.06542040407657623 train acc 0.9739665645989727\n",
            "epoch 30 batch id 2541 loss 0.07238364219665527 train acc 0.973952184179457\n",
            "epoch 30 batch id 2551 loss 0.05565575882792473 train acc 0.9739379165033321\n",
            "epoch 30 batch id 2561 loss 0.1220269426703453 train acc 0.9738932545880515\n",
            "epoch 30 batch id 2571 loss 0.14889982342720032 train acc 0.9739097141190198\n",
            "epoch 30 batch id 2581 loss 0.06177491694688797 train acc 0.9739018306857807\n",
            "epoch 30 batch id 2591 loss 0.024867579340934753 train acc 0.973900038595137\n",
            "epoch 30 batch id 2601 loss 0.12333845347166061 train acc 0.9739403114186851\n",
            "epoch 30 batch id 2611 loss 0.0029663066379725933 train acc 0.973974291459211\n",
            "epoch 30 batch id 2621 loss 0.05532899126410484 train acc 0.9739960892789011\n",
            "epoch 30 batch id 2631 loss 0.175101175904274 train acc 0.9739702109464082\n",
            "epoch 30 batch id 2641 loss 0.03187183290719986 train acc 0.9739918591442636\n",
            "epoch 30 batch id 2651 loss 0.002447531558573246 train acc 0.9739838740098076\n",
            "epoch 30 batch id 2661 loss 0.10393665730953217 train acc 0.9739759488913942\n",
            "epoch 30 batch id 2671 loss 0.01640336588025093 train acc 0.973997332459753\n",
            "epoch 30 batch id 2681 loss 0.09645728021860123 train acc 0.9739661040656471\n",
            "epoch 30 batch id 2691 loss 0.04247039556503296 train acc 0.9739757525083612\n",
            "epoch 30 batch id 2701 loss 0.05527652055025101 train acc 0.9739679748241392\n",
            "epoch 30 batch id 2711 loss 0.01472070999443531 train acc 0.9739429638509775\n",
            "epoch 30 batch id 2721 loss 0.018723487854003906 train acc 0.973946848585079\n",
            "epoch 30 batch id 2731 loss 0.012419228442013264 train acc 0.9739564262175028\n",
            "epoch 30 batch id 2741 loss 0.11264992505311966 train acc 0.9739146296971908\n",
            "epoch 30 batch id 2751 loss 0.0864046961069107 train acc 0.9738844965467103\n",
            "epoch 30 batch id 2761 loss 0.03558419272303581 train acc 0.9739168326693227\n",
            "epoch 30 batch id 2771 loss 0.011756489984691143 train acc 0.97393765788524\n",
            "epoch 30 batch id 2781 loss 0.18704810738563538 train acc 0.9739583333333334\n",
            "epoch 30 batch id 2791 loss 0.02401209995150566 train acc 0.9739732622715872\n",
            "epoch 30 batch id 2801 loss 0.041454482823610306 train acc 0.9739713495180293\n",
            "epoch 30 batch id 2811 loss 0.024325640872120857 train acc 0.9739972429740306\n",
            "epoch 30 batch id 2821 loss 0.18096871674060822 train acc 0.9739841811414393\n",
            "epoch 30 batch id 2831 loss 0.019874466583132744 train acc 0.9739932885906041\n",
            "epoch 30 batch id 2841 loss 0.03332377225160599 train acc 0.974007831749384\n",
            "epoch 30 batch id 2851 loss 0.05699540302157402 train acc 0.9739893896878288\n",
            "epoch 30 batch id 2861 loss 0.08373063802719116 train acc 0.9739546924152395\n",
            "epoch 30 batch id 2871 loss 0.15017765760421753 train acc 0.9739474486241727\n",
            "epoch 30 batch id 2881 loss 0.04276590421795845 train acc 0.9739348316556751\n",
            "epoch 30 batch id 2891 loss 0.008777149952948093 train acc 0.9739601349014182\n",
            "epoch 30 batch id 2901 loss 0.025332225486636162 train acc 0.9739852637021716\n",
            "epoch 30 batch id 2911 loss 0.024183163419365883 train acc 0.973983381999313\n",
            "epoch 30 batch id 2921 loss 0.013353589922189713 train acc 0.9739547672030127\n",
            "epoch 30 batch id 2931 loss 0.0106787895783782 train acc 0.9739530023882634\n",
            "epoch 30 batch id 2941 loss 0.0014493984635919333 train acc 0.973972500850051\n",
            "epoch 30 batch id 2951 loss 0.026768645271658897 train acc 0.9740077516096238\n",
            "epoch 30 batch id 2961 loss 0.07824890315532684 train acc 0.9739952718676123\n",
            "epoch 30 batch id 2971 loss 0.1951383501291275 train acc 0.9739828761359811\n",
            "epoch 30 batch id 2981 loss 0.03898107260465622 train acc 0.973970563569272\n",
            "epoch 30 batch id 2991 loss 0.04187464714050293 train acc 0.9739583333333334\n",
            "epoch 30 batch id 3001 loss 0.0015063146129250526 train acc 0.9739878373875375\n",
            "epoch 30 batch id 3011 loss 0.0202086940407753 train acc 0.9739860096313517\n",
            "epoch 30 batch id 3021 loss 0.033271800726652145 train acc 0.9739790218470705\n",
            "epoch 30 batch id 3031 loss 0.1870562732219696 train acc 0.9739463048498845\n",
            "epoch 30 batch id 3041 loss 0.24603398144245148 train acc 0.9739240792502466\n",
            "epoch 30 batch id 3051 loss 0.003143157111480832 train acc 0.973948090789905\n",
            "epoch 30 batch id 3061 loss 0.1268174946308136 train acc 0.9739617363606664\n",
            "epoch 30 batch id 3071 loss 0.0018574642017483711 train acc 0.9739752930641485\n",
            "epoch 30 batch id 3081 loss 0.18528831005096436 train acc 0.9739735475494969\n",
            "epoch 30 batch id 3091 loss 0.03652665764093399 train acc 0.9739667583306373\n",
            "epoch 30 batch id 3101 loss 0.09147768467664719 train acc 0.973939858110287\n",
            "epoch 30 batch id 3111 loss 0.21020807325839996 train acc 0.973913130826101\n",
            "epoch 30 batch id 3121 loss 0.033808089792728424 train acc 0.9739066004485741\n",
            "epoch 30 batch id 3131 loss 0.016473034396767616 train acc 0.9739450255509422\n",
            "epoch 30 batch id 3141 loss 0.1103963628411293 train acc 0.9739284861509073\n",
            "epoch 30 batch id 3151 loss 0.01916751079261303 train acc 0.9739418041891463\n",
            "epoch 30 batch id 3161 loss 0.05571173503994942 train acc 0.9739253796267004\n",
            "epoch 30 batch id 3171 loss 0.14343005418777466 train acc 0.9739238410596026\n",
            "epoch 30 batch id 3181 loss 0.027356870472431183 train acc 0.973902664256523\n",
            "epoch 30 batch id 3191 loss 0.02337898500263691 train acc 0.9739354826073331\n",
            "epoch 30 batch id 3201 loss 0.0798216238617897 train acc 0.9739388081849422\n",
            "epoch 30 batch id 3211 loss 0.08250398933887482 train acc 0.9739469791342261\n",
            "epoch 30 batch id 3221 loss 0.1916266679763794 train acc 0.9739162915243713\n",
            "epoch 30 batch id 3231 loss 0.08161521703004837 train acc 0.9739051377282575\n",
            "epoch 30 batch id 3241 loss 0.11909632384777069 train acc 0.9739085158901574\n",
            "epoch 30 batch id 3251 loss 0.10569754987955093 train acc 0.9738830359889264\n",
            "epoch 30 batch id 3261 loss 0.1466200351715088 train acc 0.9738529208831647\n",
            "epoch 30 batch id 3271 loss 0.06137630715966225 train acc 0.9738373203913177\n",
            "epoch 30 batch id 3281 loss 0.08712100237607956 train acc 0.9738122904602255\n",
            "epoch 30 batch id 3291 loss 0.03593546152114868 train acc 0.9738158994226679\n",
            "epoch 30 batch id 3301 loss 0.058622390031814575 train acc 0.9738242199333536\n",
            "epoch 30 batch id 3311 loss 0.023657094687223434 train acc 0.9738560857746904\n",
            "epoch 30 batch id 3321 loss 0.026695141568779945 train acc 0.9738642351701294\n",
            "epoch 30 batch id 3331 loss 0.05709557235240936 train acc 0.9738395001501051\n",
            "epoch 30 batch id 3341 loss 0.1223718672990799 train acc 0.9738476504040706\n",
            "epoch 30 batch id 3351 loss 0.10493792593479156 train acc 0.9738091241420471\n",
            "epoch 30 batch id 3361 loss 0.022502804175019264 train acc 0.9738312630169592\n",
            "epoch 30 batch id 3371 loss 0.11623663455247879 train acc 0.9738208246811035\n",
            "epoch 30 batch id 3381 loss 0.03312388435006142 train acc 0.9738243123336291\n",
            "epoch 30 batch id 3391 loss 0.11351680010557175 train acc 0.9738047404895311\n",
            "epoch 30 batch id 3401 loss 0.07027037441730499 train acc 0.9738082549250221\n",
            "epoch 30 batch id 3411 loss 0.1111377626657486 train acc 0.9738300718264439\n",
            "epoch 30 batch id 3421 loss 0.046114105731248856 train acc 0.9738471938029816\n",
            "epoch 30 batch id 3431 loss 0.17135317623615265 train acc 0.9738459997085398\n",
            "epoch 30 batch id 3441 loss 0.0618998259305954 train acc 0.9738584350479512\n",
            "epoch 30 batch id 3451 loss 0.03420943766832352 train acc 0.9738481599536366\n",
            "epoch 30 batch id 3461 loss 0.1053929477930069 train acc 0.9738289150534527\n",
            "epoch 30 batch id 3471 loss 0.09503164142370224 train acc 0.9738232857966004\n",
            "epoch 30 batch id 3481 loss 0.08230859041213989 train acc 0.9738221775351911\n",
            "epoch 30 batch id 3491 loss 0.005191573407500982 train acc 0.973830027212833\n",
            "epoch 30 batch id 3501 loss 0.019113615155220032 train acc 0.9738467580691231\n",
            "epoch 30 batch id 3511 loss 0.055151622742414474 train acc 0.9738322415266306\n",
            "epoch 30 batch id 3521 loss 0.11730314791202545 train acc 0.9738488710593581\n",
            "epoch 30 batch id 3531 loss 0.03185971453785896 train acc 0.9738609813084113\n",
            "epoch 30 batch id 3541 loss 0.13695666193962097 train acc 0.9738509601807399\n",
            "epoch 30 batch id 3551 loss 0.02056371420621872 train acc 0.9738277949873275\n",
            "epoch 30 batch id 3561 loss 0.041757453233003616 train acc 0.9738486380230272\n",
            "epoch 30 batch id 3571 loss 0.03288695216178894 train acc 0.9738693643237188\n",
            "epoch 30 batch id 3581 loss 0.07629051804542542 train acc 0.9738856115610165\n",
            "epoch 30 batch id 3591 loss 0.15806162357330322 train acc 0.973901768309663\n",
            "epoch 30 batch id 3601 loss 0.07058548927307129 train acc 0.9738831227436823\n",
            "epoch 30 batch id 3611 loss 0.0646846815943718 train acc 0.9739078510108004\n",
            "epoch 30 batch id 3621 loss 0.09868788719177246 train acc 0.9739065520574427\n",
            "epoch 30 batch id 3631 loss 0.028949636965990067 train acc 0.9738966538143762\n",
            "epoch 30 batch id 3641 loss 0.04470148682594299 train acc 0.9739168497665477\n",
            "epoch 30 batch id 3651 loss 0.15387238562107086 train acc 0.9738898589427554\n",
            "epoch 30 batch id 3661 loss 0.025461696088314056 train acc 0.9738630155695165\n",
            "epoch 30 batch id 3671 loss 0.17578166723251343 train acc 0.9738746254426587\n",
            "epoch 30 batch id 3681 loss 0.09393520653247833 train acc 0.9738946617766912\n",
            "epoch 30 batch id 3691 loss 0.03228368982672691 train acc 0.97390188973178\n",
            "epoch 30 batch id 3701 loss 0.03838023170828819 train acc 0.9738710821399622\n",
            "epoch 30 batch id 3711 loss 0.013168246485292912 train acc 0.9738867555914847\n",
            "epoch 30 batch id 3721 loss 0.08590548485517502 train acc 0.9738561542596076\n",
            "epoch 30 batch id 3731 loss 0.03679117187857628 train acc 0.9738675958188153\n",
            "epoch 30 batch id 3741 loss 0.04491696506738663 train acc 0.9738664461373964\n",
            "epoch 30 batch id 3751 loss 0.006285225972533226 train acc 0.973886130365236\n",
            "epoch 30 batch id 3761 loss 0.22847643494606018 train acc 0.9738724740760436\n",
            "epoch 30 batch id 3771 loss 0.056727420538663864 train acc 0.9738381728984354\n",
            "epoch 30 batch id 3781 loss 0.1661716252565384 train acc 0.9738164506744248\n",
            "epoch 30 batch id 3791 loss 0.004633594769984484 train acc 0.9738360590873121\n",
            "epoch 30 batch id 3801 loss 0.08364029228687286 train acc 0.9738021244409366\n",
            "epoch 30 batch id 3811 loss 0.06459054350852966 train acc 0.9738134675938074\n",
            "epoch 30 batch id 3821 loss 0.04847932234406471 train acc 0.9738165728866789\n",
            "epoch 30 batch id 3831 loss 0.026541316881775856 train acc 0.9738033476898982\n",
            "epoch 30 batch id 3841 loss 0.06036665663123131 train acc 0.9737861234053632\n",
            "epoch 30 batch id 3851 loss 0.016488943248987198 train acc 0.9737973902882369\n",
            "epoch 30 batch id 3861 loss 0.04195045679807663 train acc 0.9738085988085988\n",
            "epoch 30 batch id 3871 loss 0.20289117097854614 train acc 0.9738076401446655\n",
            "epoch 30 batch id 3881 loss 0.05799959599971771 train acc 0.9738187644936872\n",
            "epoch 30 batch id 3891 loss 0.05895098298788071 train acc 0.9738298316628116\n",
            "epoch 30 batch id 3901 loss 0.07507038861513138 train acc 0.9738047936426557\n",
            "epoch 30 batch id 3911 loss 0.027064312249422073 train acc 0.9738078496548197\n",
            "epoch 30 batch id 3921 loss 0.009273647330701351 train acc 0.9738308148431523\n",
            "epoch 30 batch id 3931 loss 0.029302000999450684 train acc 0.9738019905876367\n",
            "epoch 30 batch id 3941 loss 0.05053234472870827 train acc 0.9738129599086526\n",
            "epoch 30 batch id 3951 loss 0.021469280123710632 train acc 0.9738120096178182\n",
            "epoch 30 batch id 3961 loss 0.09043291211128235 train acc 0.9737992299924262\n",
            "epoch 30 batch id 3971 loss 0.1090662032365799 train acc 0.973794384286074\n",
            "epoch 30 batch id 3981 loss 0.01969888247549534 train acc 0.9737895629238885\n",
            "epoch 30 batch id 3991 loss 0.1531255841255188 train acc 0.9737965108995239\n",
            "epoch 30 batch id 4001 loss 0.08432050049304962 train acc 0.9737838977755561\n",
            "epoch 30 batch id 4011 loss 0.04479018226265907 train acc 0.9737479743206183\n",
            "epoch 30 batch id 4021 loss 0.035314250737428665 train acc 0.9737433163392191\n",
            "epoch 30 batch id 4031 loss 0.044309403747320175 train acc 0.9737503100967502\n",
            "epoch 30 batch id 4041 loss 0.12031516432762146 train acc 0.9737418027715912\n",
            "epoch 30 batch id 4051 loss 0.1159110739827156 train acc 0.9737101950135769\n",
            "epoch 30 batch id 4061 loss 0.03593996912240982 train acc 0.9737326089633095\n",
            "epoch 30 batch id 4071 loss 0.0436718612909317 train acc 0.9737549127978383\n",
            "epoch 30 batch id 4081 loss 0.023153359070420265 train acc 0.9737885934819898\n",
            "epoch 30 batch id 4091 loss 0.08533286303281784 train acc 0.9737915546321193\n",
            "epoch 30 batch id 4101 loss 0.007743271067738533 train acc 0.9738059314801268\n",
            "epoch 30 batch id 4111 loss 0.04131753370165825 train acc 0.9737860313792265\n",
            "epoch 30 batch id 4121 loss 0.012174244038760662 train acc 0.9738003518563455\n",
            "epoch 30 batch id 4131 loss 0.07307592034339905 train acc 0.9737994734931009\n",
            "epoch 30 batch id 4141 loss 0.019139086827635765 train acc 0.9738061458584882\n",
            "epoch 30 batch id 4151 loss 0.09348952025175095 train acc 0.9738203143820766\n",
            "epoch 30 batch id 4161 loss 0.055510781705379486 train acc 0.9738118841624609\n",
            "epoch 30 batch id 4171 loss 0.15013019740581512 train acc 0.973810986573963\n",
            "epoch 30 batch id 4181 loss 0.013875660486519337 train acc 0.9737914075580005\n",
            "epoch 30 batch id 4191 loss 0.29021450877189636 train acc 0.9737570090670484\n",
            "epoch 30 batch id 4201 loss 0.0654788464307785 train acc 0.9737860033325398\n",
            "epoch 30 batch id 4211 loss 0.10870163887739182 train acc 0.9737629126098314\n",
            "epoch 30 batch id 4221 loss 0.09553589671850204 train acc 0.9737510364842454\n",
            "epoch 30 batch id 4231 loss 0.04659533128142357 train acc 0.9737281375561333\n",
            "epoch 30 batch id 4241 loss 0.1444309651851654 train acc 0.9737421893421363\n",
            "epoch 30 batch id 4251 loss 0.2206505537033081 train acc 0.9737377969889438\n",
            "epoch 30 batch id 4261 loss 0.05995982885360718 train acc 0.973744426191035\n",
            "epoch 30 batch id 4271 loss 0.04069323092699051 train acc 0.9737619995317256\n",
            "epoch 30 batch id 4281 loss 0.12898817658424377 train acc 0.9737831406213502\n",
            "epoch 30 batch id 4291 loss 0.016034793108701706 train acc 0.9737750524353298\n",
            "epoch 30 batch id 4301 loss 0.058064885437488556 train acc 0.9737633689839572\n",
            "epoch 30 batch id 4311 loss 0.06215355172753334 train acc 0.9737734864300627\n",
            "epoch 30 batch id 4321 loss 0.03614700958132744 train acc 0.9737799409858829\n",
            "epoch 30 batch id 4331 loss 0.10032045841217041 train acc 0.9737791503117063\n",
            "epoch 30 batch id 4341 loss 0.027918774634599686 train acc 0.9737783632803502\n",
            "epoch 30 batch id 4351 loss 0.12586745619773865 train acc 0.973738077453459\n",
            "epoch 30 batch id 4361 loss 0.06969817727804184 train acc 0.9737588855767025\n",
            "epoch 30 batch id 4371 loss 0.05326016992330551 train acc 0.9737688743994509\n",
            "epoch 30 batch id 4381 loss 0.2107333391904831 train acc 0.9737395857110249\n",
            "epoch 30 batch id 4391 loss 0.11097557097673416 train acc 0.973731780915509\n",
            "epoch 30 batch id 4401 loss 0.12167812138795853 train acc 0.973709810270393\n",
            "epoch 30 batch id 4411 loss 0.17995980381965637 train acc 0.9737091929267739\n",
            "epoch 30 batch id 4421 loss 0.12058954685926437 train acc 0.9737191811807283\n",
            "epoch 30 batch id 4431 loss 0.022237498313188553 train acc 0.9737220717670955\n",
            "epoch 30 batch id 4441 loss 0.01580197736620903 train acc 0.9737284676874578\n",
            "epoch 30 batch id 4451 loss 0.1571662724018097 train acc 0.9737278139743878\n",
            "epoch 30 batch id 4461 loss 0.016317127272486687 train acc 0.9737411735036987\n",
            "epoch 30 batch id 4471 loss 0.14355044066905975 train acc 0.9737439890404831\n",
            "epoch 30 batch id 4481 loss 0.04206244274973869 train acc 0.9737363311760767\n",
            "epoch 30 batch id 4491 loss 0.08577462285757065 train acc 0.9737391449565799\n",
            "epoch 30 batch id 4501 loss 0.020023882389068604 train acc 0.9737315318818041\n",
            "epoch 30 batch id 4511 loss 0.25814923644065857 train acc 0.9737031700288185\n",
            "epoch 30 batch id 4521 loss 0.05286245793104172 train acc 0.973709494580845\n",
            "epoch 30 batch id 4531 loss 0.0028557218611240387 train acc 0.9737123427499448\n",
            "epoch 30 batch id 4541 loss 0.09142030030488968 train acc 0.9737151783748074\n",
            "epoch 30 batch id 4551 loss 0.13749513030052185 train acc 0.9737180015381235\n",
            "epoch 30 batch id 4561 loss 0.14944130182266235 train acc 0.9736968318351239\n",
            "epoch 30 batch id 4571 loss 0.08510445803403854 train acc 0.9736962644935463\n",
            "epoch 30 batch id 4581 loss 0.06801541894674301 train acc 0.9736854671469112\n",
            "epoch 30 batch id 4591 loss 0.14823120832443237 train acc 0.9736917338270529\n",
            "epoch 30 batch id 4601 loss 0.12446372956037521 train acc 0.9736708052597262\n",
            "epoch 30 batch id 4611 loss 0.03439825028181076 train acc 0.9736872424636738\n",
            "epoch 30 batch id 4621 loss 0.009970836341381073 train acc 0.9736833207098031\n",
            "epoch 30 batch id 4631 loss 0.1234482079744339 train acc 0.9736861638954869\n",
            "epoch 30 batch id 4641 loss 0.1537344604730606 train acc 0.9737024617539324\n",
            "epoch 30 batch id 4651 loss 0.13566668331623077 train acc 0.9737153300365513\n",
            "epoch 30 batch id 4661 loss 0.08025168627500534 train acc 0.9737348476721733\n",
            "epoch 30 batch id 4671 loss 0.008124745450913906 train acc 0.9737609719546135\n",
            "epoch 30 batch id 4681 loss 0.325045108795166 train acc 0.9737636188848536\n",
            "epoch 30 batch id 4691 loss 0.02260386385023594 train acc 0.9737829087614581\n",
            "epoch 30 batch id 4701 loss 0.1464805155992508 train acc 0.9737788502446288\n",
            "epoch 30 batch id 4711 loss 0.0649714469909668 train acc 0.9737748089577585\n",
            "epoch 30 batch id 4721 loss 0.012837765738368034 train acc 0.9737707847913578\n",
            "epoch 30 batch id 4731 loss 0.09921640157699585 train acc 0.9737535668991757\n",
            "epoch 30 batch id 4741 loss 0.10330300033092499 train acc 0.9737594916684243\n",
            "epoch 30 batch id 4751 loss 0.20304226875305176 train acc 0.9737456588086718\n",
            "epoch 30 batch id 4761 loss 0.04650299996137619 train acc 0.9737614209199748\n",
            "epoch 30 batch id 4771 loss 0.07185153663158417 train acc 0.9737705669670929\n",
            "epoch 30 batch id 4781 loss 0.0971812978386879 train acc 0.9737567977410584\n",
            "epoch 30 batch id 4791 loss 0.25557106733322144 train acc 0.9737333020246295\n",
            "epoch 30 batch id 4801 loss 0.08250334858894348 train acc 0.9737294313684649\n",
            "epoch 30 batch id 4811 loss 0.028401033952832222 train acc 0.9737418156308459\n",
            "epoch 30 batch id 4821 loss 0.1644875556230545 train acc 0.9737379433727442\n",
            "epoch 30 batch id 4831 loss 0.028797132894396782 train acc 0.9737664303456841\n",
            "epoch 30 batch id 4841 loss 0.13183091580867767 train acc 0.9737625232390003\n",
            "epoch 30 batch id 4851 loss 0.033893097192049026 train acc 0.973781179138322\n",
            "epoch 30 batch id 4861 loss 0.007617718540132046 train acc 0.9737708290475211\n",
            "epoch 30 batch id 4871 loss 0.016494223847985268 train acc 0.9737765602545678\n",
            "epoch 30 batch id 4881 loss 0.09857798367738724 train acc 0.9737822679778734\n",
            "epoch 30 batch id 4891 loss 0.06798074394464493 train acc 0.9737783684318135\n",
            "epoch 30 batch id 4901 loss 0.01602272503077984 train acc 0.9737808610487656\n",
            "epoch 30 batch id 4911 loss 0.23623917996883392 train acc 0.9737706169822846\n",
            "epoch 30 batch id 4921 loss 0.320971816778183 train acc 0.973766764885186\n",
            "epoch 30 batch id 4931 loss 0.09088604897260666 train acc 0.9737439160413709\n",
            "epoch 30 batch id 4941 loss 0.042319994419813156 train acc 0.97374329589152\n",
            "epoch 30 batch id 4951 loss 0.1368146538734436 train acc 0.9737363663906281\n",
            "epoch 30 train acc 0.973739787169659\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b885e4c1df4bbca01c9d61fb7f6f61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 30 loss 0.667165219783783 test acc 0.841131323313783\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ab5efc7d7dc40e9a2d938629424dc99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 31 batch id 1 loss 0.09356139600276947 train acc 0.984375\n",
            "epoch 31 batch id 11 loss 0.09201304614543915 train acc 0.9744318181818182\n",
            "epoch 31 batch id 21 loss 0.019454902037978172 train acc 0.9769345238095238\n",
            "epoch 31 batch id 31 loss 0.12533362209796906 train acc 0.9742943548387096\n",
            "epoch 31 batch id 41 loss 0.022648466750979424 train acc 0.9752286585365854\n",
            "epoch 31 batch id 51 loss 0.08684271574020386 train acc 0.9745710784313726\n",
            "epoch 31 batch id 61 loss 0.06102512776851654 train acc 0.9718237704918032\n",
            "epoch 31 batch id 71 loss 0.16794538497924805 train acc 0.9722711267605634\n",
            "epoch 31 batch id 81 loss 0.20345550775527954 train acc 0.9718364197530864\n",
            "epoch 31 batch id 91 loss 0.08860871195793152 train acc 0.9720123626373627\n",
            "epoch 31 batch id 101 loss 0.043243106454610825 train acc 0.9716893564356436\n",
            "epoch 31 batch id 111 loss 0.001161331427283585 train acc 0.971706081081081\n",
            "epoch 31 batch id 121 loss 0.04313769191503525 train acc 0.9728822314049587\n",
            "epoch 31 batch id 131 loss 0.10022764652967453 train acc 0.9725667938931297\n",
            "epoch 31 batch id 141 loss 0.004898793995380402 train acc 0.9730718085106383\n",
            "epoch 31 batch id 151 loss 0.061011143028736115 train acc 0.9733029801324503\n",
            "epoch 31 batch id 161 loss 0.08193743228912354 train acc 0.9737965838509317\n",
            "epoch 31 batch id 171 loss 0.03981466591358185 train acc 0.9740497076023392\n",
            "epoch 31 batch id 181 loss 0.07328600436449051 train acc 0.9744475138121547\n",
            "epoch 31 batch id 191 loss 0.038590606302022934 train acc 0.9742310209424084\n",
            "epoch 31 batch id 201 loss 0.08464217185974121 train acc 0.9741915422885572\n",
            "epoch 31 batch id 211 loss 0.014874850399792194 train acc 0.9741558056872038\n",
            "epoch 31 batch id 221 loss 0.0777868703007698 train acc 0.9739819004524887\n",
            "epoch 31 batch id 231 loss 0.02793748676776886 train acc 0.9744318181818182\n",
            "epoch 31 batch id 241 loss 0.0877070501446724 train acc 0.9743257261410788\n",
            "epoch 31 batch id 251 loss 0.13437306880950928 train acc 0.9744148406374502\n",
            "epoch 31 batch id 261 loss 0.08706719428300858 train acc 0.9744971264367817\n",
            "epoch 31 batch id 271 loss 0.026352424174547195 train acc 0.9746886531365314\n",
            "epoch 31 batch id 281 loss 0.008854197338223457 train acc 0.9749777580071174\n",
            "epoch 31 batch id 291 loss 0.0555802658200264 train acc 0.9750322164948454\n",
            "epoch 31 batch id 301 loss 0.08191008120775223 train acc 0.9749273255813954\n",
            "epoch 31 batch id 311 loss 0.023402702063322067 train acc 0.9752813504823151\n",
            "epoch 31 batch id 321 loss 0.1307569444179535 train acc 0.9752725856697819\n",
            "epoch 31 batch id 331 loss 0.07242127507925034 train acc 0.9752171450151057\n",
            "epoch 31 batch id 341 loss 0.3039701282978058 train acc 0.9751649560117303\n",
            "epoch 31 batch id 351 loss 0.05389059707522392 train acc 0.9752938034188035\n",
            "epoch 31 batch id 361 loss 0.05574577674269676 train acc 0.975415512465374\n",
            "epoch 31 batch id 371 loss 0.04980042949318886 train acc 0.9756991239892183\n",
            "epoch 31 batch id 381 loss 0.03558710217475891 train acc 0.9759678477690289\n",
            "epoch 31 batch id 391 loss 0.141571044921875 train acc 0.9759031329923273\n",
            "epoch 31 batch id 401 loss 0.0005269022076390684 train acc 0.9759195760598504\n",
            "epoch 31 batch id 411 loss 0.0013035096926614642 train acc 0.9760872871046229\n",
            "epoch 31 batch id 421 loss 0.10276415944099426 train acc 0.9759872327790974\n",
            "epoch 31 batch id 431 loss 0.08195821195840836 train acc 0.9757830626450116\n",
            "epoch 31 batch id 441 loss 0.10023529827594757 train acc 0.9756944444444444\n",
            "epoch 31 batch id 451 loss 0.02882353588938713 train acc 0.9758176274944568\n",
            "epoch 31 batch id 461 loss 0.012372525408864021 train acc 0.97566431670282\n",
            "epoch 31 batch id 471 loss 0.046009596437215805 train acc 0.9759156050955414\n",
            "epoch 31 batch id 481 loss 0.07304694503545761 train acc 0.9758965696465697\n",
            "epoch 31 batch id 491 loss 0.28711673617362976 train acc 0.9756873727087576\n",
            "epoch 31 batch id 501 loss 0.12937656044960022 train acc 0.9757672155688623\n",
            "epoch 31 batch id 511 loss 0.10864449292421341 train acc 0.9756298923679061\n",
            "epoch 31 batch id 521 loss 0.10176999121904373 train acc 0.975797744721689\n",
            "epoch 31 batch id 531 loss 0.02738996222615242 train acc 0.9756944444444444\n",
            "epoch 31 batch id 541 loss 0.04580732434988022 train acc 0.9755660813308688\n",
            "epoch 31 batch id 551 loss 0.10888923704624176 train acc 0.9754990925589837\n",
            "epoch 31 batch id 561 loss 0.0881391242146492 train acc 0.9755180481283422\n",
            "epoch 31 batch id 571 loss 0.06112028285861015 train acc 0.9754268826619965\n",
            "epoch 31 batch id 581 loss 0.053343433886766434 train acc 0.975473321858864\n",
            "epoch 31 batch id 591 loss 0.03308340534567833 train acc 0.9754917512690355\n",
            "epoch 31 batch id 601 loss 0.024314673617482185 train acc 0.9755355657237936\n",
            "epoch 31 batch id 611 loss 0.0481913797557354 train acc 0.9754500818330606\n",
            "epoch 31 batch id 621 loss 0.08243820071220398 train acc 0.9755434782608695\n",
            "epoch 31 batch id 631 loss 0.08368529379367828 train acc 0.9755843898573693\n",
            "epoch 31 batch id 641 loss 0.022809959948062897 train acc 0.9756484009360374\n",
            "epoch 31 batch id 651 loss 0.10789448767900467 train acc 0.9755184331797235\n",
            "epoch 31 batch id 661 loss 0.01118666771799326 train acc 0.9756287821482602\n",
            "epoch 31 batch id 671 loss 0.04065881296992302 train acc 0.9756892697466468\n",
            "epoch 31 batch id 681 loss 0.11853228509426117 train acc 0.9756791483113069\n",
            "epoch 31 batch id 691 loss 0.03306371718645096 train acc 0.9755788712011577\n",
            "epoch 31 batch id 701 loss 0.07121091336011887 train acc 0.9755260342368046\n",
            "epoch 31 batch id 711 loss 0.16898204386234283 train acc 0.9754966596343179\n",
            "epoch 31 batch id 721 loss 0.11763212084770203 train acc 0.9753380721220527\n",
            "epoch 31 batch id 731 loss 0.035515956580638885 train acc 0.9754189466484268\n",
            "epoch 31 batch id 741 loss 0.01560463011264801 train acc 0.9754554655870445\n",
            "epoch 31 batch id 751 loss 0.0455191396176815 train acc 0.9754077896138482\n",
            "epoch 31 batch id 761 loss 0.014779480174183846 train acc 0.9753203022339028\n",
            "epoch 31 batch id 771 loss 0.11188452690839767 train acc 0.9752756160830091\n",
            "epoch 31 batch id 781 loss 0.01906026341021061 train acc 0.9752320742637645\n",
            "epoch 31 batch id 791 loss 0.02172110415995121 train acc 0.9753279077117573\n",
            "epoch 31 batch id 801 loss 0.014103308320045471 train acc 0.9753628277153558\n",
            "epoch 31 batch id 811 loss 0.08839955925941467 train acc 0.9754739519112207\n",
            "epoch 31 batch id 821 loss 0.03644319623708725 train acc 0.9755252740560292\n",
            "epoch 31 batch id 831 loss 0.11177145689725876 train acc 0.9755001504211793\n",
            "epoch 31 batch id 841 loss 0.08532910794019699 train acc 0.9755127824019025\n",
            "epoch 31 batch id 851 loss 0.053881049156188965 train acc 0.9754516745005876\n",
            "epoch 31 batch id 861 loss 0.05200245603919029 train acc 0.975428281068525\n",
            "epoch 31 batch id 871 loss 0.00977710448205471 train acc 0.9755130597014925\n",
            "epoch 31 batch id 881 loss 0.02669498138129711 train acc 0.9754895005675369\n",
            "epoch 31 batch id 891 loss 0.06436453014612198 train acc 0.9755190796857464\n",
            "epoch 31 batch id 901 loss 0.04041343927383423 train acc 0.9755306603773585\n",
            "epoch 31 batch id 911 loss 0.041892554610967636 train acc 0.97555913830955\n",
            "epoch 31 batch id 921 loss 0.01090783067047596 train acc 0.9755191368078175\n",
            "epoch 31 batch id 931 loss 0.02142111212015152 train acc 0.9753960794844253\n",
            "epoch 31 batch id 941 loss 0.023940207436680794 train acc 0.9754914984059511\n",
            "epoch 31 batch id 951 loss 0.082536481320858 train acc 0.9753713196635121\n",
            "epoch 31 batch id 961 loss 0.07510340213775635 train acc 0.9753349375650364\n",
            "epoch 31 batch id 971 loss 0.1627531200647354 train acc 0.975347579814624\n",
            "epoch 31 batch id 981 loss 0.10188847035169601 train acc 0.9753121814475025\n",
            "epoch 31 batch id 991 loss 0.07800909876823425 train acc 0.9753405650857719\n",
            "epoch 31 batch id 1001 loss 0.03508167341351509 train acc 0.9754464285714286\n",
            "epoch 31 batch id 1011 loss 0.009261912666261196 train acc 0.9754574678536103\n",
            "epoch 31 batch id 1021 loss 0.033394671976566315 train acc 0.9755601126346719\n",
            "epoch 31 batch id 1031 loss 0.023969097062945366 train acc 0.9756001454898157\n",
            "epoch 31 batch id 1041 loss 0.15637391805648804 train acc 0.9755793707973103\n",
            "epoch 31 batch id 1051 loss 0.0912206619977951 train acc 0.9753954567078973\n",
            "epoch 31 batch id 1061 loss 0.033998601138591766 train acc 0.9753328228086711\n",
            "epoch 31 batch id 1071 loss 0.004079377744346857 train acc 0.975358893557423\n",
            "epoch 31 batch id 1081 loss 0.11002745479345322 train acc 0.9753700277520814\n",
            "epoch 31 batch id 1091 loss 0.13083061575889587 train acc 0.9753523143904674\n",
            "epoch 31 batch id 1101 loss 0.05241537094116211 train acc 0.9753633060853769\n",
            "epoch 31 batch id 1111 loss 0.07909231632947922 train acc 0.9752615886588659\n",
            "epoch 31 batch id 1121 loss 0.06303617358207703 train acc 0.9753707627118644\n",
            "epoch 31 batch id 1131 loss 0.02774541825056076 train acc 0.9753122236958444\n",
            "epoch 31 batch id 1141 loss 0.1347344070672989 train acc 0.9752684049079755\n",
            "epoch 31 batch id 1151 loss 0.03837866336107254 train acc 0.9751981972198088\n",
            "epoch 31 batch id 1161 loss 0.06706107407808304 train acc 0.9751561154177433\n",
            "epoch 31 batch id 1171 loss 0.012073749676346779 train acc 0.9751014090520922\n",
            "epoch 31 batch id 1181 loss 0.00991298072040081 train acc 0.9751799322607959\n",
            "epoch 31 batch id 1191 loss 0.01792178489267826 train acc 0.9752046599496221\n",
            "epoch 31 batch id 1201 loss 0.09192647784948349 train acc 0.9752419858451291\n",
            "epoch 31 batch id 1211 loss 0.000549451622646302 train acc 0.9752141824938068\n",
            "epoch 31 batch id 1221 loss 0.08511456847190857 train acc 0.9751996314496314\n",
            "epoch 31 batch id 1231 loss 0.1295035183429718 train acc 0.9751472380178716\n",
            "epoch 31 batch id 1241 loss 0.014068919233977795 train acc 0.9751838235294118\n",
            "epoch 31 batch id 1251 loss 0.29747968912124634 train acc 0.9751823541167066\n",
            "epoch 31 batch id 1261 loss 0.020718520507216454 train acc 0.9751809080095163\n",
            "epoch 31 batch id 1271 loss 0.14035464823246002 train acc 0.9750688434303698\n",
            "epoch 31 batch id 1281 loss 0.08361422270536423 train acc 0.9750927010148321\n",
            "epoch 31 batch id 1291 loss 0.0700245127081871 train acc 0.9751646010844307\n",
            "epoch 31 batch id 1301 loss 0.0037790238857269287 train acc 0.9750792659492697\n",
            "epoch 31 batch id 1311 loss 0.12154214084148407 train acc 0.9750667429443173\n",
            "epoch 31 batch id 1321 loss 0.01946406252682209 train acc 0.9750070968962907\n",
            "epoch 31 batch id 1331 loss 0.17685364186763763 train acc 0.974948347107438\n",
            "epoch 31 batch id 1341 loss 0.2013433873653412 train acc 0.9749487322893363\n",
            "epoch 31 batch id 1351 loss 0.08445912599563599 train acc 0.9749375462620281\n",
            "epoch 31 batch id 1361 loss 0.0710965245962143 train acc 0.9748920830271859\n",
            "epoch 31 batch id 1371 loss 0.04330877214670181 train acc 0.974881473377097\n",
            "epoch 31 batch id 1381 loss 0.037707820534706116 train acc 0.9749275887038378\n",
            "epoch 31 batch id 1391 loss 0.01253003440797329 train acc 0.974871944644141\n",
            "epoch 31 batch id 1401 loss 0.013568665832281113 train acc 0.9748728586723768\n",
            "epoch 31 batch id 1411 loss 0.019515275955200195 train acc 0.974906980864635\n",
            "epoch 31 batch id 1421 loss 0.08901920169591904 train acc 0.9749076354679803\n",
            "epoch 31 batch id 1431 loss 0.15479056537151337 train acc 0.974864605171209\n",
            "epoch 31 batch id 1441 loss 0.0648908019065857 train acc 0.9748221721027065\n",
            "epoch 31 batch id 1451 loss 0.1420242190361023 train acc 0.9748987767057202\n",
            "epoch 31 batch id 1461 loss 0.01010256540030241 train acc 0.97489946954141\n",
            "epoch 31 batch id 1471 loss 0.0347268171608448 train acc 0.9749532630863358\n",
            "epoch 31 batch id 1481 loss 0.052365493029356 train acc 0.9749008271438218\n",
            "epoch 31 batch id 1491 loss 0.10498420149087906 train acc 0.9749014922870557\n",
            "epoch 31 batch id 1501 loss 0.06399539858102798 train acc 0.974975016655563\n",
            "epoch 31 batch id 1511 loss 0.028851285576820374 train acc 0.9750062045003309\n",
            "epoch 31 batch id 1521 loss 0.0006784768193028867 train acc 0.9751088921761999\n",
            "epoch 31 batch id 1531 loss 0.023064086213707924 train acc 0.9751081809274984\n",
            "epoch 31 batch id 1541 loss 0.013271083123981953 train acc 0.9750973393900065\n",
            "epoch 31 batch id 1551 loss 0.041563503444194794 train acc 0.975086637653127\n",
            "epoch 31 batch id 1561 loss 0.21903736889362335 train acc 0.9750660634208841\n",
            "epoch 31 batch id 1571 loss 0.013548702001571655 train acc 0.9750855346912795\n",
            "epoch 31 batch id 1581 loss 0.03598517179489136 train acc 0.9750256957621758\n",
            "epoch 31 batch id 1591 loss 0.15674908459186554 train acc 0.9750353551225645\n",
            "epoch 31 batch id 1601 loss 0.13409382104873657 train acc 0.9750058557151781\n",
            "epoch 31 batch id 1611 loss 0.1017174944281578 train acc 0.9750058193668529\n",
            "epoch 31 batch id 1621 loss 0.047139428555965424 train acc 0.9749672270203578\n",
            "epoch 31 batch id 1631 loss 0.00566503033041954 train acc 0.974996167995095\n",
            "epoch 31 batch id 1641 loss 0.0869656577706337 train acc 0.9750342778793418\n",
            "epoch 31 batch id 1651 loss 0.05081428587436676 train acc 0.9749962144155058\n",
            "epoch 31 batch id 1661 loss 0.25101760029792786 train acc 0.974968016255268\n",
            "epoch 31 batch id 1671 loss 0.14556501805782318 train acc 0.9749401555954518\n",
            "epoch 31 batch id 1681 loss 0.13022194802761078 train acc 0.9749033313503866\n",
            "epoch 31 batch id 1691 loss 0.15256844460964203 train acc 0.974922383205204\n",
            "epoch 31 batch id 1701 loss 0.12279092520475388 train acc 0.9749228395061729\n",
            "epoch 31 batch id 1711 loss 0.038702551275491714 train acc 0.9748411016949152\n",
            "epoch 31 batch id 1721 loss 0.060643281787633896 train acc 0.9748238669378269\n",
            "epoch 31 batch id 1731 loss 0.001503754174336791 train acc 0.974761698440208\n",
            "epoch 31 batch id 1741 loss 0.0554482527077198 train acc 0.9747630672027571\n",
            "epoch 31 batch id 1751 loss 0.05866702273488045 train acc 0.9747376499143346\n",
            "epoch 31 batch id 1761 loss 0.07056118547916412 train acc 0.9746770300965361\n",
            "epoch 31 batch id 1771 loss 0.002444107783958316 train acc 0.9747053218520609\n",
            "epoch 31 batch id 1781 loss 0.04122939333319664 train acc 0.9747157495788883\n",
            "epoch 31 batch id 1791 loss 0.06062093377113342 train acc 0.9747173366834171\n",
            "epoch 31 batch id 1801 loss 0.015729166567325592 train acc 0.9747015546918378\n",
            "epoch 31 batch id 1811 loss 0.04031270369887352 train acc 0.9747032026504694\n",
            "epoch 31 batch id 1821 loss 0.10048405081033707 train acc 0.9746533498077979\n",
            "epoch 31 batch id 1831 loss 0.02285129763185978 train acc 0.9746808438012016\n",
            "epoch 31 batch id 1841 loss 0.0007866169908083975 train acc 0.974648628462792\n",
            "epoch 31 batch id 1851 loss 0.09370215237140656 train acc 0.9746336439762291\n",
            "epoch 31 batch id 1861 loss 0.03138644993305206 train acc 0.9746524046211714\n",
            "epoch 31 batch id 1871 loss 0.08935239911079407 train acc 0.9746709647247461\n",
            "epoch 31 batch id 1881 loss 0.13761460781097412 train acc 0.9746893274853801\n",
            "epoch 31 batch id 1891 loss 0.06182538717985153 train acc 0.9747157588577472\n",
            "epoch 31 batch id 1901 loss 0.15536266565322876 train acc 0.9747419121514992\n",
            "epoch 31 batch id 1911 loss 0.15183529257774353 train acc 0.9747432626896912\n",
            "epoch 31 batch id 1921 loss 0.008431121706962585 train acc 0.974728331598126\n",
            "epoch 31 batch id 1931 loss 0.1597629189491272 train acc 0.974745921802175\n",
            "epoch 31 batch id 1941 loss 0.0016002581687644124 train acc 0.9747552807831015\n",
            "epoch 31 batch id 1951 loss 0.16909661889076233 train acc 0.9747645438236802\n",
            "epoch 31 batch id 1961 loss 0.11976243555545807 train acc 0.9748294875063743\n",
            "epoch 31 batch id 1971 loss 0.01461542584002018 train acc 0.9748541349568747\n",
            "epoch 31 batch id 1981 loss 0.03398672863841057 train acc 0.9748548712771328\n",
            "epoch 31 batch id 1991 loss 0.1375620812177658 train acc 0.9748477523857358\n",
            "epoch 31 batch id 2001 loss 0.08067994564771652 train acc 0.9748485132433783\n",
            "epoch 31 batch id 2011 loss 0.07326172292232513 train acc 0.9748803455992043\n",
            "epoch 31 batch id 2021 loss 0.13189342617988586 train acc 0.9748886689757545\n",
            "epoch 31 batch id 2031 loss 0.0022906321100890636 train acc 0.9748815238798622\n",
            "epoch 31 batch id 2041 loss 0.029863057658076286 train acc 0.9749203821656051\n",
            "epoch 31 batch id 2051 loss 0.240060493350029 train acc 0.9748598244758654\n",
            "epoch 31 batch id 2061 loss 0.021854955703020096 train acc 0.9748377607957303\n",
            "epoch 31 batch id 2071 loss 0.10012206435203552 train acc 0.9748536335103815\n",
            "epoch 31 batch id 2081 loss 0.08150439709424973 train acc 0.9748092864007689\n",
            "epoch 31 batch id 2091 loss 0.07125892490148544 train acc 0.9748326159732186\n",
            "epoch 31 batch id 2101 loss 0.0038577525410801172 train acc 0.9748705973346026\n",
            "epoch 31 batch id 2111 loss 0.010469282977283001 train acc 0.9749082188536239\n",
            "epoch 31 batch id 2121 loss 0.13966654241085052 train acc 0.9749086515794436\n",
            "epoch 31 batch id 2131 loss 0.044451624155044556 train acc 0.97498973486626\n",
            "epoch 31 batch id 2141 loss 0.09406709671020508 train acc 0.974967888836992\n",
            "epoch 31 batch id 2151 loss 0.012877512723207474 train acc 0.9749970943747094\n",
            "epoch 31 batch id 2161 loss 0.07717941701412201 train acc 0.9749898773715873\n",
            "epoch 31 batch id 2171 loss 0.0027914538513869047 train acc 0.9749827268539843\n",
            "epoch 31 batch id 2181 loss 0.07375582307577133 train acc 0.9749183287482807\n",
            "epoch 31 batch id 2191 loss 0.04459807276725769 train acc 0.9749400958466453\n",
            "epoch 31 batch id 2201 loss 0.029084520414471626 train acc 0.9749474670604271\n",
            "epoch 31 batch id 2211 loss 0.001150749740190804 train acc 0.974983039348711\n",
            "epoch 31 batch id 2221 loss 0.04506661742925644 train acc 0.97495497523638\n",
            "epoch 31 batch id 2231 loss 0.011829639784991741 train acc 0.9749551770506499\n",
            "epoch 31 batch id 2241 loss 0.11501024663448334 train acc 0.9749135430611334\n",
            "epoch 31 batch id 2251 loss 0.022768085822463036 train acc 0.9749000444247001\n",
            "epoch 31 batch id 2261 loss 0.14969734847545624 train acc 0.9749281291463954\n",
            "epoch 31 batch id 2271 loss 0.055871106684207916 train acc 0.9749146851607221\n",
            "epoch 31 batch id 2281 loss 0.032381098717451096 train acc 0.9749219092503288\n",
            "epoch 31 batch id 2291 loss 0.023280154913663864 train acc 0.9749563509384548\n",
            "epoch 31 batch id 2301 loss 0.03664740175008774 train acc 0.9749701216862234\n",
            "epoch 31 batch id 2311 loss 0.11913628876209259 train acc 0.9750108178277802\n",
            "epoch 31 batch id 2321 loss 0.13421523571014404 train acc 0.9749838431710469\n",
            "epoch 31 batch id 2331 loss 0.14223194122314453 train acc 0.9749235842985843\n",
            "epoch 31 batch id 2341 loss 0.013484659604728222 train acc 0.9749305852199914\n",
            "epoch 31 batch id 2351 loss 0.02095874771475792 train acc 0.9748777116120799\n",
            "epoch 31 batch id 2361 loss 0.19407539069652557 train acc 0.9748782295637441\n",
            "epoch 31 batch id 2371 loss 0.07097527384757996 train acc 0.9749051033319275\n",
            "epoch 31 batch id 2381 loss 0.0006778044044040143 train acc 0.9749711255774884\n",
            "epoch 31 batch id 2391 loss 0.07661871612071991 train acc 0.9749712463404433\n",
            "epoch 31 batch id 2401 loss 0.12145538628101349 train acc 0.9749973969179508\n",
            "epoch 31 batch id 2411 loss 0.14120854437351227 train acc 0.9749326005806719\n",
            "epoch 31 batch id 2421 loss 0.0343567319214344 train acc 0.9749651486988847\n",
            "epoch 31 batch id 2431 loss 0.059665270149707794 train acc 0.9749845742492801\n",
            "epoch 31 batch id 2441 loss 0.004757043439894915 train acc 0.9749718353133962\n",
            "epoch 31 batch id 2451 loss 0.0033559526782482862 train acc 0.9749719502243982\n",
            "epoch 31 batch id 2461 loss 0.07184891402721405 train acc 0.9749784132466477\n",
            "epoch 31 batch id 2471 loss 0.15520595014095306 train acc 0.9749468838526912\n",
            "epoch 31 batch id 2481 loss 0.017074279487133026 train acc 0.9749974808544941\n",
            "epoch 31 batch id 2491 loss 0.2108294665813446 train acc 0.9749473103171417\n",
            "epoch 31 batch id 2501 loss 0.007799670100212097 train acc 0.9749787584966013\n",
            "epoch 31 batch id 2511 loss 0.01795795001089573 train acc 0.9750161788132218\n",
            "epoch 31 batch id 2521 loss 0.026141729205846786 train acc 0.9749975208250694\n",
            "epoch 31 batch id 2531 loss 0.041675932705402374 train acc 0.9749851837218491\n",
            "epoch 31 batch id 2541 loss 0.0303727425634861 train acc 0.9749852420306966\n",
            "epoch 31 batch id 2551 loss 0.08619725704193115 train acc 0.9749424245393963\n",
            "epoch 31 batch id 2561 loss 0.10501042008399963 train acc 0.9749304470909801\n",
            "epoch 31 batch id 2571 loss 0.15260808169841766 train acc 0.9749003306106573\n",
            "epoch 31 batch id 2581 loss 0.050674159079790115 train acc 0.9749067706315382\n",
            "epoch 31 batch id 2591 loss 0.12387147545814514 train acc 0.9748950694712466\n",
            "epoch 31 batch id 2601 loss 0.11162430793046951 train acc 0.9749375240292195\n",
            "epoch 31 batch id 2611 loss 0.006348020397126675 train acc 0.9749736690923018\n",
            "epoch 31 batch id 2621 loss 0.04272418096661568 train acc 0.9749856924837849\n",
            "epoch 31 batch id 2631 loss 0.1159282699227333 train acc 0.974956052831623\n",
            "epoch 31 batch id 2641 loss 0.11688148975372314 train acc 0.9749325539568345\n",
            "epoch 31 batch id 2651 loss 0.00582647230476141 train acc 0.9749328083741984\n",
            "epoch 31 batch id 2661 loss 0.1051812693476677 train acc 0.9749330608793687\n",
            "epoch 31 batch id 2671 loss 0.17340467870235443 train acc 0.9748982122800449\n",
            "epoch 31 batch id 2681 loss 0.0319489948451519 train acc 0.974886935844834\n",
            "epoch 31 batch id 2691 loss 0.038729678839445114 train acc 0.9748583240431067\n",
            "epoch 31 batch id 2701 loss 0.16112853586673737 train acc 0.9748530636801185\n",
            "epoch 31 batch id 2711 loss 0.013735290616750717 train acc 0.9748132607893766\n",
            "epoch 31 batch id 2721 loss 0.05357497185468674 train acc 0.9747967199558986\n",
            "epoch 31 batch id 2731 loss 0.010840288363397121 train acc 0.974763136213841\n",
            "epoch 31 batch id 2741 loss 0.1485229879617691 train acc 0.9747525994162715\n",
            "epoch 31 batch id 2751 loss 0.09116102010011673 train acc 0.9747250999636495\n",
            "epoch 31 batch id 2761 loss 0.10937833786010742 train acc 0.9747543915248098\n",
            "epoch 31 batch id 2771 loss 0.08973688632249832 train acc 0.9747665553951642\n",
            "epoch 31 batch id 2781 loss 0.10844362527132034 train acc 0.9748011057173679\n",
            "epoch 31 batch id 2791 loss 0.19179126620292664 train acc 0.9748242117520602\n",
            "epoch 31 batch id 2801 loss 0.09996703267097473 train acc 0.9748415744377008\n",
            "epoch 31 batch id 2811 loss 0.03807084262371063 train acc 0.9748643721095696\n",
            "epoch 31 batch id 2821 loss 0.027916274964809418 train acc 0.9748870081531372\n",
            "epoch 31 batch id 2831 loss 0.022623391821980476 train acc 0.9749039650300247\n",
            "epoch 31 batch id 2841 loss 0.03009989485144615 train acc 0.9748713041182682\n",
            "epoch 31 batch id 2851 loss 0.074685238301754 train acc 0.9748607944580849\n",
            "epoch 31 batch id 2861 loss 0.062436603009700775 train acc 0.9748558196434813\n",
            "epoch 31 batch id 2871 loss 0.1407945603132248 train acc 0.9748617641936608\n",
            "epoch 31 batch id 2881 loss 0.052903998643159866 train acc 0.9748676674765706\n",
            "epoch 31 batch id 2891 loss 0.2189473956823349 train acc 0.9748627205119336\n",
            "epoch 31 batch id 2901 loss 0.01111061591655016 train acc 0.974852421578766\n",
            "epoch 31 batch id 2911 loss 0.04646160081028938 train acc 0.9748421934043284\n",
            "epoch 31 batch id 2921 loss 0.000593582633882761 train acc 0.9748373844573777\n",
            "epoch 31 batch id 2931 loss 0.032112907618284225 train acc 0.9748432702149437\n",
            "epoch 31 batch id 2941 loss 0.07460905611515045 train acc 0.974817239034342\n",
            "epoch 31 batch id 2951 loss 0.06972452998161316 train acc 0.9748549220603185\n",
            "epoch 31 batch id 2961 loss 0.018468309193849564 train acc 0.9748870736237758\n",
            "epoch 31 batch id 2971 loss 0.04384685680270195 train acc 0.9748821945472905\n",
            "epoch 31 batch id 2981 loss 0.07834640890359879 train acc 0.9748616236162362\n",
            "epoch 31 batch id 2991 loss 0.06968596577644348 train acc 0.9748725342694751\n",
            "epoch 31 batch id 3001 loss 0.001744925044476986 train acc 0.9748885788070644\n",
            "epoch 31 batch id 3011 loss 0.11089324206113815 train acc 0.9748941381600797\n",
            "epoch 31 batch id 3021 loss 0.052963972091674805 train acc 0.9748738000662033\n",
            "epoch 31 batch id 3031 loss 0.12275879085063934 train acc 0.9748793714945563\n",
            "epoch 31 batch id 3041 loss 0.1963883489370346 train acc 0.9748746300559027\n",
            "epoch 31 batch id 3051 loss 0.024644628167152405 train acc 0.9748647984267453\n",
            "epoch 31 batch id 3061 loss 0.1460789442062378 train acc 0.974860135576609\n",
            "epoch 31 batch id 3071 loss 0.0006421132129617035 train acc 0.9748402393357213\n",
            "epoch 31 batch id 3081 loss 0.061449892818927765 train acc 0.974845829276209\n",
            "epoch 31 batch id 3091 loss 0.0439470037817955 train acc 0.9748362180524103\n",
            "epoch 31 batch id 3101 loss 0.07021386176347733 train acc 0.974806514027733\n",
            "epoch 31 batch id 3111 loss 0.05744820088148117 train acc 0.9747870459659274\n",
            "epoch 31 batch id 3121 loss 0.05892300233244896 train acc 0.974797741108619\n",
            "epoch 31 batch id 3131 loss 0.006206205580383539 train acc 0.9748083679335675\n",
            "epoch 31 batch id 3141 loss 0.12480540573596954 train acc 0.9748089780324737\n",
            "epoch 31 batch id 3151 loss 0.008922460488975048 train acc 0.9748095842589654\n",
            "epoch 31 batch id 3161 loss 0.0281484667211771 train acc 0.9748151297057893\n",
            "epoch 31 batch id 3171 loss 0.021864503622055054 train acc 0.9748157127089246\n",
            "epoch 31 batch id 3181 loss 0.03326563909649849 train acc 0.9748212040238918\n",
            "epoch 31 batch id 3191 loss 0.03030889853835106 train acc 0.9748266609213413\n",
            "epoch 31 batch id 3201 loss 0.09119132161140442 train acc 0.974817439862543\n",
            "epoch 31 batch id 3211 loss 0.1456969678401947 train acc 0.9747936779819371\n",
            "epoch 31 batch id 3221 loss 0.22837209701538086 train acc 0.974745808755045\n",
            "epoch 31 batch id 3231 loss 0.10057955980300903 train acc 0.9747369235530795\n",
            "epoch 31 batch id 3241 loss 0.1531151682138443 train acc 0.9747521983955569\n",
            "epoch 31 batch id 3251 loss 0.04698904603719711 train acc 0.9747433482005536\n",
            "epoch 31 batch id 3261 loss 0.1141020879149437 train acc 0.9747153863845446\n",
            "epoch 31 batch id 3271 loss 0.024151798337697983 train acc 0.9746971491898502\n",
            "epoch 31 batch id 3281 loss 0.12422201037406921 train acc 0.974698072234075\n",
            "epoch 31 batch id 3291 loss 0.009980713948607445 train acc 0.9747037374658158\n",
            "epoch 31 batch id 3301 loss 0.025348637253046036 train acc 0.9747046349591033\n",
            "epoch 31 batch id 3311 loss 0.0262922216206789 train acc 0.9747149652672908\n",
            "epoch 31 batch id 3321 loss 0.01607503369450569 train acc 0.9747205284552846\n",
            "epoch 31 batch id 3331 loss 0.031033840030431747 train acc 0.9747072951065746\n",
            "epoch 31 batch id 3341 loss 0.06982189416885376 train acc 0.9747175246932056\n",
            "epoch 31 batch id 3351 loss 0.031966984272003174 train acc 0.9747090420769919\n",
            "epoch 31 batch id 3361 loss 0.08362811803817749 train acc 0.9746959610235049\n",
            "epoch 31 batch id 3371 loss 0.14689980447292328 train acc 0.9746922278255711\n",
            "epoch 31 batch id 3381 loss 0.0013946985127404332 train acc 0.974697759538598\n",
            "epoch 31 batch id 3391 loss 0.15863412618637085 train acc 0.9746894352698319\n",
            "epoch 31 batch id 3401 loss 0.0843624696135521 train acc 0.9746903484269333\n",
            "epoch 31 batch id 3411 loss 0.16209207475185394 train acc 0.974700417766051\n",
            "epoch 31 batch id 3421 loss 0.02106272429227829 train acc 0.9747423998830751\n",
            "epoch 31 batch id 3431 loss 0.14374707639217377 train acc 0.9747385966190615\n",
            "epoch 31 batch id 3441 loss 0.1729714721441269 train acc 0.9747438971229294\n",
            "epoch 31 batch id 3451 loss 0.06339064985513687 train acc 0.9747401115618661\n",
            "epoch 31 batch id 3461 loss 0.023188386112451553 train acc 0.9747498916498122\n",
            "epoch 31 batch id 3471 loss 0.02000388689339161 train acc 0.9747686185537309\n",
            "epoch 31 batch id 3481 loss 0.05763465538620949 train acc 0.9747513286411951\n",
            "epoch 31 batch id 3491 loss 0.01796547695994377 train acc 0.9747341377828702\n",
            "epoch 31 batch id 3501 loss 0.022865284234285355 train acc 0.9747438231933733\n",
            "epoch 31 batch id 3511 loss 0.026052428409457207 train acc 0.9747445528339505\n",
            "epoch 31 batch id 3521 loss 0.0861329585313797 train acc 0.9747630289690429\n",
            "epoch 31 batch id 3531 loss 0.11301723122596741 train acc 0.9747637000849617\n",
            "epoch 31 batch id 3541 loss 0.05483179911971092 train acc 0.974759954815024\n",
            "epoch 31 batch id 3551 loss 0.042661119252443314 train acc 0.9747430301323571\n",
            "epoch 31 batch id 3561 loss 0.048691459000110626 train acc 0.9747700786295984\n",
            "epoch 31 batch id 3571 loss 0.08607297390699387 train acc 0.9747750980117614\n",
            "epoch 31 batch id 3581 loss 0.09556601196527481 train acc 0.9747844526668529\n",
            "epoch 31 batch id 3591 loss 0.10792390257120132 train acc 0.9747807017543859\n",
            "epoch 31 batch id 3601 loss 0.19758981466293335 train acc 0.9747899888919744\n",
            "epoch 31 batch id 3611 loss 0.014736242592334747 train acc 0.9748165328163944\n",
            "epoch 31 batch id 3621 loss 0.130146324634552 train acc 0.9748256697045016\n",
            "epoch 31 batch id 3631 loss 0.038370292633771896 train acc 0.9748347562654917\n",
            "epoch 31 batch id 3641 loss 0.01850605383515358 train acc 0.974839501510574\n",
            "epoch 31 batch id 3651 loss 0.055236976593732834 train acc 0.9748185428649685\n",
            "epoch 31 batch id 3661 loss 0.03217320144176483 train acc 0.9748019666757717\n",
            "epoch 31 batch id 3671 loss 0.05628563463687897 train acc 0.9748152751293926\n",
            "epoch 31 batch id 3681 loss 0.1985427886247635 train acc 0.9748412455854387\n",
            "epoch 31 batch id 3691 loss 0.0014390519354492426 train acc 0.9748543755079924\n",
            "epoch 31 batch id 3701 loss 0.11764737963676453 train acc 0.9748125506619832\n",
            "epoch 31 batch id 3711 loss 0.07737642526626587 train acc 0.9748298976017246\n",
            "epoch 31 batch id 3721 loss 0.07896111905574799 train acc 0.9748345538833647\n",
            "epoch 31 batch id 3731 loss 0.011473637074232101 train acc 0.9748601246314661\n",
            "epoch 31 batch id 3741 loss 0.04859469085931778 train acc 0.9748604985298048\n",
            "epoch 31 batch id 3751 loss 0.0034731586929410696 train acc 0.9748692015462543\n",
            "epoch 31 batch id 3761 loss 0.1408320814371109 train acc 0.9748653948417974\n",
            "epoch 31 batch id 3771 loss 0.23521636426448822 train acc 0.9748243171572527\n",
            "epoch 31 batch id 3781 loss 0.12038424611091614 train acc 0.974828914308384\n",
            "epoch 31 batch id 3791 loss 0.0006670142756775022 train acc 0.9748499736217356\n",
            "epoch 31 batch id 3801 loss 0.07437802851200104 train acc 0.9748544790844514\n",
            "epoch 31 batch id 3811 loss 0.09805399179458618 train acc 0.9748507609551299\n",
            "epoch 31 batch id 3821 loss 0.028950160369277 train acc 0.9748634192619733\n",
            "epoch 31 batch id 3831 loss 0.0011163995368406177 train acc 0.9748678543461238\n",
            "epoch 31 batch id 3841 loss 0.03157391771674156 train acc 0.9748600624837281\n",
            "epoch 31 batch id 3851 loss 0.004515341017395258 train acc 0.9748604258634121\n",
            "epoch 31 batch id 3861 loss 0.03214540332555771 train acc 0.9748567404817405\n",
            "epoch 31 batch id 3871 loss 0.1351161003112793 train acc 0.9748611469904418\n",
            "epoch 31 batch id 3881 loss 0.09374493360519409 train acc 0.9748695568152538\n",
            "epoch 31 batch id 3891 loss 0.025274787098169327 train acc 0.974865876381393\n",
            "epoch 31 batch id 3901 loss 0.008255391381680965 train acc 0.9748582094334786\n",
            "epoch 31 batch id 3911 loss 0.02529093623161316 train acc 0.9748425914088469\n",
            "epoch 31 batch id 3921 loss 0.015756696462631226 train acc 0.9748429928589646\n",
            "epoch 31 batch id 3931 loss 0.10832874476909637 train acc 0.974823518188756\n",
            "epoch 31 batch id 3941 loss 0.04208621755242348 train acc 0.9748517191068257\n",
            "epoch 31 batch id 3951 loss 0.07469014823436737 train acc 0.974820456846368\n",
            "epoch 31 batch id 3961 loss 0.09416564553976059 train acc 0.9748287995455693\n",
            "epoch 31 batch id 3971 loss 0.15012462437152863 train acc 0.9748213611181062\n",
            "epoch 31 batch id 3981 loss 0.003639100817963481 train acc 0.9747943355940718\n",
            "epoch 31 batch id 3991 loss 0.13350804150104523 train acc 0.9748105111500877\n",
            "epoch 31 batch id 4001 loss 0.07991385459899902 train acc 0.9748031742064484\n",
            "epoch 31 batch id 4011 loss 0.17545288801193237 train acc 0.9747686050860135\n",
            "epoch 31 batch id 4021 loss 0.09920541942119598 train acc 0.9747691805521015\n",
            "epoch 31 batch id 4031 loss 0.003174574812874198 train acc 0.9748201438848921\n",
            "epoch 31 batch id 4041 loss 0.0422978550195694 train acc 0.9748051224944321\n",
            "epoch 31 batch id 4051 loss 0.0207515899091959 train acc 0.9747978894100222\n",
            "epoch 31 batch id 4061 loss 0.12192972004413605 train acc 0.9747945395222851\n",
            "epoch 31 batch id 4071 loss 0.040992993861436844 train acc 0.9748065585851142\n",
            "epoch 31 batch id 4081 loss 0.04658253863453865 train acc 0.9748146900269542\n",
            "epoch 31 batch id 4091 loss 0.03443799912929535 train acc 0.9748151429968223\n",
            "epoch 31 batch id 4101 loss 0.030213292688131332 train acc 0.9748232138502804\n",
            "epoch 31 batch id 4111 loss 0.014543808996677399 train acc 0.9748274446606665\n",
            "epoch 31 batch id 4121 loss 0.06733056157827377 train acc 0.9748013224945402\n",
            "epoch 31 batch id 4131 loss 0.08064478635787964 train acc 0.9748018034374244\n",
            "epoch 31 batch id 4141 loss 0.00627680541947484 train acc 0.9748249215165419\n",
            "epoch 31 batch id 4151 loss 0.0653209239244461 train acc 0.9748328715972054\n",
            "epoch 31 batch id 4161 loss 0.08210693299770355 train acc 0.9748107426099495\n",
            "epoch 31 batch id 4171 loss 0.14000347256660461 train acc 0.9747999580436346\n",
            "epoch 31 batch id 4181 loss 0.005301889963448048 train acc 0.9747966993542215\n",
            "epoch 31 batch id 4191 loss 0.1654045432806015 train acc 0.9748009126700071\n",
            "epoch 31 batch id 4201 loss 0.03350605443120003 train acc 0.9748274220423708\n",
            "epoch 31 batch id 4211 loss 0.11278189718723297 train acc 0.9748167003087153\n",
            "epoch 31 batch id 4221 loss 0.1069989874958992 train acc 0.9747949241885809\n",
            "epoch 31 batch id 4231 loss 0.048046842217445374 train acc 0.9747843299456393\n",
            "epoch 31 batch id 4241 loss 0.06317108124494553 train acc 0.9747995755717991\n",
            "epoch 31 batch id 4251 loss 0.11236192286014557 train acc 0.9748147494707128\n",
            "epoch 31 batch id 4261 loss 0.05069190263748169 train acc 0.9748298521473833\n",
            "epoch 31 batch id 4271 loss 0.0026149051263928413 train acc 0.9748302505268087\n",
            "epoch 31 batch id 4281 loss 0.05244021862745285 train acc 0.9748415965895819\n",
            "epoch 31 batch id 4291 loss 0.0550321601331234 train acc 0.9748274003728734\n",
            "epoch 31 batch id 4301 loss 0.029046930372714996 train acc 0.9748060044175773\n",
            "epoch 31 batch id 4311 loss 0.02510663866996765 train acc 0.9748282011134307\n",
            "epoch 31 batch id 4321 loss 0.01875229924917221 train acc 0.9748249826429067\n",
            "epoch 31 batch id 4331 loss 0.09672129899263382 train acc 0.9748362098822443\n",
            "epoch 31 batch id 4341 loss 0.0880432277917862 train acc 0.9748401865929509\n",
            "epoch 31 batch id 4351 loss 0.03581153228878975 train acc 0.9748225982532751\n",
            "epoch 31 batch id 4361 loss 0.05437289550900459 train acc 0.9748301708323779\n",
            "epoch 31 batch id 4371 loss 0.048512354493141174 train acc 0.9748448581560284\n",
            "epoch 31 batch id 4381 loss 0.10952166467905045 train acc 0.9748309461310203\n",
            "epoch 31 batch id 4391 loss 0.049629781395196915 train acc 0.9748384479617399\n",
            "epoch 31 batch id 4401 loss 0.05031915009021759 train acc 0.9747997614178596\n",
            "epoch 31 batch id 4411 loss 0.09762898087501526 train acc 0.9747895885286783\n",
            "epoch 31 batch id 4421 loss 0.1268618255853653 train acc 0.9748077358063787\n",
            "epoch 31 batch id 4431 loss 0.1348731964826584 train acc 0.9747905382532159\n",
            "epoch 31 batch id 4441 loss 0.013647431507706642 train acc 0.974794528259401\n",
            "epoch 31 batch id 4451 loss 0.03413282334804535 train acc 0.9748020107840935\n",
            "epoch 31 batch id 4461 loss 0.010017585940659046 train acc 0.9748024546065904\n",
            "epoch 31 batch id 4471 loss 0.0455055758357048 train acc 0.9748063911876538\n",
            "epoch 31 batch id 4481 loss 0.047665853053331375 train acc 0.9747963624191028\n",
            "epoch 31 batch id 4491 loss 0.10457971692085266 train acc 0.9748002950345135\n",
            "epoch 31 batch id 4501 loss 0.031093329191207886 train acc 0.9747972672739391\n",
            "epoch 31 batch id 4511 loss 0.08182049542665482 train acc 0.9747977166925293\n",
            "epoch 31 batch id 4521 loss 0.03875133395195007 train acc 0.9747947080291971\n",
            "epoch 31 batch id 4531 loss 0.024285072460770607 train acc 0.9747813672478481\n",
            "epoch 31 batch id 4541 loss 0.06644026935100555 train acc 0.9747956121999559\n",
            "epoch 31 batch id 4551 loss 0.21202675998210907 train acc 0.9747891946824874\n",
            "epoch 31 batch id 4561 loss 0.11043620109558105 train acc 0.9747828053058539\n",
            "epoch 31 batch id 4571 loss 0.09906056523323059 train acc 0.9747798621745789\n",
            "epoch 31 batch id 4581 loss 0.20825578272342682 train acc 0.9747871643745907\n",
            "epoch 31 batch id 4591 loss 0.09978043287992477 train acc 0.9747842245698105\n",
            "epoch 31 batch id 4601 loss 0.09204654395580292 train acc 0.9747779015431428\n",
            "epoch 31 batch id 4611 loss 0.04312494397163391 train acc 0.9747987150292778\n",
            "epoch 31 batch id 4621 loss 0.03379148244857788 train acc 0.9748092945249945\n",
            "epoch 31 batch id 4631 loss 0.08203383535146713 train acc 0.9748164543295185\n",
            "epoch 31 batch id 4641 loss 0.13794037699699402 train acc 0.9748168498168498\n",
            "epoch 31 batch id 4651 loss 0.2764887511730194 train acc 0.9748206030961084\n",
            "epoch 31 batch id 4661 loss 0.03491060063242912 train acc 0.9748411016949152\n",
            "epoch 31 batch id 4671 loss 0.011724160052835941 train acc 0.9748514771997431\n",
            "epoch 31 batch id 4681 loss 0.24802733957767487 train acc 0.9748584704123051\n",
            "epoch 31 batch id 4691 loss 0.010949673131108284 train acc 0.9748720955020251\n",
            "epoch 31 batch id 4701 loss 0.06861773133277893 train acc 0.9748856626249734\n",
            "epoch 31 batch id 4711 loss 0.041056085377931595 train acc 0.9748892220335386\n",
            "epoch 31 batch id 4721 loss 0.015937836840748787 train acc 0.9748695986019911\n",
            "epoch 31 batch id 4731 loss 0.016021035611629486 train acc 0.9748599661805115\n",
            "epoch 31 batch id 4741 loss 0.049772948026657104 train acc 0.974880035857414\n",
            "epoch 31 batch id 4751 loss 0.1977800577878952 train acc 0.9748868659229636\n",
            "epoch 31 batch id 4761 loss 0.16151221096515656 train acc 0.9748936672967864\n",
            "epoch 31 batch id 4771 loss 0.0776199996471405 train acc 0.9749004401592958\n",
            "epoch 31 batch id 4781 loss 0.11849914491176605 train acc 0.9748973802551767\n",
            "epoch 31 batch id 4791 loss 0.13320939242839813 train acc 0.974884549154665\n",
            "epoch 31 batch id 4801 loss 0.10797863453626633 train acc 0.9748815350968548\n",
            "epoch 31 batch id 4811 loss 0.10036889463663101 train acc 0.9748850290999792\n",
            "epoch 31 batch id 4821 loss 0.10475345700979233 train acc 0.9748820265505082\n",
            "epoch 31 batch id 4831 loss 0.023485448211431503 train acc 0.9748919737114469\n",
            "epoch 31 batch id 4841 loss 0.16635796427726746 train acc 0.9748889692212352\n",
            "epoch 31 batch id 4851 loss 0.057962335646152496 train acc 0.9748956400742115\n",
            "epoch 31 batch id 4861 loss 0.08935501426458359 train acc 0.9748862116848385\n",
            "epoch 31 batch id 4871 loss 0.021013841032981873 train acc 0.9748960685690823\n",
            "epoch 31 batch id 4881 loss 0.03870667144656181 train acc 0.9748994826879738\n",
            "epoch 31 batch id 4891 loss 0.024288304150104523 train acc 0.9748964935595993\n",
            "epoch 31 batch id 4901 loss 0.00045388005673885345 train acc 0.9749126453784942\n",
            "epoch 31 batch id 4911 loss 0.2715786099433899 train acc 0.9748905518224394\n",
            "epoch 31 batch id 4921 loss 0.22996298968791962 train acc 0.9748875990652306\n",
            "epoch 31 batch id 4931 loss 0.07476197183132172 train acc 0.9748719833705131\n",
            "epoch 31 batch id 4941 loss 0.079845130443573 train acc 0.9748659178303987\n",
            "epoch 31 batch id 4951 loss 0.08496517688035965 train acc 0.9748535649363765\n",
            "epoch 31 train acc 0.9748478964549673\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49b3684725d74cb490d964ab7b8e91d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 31 loss 1.4682694673538208 test acc 0.8408838892961877\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a73eff93c2744d3ab55da08aaaf5a38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 32 batch id 1 loss 0.04471642151474953 train acc 0.984375\n",
            "epoch 32 batch id 11 loss 0.01930500753223896 train acc 0.984375\n",
            "epoch 32 batch id 21 loss 0.01315850019454956 train acc 0.9776785714285714\n",
            "epoch 32 batch id 31 loss 0.05201474204659462 train acc 0.9747983870967742\n",
            "epoch 32 batch id 41 loss 0.032641734927892685 train acc 0.975609756097561\n",
            "epoch 32 batch id 51 loss 0.018766874447464943 train acc 0.9761029411764706\n",
            "epoch 32 batch id 61 loss 0.04705962538719177 train acc 0.9733606557377049\n",
            "epoch 32 batch id 71 loss 0.20093998312950134 train acc 0.9746919014084507\n",
            "epoch 32 batch id 81 loss 0.12422681599855423 train acc 0.9751157407407407\n",
            "epoch 32 batch id 91 loss 0.14690274000167847 train acc 0.9752747252747253\n",
            "epoch 32 batch id 101 loss 0.19284653663635254 train acc 0.974009900990099\n",
            "epoch 32 batch id 111 loss 0.0033492844086140394 train acc 0.9743806306306306\n",
            "epoch 32 batch id 121 loss 0.18593987822532654 train acc 0.9744318181818182\n",
            "epoch 32 batch id 131 loss 0.04752681404352188 train acc 0.974594465648855\n",
            "epoch 32 batch id 141 loss 0.010899420827627182 train acc 0.9750664893617021\n",
            "epoch 32 batch id 151 loss 0.06232181563973427 train acc 0.9754759933774835\n",
            "epoch 32 batch id 161 loss 0.0034914070274680853 train acc 0.9755434782608695\n",
            "epoch 32 batch id 171 loss 0.08499809354543686 train acc 0.9756030701754386\n",
            "epoch 32 batch id 181 loss 0.019043991342186928 train acc 0.9760877071823204\n",
            "epoch 32 batch id 191 loss 0.08192756772041321 train acc 0.9761943717277487\n",
            "epoch 32 batch id 201 loss 0.16059722006320953 train acc 0.976601368159204\n",
            "epoch 32 batch id 211 loss 0.03206795081496239 train acc 0.9765254739336493\n",
            "epoch 32 batch id 221 loss 0.04287268966436386 train acc 0.9766685520361991\n",
            "epoch 32 batch id 231 loss 0.03904873505234718 train acc 0.9770021645021645\n",
            "epoch 32 batch id 241 loss 0.16408368945121765 train acc 0.9764004149377593\n",
            "epoch 32 batch id 251 loss 0.08700665086507797 train acc 0.976531374501992\n",
            "epoch 32 batch id 261 loss 0.05858943238854408 train acc 0.976352969348659\n",
            "epoch 32 batch id 271 loss 0.0002998148265760392 train acc 0.9765336715867159\n",
            "epoch 32 batch id 281 loss 0.09995024651288986 train acc 0.9767015124555161\n",
            "epoch 32 batch id 291 loss 0.1294259876012802 train acc 0.9769115120274914\n",
            "epoch 32 batch id 301 loss 0.16300132870674133 train acc 0.9766922757475083\n",
            "epoch 32 batch id 311 loss 0.031853847205638885 train acc 0.9768890675241158\n",
            "epoch 32 batch id 321 loss 0.39366015791893005 train acc 0.9767328660436138\n",
            "epoch 32 batch id 331 loss 0.00924218725413084 train acc 0.9764444864048338\n",
            "epoch 32 batch id 341 loss 0.2923012673854828 train acc 0.9764479472140762\n",
            "epoch 32 batch id 351 loss 0.03196019306778908 train acc 0.9765847578347578\n",
            "epoch 32 batch id 361 loss 0.059460584074258804 train acc 0.9766707063711911\n",
            "epoch 32 batch id 371 loss 0.035838838666677475 train acc 0.9769626010781671\n",
            "epoch 32 batch id 381 loss 0.020184164866805077 train acc 0.9769931102362205\n",
            "epoch 32 batch id 391 loss 0.1074480339884758 train acc 0.9769421355498721\n",
            "epoch 32 batch id 401 loss 0.002226777607575059 train acc 0.976932668329177\n",
            "epoch 32 batch id 411 loss 0.04387788474559784 train acc 0.9768856447688564\n",
            "epoch 32 batch id 421 loss 0.1220039576292038 train acc 0.9767666270783848\n",
            "epoch 32 batch id 431 loss 0.05235963687300682 train acc 0.97665313225058\n",
            "epoch 32 batch id 441 loss 0.12048934400081635 train acc 0.9765447845804989\n",
            "epoch 32 batch id 451 loss 0.09895265847444534 train acc 0.9763719512195121\n",
            "epoch 32 batch id 461 loss 0.019223978742957115 train acc 0.9760710412147505\n",
            "epoch 32 batch id 471 loss 0.07224760949611664 train acc 0.9762141719745223\n",
            "epoch 32 batch id 481 loss 0.0950576663017273 train acc 0.9761239604989606\n",
            "epoch 32 batch id 491 loss 0.016978159546852112 train acc 0.9760374236252546\n",
            "epoch 32 batch id 501 loss 0.19854798913002014 train acc 0.9761726546906188\n",
            "epoch 32 batch id 511 loss 0.12894578278064728 train acc 0.9759050880626223\n",
            "epoch 32 batch id 521 loss 0.09712845832109451 train acc 0.9760676583493282\n",
            "epoch 32 batch id 531 loss 0.04518536105751991 train acc 0.9759004237288136\n",
            "epoch 32 batch id 541 loss 0.014690491370856762 train acc 0.9757682532347505\n",
            "epoch 32 batch id 551 loss 0.02959257923066616 train acc 0.9757826678765881\n",
            "epoch 32 batch id 561 loss 0.05263858288526535 train acc 0.9756851604278075\n",
            "epoch 32 batch id 571 loss 0.05661650747060776 train acc 0.9758099824868651\n",
            "epoch 32 batch id 581 loss 0.04386178031563759 train acc 0.9759036144578314\n",
            "epoch 32 batch id 591 loss 0.04564011096954346 train acc 0.9760469543147208\n",
            "epoch 32 batch id 601 loss 0.007954436354339123 train acc 0.9761595257903494\n",
            "epoch 32 batch id 611 loss 0.12521231174468994 train acc 0.9761916939443536\n",
            "epoch 32 batch id 621 loss 0.16319240629673004 train acc 0.976147342995169\n",
            "epoch 32 batch id 631 loss 0.05654342472553253 train acc 0.9761291600633915\n",
            "epoch 32 batch id 641 loss 0.0339745357632637 train acc 0.9762334243369735\n",
            "epoch 32 batch id 651 loss 0.03565126657485962 train acc 0.9760704685099847\n",
            "epoch 32 batch id 661 loss 0.11730446666479111 train acc 0.9760542738275341\n",
            "epoch 32 batch id 671 loss 0.0076163653284311295 train acc 0.9759454172876304\n",
            "epoch 32 batch id 681 loss 0.09022070467472076 train acc 0.9759085903083701\n",
            "epoch 32 batch id 691 loss 0.04256010800600052 train acc 0.9759858900144718\n",
            "epoch 32 batch id 701 loss 0.05947314202785492 train acc 0.9760609843081313\n",
            "epoch 32 batch id 711 loss 0.12836147844791412 train acc 0.9760460618846695\n",
            "epoch 32 batch id 721 loss 0.06937598437070847 train acc 0.9759231969486823\n",
            "epoch 32 batch id 731 loss 0.010702120140194893 train acc 0.9758678180574556\n",
            "epoch 32 batch id 741 loss 0.03281668946146965 train acc 0.9759615384615384\n",
            "epoch 32 batch id 751 loss 0.18312449753284454 train acc 0.9758863182423435\n",
            "epoch 32 batch id 761 loss 0.015133441425859928 train acc 0.975977332457293\n",
            "epoch 32 batch id 771 loss 0.04978524148464203 train acc 0.976045719844358\n",
            "epoch 32 batch id 781 loss 0.19831548631191254 train acc 0.9759923175416133\n",
            "epoch 32 batch id 791 loss 0.01753496378660202 train acc 0.9760785398230089\n",
            "epoch 32 batch id 801 loss 0.02552376687526703 train acc 0.9760650749063671\n",
            "epoch 32 batch id 811 loss 0.21020330488681793 train acc 0.9761290073982737\n",
            "epoch 32 batch id 821 loss 0.07227115333080292 train acc 0.9761533191230207\n",
            "epoch 32 batch id 831 loss 0.10854368656873703 train acc 0.9761018351383874\n",
            "epoch 32 batch id 841 loss 0.08774620294570923 train acc 0.976032996432818\n",
            "epoch 32 batch id 851 loss 0.1640111207962036 train acc 0.9759106933019976\n",
            "epoch 32 batch id 861 loss 0.10439476370811462 train acc 0.9759001161440186\n",
            "epoch 32 batch id 871 loss 0.01622188650071621 train acc 0.9759794776119403\n",
            "epoch 32 batch id 881 loss 0.02207251451909542 train acc 0.975897417707151\n",
            "epoch 32 batch id 891 loss 0.06501038372516632 train acc 0.9759925645342312\n",
            "epoch 32 batch id 901 loss 0.031657855957746506 train acc 0.9760509156492786\n",
            "epoch 32 batch id 911 loss 0.16758134961128235 train acc 0.9759536223929748\n",
            "epoch 32 batch id 921 loss 0.0007043858640827239 train acc 0.9758754071661238\n",
            "epoch 32 batch id 931 loss 0.058617837727069855 train acc 0.9758660042964554\n",
            "epoch 32 batch id 941 loss 0.04246140643954277 train acc 0.9759398246546227\n",
            "epoch 32 batch id 951 loss 0.02858843095600605 train acc 0.9758149316508938\n",
            "epoch 32 batch id 961 loss 0.03705921769142151 train acc 0.9758877471383975\n",
            "epoch 32 batch id 971 loss 0.06323730200529099 train acc 0.9758625128733265\n",
            "epoch 32 batch id 981 loss 0.1604461520910263 train acc 0.9759174311926605\n",
            "epoch 32 batch id 991 loss 0.15459899604320526 train acc 0.9759870080726539\n",
            "epoch 32 batch id 1001 loss 0.0422837994992733 train acc 0.9760551948051948\n",
            "epoch 32 batch id 1011 loss 0.005389376077800989 train acc 0.9760911226508407\n",
            "epoch 32 batch id 1021 loss 0.061661798506975174 train acc 0.9761875612144956\n",
            "epoch 32 batch id 1031 loss 0.02317219413816929 train acc 0.9762366634335596\n",
            "epoch 32 batch id 1041 loss 0.0305397417396307 train acc 0.9762698126801153\n",
            "epoch 32 batch id 1051 loss 0.022396307438611984 train acc 0.9761536631779257\n",
            "epoch 32 batch id 1061 loss 0.12541663646697998 train acc 0.9760838831291234\n",
            "epoch 32 batch id 1071 loss 0.0006307605654001236 train acc 0.9761467086834734\n",
            "epoch 32 batch id 1081 loss 0.05976967141032219 train acc 0.976063829787234\n",
            "epoch 32 batch id 1091 loss 0.21631576120853424 train acc 0.9760540788267644\n",
            "epoch 32 batch id 1101 loss 0.06995414942502975 train acc 0.9760445049954587\n",
            "epoch 32 batch id 1111 loss 0.1995949149131775 train acc 0.9759929117911791\n",
            "epoch 32 batch id 1121 loss 0.07911534607410431 train acc 0.9759979928635147\n",
            "epoch 32 batch id 1131 loss 0.06415484845638275 train acc 0.9759615384615384\n",
            "epoch 32 batch id 1141 loss 0.048042334616184235 train acc 0.9758846406660824\n",
            "epoch 32 batch id 1151 loss 0.19258244335651398 train acc 0.9757819287576021\n",
            "epoch 32 batch id 1161 loss 0.1009058803319931 train acc 0.9756540697674418\n",
            "epoch 32 batch id 1171 loss 0.054036207497119904 train acc 0.9756351409052092\n",
            "epoch 32 batch id 1181 loss 0.01918867416679859 train acc 0.9757091447925487\n",
            "epoch 32 batch id 1191 loss 0.057364579290151596 train acc 0.9757031905961377\n",
            "epoch 32 batch id 1201 loss 0.08348987251520157 train acc 0.9757753955037469\n",
            "epoch 32 batch id 1211 loss 0.13177825510501862 train acc 0.9758206028075971\n",
            "epoch 32 batch id 1221 loss 0.06376788020133972 train acc 0.9758010851760852\n",
            "epoch 32 batch id 1231 loss 0.027582479640841484 train acc 0.9757564987814785\n",
            "epoch 32 batch id 1241 loss 0.016027113422751427 train acc 0.9757504029008863\n",
            "epoch 32 batch id 1251 loss 0.0928569808602333 train acc 0.9757444044764189\n",
            "epoch 32 batch id 1261 loss 0.06316141039133072 train acc 0.9757508921490881\n",
            "epoch 32 batch id 1271 loss 0.02865685522556305 train acc 0.9758064516129032\n",
            "epoch 32 batch id 1281 loss 0.14650915563106537 train acc 0.9757391686182669\n",
            "epoch 32 batch id 1291 loss 0.09224328398704529 train acc 0.9756971340046475\n",
            "epoch 32 batch id 1301 loss 0.036850329488515854 train acc 0.9757157955418908\n",
            "epoch 32 batch id 1311 loss 0.04342682659626007 train acc 0.9757103356216629\n",
            "epoch 32 batch id 1321 loss 0.024233130738139153 train acc 0.9756694738834216\n",
            "epoch 32 batch id 1331 loss 0.03843057528138161 train acc 0.9756409654395192\n",
            "epoch 32 batch id 1341 loss 0.11927597969770432 train acc 0.9756361856823266\n",
            "epoch 32 batch id 1351 loss 0.09113430231809616 train acc 0.9756314766839378\n",
            "epoch 32 batch id 1361 loss 0.015190907754004002 train acc 0.9755694342395298\n",
            "epoch 32 batch id 1371 loss 0.007161460816860199 train acc 0.9756222647702407\n",
            "epoch 32 batch id 1381 loss 0.1241588369011879 train acc 0.9756856444605359\n",
            "epoch 32 batch id 1391 loss 0.011215940117835999 train acc 0.9756919482386772\n",
            "epoch 32 batch id 1401 loss 0.05912863463163376 train acc 0.9756647037830122\n",
            "epoch 32 batch id 1411 loss 0.022507624700665474 train acc 0.9757596562721474\n",
            "epoch 32 batch id 1421 loss 0.011831940151751041 train acc 0.9757872976776918\n",
            "epoch 32 batch id 1431 loss 0.10888103395700455 train acc 0.9757708770090846\n",
            "epoch 32 batch id 1441 loss 0.044682107865810394 train acc 0.9757113115891742\n",
            "epoch 32 batch id 1451 loss 0.009457198902964592 train acc 0.9757817884217781\n",
            "epoch 32 batch id 1461 loss 0.006766891106963158 train acc 0.9757550479123888\n",
            "epoch 32 batch id 1471 loss 0.05736269801855087 train acc 0.9758030251529571\n",
            "epoch 32 batch id 1481 loss 0.0840403288602829 train acc 0.9757448514517219\n",
            "epoch 32 batch id 1491 loss 0.0941646471619606 train acc 0.975802733065057\n",
            "epoch 32 batch id 1501 loss 0.07306384295225143 train acc 0.975870253164557\n",
            "epoch 32 batch id 1511 loss 0.022278066724538803 train acc 0.975926538716082\n",
            "epoch 32 batch id 1521 loss 0.021820884197950363 train acc 0.9759718113083498\n",
            "epoch 32 batch id 1531 loss 0.027667876332998276 train acc 0.975985875244938\n",
            "epoch 32 batch id 1541 loss 0.053644370287656784 train acc 0.9760098961713173\n",
            "epoch 32 batch id 1551 loss 0.03207448869943619 train acc 0.9759630883301096\n",
            "epoch 32 batch id 1561 loss 0.0637383833527565 train acc 0.975896860986547\n",
            "epoch 32 batch id 1571 loss 0.00811073649674654 train acc 0.9758712603437301\n",
            "epoch 32 batch id 1581 loss 0.08468696475028992 train acc 0.9758657495256167\n",
            "epoch 32 batch id 1591 loss 0.15274912118911743 train acc 0.9758995914519171\n",
            "epoch 32 batch id 1601 loss 0.14525645971298218 train acc 0.9759720487195502\n",
            "epoch 32 batch id 1611 loss 0.2083515077829361 train acc 0.9759563159528243\n",
            "epoch 32 batch id 1621 loss 0.048777010291814804 train acc 0.9759889728562615\n",
            "epoch 32 batch id 1631 loss 0.013069480657577515 train acc 0.976030809319436\n",
            "epoch 32 batch id 1641 loss 0.0681290328502655 train acc 0.9760530926264472\n",
            "epoch 32 batch id 1651 loss 0.07121971994638443 train acc 0.976046714112659\n",
            "epoch 32 batch id 1661 loss 0.033938612788915634 train acc 0.9760780403371463\n",
            "epoch 32 batch id 1671 loss 0.05887709930539131 train acc 0.9760622381807301\n",
            "epoch 32 batch id 1681 loss 0.18997320532798767 train acc 0.9760373289708507\n",
            "epoch 32 batch id 1691 loss 0.128206267952919 train acc 0.9760773950325251\n",
            "epoch 32 batch id 1701 loss 0.03577236086130142 train acc 0.9761169900058789\n",
            "epoch 32 batch id 1711 loss 0.09582091122865677 train acc 0.9761104617182934\n",
            "epoch 32 batch id 1721 loss 0.09690291434526443 train acc 0.9761675624636839\n",
            "epoch 32 batch id 1731 loss 0.0024083766620606184 train acc 0.9761969237435009\n",
            "epoch 32 batch id 1741 loss 0.039015043526887894 train acc 0.976225947731189\n",
            "epoch 32 batch id 1751 loss 0.03974774852395058 train acc 0.9762100228440891\n",
            "epoch 32 batch id 1761 loss 0.045661069452762604 train acc 0.9761676604202157\n",
            "epoch 32 batch id 1771 loss 0.0062052044086158276 train acc 0.9762051806888763\n",
            "epoch 32 batch id 1781 loss 0.0013022730126976967 train acc 0.9761896406513195\n",
            "epoch 32 batch id 1791 loss 0.07059593498706818 train acc 0.9761829983249581\n",
            "epoch 32 batch id 1801 loss 0.0928928479552269 train acc 0.9762198084397556\n",
            "epoch 32 batch id 1811 loss 0.05694795399904251 train acc 0.9762217007178354\n",
            "epoch 32 batch id 1821 loss 0.10204395651817322 train acc 0.9761892504118617\n",
            "epoch 32 batch id 1831 loss 0.021095599979162216 train acc 0.9761656881485528\n",
            "epoch 32 batch id 1841 loss 0.06400154531002045 train acc 0.9761593563280826\n",
            "epoch 32 batch id 1851 loss 0.10342580825090408 train acc 0.9761699756888168\n",
            "epoch 32 batch id 1861 loss 0.037311941385269165 train acc 0.9761804809242343\n",
            "epoch 32 batch id 1871 loss 0.10576339811086655 train acc 0.9762576830571886\n",
            "epoch 32 batch id 1881 loss 0.017758486792445183 train acc 0.9762509968102073\n",
            "epoch 32 batch id 1891 loss 0.024359937757253647 train acc 0.9762691697514543\n",
            "epoch 32 batch id 1901 loss 0.11633158475160599 train acc 0.9762624934245134\n",
            "epoch 32 batch id 1911 loss 0.023751668632030487 train acc 0.9762313579277865\n",
            "epoch 32 batch id 1921 loss 0.05374009162187576 train acc 0.9761598776678813\n",
            "epoch 32 batch id 1931 loss 0.05516604334115982 train acc 0.9761538710512687\n",
            "epoch 32 batch id 1941 loss 0.0038503610994666815 train acc 0.976172076249356\n",
            "epoch 32 batch id 1951 loss 0.03597186133265495 train acc 0.9762141209636084\n",
            "epoch 32 batch id 1961 loss 0.0448278971016407 train acc 0.9762398011218766\n",
            "epoch 32 batch id 1971 loss 0.008224393241107464 train acc 0.9762335109081685\n",
            "epoch 32 batch id 1981 loss 0.07611676305532455 train acc 0.9762351716304897\n",
            "epoch 32 batch id 1991 loss 0.27931565046310425 train acc 0.9761818809643396\n",
            "epoch 32 batch id 2001 loss 0.007874595932662487 train acc 0.9761994002998501\n",
            "epoch 32 batch id 2011 loss 0.07789423316717148 train acc 0.9762167454002983\n",
            "epoch 32 batch id 2021 loss 0.18137764930725098 train acc 0.9762339188520535\n",
            "epoch 32 batch id 2031 loss 0.09780963510274887 train acc 0.9762201501723289\n",
            "epoch 32 batch id 2041 loss 0.027653859928250313 train acc 0.9761988608525233\n",
            "epoch 32 batch id 2051 loss 0.16423994302749634 train acc 0.9762006338371526\n",
            "epoch 32 batch id 2061 loss 0.01425749808549881 train acc 0.9762630397865114\n",
            "epoch 32 batch id 2071 loss 0.09238769114017487 train acc 0.9762795750845003\n",
            "epoch 32 batch id 2081 loss 0.0474490225315094 train acc 0.9762809346468044\n",
            "epoch 32 batch id 2091 loss 0.07511646300554276 train acc 0.9762598637015782\n",
            "epoch 32 batch id 2101 loss 0.0035549874883145094 train acc 0.9762687410756783\n",
            "epoch 32 batch id 2111 loss 0.01320938766002655 train acc 0.9762701326385599\n",
            "epoch 32 batch id 2121 loss 0.02575072832405567 train acc 0.9762936115040075\n",
            "epoch 32 batch id 2131 loss 0.0071470108814537525 train acc 0.9763315344908494\n",
            "epoch 32 batch id 2141 loss 0.13698525726795197 train acc 0.976296123306866\n",
            "epoch 32 batch id 2151 loss 0.013754713349044323 train acc 0.9763554741980475\n",
            "epoch 32 batch id 2161 loss 0.2189650684595108 train acc 0.9763058190652476\n",
            "epoch 32 batch id 2171 loss 0.012683015316724777 train acc 0.9763357899585444\n",
            "epoch 32 batch id 2181 loss 0.08983854204416275 train acc 0.9763225011462632\n",
            "epoch 32 batch id 2191 loss 0.027489453554153442 train acc 0.9763663852122318\n",
            "epoch 32 batch id 2201 loss 0.011386947706341743 train acc 0.9764169695592912\n",
            "epoch 32 batch id 2211 loss 0.000996388029307127 train acc 0.9764388285843509\n",
            "epoch 32 batch id 2221 loss 0.05090252682566643 train acc 0.9764042098153984\n",
            "epoch 32 batch id 2231 loss 0.019982321187853813 train acc 0.9764119229045272\n",
            "epoch 32 batch id 2241 loss 0.1148190051317215 train acc 0.9764125948237394\n",
            "epoch 32 batch id 2251 loss 0.0006317249499261379 train acc 0.9764618502887605\n",
            "epoch 32 batch id 2261 loss 0.040407512336969376 train acc 0.9764968487394958\n",
            "epoch 32 batch id 2271 loss 0.05350334569811821 train acc 0.9764764971378247\n",
            "epoch 32 batch id 2281 loss 0.03551184758543968 train acc 0.9765179745725558\n",
            "epoch 32 batch id 2291 loss 0.08894715458154678 train acc 0.9765386294194675\n",
            "epoch 32 batch id 2301 loss 0.02970922365784645 train acc 0.9765658952629291\n",
            "epoch 32 batch id 2311 loss 0.049236591905355453 train acc 0.976606447425357\n",
            "epoch 32 batch id 2321 loss 0.16124442219734192 train acc 0.9765793300301594\n",
            "epoch 32 batch id 2331 loss 0.05120229348540306 train acc 0.9765591484341485\n",
            "epoch 32 batch id 2341 loss 0.04540311172604561 train acc 0.9765858607432721\n",
            "epoch 32 batch id 2351 loss 0.0364636555314064 train acc 0.9765525308379412\n",
            "epoch 32 batch id 2361 loss 0.0640702024102211 train acc 0.9765327191867853\n",
            "epoch 32 batch id 2371 loss 0.02159763313829899 train acc 0.976559204976803\n",
            "epoch 32 batch id 2381 loss 0.03083941899240017 train acc 0.9765526564468711\n",
            "epoch 32 batch id 2391 loss 0.023297011852264404 train acc 0.9765592325386867\n",
            "epoch 32 batch id 2401 loss 0.017322923988103867 train acc 0.9765592461474386\n",
            "epoch 32 batch id 2411 loss 0.0274669099599123 train acc 0.9765722210700954\n",
            "epoch 32 batch id 2421 loss 0.011670410633087158 train acc 0.9765721809169765\n",
            "epoch 32 batch id 2431 loss 0.05713111162185669 train acc 0.9765914232825997\n",
            "epoch 32 batch id 2441 loss 0.044513918459415436 train acc 0.9765400962720197\n",
            "epoch 32 batch id 2451 loss 0.0034260035026818514 train acc 0.976514687882497\n",
            "epoch 32 batch id 2461 loss 0.04044190049171448 train acc 0.9765148821617229\n",
            "epoch 32 batch id 2471 loss 0.11659014225006104 train acc 0.9764771347632537\n",
            "epoch 32 batch id 2481 loss 0.2672785520553589 train acc 0.9765152660217654\n",
            "epoch 32 batch id 2491 loss 0.07206184417009354 train acc 0.9765217282215978\n",
            "epoch 32 batch id 2501 loss 0.017978355288505554 train acc 0.976484406237505\n",
            "epoch 32 batch id 2511 loss 0.05763256549835205 train acc 0.9764971624850657\n",
            "epoch 32 batch id 2521 loss 0.024151740595698357 train acc 0.9764850257834192\n",
            "epoch 32 batch id 2531 loss 0.014701846055686474 train acc 0.9765161991307784\n",
            "epoch 32 batch id 2541 loss 0.020787188783288002 train acc 0.976534828807556\n",
            "epoch 32 batch id 2551 loss 0.056266214698553085 train acc 0.9765288122304978\n",
            "epoch 32 batch id 2561 loss 0.14141498506069183 train acc 0.9765228426395939\n",
            "epoch 32 batch id 2571 loss 0.2565769553184509 train acc 0.9764926098794243\n",
            "epoch 32 batch id 2581 loss 0.05125995725393295 train acc 0.9765110422316932\n",
            "epoch 32 batch id 2591 loss 0.016298003494739532 train acc 0.9765534542647626\n",
            "epoch 32 batch id 2601 loss 0.04139738902449608 train acc 0.9765594963475587\n",
            "epoch 32 batch id 2611 loss 0.01531696692109108 train acc 0.9765774607430103\n",
            "epoch 32 batch id 2621 loss 0.05474599450826645 train acc 0.9765654807325448\n",
            "epoch 32 batch id 2631 loss 0.11484717577695847 train acc 0.9765179589509693\n",
            "epoch 32 batch id 2641 loss 0.16556420922279358 train acc 0.9764648807269973\n",
            "epoch 32 batch id 2651 loss 0.0030246006790548563 train acc 0.9764652489626556\n",
            "epoch 32 batch id 2661 loss 0.012093639001250267 train acc 0.976489101841413\n",
            "epoch 32 batch id 2671 loss 0.129534050822258 train acc 0.9764718270310745\n",
            "epoch 32 batch id 2681 loss 0.01975097507238388 train acc 0.9764546810891458\n",
            "epoch 32 batch id 2691 loss 0.012685533612966537 train acc 0.9764608881456708\n",
            "epoch 32 batch id 2701 loss 0.06353055685758591 train acc 0.9764612643465383\n",
            "epoch 32 batch id 2711 loss 0.0026799195911735296 train acc 0.9764270564367392\n",
            "epoch 32 batch id 2721 loss 0.00884531531482935 train acc 0.9763873575891217\n",
            "epoch 32 batch id 2731 loss 0.0014220100129023194 train acc 0.9764051629439766\n",
            "epoch 32 batch id 2741 loss 0.14942649006843567 train acc 0.9764000364830354\n",
            "epoch 32 batch id 2751 loss 0.07168973237276077 train acc 0.9763835877862596\n",
            "epoch 32 batch id 2761 loss 0.03394081071019173 train acc 0.9763842357841361\n",
            "epoch 32 batch id 2771 loss 0.00864887423813343 train acc 0.9763848791050163\n",
            "epoch 32 batch id 2781 loss 0.04109992831945419 train acc 0.976391136281913\n",
            "epoch 32 batch id 2791 loss 0.023939890787005424 train acc 0.9763637585094948\n",
            "epoch 32 batch id 2801 loss 0.2332732081413269 train acc 0.9763254194930382\n",
            "epoch 32 batch id 2811 loss 0.12251822650432587 train acc 0.9763429384560655\n",
            "epoch 32 batch id 2821 loss 0.012563117779791355 train acc 0.9763603332151719\n",
            "epoch 32 batch id 2831 loss 0.06102800741791725 train acc 0.9763444895796538\n",
            "epoch 32 batch id 2841 loss 0.07915689051151276 train acc 0.9763397571277719\n",
            "epoch 32 batch id 2851 loss 0.009540940634906292 train acc 0.9763569800070151\n",
            "epoch 32 batch id 2861 loss 0.052372660487890244 train acc 0.9763686211114995\n",
            "epoch 32 batch id 2871 loss 0.1258525401353836 train acc 0.9763747387669801\n",
            "epoch 32 batch id 2881 loss 0.052311673760414124 train acc 0.9763808139534884\n",
            "epoch 32 batch id 2891 loss 0.026297137141227722 train acc 0.9763544188861986\n",
            "epoch 32 batch id 2901 loss 0.024790963158011436 train acc 0.9763605222337125\n",
            "epoch 32 batch id 2911 loss 0.019724858924746513 train acc 0.97632364307798\n",
            "epoch 32 batch id 2921 loss 0.05308688059449196 train acc 0.9762923656282095\n",
            "epoch 32 batch id 2931 loss 0.008427879773080349 train acc 0.9763039491641078\n",
            "epoch 32 batch id 2941 loss 0.003063882701098919 train acc 0.9763048282896973\n",
            "epoch 32 batch id 2951 loss 0.13881848752498627 train acc 0.9763533547949848\n",
            "epoch 32 batch id 2961 loss 0.07618109881877899 train acc 0.9763646149949341\n",
            "epoch 32 batch id 2971 loss 0.05985178053379059 train acc 0.9763337260181757\n",
            "epoch 32 batch id 2981 loss 0.018952777609229088 train acc 0.9763082858101308\n",
            "epoch 32 batch id 2991 loss 0.029561813920736313 train acc 0.976335255767302\n",
            "epoch 32 batch id 3001 loss 0.0017044682754203677 train acc 0.9763672525824725\n",
            "epoch 32 batch id 3011 loss 0.010383178479969501 train acc 0.9763730903354367\n",
            "epoch 32 batch id 3021 loss 0.029526205733418465 train acc 0.9763737173121483\n",
            "epoch 32 batch id 3031 loss 0.15810202062129974 train acc 0.9763588749587595\n",
            "epoch 32 batch id 3041 loss 0.1464949995279312 train acc 0.9763338539953963\n",
            "epoch 32 batch id 3051 loss 0.16892947256565094 train acc 0.976349967223861\n",
            "epoch 32 batch id 3061 loss 0.10096870362758636 train acc 0.9763353479255146\n",
            "epoch 32 batch id 3071 loss 0.004040051717311144 train acc 0.9763462634321068\n",
            "epoch 32 batch id 3081 loss 0.10075951367616653 train acc 0.9763520366764038\n",
            "epoch 32 batch id 3091 loss 0.04865824058651924 train acc 0.9763577725655128\n",
            "epoch 32 batch id 3101 loss 0.06044888496398926 train acc 0.9763533940664302\n",
            "epoch 32 batch id 3111 loss 0.04656847193837166 train acc 0.9763540662166506\n",
            "epoch 32 batch id 3121 loss 0.022875182330608368 train acc 0.9764047981416213\n",
            "epoch 32 batch id 3131 loss 0.0023027868010103703 train acc 0.9764352443308847\n",
            "epoch 32 batch id 3141 loss 0.0751672312617302 train acc 0.9764555475963069\n",
            "epoch 32 batch id 3151 loss 0.0015542196342721581 train acc 0.9764509282767375\n",
            "epoch 32 batch id 3161 loss 0.11279244720935822 train acc 0.9764710534640937\n",
            "epoch 32 batch id 3171 loss 0.021772030740976334 train acc 0.9764516319772942\n",
            "epoch 32 batch id 3181 loss 0.06698758900165558 train acc 0.9764421565545426\n",
            "epoch 32 batch id 3191 loss 0.03134214133024216 train acc 0.9764670166092134\n",
            "epoch 32 batch id 3201 loss 0.07518956065177917 train acc 0.9764575523273977\n",
            "epoch 32 batch id 3211 loss 0.10534561425447464 train acc 0.976443280909374\n",
            "epoch 32 batch id 3221 loss 0.11412712931632996 train acc 0.976390290282521\n",
            "epoch 32 batch id 3231 loss 0.040818311274051666 train acc 0.976366643454039\n",
            "epoch 32 batch id 3241 loss 0.10155818611383438 train acc 0.9763624267201481\n",
            "epoch 32 batch id 3251 loss 0.11151262372732162 train acc 0.9763486235004614\n",
            "epoch 32 batch id 3261 loss 0.0979769229888916 train acc 0.9763061560870898\n",
            "epoch 32 batch id 3271 loss 0.03040136583149433 train acc 0.9762782788138185\n",
            "epoch 32 batch id 3281 loss 0.0924508273601532 train acc 0.9762743828101189\n",
            "epoch 32 batch id 3291 loss 0.031332407146692276 train acc 0.9762610148890914\n",
            "epoch 32 batch id 3301 loss 0.04155321046710014 train acc 0.9762713950318086\n",
            "epoch 32 batch id 3311 loss 0.017763784155249596 train acc 0.9762769933554817\n",
            "epoch 32 batch id 3321 loss 0.03430444374680519 train acc 0.9762966726889492\n",
            "epoch 32 batch id 3331 loss 0.05555546283721924 train acc 0.9762787075953168\n",
            "epoch 32 batch id 3341 loss 0.08882049471139908 train acc 0.9762655267883867\n",
            "epoch 32 batch id 3351 loss 0.011898424476385117 train acc 0.9762337735004476\n",
            "epoch 32 batch id 3361 loss 0.1008700579404831 train acc 0.9762579961321035\n",
            "epoch 32 batch id 3371 loss 0.07817377895116806 train acc 0.9762496291901512\n",
            "epoch 32 batch id 3381 loss 0.0002704565995372832 train acc 0.9762736616385684\n",
            "epoch 32 batch id 3391 loss 0.07188552618026733 train acc 0.9763021601297552\n",
            "epoch 32 batch id 3401 loss 0.07739502191543579 train acc 0.9763075198471038\n",
            "epoch 32 batch id 3411 loss 0.11806811392307281 train acc 0.9763128481383758\n",
            "epoch 32 batch id 3421 loss 0.04487019404768944 train acc 0.9763409821689565\n",
            "epoch 32 batch id 3431 loss 0.12649083137512207 train acc 0.9763416278053045\n",
            "epoch 32 batch id 3441 loss 0.0467267744243145 train acc 0.9763558921825051\n",
            "epoch 32 batch id 3451 loss 0.04470178112387657 train acc 0.976356490872211\n",
            "epoch 32 batch id 3461 loss 0.01117649395018816 train acc 0.9763345131464894\n",
            "epoch 32 batch id 3471 loss 0.03759271651506424 train acc 0.9763396715643907\n",
            "epoch 32 batch id 3481 loss 0.0857783704996109 train acc 0.9763268457339845\n",
            "epoch 32 batch id 3491 loss 0.0033841629046946764 train acc 0.9763275207676884\n",
            "epoch 32 batch id 3501 loss 0.031871676445007324 train acc 0.976354970008569\n",
            "epoch 32 batch id 3511 loss 0.019574157893657684 train acc 0.9763377598974651\n",
            "epoch 32 batch id 3521 loss 0.12880609929561615 train acc 0.9763428358420904\n",
            "epoch 32 batch id 3531 loss 0.10337917506694794 train acc 0.9763788586802605\n",
            "epoch 32 batch id 3541 loss 0.05821484699845314 train acc 0.9763705521039254\n",
            "epoch 32 batch id 3551 loss 0.02546313777565956 train acc 0.9763710926499578\n",
            "epoch 32 batch id 3561 loss 0.010673179291188717 train acc 0.9763847935973041\n",
            "epoch 32 batch id 3571 loss 0.022405611351132393 train acc 0.9764159199103892\n",
            "epoch 32 batch id 3581 loss 0.14547991752624512 train acc 0.9764076026249651\n",
            "epoch 32 batch id 3591 loss 0.04083709791302681 train acc 0.9764254385964912\n",
            "epoch 32 batch id 3601 loss 0.1206585019826889 train acc 0.9764388364343238\n",
            "epoch 32 batch id 3611 loss 0.023517997935414314 train acc 0.9764435059540294\n",
            "epoch 32 batch id 3621 loss 0.06079619377851486 train acc 0.976443834576084\n",
            "epoch 32 batch id 3631 loss 0.03194292262196541 train acc 0.9764441613880473\n",
            "epoch 32 batch id 3641 loss 0.17168214917182922 train acc 0.9764444864048338\n",
            "epoch 32 batch id 3651 loss 0.05740116909146309 train acc 0.9764276910435498\n",
            "epoch 32 batch id 3661 loss 0.04798509180545807 train acc 0.9763896476372576\n",
            "epoch 32 batch id 3671 loss 0.04790905863046646 train acc 0.9764199128302915\n",
            "epoch 32 batch id 3681 loss 0.07302922010421753 train acc 0.9764585031241511\n",
            "epoch 32 batch id 3691 loss 0.08564934134483337 train acc 0.9764841845028448\n",
            "epoch 32 batch id 3701 loss 0.06001119688153267 train acc 0.9764759524452851\n",
            "epoch 32 batch id 3711 loss 0.05844869464635849 train acc 0.9764846065750472\n",
            "epoch 32 batch id 3721 loss 0.07377956807613373 train acc 0.9764932141897339\n",
            "epoch 32 batch id 3731 loss 0.020101692527532578 train acc 0.9765101514339319\n",
            "epoch 32 batch id 3741 loss 0.06444709002971649 train acc 0.9764935846030474\n",
            "epoch 32 batch id 3751 loss 0.04126395657658577 train acc 0.9765104305518528\n",
            "epoch 32 batch id 3761 loss 0.13433218002319336 train acc 0.9764773331560755\n",
            "epoch 32 batch id 3771 loss 0.02233707346022129 train acc 0.9764609851498276\n",
            "epoch 32 batch id 3781 loss 0.10877203941345215 train acc 0.9764488561227188\n",
            "epoch 32 batch id 3791 loss 0.0019914263393729925 train acc 0.9764780071221314\n",
            "epoch 32 batch id 3801 loss 0.21504813432693481 train acc 0.9764494540910287\n",
            "epoch 32 batch id 3811 loss 0.0317499078810215 train acc 0.9764620506428758\n",
            "epoch 32 batch id 3821 loss 0.06742148846387863 train acc 0.9764786705051034\n",
            "epoch 32 batch id 3831 loss 0.011005141772329807 train acc 0.9764503393369878\n",
            "epoch 32 batch id 3841 loss 0.0005625333869829774 train acc 0.9764302915907316\n",
            "epoch 32 batch id 3851 loss 0.012494909577071667 train acc 0.976454979226175\n",
            "epoch 32 batch id 3861 loss 0.030716819688677788 train acc 0.9764673983423984\n",
            "epoch 32 batch id 3871 loss 0.16063614189624786 train acc 0.9764797532937226\n",
            "epoch 32 batch id 3881 loss 0.06663848459720612 train acc 0.9764920445761401\n",
            "epoch 32 batch id 3891 loss 0.012625906616449356 train acc 0.9764922256489335\n",
            "epoch 32 batch id 3901 loss 0.06434822082519531 train acc 0.976472378877211\n",
            "epoch 32 batch id 3911 loss 0.08072254061698914 train acc 0.9764686141651752\n",
            "epoch 32 batch id 3921 loss 0.11032476276159286 train acc 0.9764808084672277\n",
            "epoch 32 batch id 3931 loss 0.07452190667390823 train acc 0.9764651170185703\n",
            "epoch 32 batch id 3941 loss 0.032122835516929626 train acc 0.9764851877696016\n",
            "epoch 32 batch id 3951 loss 0.019096652045845985 train acc 0.9764774740572008\n",
            "epoch 32 batch id 3961 loss 0.02106292173266411 train acc 0.976477688714971\n",
            "epoch 32 batch id 3971 loss 0.1463584154844284 train acc 0.97647396751448\n",
            "epoch 32 batch id 3981 loss 0.03992697224020958 train acc 0.9764624152223059\n",
            "epoch 32 batch id 3991 loss 0.13493752479553223 train acc 0.9764626659984966\n",
            "epoch 32 batch id 4001 loss 0.10883200913667679 train acc 0.9764472944263934\n",
            "epoch 32 batch id 4011 loss 0.14708943665027618 train acc 0.9764125218150087\n",
            "epoch 32 batch id 4021 loss 0.03762109577655792 train acc 0.976412894802288\n",
            "epoch 32 batch id 4031 loss 0.029956312850117683 train acc 0.9764442756139916\n",
            "epoch 32 batch id 4041 loss 0.08847367018461227 train acc 0.976456168027716\n",
            "epoch 32 batch id 4051 loss 0.029329944401979446 train acc 0.9764602875833127\n",
            "epoch 32 batch id 4061 loss 0.022183120250701904 train acc 0.9764605392760404\n",
            "epoch 32 batch id 4071 loss 0.0454462468624115 train acc 0.9764569516089413\n",
            "epoch 32 batch id 4081 loss 0.08396240323781967 train acc 0.976472525116393\n",
            "epoch 32 batch id 4091 loss 0.053997185081243515 train acc 0.9764765644096798\n",
            "epoch 32 batch id 4101 loss 0.0163703765720129 train acc 0.9764729639112412\n",
            "epoch 32 batch id 4111 loss 0.012353388592600822 train acc 0.9764655801508149\n",
            "epoch 32 batch id 4121 loss 0.03450821712613106 train acc 0.9764620237806357\n",
            "epoch 32 batch id 4131 loss 0.20641116797924042 train acc 0.9764547022512708\n",
            "epoch 32 batch id 4141 loss 0.000871468975674361 train acc 0.9764625090557836\n",
            "epoch 32 batch id 4151 loss 0.09168475866317749 train acc 0.9764552216333413\n",
            "epoch 32 batch id 4161 loss 0.09337496012449265 train acc 0.9764367039173276\n",
            "epoch 32 batch id 4171 loss 0.08375339210033417 train acc 0.9764370055142652\n",
            "epoch 32 batch id 4181 loss 0.0091062281280756 train acc 0.9764373056685004\n",
            "epoch 32 batch id 4191 loss 0.11324537545442581 train acc 0.9764189632545932\n",
            "epoch 32 batch id 4201 loss 0.0405084565281868 train acc 0.9764230242799333\n",
            "epoch 32 batch id 4211 loss 0.16493314504623413 train acc 0.9764233554975066\n",
            "epoch 32 batch id 4221 loss 0.10129748284816742 train acc 0.9764088782279081\n",
            "epoch 32 batch id 4231 loss 0.02511250413954258 train acc 0.9764055483337273\n",
            "epoch 32 batch id 4241 loss 0.05000833421945572 train acc 0.9764169712331997\n",
            "epoch 32 batch id 4251 loss 0.11506183445453644 train acc 0.976413637967537\n",
            "epoch 32 batch id 4261 loss 0.10733455419540405 train acc 0.9764139873269185\n",
            "epoch 32 batch id 4271 loss 0.003675996558740735 train acc 0.976421651837977\n",
            "epoch 32 batch id 4281 loss 0.07575763761997223 train acc 0.9764183309974305\n",
            "epoch 32 batch id 4291 loss 0.0612000934779644 train acc 0.9764041016080168\n",
            "epoch 32 batch id 4301 loss 0.04507201537489891 train acc 0.9763863055103464\n",
            "epoch 32 batch id 4311 loss 0.03446080908179283 train acc 0.9764084609139411\n",
            "epoch 32 batch id 4321 loss 0.02408093959093094 train acc 0.9763943531589909\n",
            "epoch 32 batch id 4331 loss 0.042279064655303955 train acc 0.9764055645347495\n",
            "epoch 32 batch id 4341 loss 0.05010070651769638 train acc 0.9763987272517853\n",
            "epoch 32 batch id 4351 loss 0.038445401936769485 train acc 0.976377556883475\n",
            "epoch 32 batch id 4361 loss 0.026294590905308723 train acc 0.9763815638614997\n",
            "epoch 32 batch id 4371 loss 0.060083866119384766 train acc 0.976403425989476\n",
            "epoch 32 batch id 4381 loss 0.12038292735815048 train acc 0.9763859564026478\n",
            "epoch 32 batch id 4391 loss 0.0009077201830223203 train acc 0.976389916875427\n",
            "epoch 32 batch id 4401 loss 0.04113329201936722 train acc 0.976383208361736\n",
            "epoch 32 batch id 4411 loss 0.005949595011770725 train acc 0.9763906993878939\n",
            "epoch 32 batch id 4421 loss 0.10374254733324051 train acc 0.976398156525673\n",
            "epoch 32 batch id 4431 loss 0.04022550582885742 train acc 0.976373843376213\n",
            "epoch 32 batch id 4441 loss 0.01556819025427103 train acc 0.9763953782931772\n",
            "epoch 32 batch id 4451 loss 0.0012666045222431421 train acc 0.9764027746573803\n",
            "epoch 32 batch id 4461 loss 0.07027014344930649 train acc 0.976410137861466\n",
            "epoch 32 batch id 4471 loss 0.03980739787220955 train acc 0.9763999944084097\n",
            "epoch 32 batch id 4481 loss 0.04602302983403206 train acc 0.9763933831733987\n",
            "epoch 32 batch id 4491 loss 0.06257003545761108 train acc 0.9763937597417056\n",
            "epoch 32 batch id 4501 loss 0.08961399644613266 train acc 0.97638719173517\n",
            "epoch 32 batch id 4511 loss 0.16584943234920502 train acc 0.9763702615827976\n",
            "epoch 32 batch id 4521 loss 0.022441904991865158 train acc 0.9763810550763106\n",
            "epoch 32 batch id 4531 loss 0.0003493310941848904 train acc 0.9763849039947031\n",
            "epoch 32 batch id 4541 loss 0.20433543622493744 train acc 0.9763818542171327\n",
            "epoch 32 batch id 4551 loss 0.03345290943980217 train acc 0.9763822511535926\n",
            "epoch 32 batch id 4561 loss 0.17768341302871704 train acc 0.976365517430388\n",
            "epoch 32 batch id 4571 loss 0.07045837491750717 train acc 0.9763762032378035\n",
            "epoch 32 batch id 4581 loss 0.019889095798134804 train acc 0.976390253219821\n",
            "epoch 32 batch id 4591 loss 0.10183697193861008 train acc 0.9763736114136353\n",
            "epoch 32 batch id 4601 loss 0.0633011981844902 train acc 0.9763604379482721\n",
            "epoch 32 batch id 4611 loss 0.040264006704092026 train acc 0.9763710420733029\n",
            "epoch 32 batch id 4621 loss 0.016085604205727577 train acc 0.9763782190002164\n",
            "epoch 32 batch id 4631 loss 0.1055920422077179 train acc 0.9763819909306846\n",
            "epoch 32 batch id 4641 loss 0.11946036666631699 train acc 0.9764059469941823\n",
            "epoch 32 batch id 4651 loss 0.10913516581058502 train acc 0.976423081057837\n",
            "epoch 32 batch id 4661 loss 0.10670206695795059 train acc 0.9764099710362583\n",
            "epoch 32 batch id 4671 loss 0.02086726948618889 train acc 0.9764203329051595\n",
            "epoch 32 batch id 4681 loss 0.19119426608085632 train acc 0.9764206366161077\n",
            "epoch 32 batch id 4691 loss 0.010654959827661514 train acc 0.9764309315710936\n",
            "epoch 32 batch id 4701 loss 0.12002746760845184 train acc 0.9764478302488833\n",
            "epoch 32 batch id 4711 loss 0.04206272214651108 train acc 0.9764414402462323\n",
            "epoch 32 batch id 4721 loss 0.09968717396259308 train acc 0.9764416966744334\n",
            "epoch 32 batch id 4731 loss 0.022022061049938202 train acc 0.9764386493341788\n",
            "epoch 32 batch id 4741 loss 0.07074346393346786 train acc 0.9764455020037967\n",
            "epoch 32 batch id 4751 loss 0.19020754098892212 train acc 0.976439170700905\n",
            "epoch 32 batch id 4761 loss 0.06764353066682816 train acc 0.9764558391094308\n",
            "epoch 32 batch id 4771 loss 0.06519827991724014 train acc 0.9764724376440997\n",
            "epoch 32 batch id 4781 loss 0.1026652455329895 train acc 0.9764562852959632\n",
            "epoch 32 batch id 4791 loss 0.2122175693511963 train acc 0.9764304164057608\n",
            "epoch 32 batch id 4801 loss 0.11973975598812103 train acc 0.976424182461987\n",
            "epoch 32 batch id 4811 loss 0.04078506678342819 train acc 0.9764374610268135\n",
            "epoch 32 batch id 4821 loss 0.07035893946886063 train acc 0.9764474434764572\n",
            "epoch 32 batch id 4831 loss 0.03661580756306648 train acc 0.9764606189194783\n",
            "epoch 32 batch id 4841 loss 0.1451515108346939 train acc 0.9764576017351787\n",
            "epoch 32 batch id 4851 loss 0.0829867273569107 train acc 0.9764739229024944\n",
            "epoch 32 batch id 4861 loss 0.0005551961367018521 train acc 0.9764676764040321\n",
            "epoch 32 batch id 4871 loss 0.09626050293445587 train acc 0.9764871176349825\n",
            "epoch 32 batch id 4881 loss 0.07013095915317535 train acc 0.9764808696988322\n",
            "epoch 32 batch id 4891 loss 0.021243255585432053 train acc 0.976484231241055\n",
            "epoch 32 batch id 4901 loss 0.001309026381932199 train acc 0.9764812028157519\n",
            "epoch 32 batch id 4911 loss 0.11344440281391144 train acc 0.9764718234575442\n",
            "epoch 32 batch id 4921 loss 0.1347137987613678 train acc 0.9764720077220077\n",
            "epoch 32 batch id 4931 loss 0.0696515291929245 train acc 0.9764563475968363\n",
            "epoch 32 batch id 4941 loss 0.07453396171331406 train acc 0.9764439131754705\n",
            "epoch 32 batch id 4951 loss 0.14766855537891388 train acc 0.976415749343567\n",
            "epoch 32 train acc 0.9764127748638289\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88bea62427ce45c4b28702e177d191bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 32 loss 1.0434470176696777 test acc 0.8448657441348973\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d05434672e64603a2016fe572052fc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 33 batch id 1 loss 0.033210109919309616 train acc 0.984375\n",
            "epoch 33 batch id 11 loss 0.07485900819301605 train acc 0.9801136363636364\n",
            "epoch 33 batch id 21 loss 0.009303655475378036 train acc 0.9799107142857143\n",
            "epoch 33 batch id 31 loss 0.12485969066619873 train acc 0.9753024193548387\n",
            "epoch 33 batch id 41 loss 0.021570397540926933 train acc 0.9782774390243902\n",
            "epoch 33 batch id 51 loss 0.011732655577361584 train acc 0.9791666666666666\n",
            "epoch 33 batch id 61 loss 0.03566620126366615 train acc 0.975922131147541\n",
            "epoch 33 batch id 71 loss 0.08860362321138382 train acc 0.976012323943662\n",
            "epoch 33 batch id 81 loss 0.05898279324173927 train acc 0.9753086419753086\n",
            "epoch 33 batch id 91 loss 0.07648638635873795 train acc 0.9754464285714286\n",
            "epoch 33 batch id 101 loss 0.06292736530303955 train acc 0.9744740099009901\n",
            "epoch 33 batch id 111 loss 0.000335610267939046 train acc 0.9748029279279279\n",
            "epoch 33 batch id 121 loss 0.037263937294483185 train acc 0.9755940082644629\n",
            "epoch 33 batch id 131 loss 0.02918442152440548 train acc 0.9754293893129771\n",
            "epoch 33 batch id 141 loss 0.2321547120809555 train acc 0.9756205673758865\n",
            "epoch 33 batch id 151 loss 0.06455294787883759 train acc 0.9754759933774835\n",
            "epoch 33 batch id 161 loss 0.007730109617114067 train acc 0.9759316770186336\n",
            "epoch 33 batch id 171 loss 0.026092275977134705 train acc 0.9758771929824561\n",
            "epoch 33 batch id 181 loss 0.025124868378043175 train acc 0.9763466850828729\n",
            "epoch 33 batch id 191 loss 0.05376189574599266 train acc 0.9761125654450262\n",
            "epoch 33 batch id 201 loss 0.09453141689300537 train acc 0.976523631840796\n",
            "epoch 33 batch id 211 loss 0.11402682960033417 train acc 0.9764514218009479\n",
            "epoch 33 batch id 221 loss 0.15589849650859833 train acc 0.9762443438914027\n",
            "epoch 33 batch id 231 loss 0.06186792999505997 train acc 0.976663961038961\n",
            "epoch 33 batch id 241 loss 0.03490331396460533 train acc 0.9767894190871369\n",
            "epoch 33 batch id 251 loss 0.10724928975105286 train acc 0.976531374501992\n",
            "epoch 33 batch id 261 loss 0.17867164313793182 train acc 0.9765924329501916\n",
            "epoch 33 batch id 271 loss 0.004718865733593702 train acc 0.9768796125461254\n",
            "epoch 33 batch id 281 loss 0.10470543801784515 train acc 0.9769239323843416\n",
            "epoch 33 batch id 291 loss 0.10729775577783585 train acc 0.9771799828178694\n",
            "epoch 33 batch id 301 loss 0.1586884707212448 train acc 0.9768480066445183\n",
            "epoch 33 batch id 311 loss 0.03372037038207054 train acc 0.976939308681672\n",
            "epoch 33 batch id 321 loss 0.1291234940290451 train acc 0.976976246105919\n",
            "epoch 33 batch id 331 loss 0.08335564285516739 train acc 0.9770109516616314\n",
            "epoch 33 batch id 341 loss 0.17215269804000854 train acc 0.9771352639296188\n",
            "epoch 33 batch id 351 loss 0.13358208537101746 train acc 0.9772079772079773\n",
            "epoch 33 batch id 361 loss 0.1576569378376007 train acc 0.9771035318559557\n",
            "epoch 33 batch id 371 loss 0.025635521858930588 train acc 0.9773837601078167\n",
            "epoch 33 batch id 381 loss 0.021656906232237816 train acc 0.9776492782152231\n",
            "epoch 33 batch id 391 loss 0.1343906968832016 train acc 0.9778212915601023\n",
            "epoch 33 batch id 401 loss 0.0025216753128916025 train acc 0.9777509351620948\n",
            "epoch 33 batch id 411 loss 0.0023574414663016796 train acc 0.9777600364963503\n",
            "epoch 33 batch id 421 loss 0.07932960242033005 train acc 0.97791716152019\n",
            "epoch 33 batch id 431 loss 0.07234637439250946 train acc 0.9778494779582366\n",
            "epoch 33 batch id 441 loss 0.11476774513721466 train acc 0.9775722789115646\n",
            "epoch 33 batch id 451 loss 0.15626904368400574 train acc 0.9776538248337029\n",
            "epoch 33 batch id 461 loss 0.07134976238012314 train acc 0.97746068329718\n",
            "epoch 33 batch id 471 loss 0.10778551548719406 train acc 0.9776074840764332\n",
            "epoch 33 batch id 481 loss 0.20417451858520508 train acc 0.9778131496881497\n",
            "epoch 33 batch id 491 loss 0.048811137676239014 train acc 0.9778831466395111\n",
            "epoch 33 batch id 501 loss 0.11197349429130554 train acc 0.9780127245508982\n",
            "epoch 33 batch id 511 loss 0.08693365007638931 train acc 0.9777703033268101\n",
            "epoch 33 batch id 521 loss 0.10945923626422882 train acc 0.9778670825335892\n",
            "epoch 33 batch id 531 loss 0.0198731180280447 train acc 0.9776365348399246\n",
            "epoch 33 batch id 541 loss 0.19103094935417175 train acc 0.9775011552680222\n",
            "epoch 33 batch id 551 loss 0.14241783320903778 train acc 0.9775124773139746\n",
            "epoch 33 batch id 561 loss 0.10202992707490921 train acc 0.977328431372549\n",
            "epoch 33 batch id 571 loss 0.061378225684165955 train acc 0.9773971103327496\n",
            "epoch 33 batch id 581 loss 0.031163092702627182 train acc 0.9775441049913941\n",
            "epoch 33 batch id 591 loss 0.026742352172732353 train acc 0.9775274957698815\n",
            "epoch 33 batch id 601 loss 0.019283803179860115 train acc 0.9775114392678869\n",
            "epoch 33 batch id 611 loss 0.06820081174373627 train acc 0.9773680441898527\n",
            "epoch 33 batch id 621 loss 0.1431974619626999 train acc 0.9773047504025765\n",
            "epoch 33 batch id 631 loss 0.0657699853181839 train acc 0.9773920364500792\n",
            "epoch 33 batch id 641 loss 0.025662455707788467 train acc 0.9774278471138845\n",
            "epoch 33 batch id 651 loss 0.020737528800964355 train acc 0.9773665514592934\n",
            "epoch 33 batch id 661 loss 0.02829526551067829 train acc 0.9774016641452344\n",
            "epoch 33 batch id 671 loss 0.02575581520795822 train acc 0.9773891579731744\n",
            "epoch 33 batch id 681 loss 0.10911685228347778 train acc 0.9774229074889867\n",
            "epoch 33 batch id 691 loss 0.16990041732788086 train acc 0.9773878437047757\n",
            "epoch 33 batch id 701 loss 0.06549212336540222 train acc 0.9773537803138374\n",
            "epoch 33 batch id 711 loss 0.12968701124191284 train acc 0.977232770745429\n",
            "epoch 33 batch id 721 loss 0.06317494809627533 train acc 0.9771151178918169\n",
            "epoch 33 batch id 731 loss 0.023367250338196754 train acc 0.977171682626539\n",
            "epoch 33 batch id 741 loss 0.1306600719690323 train acc 0.9772267206477733\n",
            "epoch 33 batch id 751 loss 0.08859230577945709 train acc 0.9769057922769641\n",
            "epoch 33 batch id 761 loss 0.004665504675358534 train acc 0.9770244743758213\n",
            "epoch 33 batch id 771 loss 0.06823252141475677 train acc 0.9770590142671854\n",
            "epoch 33 batch id 781 loss 0.02945788949728012 train acc 0.9771526888604354\n",
            "epoch 33 batch id 791 loss 0.027849722653627396 train acc 0.9772044879898862\n",
            "epoch 33 batch id 801 loss 0.0032531297765672207 train acc 0.9771379525593009\n",
            "epoch 33 batch id 811 loss 0.09869373589754105 train acc 0.9769767262638718\n",
            "epoch 33 batch id 821 loss 0.01969245634973049 train acc 0.9769526492082826\n",
            "epoch 33 batch id 831 loss 0.11710613965988159 train acc 0.9769667569193743\n",
            "epoch 33 batch id 841 loss 0.07134775817394257 train acc 0.9769619500594531\n",
            "epoch 33 batch id 851 loss 0.043194446712732315 train acc 0.976883813160987\n",
            "epoch 33 batch id 861 loss 0.05147397145628929 train acc 0.9768619337979094\n",
            "epoch 33 batch id 871 loss 0.016359737142920494 train acc 0.9769840700344432\n",
            "epoch 33 batch id 881 loss 0.07645481079816818 train acc 0.9768551362088536\n",
            "epoch 33 batch id 891 loss 0.011499588377773762 train acc 0.9769219977553311\n",
            "epoch 33 batch id 901 loss 0.10776617377996445 train acc 0.97695269145394\n",
            "epoch 33 batch id 911 loss 0.10620629787445068 train acc 0.9769998627881449\n",
            "epoch 33 batch id 921 loss 0.08974340558052063 train acc 0.9769102877307275\n",
            "epoch 33 batch id 931 loss 0.003468523034825921 train acc 0.9769401181525241\n",
            "epoch 33 batch id 941 loss 0.046699102967977524 train acc 0.9769693145589798\n",
            "epoch 33 batch id 951 loss 0.05304696410894394 train acc 0.9768500262881178\n",
            "epoch 33 batch id 961 loss 0.024329060688614845 train acc 0.9768795525494277\n",
            "epoch 33 batch id 971 loss 0.01670556515455246 train acc 0.9768923789907312\n",
            "epoch 33 batch id 981 loss 0.036276593804359436 train acc 0.9769527268093782\n",
            "epoch 33 batch id 991 loss 0.08588306605815887 train acc 0.9769172552976791\n",
            "epoch 33 batch id 1001 loss 0.1271631419658661 train acc 0.9768981018981019\n",
            "epoch 33 batch id 1011 loss 0.0021769620943814516 train acc 0.9768638724035609\n",
            "epoch 33 batch id 1021 loss 0.04864470660686493 train acc 0.9769068315377081\n",
            "epoch 33 batch id 1031 loss 0.02187059074640274 train acc 0.9769489573229874\n",
            "epoch 33 batch id 1041 loss 0.029484247788786888 train acc 0.9769902737752162\n",
            "epoch 33 batch id 1051 loss 0.01798522286117077 train acc 0.9768821360608944\n",
            "epoch 33 batch id 1061 loss 0.019574923440814018 train acc 0.9768349434495759\n",
            "epoch 33 batch id 1071 loss 0.0003352835774421692 train acc 0.9768323996265172\n",
            "epoch 33 batch id 1081 loss 0.11040029674768448 train acc 0.9767142691951897\n",
            "epoch 33 batch id 1091 loss 0.04614894464612007 train acc 0.9767128780934922\n",
            "epoch 33 batch id 1101 loss 0.056346625089645386 train acc 0.9766547456857403\n",
            "epoch 33 batch id 1111 loss 0.13675378262996674 train acc 0.9765273402340234\n",
            "epoch 33 batch id 1121 loss 0.06509052962064743 train acc 0.9766112845673506\n",
            "epoch 33 batch id 1131 loss 0.028373539447784424 train acc 0.9765694076038903\n",
            "epoch 33 batch id 1141 loss 0.12799711525440216 train acc 0.976555652936021\n",
            "epoch 33 batch id 1151 loss 0.06124741584062576 train acc 0.9764606863596872\n",
            "epoch 33 batch id 1161 loss 0.11693670600652695 train acc 0.976475021533161\n",
            "epoch 33 batch id 1171 loss 0.020328961312770844 train acc 0.9764090520922288\n",
            "epoch 33 batch id 1181 loss 0.1475372165441513 train acc 0.9764235817104149\n",
            "epoch 33 batch id 1191 loss 0.01538961660116911 train acc 0.9763722712006717\n",
            "epoch 33 batch id 1201 loss 0.03622809052467346 train acc 0.9763738551207327\n",
            "epoch 33 batch id 1211 loss 0.0013766676420345902 train acc 0.9764012180016516\n",
            "epoch 33 batch id 1221 loss 0.16966041922569275 train acc 0.9763513513513513\n",
            "epoch 33 batch id 1231 loss 0.10915520787239075 train acc 0.9763276807473599\n",
            "epoch 33 batch id 1241 loss 0.005679621361196041 train acc 0.9763673448831588\n",
            "epoch 33 batch id 1251 loss 0.15208123624324799 train acc 0.9764188649080735\n",
            "epoch 33 batch id 1261 loss 0.06841949373483658 train acc 0.9763828310864393\n",
            "epoch 33 batch id 1271 loss 0.04360557720065117 train acc 0.9764088316286389\n",
            "epoch 33 batch id 1281 loss 0.03780294209718704 train acc 0.9763978337236534\n",
            "epoch 33 batch id 1291 loss 0.09335527569055557 train acc 0.9764354182804028\n",
            "epoch 33 batch id 1301 loss 0.14253926277160645 train acc 0.9763883551114527\n",
            "epoch 33 batch id 1311 loss 0.01938752271234989 train acc 0.976377765064836\n",
            "epoch 33 batch id 1321 loss 0.015025115571916103 train acc 0.9763200227100681\n",
            "epoch 33 batch id 1331 loss 0.024363137781620026 train acc 0.976286626596544\n",
            "epoch 33 batch id 1341 loss 0.12929365038871765 train acc 0.9762886838180462\n",
            "epoch 33 batch id 1351 loss 0.003423531074076891 train acc 0.9763254071058475\n",
            "epoch 33 batch id 1361 loss 0.04897376894950867 train acc 0.9763501102130786\n",
            "epoch 33 batch id 1371 loss 0.24051326513290405 train acc 0.9763402625820569\n",
            "epoch 33 batch id 1381 loss 0.09800006449222565 train acc 0.9763984431571325\n",
            "epoch 33 batch id 1391 loss 0.008692357689142227 train acc 0.9763883896477354\n",
            "epoch 33 batch id 1401 loss 0.010248848237097263 train acc 0.9764453961456103\n",
            "epoch 33 batch id 1411 loss 0.016142528504133224 train acc 0.9765348157335223\n",
            "epoch 33 batch id 1421 loss 0.008138411678373814 train acc 0.9765460063335679\n",
            "epoch 33 batch id 1431 loss 0.017247211188077927 train acc 0.9766116352201258\n",
            "epoch 33 batch id 1441 loss 0.0315936841070652 train acc 0.9765679215822346\n",
            "epoch 33 batch id 1451 loss 0.014823432080447674 train acc 0.9766324948311509\n",
            "epoch 33 batch id 1461 loss 0.07397757470607758 train acc 0.9766427104722792\n",
            "epoch 33 batch id 1471 loss 0.05291888862848282 train acc 0.9766634092454113\n",
            "epoch 33 batch id 1481 loss 0.049062419682741165 train acc 0.9766732781904118\n",
            "epoch 33 batch id 1491 loss 0.10047361254692078 train acc 0.9767354124748491\n",
            "epoch 33 batch id 1501 loss 0.09333644062280655 train acc 0.9767654896735509\n",
            "epoch 33 batch id 1511 loss 0.03467520698904991 train acc 0.9767848279285242\n",
            "epoch 33 batch id 1521 loss 0.10632525384426117 train acc 0.9767422748191978\n",
            "epoch 33 batch id 1531 loss 0.01962791569530964 train acc 0.9767615120836055\n",
            "epoch 33 batch id 1541 loss 0.018878506496548653 train acc 0.976770360155743\n",
            "epoch 33 batch id 1551 loss 0.031765107065439224 train acc 0.9767589458413927\n",
            "epoch 33 batch id 1561 loss 0.2155599594116211 train acc 0.9767076393337604\n",
            "epoch 33 batch id 1571 loss 0.00795046053826809 train acc 0.9767067154678549\n",
            "epoch 33 batch id 1581 loss 0.03344512730836868 train acc 0.9766662713472486\n",
            "epoch 33 batch id 1591 loss 0.1534290313720703 train acc 0.9766852608422376\n",
            "epoch 33 batch id 1601 loss 0.27954134345054626 train acc 0.9766844940662086\n",
            "epoch 33 batch id 1611 loss 0.06368757039308548 train acc 0.9766643389199255\n",
            "epoch 33 batch id 1621 loss 0.09395111352205276 train acc 0.9766733497840839\n",
            "epoch 33 batch id 1631 loss 0.0002049533650279045 train acc 0.9767301502145923\n",
            "epoch 33 batch id 1641 loss 0.07500119507312775 train acc 0.9767481718464351\n",
            "epoch 33 batch id 1651 loss 0.034415923058986664 train acc 0.9766713355542096\n",
            "epoch 33 batch id 1661 loss 0.053057245910167694 train acc 0.9767459361830223\n",
            "epoch 33 batch id 1671 loss 0.10602317005395889 train acc 0.9767448384201077\n",
            "epoch 33 batch id 1681 loss 0.1152164563536644 train acc 0.976743753718025\n",
            "epoch 33 batch id 1691 loss 0.06642547994852066 train acc 0.9767426818450621\n",
            "epoch 33 batch id 1701 loss 0.04158098250627518 train acc 0.9767599941211053\n",
            "epoch 33 batch id 1711 loss 0.0650535374879837 train acc 0.9767771040327294\n",
            "epoch 33 batch id 1721 loss 0.009359790943562984 train acc 0.9768121731551423\n",
            "epoch 33 batch id 1731 loss 0.11456271260976791 train acc 0.9768197573656846\n",
            "epoch 33 batch id 1741 loss 0.1159496083855629 train acc 0.9768272544514647\n",
            "epoch 33 batch id 1751 loss 0.03828565776348114 train acc 0.9768168189605939\n",
            "epoch 33 batch id 1761 loss 0.027161454781889915 train acc 0.9768774843838728\n",
            "epoch 33 batch id 1771 loss 0.007666186895221472 train acc 0.9769286420101637\n",
            "epoch 33 batch id 1781 loss 0.00026530743343755603 train acc 0.9769616788321168\n",
            "epoch 33 batch id 1791 loss 0.15039540827274323 train acc 0.9769420016750419\n",
            "epoch 33 batch id 1801 loss 0.021785998716950417 train acc 0.9769919489172681\n",
            "epoch 33 batch id 1811 loss 0.0349978543817997 train acc 0.9769982054113749\n",
            "epoch 33 batch id 1821 loss 0.10082077980041504 train acc 0.9769700713893466\n",
            "epoch 33 batch id 1831 loss 0.01610460877418518 train acc 0.9770019797924632\n",
            "epoch 33 batch id 1841 loss 0.06079603359103203 train acc 0.9769486692015209\n",
            "epoch 33 batch id 1851 loss 0.08504799008369446 train acc 0.9769550243111832\n",
            "epoch 33 batch id 1861 loss 0.025264935567975044 train acc 0.9769864991939817\n",
            "epoch 33 batch id 1871 loss 0.0893615111708641 train acc 0.9769925841795831\n",
            "epoch 33 batch id 1881 loss 0.02737712487578392 train acc 0.9770069112174375\n",
            "epoch 33 batch id 1891 loss 0.04666658490896225 train acc 0.9770293495505024\n",
            "epoch 33 batch id 1901 loss 0.1354781538248062 train acc 0.9770268937401367\n",
            "epoch 33 batch id 1911 loss 0.03430400416254997 train acc 0.9769917582417582\n",
            "epoch 33 batch id 1921 loss 0.001170697039924562 train acc 0.9769569885476315\n",
            "epoch 33 batch id 1931 loss 0.1184273213148117 train acc 0.9769711289487313\n",
            "epoch 33 batch id 1941 loss 0.012385264970362186 train acc 0.9769690236991242\n",
            "epoch 33 batch id 1951 loss 0.07734372466802597 train acc 0.9769349051768323\n",
            "epoch 33 batch id 1961 loss 0.16229891777038574 train acc 0.9769808133605303\n",
            "epoch 33 batch id 1971 loss 0.0070852539502084255 train acc 0.9769945459157788\n",
            "epoch 33 batch id 1981 loss 0.10805555433034897 train acc 0.976976590106007\n",
            "epoch 33 batch id 1991 loss 0.1319030076265335 train acc 0.9769509668508287\n",
            "epoch 33 batch id 2001 loss 0.18476226925849915 train acc 0.9769490254872564\n",
            "epoch 33 batch id 2011 loss 0.10267908126115799 train acc 0.976931563898558\n",
            "epoch 33 batch id 2021 loss 0.15110832452774048 train acc 0.9769220064324592\n",
            "epoch 33 batch id 2031 loss 0.007750726770609617 train acc 0.9769048498276711\n",
            "epoch 33 batch id 2041 loss 0.009491559118032455 train acc 0.9768495835374816\n",
            "epoch 33 batch id 2051 loss 0.23666535317897797 train acc 0.9768024744027304\n",
            "epoch 33 batch id 2061 loss 0.0177862960845232 train acc 0.9768013100436681\n",
            "epoch 33 batch id 2071 loss 0.15322229266166687 train acc 0.9768001569290198\n",
            "epoch 33 batch id 2081 loss 0.10043665021657944 train acc 0.9767764896684287\n",
            "epoch 33 batch id 2091 loss 0.05195377767086029 train acc 0.9767605212816834\n",
            "epoch 33 batch id 2101 loss 0.005660688038915396 train acc 0.97682651118515\n",
            "epoch 33 batch id 2111 loss 0.014369162730872631 train acc 0.9768400639507342\n",
            "epoch 33 batch id 2121 loss 0.01874459721148014 train acc 0.9768166548797736\n",
            "epoch 33 batch id 2131 loss 0.1317109614610672 train acc 0.9768521234162365\n",
            "epoch 33 batch id 2141 loss 0.10864748060703278 train acc 0.976828876693134\n",
            "epoch 33 batch id 2151 loss 0.0219713244587183 train acc 0.9768857508135751\n",
            "epoch 33 batch id 2161 loss 0.10938732326030731 train acc 0.9768770245256826\n",
            "epoch 33 batch id 2171 loss 0.0801495686173439 train acc 0.9769043643482266\n",
            "epoch 33 batch id 2181 loss 0.25142720341682434 train acc 0.9768669761577258\n",
            "epoch 33 batch id 2191 loss 0.1526973843574524 train acc 0.9768869808306709\n",
            "epoch 33 batch id 2201 loss 0.018012912943959236 train acc 0.9768926056338029\n",
            "epoch 33 batch id 2211 loss 0.0009456691914238036 train acc 0.9768769787426503\n",
            "epoch 33 batch id 2221 loss 0.043738916516304016 train acc 0.9768896330481764\n",
            "epoch 33 batch id 2231 loss 0.012922832742333412 train acc 0.9769161810847153\n",
            "epoch 33 batch id 2241 loss 0.07293417304754257 train acc 0.9768867135207496\n",
            "epoch 33 batch id 2251 loss 0.036831650882959366 train acc 0.9768436250555309\n",
            "epoch 33 batch id 2261 loss 0.009375627152621746 train acc 0.9768631136665192\n",
            "epoch 33 batch id 2271 loss 0.023788882419466972 train acc 0.976875550418318\n",
            "epoch 33 batch id 2281 loss 0.14352241158485413 train acc 0.9768467777290663\n",
            "epoch 33 batch id 2291 loss 0.016201552003622055 train acc 0.9768659973810563\n",
            "epoch 33 batch id 2301 loss 0.02078246884047985 train acc 0.9769122120817036\n",
            "epoch 33 batch id 2311 loss 0.07019875198602676 train acc 0.976937743401125\n",
            "epoch 33 batch id 2321 loss 0.1431705206632614 train acc 0.9769091986212839\n",
            "epoch 33 batch id 2331 loss 0.06169411540031433 train acc 0.9768876018876019\n",
            "epoch 33 batch id 2341 loss 0.030679889023303986 train acc 0.9769062366510038\n",
            "epoch 33 batch id 2351 loss 0.07038386166095734 train acc 0.9768648979157806\n",
            "epoch 33 batch id 2361 loss 0.0684535801410675 train acc 0.9769033248623464\n",
            "epoch 33 batch id 2371 loss 0.027384011074900627 train acc 0.976921657528469\n",
            "epoch 33 batch id 2381 loss 0.007174480706453323 train acc 0.9769267114657707\n",
            "epoch 33 batch id 2391 loss 0.0223278496414423 train acc 0.9769186532831451\n",
            "epoch 33 batch id 2401 loss 0.029440894722938538 train acc 0.9768911391087047\n",
            "epoch 33 batch id 2411 loss 0.1089344397187233 train acc 0.9768768145997512\n",
            "epoch 33 batch id 2421 loss 0.0033853217028081417 train acc 0.9769529636513837\n",
            "epoch 33 batch id 2431 loss 0.05748290196061134 train acc 0.9769256478815302\n",
            "epoch 33 batch id 2441 loss 0.0030907434411346912 train acc 0.9768729516591561\n",
            "epoch 33 batch id 2451 loss 0.0004842507478315383 train acc 0.9768525601795186\n",
            "epoch 33 batch id 2461 loss 0.07760018855333328 train acc 0.9768704286875254\n",
            "epoch 33 batch id 2471 loss 0.1259310394525528 train acc 0.9768185957102388\n",
            "epoch 33 batch id 2481 loss 0.06751731038093567 train acc 0.9768679463925837\n",
            "epoch 33 batch id 2491 loss 0.10912274569272995 train acc 0.9768729927739863\n",
            "epoch 33 batch id 2501 loss 0.02868649922311306 train acc 0.9769092363054778\n",
            "epoch 33 batch id 2511 loss 0.035390354692935944 train acc 0.9769078554360813\n",
            "epoch 33 batch id 2521 loss 0.10949347913265228 train acc 0.9768630999603332\n",
            "epoch 33 batch id 2531 loss 0.016003970056772232 train acc 0.9769051264322403\n",
            "epoch 33 batch id 2541 loss 0.04978295788168907 train acc 0.9768730322707595\n",
            "epoch 33 batch id 2551 loss 0.04470069333910942 train acc 0.9768411897295178\n",
            "epoch 33 batch id 2561 loss 0.1502051055431366 train acc 0.9768278992581023\n",
            "epoch 33 batch id 2571 loss 0.13026949763298035 train acc 0.9768207895760405\n",
            "epoch 33 batch id 2581 loss 0.04540971294045448 train acc 0.9768076811313444\n",
            "epoch 33 batch id 2591 loss 0.018013261258602142 train acc 0.9768187958317252\n",
            "epoch 33 batch id 2601 loss 0.06709253042936325 train acc 0.9768178104575164\n",
            "epoch 33 batch id 2611 loss 0.006636218633502722 train acc 0.976828801225584\n",
            "epoch 33 batch id 2621 loss 0.0639912486076355 train acc 0.9768218237314003\n",
            "epoch 33 batch id 2631 loss 0.10766041278839111 train acc 0.9768030216647663\n",
            "epoch 33 batch id 2641 loss 0.14269043505191803 train acc 0.9768198599015524\n",
            "epoch 33 batch id 2651 loss 0.028025854378938675 train acc 0.9768012070916635\n",
            "epoch 33 batch id 2661 loss 0.0883435308933258 train acc 0.9768061818865088\n",
            "epoch 33 batch id 2671 loss 0.006010180339217186 train acc 0.9768169692998877\n",
            "epoch 33 batch id 2681 loss 0.03073734976351261 train acc 0.9768160201417382\n",
            "epoch 33 batch id 2691 loss 0.07715480029582977 train acc 0.976832497212932\n",
            "epoch 33 batch id 2701 loss 0.041435930877923965 train acc 0.9768488522769345\n",
            "epoch 33 batch id 2711 loss 0.004828681703656912 train acc 0.9768305053485798\n",
            "epoch 33 batch id 2721 loss 0.009973470121622086 train acc 0.9768467475192943\n",
            "epoch 33 batch id 2731 loss 0.0018420096021145582 train acc 0.9768628707433175\n",
            "epoch 33 batch id 2741 loss 0.14042480289936066 train acc 0.9768845767967895\n",
            "epoch 33 batch id 2751 loss 0.09150499105453491 train acc 0.9768834060341693\n",
            "epoch 33 batch id 2761 loss 0.0064369188621640205 train acc 0.9769105396595437\n",
            "epoch 33 batch id 2771 loss 0.0023801918141543865 train acc 0.9769431162035366\n",
            "epoch 33 batch id 2781 loss 0.10612249374389648 train acc 0.9769810769507371\n",
            "epoch 33 batch id 2791 loss 0.12974244356155396 train acc 0.9769683805087782\n",
            "epoch 33 batch id 2801 loss 0.10527661442756653 train acc 0.9769613530881828\n",
            "epoch 33 batch id 2811 loss 0.01930980011820793 train acc 0.9769766097474208\n",
            "epoch 33 batch id 2821 loss 0.015983177348971367 train acc 0.9769862194257356\n",
            "epoch 33 batch id 2831 loss 0.01755785010755062 train acc 0.9769681649593783\n",
            "epoch 33 batch id 2841 loss 0.10422677546739578 train acc 0.9769997360084477\n",
            "epoch 33 batch id 2851 loss 0.008086380548775196 train acc 0.9770146439845668\n",
            "epoch 33 batch id 2861 loss 0.10380295664072037 train acc 0.97700760223698\n",
            "epoch 33 batch id 2871 loss 0.15312987565994263 train acc 0.977000609543713\n",
            "epoch 33 batch id 2881 loss 0.07789081335067749 train acc 0.9770099357861853\n",
            "epoch 33 batch id 2891 loss 0.003611442632973194 train acc 0.9770462210307852\n",
            "epoch 33 batch id 2901 loss 0.04998492822051048 train acc 0.9770768700448121\n",
            "epoch 33 batch id 2911 loss 0.06161234900355339 train acc 0.9770697354860872\n",
            "epoch 33 batch id 2921 loss 0.0004593105404637754 train acc 0.9770252054091064\n",
            "epoch 33 batch id 2931 loss 0.011732475832104683 train acc 0.97701829580348\n",
            "epoch 33 batch id 2941 loss 0.02187998592853546 train acc 0.9770167460047603\n",
            "epoch 33 batch id 2951 loss 0.04149849712848663 train acc 0.9770205015249068\n",
            "epoch 33 batch id 2961 loss 0.013064831495285034 train acc 0.9770453394123607\n",
            "epoch 33 batch id 2971 loss 0.13787978887557983 train acc 0.9770384550656345\n",
            "epoch 33 batch id 2981 loss 0.08062246441841125 train acc 0.9770473414961423\n",
            "epoch 33 batch id 2991 loss 0.020502030849456787 train acc 0.9770666165162153\n",
            "epoch 33 batch id 3001 loss 0.007989936508238316 train acc 0.9770857630789737\n",
            "epoch 33 batch id 3011 loss 0.12852148711681366 train acc 0.9770892145466622\n",
            "epoch 33 batch id 3021 loss 0.13454227149486542 train acc 0.9770874710360807\n",
            "epoch 33 batch id 3031 loss 0.19948174059391022 train acc 0.9770805839656879\n",
            "epoch 33 batch id 3041 loss 0.08990640193223953 train acc 0.977048051627754\n",
            "epoch 33 batch id 3051 loss 0.0011353555601090193 train acc 0.9770618239921337\n",
            "epoch 33 batch id 3061 loss 0.12797483801841736 train acc 0.9770244609604705\n",
            "epoch 33 batch id 3071 loss 0.05707419291138649 train acc 0.9770382204493651\n",
            "epoch 33 batch id 3081 loss 0.10818172991275787 train acc 0.977036676403765\n",
            "epoch 33 batch id 3091 loss 0.015589195303618908 train acc 0.9770604173406664\n",
            "epoch 33 batch id 3101 loss 0.05860524997115135 train acc 0.9770386568848759\n",
            "epoch 33 batch id 3111 loss 0.09986218065023422 train acc 0.9770521938283511\n",
            "epoch 33 batch id 3121 loss 0.05628379061818123 train acc 0.9770406119833387\n",
            "epoch 33 batch id 3131 loss 0.06513781100511551 train acc 0.9770540562120729\n",
            "epoch 33 batch id 3141 loss 0.0651591420173645 train acc 0.9770524912448265\n",
            "epoch 33 batch id 3151 loss 0.10183616727590561 train acc 0.9770410187242146\n",
            "epoch 33 batch id 3161 loss 0.11207637190818787 train acc 0.9769950173995571\n",
            "epoch 33 batch id 3171 loss 0.06871325522661209 train acc 0.9769887259539577\n",
            "epoch 33 batch id 3181 loss 0.02309011109173298 train acc 0.9769873860421251\n",
            "epoch 33 batch id 3191 loss 0.01923488639295101 train acc 0.9770154340332184\n",
            "epoch 33 batch id 3201 loss 0.154137521982193 train acc 0.9769749687597625\n",
            "epoch 33 batch id 3211 loss 0.08724813163280487 train acc 0.9769250233572095\n",
            "epoch 33 batch id 3221 loss 0.1634078323841095 train acc 0.976855984166408\n",
            "epoch 33 batch id 3231 loss 0.0968763679265976 train acc 0.9768599117920148\n",
            "epoch 33 batch id 3241 loss 0.0728209987282753 train acc 0.9768541730947239\n",
            "epoch 33 batch id 3251 loss 0.052893463522195816 train acc 0.9768340510612119\n",
            "epoch 33 batch id 3261 loss 0.10771828889846802 train acc 0.9767996780128795\n",
            "epoch 33 batch id 3271 loss 0.08494962006807327 train acc 0.9767607383063284\n",
            "epoch 33 batch id 3281 loss 0.19543522596359253 train acc 0.9767458473026517\n",
            "epoch 33 batch id 3291 loss 0.011989995837211609 train acc 0.9767500379823761\n",
            "epoch 33 batch id 3301 loss 0.023504694923758507 train acc 0.9767731369282036\n",
            "epoch 33 batch id 3311 loss 0.020956391468644142 train acc 0.9767913772274237\n",
            "epoch 33 batch id 3321 loss 0.013126993551850319 train acc 0.9768236224028907\n",
            "epoch 33 batch id 3331 loss 0.06133907288312912 train acc 0.97679469378565\n",
            "epoch 33 batch id 3341 loss 0.012828114442527294 train acc 0.9768033522897336\n",
            "epoch 33 batch id 3351 loss 0.01682383008301258 train acc 0.9767933079677709\n",
            "epoch 33 batch id 3361 loss 0.2400076538324356 train acc 0.9767972701576911\n",
            "epoch 33 batch id 3371 loss 0.1528724879026413 train acc 0.976791938593889\n",
            "epoch 33 batch id 3381 loss 0.0029777369927614927 train acc 0.9767820171546879\n",
            "epoch 33 batch id 3391 loss 0.16933850944042206 train acc 0.9767859775877322\n",
            "epoch 33 batch id 3401 loss 0.07263371348381042 train acc 0.9767853204939724\n",
            "epoch 33 batch id 3411 loss 0.10365813970565796 train acc 0.9767800864849018\n",
            "epoch 33 batch id 3421 loss 0.06539543718099594 train acc 0.9767931525869629\n",
            "epoch 33 batch id 3431 loss 0.14604085683822632 train acc 0.9768061425240455\n",
            "epoch 33 batch id 3441 loss 0.04632152244448662 train acc 0.9767872711421098\n",
            "epoch 33 batch id 3451 loss 0.013338463380932808 train acc 0.9768047305128948\n",
            "epoch 33 batch id 3461 loss 0.029541075229644775 train acc 0.976785972262352\n",
            "epoch 33 batch id 3471 loss 0.07020098716020584 train acc 0.9767853284356093\n",
            "epoch 33 batch id 3481 loss 0.04889683425426483 train acc 0.9767891769606435\n",
            "epoch 33 batch id 3491 loss 0.015736427158117294 train acc 0.9768019550272128\n",
            "epoch 33 batch id 3501 loss 0.025926996022462845 train acc 0.9768235861182519\n",
            "epoch 33 batch id 3511 loss 0.1351069062948227 train acc 0.9768272927940758\n",
            "epoch 33 batch id 3521 loss 0.11183898150920868 train acc 0.9768576043737575\n",
            "epoch 33 batch id 3531 loss 0.05537459999322891 train acc 0.9768523435287454\n",
            "epoch 33 batch id 3541 loss 0.048058293759822845 train acc 0.9768426998023158\n",
            "epoch 33 batch id 3551 loss 0.0598142184317112 train acc 0.9768067093776401\n",
            "epoch 33 batch id 3561 loss 0.00684394920244813 train acc 0.9768411260881775\n",
            "epoch 33 batch id 3571 loss 0.09594497084617615 train acc 0.97685347241669\n",
            "epoch 33 batch id 3581 loss 0.2857496440410614 train acc 0.9768439332588662\n",
            "epoch 33 batch id 3591 loss 0.10016116499900818 train acc 0.9768475006961849\n",
            "epoch 33 batch id 3601 loss 0.12163875997066498 train acc 0.9768467092474312\n",
            "epoch 33 batch id 3611 loss 0.0026287627406418324 train acc 0.9768589033508723\n",
            "epoch 33 batch id 3621 loss 0.07165451347827911 train acc 0.9768408243579122\n",
            "epoch 33 batch id 3631 loss 0.016819648444652557 train acc 0.9768142385017902\n",
            "epoch 33 batch id 3641 loss 0.04615766927599907 train acc 0.9768392955232079\n",
            "epoch 33 batch id 3651 loss 0.10231548547744751 train acc 0.9768171391399617\n",
            "epoch 33 batch id 3661 loss 0.016948970034718513 train acc 0.9767908358372029\n",
            "epoch 33 batch id 3671 loss 0.06943570822477341 train acc 0.9767987265050395\n",
            "epoch 33 batch id 3681 loss 0.029252175241708755 train acc 0.976827798152676\n",
            "epoch 33 batch id 3691 loss 0.02440948784351349 train acc 0.9768482457328638\n",
            "epoch 33 batch id 3701 loss 0.050549283623695374 train acc 0.9768348081599567\n",
            "epoch 33 batch id 3711 loss 0.016699640080332756 train acc 0.9768593371059013\n",
            "epoch 33 batch id 3721 loss 0.07660866528749466 train acc 0.9768585393711368\n",
            "epoch 33 batch id 3731 loss 0.0988122820854187 train acc 0.9768535580273385\n",
            "epoch 33 batch id 3741 loss 0.13540887832641602 train acc 0.9768527800053461\n",
            "epoch 33 batch id 3751 loss 0.12220842391252518 train acc 0.9768478405758464\n",
            "epoch 33 batch id 3761 loss 0.13024619221687317 train acc 0.9768180005317735\n",
            "epoch 33 batch id 3771 loss 0.018517056480050087 train acc 0.976792462211615\n",
            "epoch 33 batch id 3781 loss 0.14854834973812103 train acc 0.9767959865115049\n",
            "epoch 33 batch id 3791 loss 0.002095986856147647 train acc 0.9768118570298074\n",
            "epoch 33 batch id 3801 loss 0.06907878816127777 train acc 0.9768029794790845\n",
            "epoch 33 batch id 3811 loss 0.029108475893735886 train acc 0.9768105484124902\n",
            "epoch 33 batch id 3821 loss 0.05928860232234001 train acc 0.9767976315100759\n",
            "epoch 33 batch id 3831 loss 0.03772047907114029 train acc 0.9768133320281911\n",
            "epoch 33 batch id 3841 loss 0.0008237772854045033 train acc 0.9768248828430096\n",
            "epoch 33 batch id 3851 loss 0.0036100551951676607 train acc 0.9768444884445598\n",
            "epoch 33 batch id 3861 loss 0.0256431233137846 train acc 0.9768558987308987\n",
            "epoch 33 batch id 3871 loss 0.13170498609542847 train acc 0.9768591772151899\n",
            "epoch 33 batch id 3881 loss 0.06921830773353577 train acc 0.9768624388044318\n",
            "epoch 33 batch id 3891 loss 0.00838252529501915 train acc 0.9768576522744795\n",
            "epoch 33 batch id 3901 loss 0.0066973986104130745 train acc 0.9768448795180723\n",
            "epoch 33 batch id 3911 loss 0.05162348970770836 train acc 0.9768361672206597\n",
            "epoch 33 batch id 3921 loss 0.03217040374875069 train acc 0.9768474241264984\n",
            "epoch 33 batch id 3931 loss 0.13228897750377655 train acc 0.9768347748664462\n",
            "epoch 33 batch id 3941 loss 0.013338823802769184 train acc 0.9768539076376554\n",
            "epoch 33 batch id 3951 loss 0.01913665048778057 train acc 0.9768531700835231\n",
            "epoch 33 batch id 3961 loss 0.0029713057447224855 train acc 0.9768721598081292\n",
            "epoch 33 batch id 3971 loss 0.08166488260030746 train acc 0.9768517061193654\n",
            "epoch 33 batch id 3981 loss 0.05498996376991272 train acc 0.9768352800803818\n",
            "epoch 33 batch id 3991 loss 0.16445069015026093 train acc 0.9768267664745678\n",
            "epoch 33 batch id 4001 loss 0.1752193570137024 train acc 0.9768143901524619\n",
            "epoch 33 batch id 4011 loss 0.05148976668715477 train acc 0.9767787023186237\n",
            "epoch 33 batch id 4021 loss 0.01674686186015606 train acc 0.9767975938821188\n",
            "epoch 33 batch id 4031 loss 0.003491061506792903 train acc 0.9768163917142149\n",
            "epoch 33 batch id 4041 loss 0.1371413916349411 train acc 0.9768080301905468\n",
            "epoch 33 batch id 4051 loss 0.020203813910484314 train acc 0.9768189953098001\n",
            "epoch 33 batch id 4061 loss 0.08789018541574478 train acc 0.9768068209800542\n",
            "epoch 33 batch id 4071 loss 0.0910436138510704 train acc 0.9768100589535741\n",
            "epoch 33 batch id 4081 loss 0.0461258701980114 train acc 0.9768247672139182\n",
            "epoch 33 batch id 4091 loss 0.030289707705378532 train acc 0.9768164874113908\n",
            "epoch 33 batch id 4101 loss 0.008358252234756947 train acc 0.9768234881736162\n",
            "epoch 33 batch id 4111 loss 0.01737326569855213 train acc 0.9768152517635612\n",
            "epoch 33 batch id 4121 loss 0.08615325391292572 train acc 0.9768146384372725\n",
            "epoch 33 batch id 4131 loss 0.13832014799118042 train acc 0.976814028080368\n",
            "epoch 33 batch id 4141 loss 0.009282286278903484 train acc 0.9768322868872253\n",
            "epoch 33 batch id 4151 loss 0.06176808476448059 train acc 0.9768429294145989\n",
            "epoch 33 batch id 4161 loss 0.047194525599479675 train acc 0.9768122146118722\n",
            "epoch 33 batch id 4171 loss 0.09908902645111084 train acc 0.976807869815392\n",
            "epoch 33 batch id 4181 loss 0.020309291779994965 train acc 0.9767848600813203\n",
            "epoch 33 batch id 4191 loss 0.09295617789030075 train acc 0.9767806012884753\n",
            "epoch 33 batch id 4201 loss 0.04325060546398163 train acc 0.97681355629612\n",
            "epoch 33 batch id 4211 loss 0.06056974083185196 train acc 0.9768018285442888\n",
            "epoch 33 batch id 4221 loss 0.08384817838668823 train acc 0.9767938580904999\n",
            "epoch 33 batch id 4231 loss 0.0427706316113472 train acc 0.9767822323327818\n",
            "epoch 33 batch id 4241 loss 0.02894635871052742 train acc 0.9768001355812308\n",
            "epoch 33 batch id 4251 loss 0.3090035319328308 train acc 0.9767995765702188\n",
            "epoch 33 batch id 4261 loss 0.0022601629607379436 train acc 0.9768136881013847\n",
            "epoch 33 batch id 4271 loss 0.011011180467903614 train acc 0.9768313919456801\n",
            "epoch 33 batch id 4281 loss 0.04500880092382431 train acc 0.9768380635365569\n",
            "epoch 33 batch id 4291 loss 0.008324717171490192 train acc 0.9768374213470054\n",
            "epoch 33 batch id 4301 loss 0.010717739351093769 train acc 0.9768295163915368\n",
            "epoch 33 batch id 4311 loss 0.03174019604921341 train acc 0.9768433948039897\n",
            "epoch 33 batch id 4321 loss 0.04303902015089989 train acc 0.9768391286739181\n",
            "epoch 33 batch id 4331 loss 0.1067713275551796 train acc 0.9768457053798199\n",
            "epoch 33 batch id 4341 loss 0.10389164835214615 train acc 0.9768486523842432\n",
            "epoch 33 batch id 4351 loss 0.09545277059078217 train acc 0.9768192656860492\n",
            "epoch 33 batch id 4361 loss 0.02199629321694374 train acc 0.9768365913781243\n",
            "epoch 33 batch id 4371 loss 0.09019894152879715 train acc 0.9768288149164951\n",
            "epoch 33 batch id 4381 loss 0.128911554813385 train acc 0.9768175074183977\n",
            "epoch 33 batch id 4391 loss 0.0011395805049687624 train acc 0.9768026930084264\n",
            "epoch 33 batch id 4401 loss 0.04012584313750267 train acc 0.9768021472392638\n",
            "epoch 33 batch id 4411 loss 0.032183192670345306 train acc 0.9767945193833598\n",
            "epoch 33 batch id 4421 loss 0.05687401071190834 train acc 0.9768152001809546\n",
            "epoch 33 batch id 4431 loss 0.013674492947757244 train acc 0.9768111035883548\n",
            "epoch 33 batch id 4441 loss 0.03875443711876869 train acc 0.9768105437964423\n",
            "epoch 33 batch id 4451 loss 0.02306746132671833 train acc 0.9768134969669737\n",
            "epoch 33 batch id 4461 loss 0.007979590445756912 train acc 0.9768164368975566\n",
            "epoch 33 batch id 4471 loss 0.17685897648334503 train acc 0.9767914057257884\n",
            "epoch 33 batch id 4481 loss 0.09687227010726929 train acc 0.9767804340548984\n",
            "epoch 33 batch id 4491 loss 0.08295591175556183 train acc 0.9767729904252951\n",
            "epoch 33 batch id 4501 loss 0.009941797703504562 train acc 0.976762108420351\n",
            "epoch 33 batch id 4511 loss 0.09948939085006714 train acc 0.9767547384172024\n",
            "epoch 33 batch id 4521 loss 0.04007742553949356 train acc 0.9767508571112585\n",
            "epoch 33 batch id 4531 loss 0.0049145836383104324 train acc 0.9767469929375414\n",
            "epoch 33 batch id 4541 loss 0.07352200150489807 train acc 0.9767534683990311\n",
            "epoch 33 batch id 4551 loss 0.055324818938970566 train acc 0.9767599154032081\n",
            "epoch 33 batch id 4561 loss 0.14447063207626343 train acc 0.9767492052181539\n",
            "epoch 33 batch id 4571 loss 0.02895393967628479 train acc 0.9767453784729818\n",
            "epoch 33 batch id 4581 loss 0.02549576200544834 train acc 0.9767415684348395\n",
            "epoch 33 batch id 4591 loss 0.04917038977146149 train acc 0.9767445817904596\n",
            "epoch 33 batch id 4601 loss 0.13274209201335907 train acc 0.9767339980439035\n",
            "epoch 33 batch id 4611 loss 0.03261758014559746 train acc 0.9767742897419215\n",
            "epoch 33 batch id 4621 loss 0.017297526821494102 train acc 0.9767873566327635\n",
            "epoch 33 batch id 4631 loss 0.1381119042634964 train acc 0.9767969930900453\n",
            "epoch 33 batch id 4641 loss 0.11330121010541916 train acc 0.9768032212885154\n",
            "epoch 33 batch id 4651 loss 0.13307739794254303 train acc 0.9768094227047946\n",
            "epoch 33 batch id 4661 loss 0.05911378189921379 train acc 0.9767988360866767\n",
            "epoch 33 batch id 4671 loss 0.02888588048517704 train acc 0.9768083654463712\n",
            "epoch 33 batch id 4681 loss 0.22753767669200897 train acc 0.9768145161290323\n",
            "epoch 33 batch id 4691 loss 0.007100187707692385 train acc 0.9768239714346622\n",
            "epoch 33 batch id 4701 loss 0.026784030720591545 train acc 0.9768333865135078\n",
            "epoch 33 batch id 4711 loss 0.0714666023850441 train acc 0.976836128210571\n",
            "epoch 33 batch id 4721 loss 0.006595798768103123 train acc 0.976835548612582\n",
            "epoch 33 batch id 4731 loss 0.06549382954835892 train acc 0.9768415768336504\n",
            "epoch 33 batch id 4741 loss 0.1917029470205307 train acc 0.9768409881881459\n",
            "epoch 33 batch id 4751 loss 0.18266336619853973 train acc 0.9768338244580088\n",
            "epoch 33 batch id 4761 loss 0.08957701176404953 train acc 0.976833254568368\n",
            "epoch 33 batch id 4771 loss 0.05978924036026001 train acc 0.9768359620624607\n",
            "epoch 33 batch id 4781 loss 0.09421193599700928 train acc 0.9768419263752353\n",
            "epoch 33 batch id 4791 loss 0.17302897572517395 train acc 0.9768217752035065\n",
            "epoch 33 batch id 4801 loss 0.10246162861585617 train acc 0.9768277442199542\n",
            "epoch 33 batch id 4811 loss 0.00397405494004488 train acc 0.9768466794845146\n",
            "epoch 33 batch id 4821 loss 0.1757584512233734 train acc 0.9768525720804813\n",
            "epoch 33 batch id 4831 loss 0.04318931698799133 train acc 0.9768487373214655\n",
            "epoch 33 batch id 4841 loss 0.0663735643029213 train acc 0.976838463127453\n",
            "epoch 33 batch id 4851 loss 0.045857012271881104 train acc 0.9768475572047001\n",
            "epoch 33 batch id 4861 loss 0.04935281723737717 train acc 0.976830898991977\n",
            "epoch 33 batch id 4871 loss 0.055712711066007614 train acc 0.9768463867788955\n",
            "epoch 33 batch id 4881 loss 0.08374833315610886 train acc 0.9768490063511576\n",
            "epoch 33 batch id 4891 loss 0.021565968170762062 train acc 0.9768388366387242\n",
            "epoch 33 batch id 4901 loss 0.027582181617617607 train acc 0.9768255203019792\n",
            "epoch 33 batch id 4911 loss 0.14469937980175018 train acc 0.9768313479942985\n",
            "epoch 33 batch id 4921 loss 0.12373840063810349 train acc 0.9768339768339769\n",
            "epoch 33 batch id 4931 loss 0.04572800174355507 train acc 0.9768239200973433\n",
            "epoch 33 batch id 4941 loss 0.0865672156214714 train acc 0.9767980924913985\n",
            "epoch 33 batch id 4951 loss 0.1094927042722702 train acc 0.9767913047869118\n",
            "epoch 33 train acc 0.976791027839419\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "312796c6c25b4608b8b7a381d61e5776",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 33 loss 0.7060408592224121 test acc 0.8452895894428153\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38402d1f52dc467bbb5b294db8657ee3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 34 batch id 1 loss 0.11928021907806396 train acc 0.953125\n",
            "epoch 34 batch id 11 loss 0.020022012293338776 train acc 0.9758522727272727\n",
            "epoch 34 batch id 21 loss 0.005219563841819763 train acc 0.9799107142857143\n",
            "epoch 34 batch id 31 loss 0.10309363901615143 train acc 0.9793346774193549\n",
            "epoch 34 batch id 41 loss 0.050510406494140625 train acc 0.979420731707317\n",
            "epoch 34 batch id 51 loss 0.05491871386766434 train acc 0.9788602941176471\n",
            "epoch 34 batch id 61 loss 0.046220436692237854 train acc 0.9766905737704918\n",
            "epoch 34 batch id 71 loss 0.09650194644927979 train acc 0.977112676056338\n",
            "epoch 34 batch id 81 loss 0.010046329349279404 train acc 0.9766589506172839\n",
            "epoch 34 batch id 91 loss 0.09477124363183975 train acc 0.9764766483516484\n",
            "epoch 34 batch id 101 loss 0.04140642285346985 train acc 0.9752475247524752\n",
            "epoch 34 batch id 111 loss 0.00036007960443384945 train acc 0.9755067567567568\n",
            "epoch 34 batch id 121 loss 0.11765953153371811 train acc 0.9757231404958677\n",
            "epoch 34 batch id 131 loss 0.07292903959751129 train acc 0.9756679389312977\n",
            "epoch 34 batch id 141 loss 0.004908093251287937 train acc 0.9766179078014184\n",
            "epoch 34 batch id 151 loss 0.043303556740283966 train acc 0.9775455298013245\n",
            "epoch 34 batch id 161 loss 0.0565432645380497 train acc 0.9779697204968945\n",
            "epoch 34 batch id 171 loss 0.010568976402282715 train acc 0.9778874269005848\n",
            "epoch 34 batch id 181 loss 0.07452317327260971 train acc 0.9779868784530387\n",
            "epoch 34 batch id 191 loss 0.12602803111076355 train acc 0.9775850785340314\n",
            "epoch 34 batch id 201 loss 0.21834711730480194 train acc 0.9776896766169154\n",
            "epoch 34 batch id 211 loss 0.1502389758825302 train acc 0.9778584123222749\n",
            "epoch 34 batch id 221 loss 0.07499957084655762 train acc 0.9773755656108597\n",
            "epoch 34 batch id 231 loss 0.03713545575737953 train acc 0.9777462121212122\n",
            "epoch 34 batch id 241 loss 0.19067427515983582 train acc 0.9775674273858921\n",
            "epoch 34 batch id 251 loss 0.16619297862052917 train acc 0.9772783864541833\n",
            "epoch 34 batch id 261 loss 0.06771687418222427 train acc 0.977191091954023\n",
            "epoch 34 batch id 271 loss 0.0007898651529103518 train acc 0.9775138376383764\n",
            "epoch 34 batch id 281 loss 0.0032632749062031507 train acc 0.9778136120996441\n",
            "epoch 34 batch id 291 loss 0.17407675087451935 train acc 0.9779317010309279\n",
            "epoch 34 batch id 301 loss 0.1232633888721466 train acc 0.9777304817275747\n",
            "epoch 34 batch id 311 loss 0.014149639755487442 train acc 0.9777934083601286\n",
            "epoch 34 batch id 321 loss 0.08200307190418243 train acc 0.9776577102803738\n",
            "epoch 34 batch id 331 loss 0.13143648207187653 train acc 0.9776246223564955\n",
            "epoch 34 batch id 341 loss 0.3182077705860138 train acc 0.9776392961876833\n",
            "epoch 34 batch id 351 loss 0.02485152892768383 train acc 0.9777866809116809\n",
            "epoch 34 batch id 361 loss 0.08889906108379364 train acc 0.9778393351800554\n",
            "epoch 34 batch id 371 loss 0.017338315024971962 train acc 0.9779733827493261\n",
            "epoch 34 batch id 381 loss 0.01582709141075611 train acc 0.978141404199475\n",
            "epoch 34 batch id 391 loss 0.08152033388614655 train acc 0.978300831202046\n",
            "epoch 34 batch id 401 loss 0.00045443957787938416 train acc 0.9783354114713217\n",
            "epoch 34 batch id 411 loss 0.004039138089865446 train acc 0.9784823600973236\n",
            "epoch 34 batch id 421 loss 0.06991570442914963 train acc 0.9783625296912114\n",
            "epoch 34 batch id 431 loss 0.06952687352895737 train acc 0.9782845127610209\n",
            "epoch 34 batch id 441 loss 0.1723984330892563 train acc 0.9781391723356009\n",
            "epoch 34 batch id 451 loss 0.06214345619082451 train acc 0.9782774390243902\n",
            "epoch 34 batch id 461 loss 0.0017203945899382234 train acc 0.9780368763557483\n",
            "epoch 34 batch id 471 loss 0.022035466507077217 train acc 0.9783041401273885\n",
            "epoch 34 batch id 481 loss 0.0753076821565628 train acc 0.9784303534303534\n",
            "epoch 34 batch id 491 loss 0.1354311853647232 train acc 0.9782650203665988\n",
            "epoch 34 batch id 501 loss 0.15354673564434052 train acc 0.9783869760479041\n",
            "epoch 34 batch id 511 loss 0.11896663159132004 train acc 0.9781372309197651\n",
            "epoch 34 batch id 521 loss 0.09816776216030121 train acc 0.9782269673704415\n",
            "epoch 34 batch id 531 loss 0.03093036822974682 train acc 0.978136770244821\n",
            "epoch 34 batch id 541 loss 0.019963178783655167 train acc 0.9779343807763401\n",
            "epoch 34 batch id 551 loss 0.1342204064130783 train acc 0.9779094827586207\n",
            "epoch 34 batch id 561 loss 0.04287029802799225 train acc 0.977774064171123\n",
            "epoch 34 batch id 571 loss 0.11649632453918457 train acc 0.9778075744308231\n",
            "epoch 34 batch id 581 loss 0.06893240660429001 train acc 0.9778399311531841\n",
            "epoch 34 batch id 591 loss 0.08844203501939774 train acc 0.9778183164128595\n",
            "epoch 34 batch id 601 loss 0.05256790667772293 train acc 0.9779014143094842\n",
            "epoch 34 batch id 611 loss 0.06987395882606506 train acc 0.9778283551554828\n",
            "epoch 34 batch id 621 loss 0.2335924357175827 train acc 0.9778582930756844\n",
            "epoch 34 batch id 631 loss 0.07087601721286774 train acc 0.9778625198098256\n",
            "epoch 34 batch id 641 loss 0.04661273583769798 train acc 0.9779153666146646\n",
            "epoch 34 batch id 651 loss 0.016283761709928513 train acc 0.9779905913978495\n",
            "epoch 34 batch id 661 loss 0.005123145878314972 train acc 0.9780399016641452\n",
            "epoch 34 batch id 671 loss 0.1223847046494484 train acc 0.9780644560357675\n",
            "epoch 34 batch id 681 loss 0.05532994493842125 train acc 0.9781800660792952\n",
            "epoch 34 batch id 691 loss 0.12609224021434784 train acc 0.9781340448625181\n",
            "epoch 34 batch id 701 loss 0.05391249433159828 train acc 0.9781116262482168\n",
            "epoch 34 batch id 711 loss 0.12364962697029114 train acc 0.9780678621659634\n",
            "epoch 34 batch id 721 loss 0.05173465982079506 train acc 0.9779819694868238\n",
            "epoch 34 batch id 731 loss 0.03178678825497627 train acc 0.9778984268125855\n",
            "epoch 34 batch id 741 loss 0.017137011513113976 train acc 0.9778593117408907\n",
            "epoch 34 batch id 751 loss 0.10069513320922852 train acc 0.9778004327563249\n",
            "epoch 34 batch id 761 loss 0.0765107125043869 train acc 0.9778252299605782\n",
            "epoch 34 batch id 771 loss 0.025476450100541115 train acc 0.9778291180285343\n",
            "epoch 34 batch id 781 loss 0.03146590292453766 train acc 0.977872919334187\n",
            "epoch 34 batch id 791 loss 0.102849081158638 train acc 0.9778563527180784\n",
            "epoch 34 batch id 801 loss 0.04745463281869888 train acc 0.977957240948814\n",
            "epoch 34 batch id 811 loss 0.05846405774354935 train acc 0.9779785758323057\n",
            "epoch 34 batch id 821 loss 0.011353904381394386 train acc 0.978113580998782\n",
            "epoch 34 batch id 831 loss 0.09725384414196014 train acc 0.9780761131167268\n",
            "epoch 34 batch id 841 loss 0.004013299010694027 train acc 0.9780209571938169\n",
            "epoch 34 batch id 851 loss 0.10910636931657791 train acc 0.9780038190364277\n",
            "epoch 34 batch id 861 loss 0.10362613946199417 train acc 0.9779507839721254\n",
            "epoch 34 batch id 871 loss 0.005398955196142197 train acc 0.977881027554535\n",
            "epoch 34 batch id 881 loss 0.026987897232174873 train acc 0.9778128547105562\n",
            "epoch 34 batch id 891 loss 0.002338533056899905 train acc 0.9778865039281706\n",
            "epoch 34 batch id 901 loss 0.07689541578292847 train acc 0.9777850998890122\n",
            "epoch 34 batch id 911 loss 0.0848228931427002 train acc 0.9778059824368825\n",
            "epoch 34 batch id 921 loss 0.06811266392469406 train acc 0.9777755157437568\n",
            "epoch 34 batch id 931 loss 0.006654001772403717 train acc 0.9778296186895811\n",
            "epoch 34 batch id 941 loss 0.026237446814775467 train acc 0.9779489904357067\n",
            "epoch 34 batch id 951 loss 0.0522143617272377 train acc 0.9778522607781283\n",
            "epoch 34 batch id 961 loss 0.030683185905218124 train acc 0.9778388397502601\n",
            "epoch 34 batch id 971 loss 0.00963276531547308 train acc 0.9778417868177137\n",
            "epoch 34 batch id 981 loss 0.013231834396719933 train acc 0.9779720948012233\n",
            "epoch 34 batch id 991 loss 0.07894941419363022 train acc 0.9780051715438951\n",
            "epoch 34 batch id 1001 loss 0.025717297568917274 train acc 0.9780688061938062\n",
            "epoch 34 batch id 1011 loss 0.06533611565828323 train acc 0.9780384520276953\n",
            "epoch 34 batch id 1021 loss 0.04279521107673645 train acc 0.9780699069539667\n",
            "epoch 34 batch id 1031 loss 0.11276330053806305 train acc 0.978115906886518\n",
            "epoch 34 batch id 1041 loss 0.04764540120959282 train acc 0.9781460134486071\n",
            "epoch 34 batch id 1051 loss 0.05472714453935623 train acc 0.9780268791627021\n",
            "epoch 34 batch id 1061 loss 0.02819828689098358 train acc 0.9780130772855796\n",
            "epoch 34 batch id 1071 loss 0.011929711326956749 train acc 0.9781016573295985\n",
            "epoch 34 batch id 1081 loss 0.13812373578548431 train acc 0.9779573311748381\n",
            "epoch 34 batch id 1091 loss 0.4893530607223511 train acc 0.9779302245646196\n",
            "epoch 34 batch id 1101 loss 0.09176559001207352 train acc 0.9779319936421436\n",
            "epoch 34 batch id 1111 loss 0.2008216828107834 train acc 0.9778352835283528\n",
            "epoch 34 batch id 1121 loss 0.03564849868416786 train acc 0.9778796833184656\n",
            "epoch 34 batch id 1131 loss 0.041380949318408966 train acc 0.9778265915119363\n",
            "epoch 34 batch id 1141 loss 0.056099455803632736 train acc 0.9778976774758983\n",
            "epoch 34 batch id 1151 loss 0.09410060197114944 train acc 0.9777910512597741\n",
            "epoch 34 batch id 1161 loss 0.13600867986679077 train acc 0.9778208440999139\n",
            "epoch 34 batch id 1171 loss 0.13084763288497925 train acc 0.9778234415029889\n",
            "epoch 34 batch id 1181 loss 0.11348950117826462 train acc 0.9778656858594411\n",
            "epoch 34 batch id 1191 loss 0.07141442596912384 train acc 0.9778809823677582\n",
            "epoch 34 batch id 1201 loss 0.01919141784310341 train acc 0.9779870940882598\n",
            "epoch 34 batch id 1211 loss 0.06462222337722778 train acc 0.978039843104872\n",
            "epoch 34 batch id 1221 loss 0.1775844842195511 train acc 0.9780405405405406\n",
            "epoch 34 batch id 1231 loss 0.12941133975982666 train acc 0.977952376116978\n",
            "epoch 34 batch id 1241 loss 0.007163004484027624 train acc 0.9779537671232876\n",
            "epoch 34 batch id 1251 loss 0.15318910777568817 train acc 0.9779301558752997\n",
            "epoch 34 batch id 1261 loss 0.037888746708631516 train acc 0.9779564829500397\n",
            "epoch 34 batch id 1271 loss 0.040039755403995514 train acc 0.977970102281668\n",
            "epoch 34 batch id 1281 loss 0.09166514128446579 train acc 0.9779835089773614\n",
            "epoch 34 batch id 1291 loss 0.1261434555053711 train acc 0.9779967079783114\n",
            "epoch 34 batch id 1301 loss 0.08295239508152008 train acc 0.9779736740968485\n",
            "epoch 34 batch id 1311 loss 0.025328611955046654 train acc 0.9779986651411137\n",
            "epoch 34 batch id 1321 loss 0.023325886577367783 train acc 0.9779759651778955\n",
            "epoch 34 batch id 1331 loss 0.04601118341088295 train acc 0.9778831705484599\n",
            "epoch 34 batch id 1341 loss 0.0976484939455986 train acc 0.977873322147651\n",
            "epoch 34 batch id 1351 loss 0.004032115451991558 train acc 0.9778751850481125\n",
            "epoch 34 batch id 1361 loss 0.01427857019007206 train acc 0.9779114621601763\n",
            "epoch 34 batch id 1371 loss 0.02500716969370842 train acc 0.977924416484318\n",
            "epoch 34 batch id 1381 loss 0.026071760803461075 train acc 0.9779711259956553\n",
            "epoch 34 batch id 1391 loss 0.012460276484489441 train acc 0.9779834651329978\n",
            "epoch 34 batch id 1401 loss 0.014128525741398335 train acc 0.9780290863668808\n",
            "epoch 34 batch id 1411 loss 0.02017180435359478 train acc 0.97806298724309\n",
            "epoch 34 batch id 1421 loss 0.07880829274654388 train acc 0.9780524278676987\n",
            "epoch 34 batch id 1431 loss 0.14515696465969086 train acc 0.9780420160726765\n",
            "epoch 34 batch id 1441 loss 0.029132619500160217 train acc 0.9779992192921583\n",
            "epoch 34 batch id 1451 loss 0.04098951444029808 train acc 0.9780216230186078\n",
            "epoch 34 batch id 1461 loss 0.04256502911448479 train acc 0.9779902464065708\n",
            "epoch 34 batch id 1471 loss 0.023910168558359146 train acc 0.978044272603671\n",
            "epoch 34 batch id 1481 loss 0.07059407234191895 train acc 0.9780342673869007\n",
            "epoch 34 batch id 1491 loss 0.08245910704135895 train acc 0.9780243963782697\n",
            "epoch 34 batch id 1501 loss 0.07968815416097641 train acc 0.978014656895403\n",
            "epoch 34 batch id 1511 loss 0.038097914308309555 train acc 0.97805675049636\n",
            "epoch 34 batch id 1521 loss 0.07926800847053528 train acc 0.9780880177514792\n",
            "epoch 34 batch id 1531 loss 0.024107351899147034 train acc 0.9780984650555192\n",
            "epoch 34 batch id 1541 loss 0.08234380930662155 train acc 0.9781189162881246\n",
            "epoch 34 batch id 1551 loss 0.039946965873241425 train acc 0.9781794003868471\n",
            "epoch 34 batch id 1561 loss 0.0422825962305069 train acc 0.9781790518898142\n",
            "epoch 34 batch id 1571 loss 0.13645020127296448 train acc 0.978178707829408\n",
            "epoch 34 batch id 1581 loss 0.04228643700480461 train acc 0.9781289531941809\n",
            "epoch 34 batch id 1591 loss 0.08497043699026108 train acc 0.9781485700817096\n",
            "epoch 34 batch id 1601 loss 0.04724154248833656 train acc 0.9782264990630856\n",
            "epoch 34 batch id 1611 loss 0.10721280425786972 train acc 0.9781870732464308\n",
            "epoch 34 batch id 1621 loss 0.1068018302321434 train acc 0.9781963294262801\n",
            "epoch 34 batch id 1631 loss 0.04646103084087372 train acc 0.9782437921520539\n",
            "epoch 34 batch id 1641 loss 0.07177329808473587 train acc 0.9782430682510664\n",
            "epoch 34 batch id 1651 loss 0.044515956193208694 train acc 0.9782044972743792\n",
            "epoch 34 batch id 1661 loss 0.056457094848155975 train acc 0.97823223961469\n",
            "epoch 34 batch id 1671 loss 0.023264339193701744 train acc 0.978240948533812\n",
            "epoch 34 batch id 1681 loss 0.0034646030981093645 train acc 0.978240258774539\n",
            "epoch 34 batch id 1691 loss 0.11163157224655151 train acc 0.9782488172678888\n",
            "epoch 34 batch id 1701 loss 0.022329797968268394 train acc 0.9782297178130511\n",
            "epoch 34 batch id 1711 loss 0.04172148555517197 train acc 0.9782382378725891\n",
            "epoch 34 batch id 1721 loss 0.008383574895560741 train acc 0.9782194218477629\n",
            "epoch 34 batch id 1731 loss 0.000559059961233288 train acc 0.9782279029462738\n",
            "epoch 34 batch id 1741 loss 0.015133137814700603 train acc 0.9782362866168869\n",
            "epoch 34 batch id 1751 loss 0.04052148014307022 train acc 0.9782267275842376\n",
            "epoch 34 batch id 1761 loss 0.02721402794122696 train acc 0.9782527683134583\n",
            "epoch 34 batch id 1771 loss 0.0012330661993473768 train acc 0.9782608695652174\n",
            "epoch 34 batch id 1781 loss 0.07045383006334305 train acc 0.9783039724873667\n",
            "epoch 34 batch id 1791 loss 0.047911178320646286 train acc 0.9783291457286433\n",
            "epoch 34 batch id 1801 loss 0.09107320010662079 train acc 0.9783106607440311\n",
            "epoch 34 batch id 1811 loss 0.035550933331251144 train acc 0.9783010077305356\n",
            "epoch 34 batch id 1821 loss 0.10258913040161133 train acc 0.9782657193849533\n",
            "epoch 34 batch id 1831 loss 0.08244863897562027 train acc 0.9782820180229382\n",
            "epoch 34 batch id 1841 loss 0.010270768776535988 train acc 0.9782387289516568\n",
            "epoch 34 batch id 1851 loss 0.022827396169304848 train acc 0.9782718800648298\n",
            "epoch 34 batch id 1861 loss 0.026969775557518005 train acc 0.9782375067168189\n",
            "epoch 34 batch id 1871 loss 0.07433001697063446 train acc 0.978320416889364\n",
            "epoch 34 batch id 1881 loss 0.01883808523416519 train acc 0.9783027644869751\n",
            "epoch 34 batch id 1891 loss 0.023855406790971756 train acc 0.9783348757271285\n",
            "epoch 34 batch id 1901 loss 0.09147953987121582 train acc 0.9782844555497107\n",
            "epoch 34 batch id 1911 loss 0.023043496534228325 train acc 0.9782018576661434\n",
            "epoch 34 batch id 1921 loss 0.049532920122146606 train acc 0.9781282535137948\n",
            "epoch 34 batch id 1931 loss 0.08378905802965164 train acc 0.9780877783531848\n",
            "epoch 34 batch id 1941 loss 0.05055785924196243 train acc 0.9781201700154559\n",
            "epoch 34 batch id 1951 loss 0.06376926600933075 train acc 0.9781362121988724\n",
            "epoch 34 batch id 1961 loss 0.05710316076874733 train acc 0.9781759943906171\n",
            "epoch 34 batch id 1971 loss 0.011585959233343601 train acc 0.9782153729071538\n",
            "epoch 34 batch id 1981 loss 0.08955962210893631 train acc 0.9781754795557799\n",
            "epoch 34 batch id 1991 loss 0.12211903184652328 train acc 0.9781202913108991\n",
            "epoch 34 batch id 2001 loss 0.019210997968912125 train acc 0.9781515492253873\n",
            "epoch 34 batch id 2011 loss 0.14102578163146973 train acc 0.978143647439085\n",
            "epoch 34 batch id 2021 loss 0.045017167925834656 train acc 0.9781048985650668\n",
            "epoch 34 batch id 2031 loss 0.003936021123081446 train acc 0.9781049975381585\n",
            "epoch 34 batch id 2041 loss 0.10318811982870102 train acc 0.9780821288584027\n",
            "epoch 34 batch id 2051 loss 0.239447683095932 train acc 0.9780671014139444\n",
            "epoch 34 batch id 2061 loss 0.004396920092403889 train acc 0.9780749636098981\n",
            "epoch 34 batch id 2071 loss 0.14976949989795685 train acc 0.9780827498792853\n",
            "epoch 34 batch id 2081 loss 0.09565339982509613 train acc 0.9780979697260932\n",
            "epoch 34 batch id 2091 loss 0.039240285754203796 train acc 0.9780906264945003\n",
            "epoch 34 batch id 2101 loss 0.005058621056377888 train acc 0.9781279747739172\n",
            "epoch 34 batch id 2111 loss 0.009433940052986145 train acc 0.9781501657981999\n",
            "epoch 34 batch id 2121 loss 0.041248563677072525 train acc 0.9781574139556812\n",
            "epoch 34 batch id 2131 loss 0.0016870154067873955 train acc 0.9781939230408259\n",
            "epoch 34 batch id 2141 loss 0.05156894773244858 train acc 0.9781790051377861\n",
            "epoch 34 batch id 2151 loss 0.011576524004340172 train acc 0.9782296025104602\n",
            "epoch 34 batch id 2161 loss 0.21957416832447052 train acc 0.9781929662193429\n",
            "epoch 34 batch id 2171 loss 0.0013147520367056131 train acc 0.9782142445877475\n",
            "epoch 34 batch id 2181 loss 0.09722431749105453 train acc 0.9782138353966071\n",
            "epoch 34 batch id 2191 loss 0.08549942076206207 train acc 0.9782276928343222\n",
            "epoch 34 batch id 2201 loss 0.016439206898212433 train acc 0.9781988300772376\n",
            "epoch 34 batch id 2211 loss 0.0007169291493482888 train acc 0.9782055630936228\n",
            "epoch 34 batch id 2221 loss 0.12268973886966705 train acc 0.97817002476362\n",
            "epoch 34 batch id 2231 loss 0.02378994971513748 train acc 0.9781978372926938\n",
            "epoch 34 batch id 2241 loss 0.042809274047613144 train acc 0.9781835676037484\n",
            "epoch 34 batch id 2251 loss 0.003766167676076293 train acc 0.9781416592625499\n",
            "epoch 34 batch id 2261 loss 0.07046496123075485 train acc 0.9781277642636002\n",
            "epoch 34 batch id 2271 loss 0.15178470313549042 train acc 0.9781208718626155\n",
            "epoch 34 batch id 2281 loss 0.0532425157725811 train acc 0.9781208899605436\n",
            "epoch 34 batch id 2291 loss 0.04349115490913391 train acc 0.9781277280663466\n",
            "epoch 34 batch id 2301 loss 0.01655995100736618 train acc 0.9781752498913516\n",
            "epoch 34 batch id 2311 loss 0.07240595668554306 train acc 0.9782088381652964\n",
            "epoch 34 batch id 2321 loss 0.07737241685390472 train acc 0.9781546208530806\n",
            "epoch 34 batch id 2331 loss 0.05661405622959137 train acc 0.9781343843843844\n",
            "epoch 34 batch id 2341 loss 0.008787339553236961 train acc 0.9781677167876975\n",
            "epoch 34 batch id 2351 loss 0.023320304229855537 train acc 0.9781675350914505\n",
            "epoch 34 batch id 2361 loss 0.18183347582817078 train acc 0.9781541190173655\n",
            "epoch 34 batch id 2371 loss 0.0021261083893477917 train acc 0.9782133066216786\n",
            "epoch 34 batch id 2381 loss 0.002100970596075058 train acc 0.9782457475850483\n",
            "epoch 34 batch id 2391 loss 0.01620851643383503 train acc 0.978245242576328\n",
            "epoch 34 batch id 2401 loss 0.03456370159983635 train acc 0.9782252186588921\n",
            "epoch 34 batch id 2411 loss 0.022424258291721344 train acc 0.9782118415595189\n",
            "epoch 34 batch id 2421 loss 0.0070621296763420105 train acc 0.9782502065262288\n",
            "epoch 34 batch id 2431 loss 0.06571085751056671 train acc 0.9782561188811189\n",
            "epoch 34 batch id 2441 loss 0.0002243351482320577 train acc 0.9782491806636624\n",
            "epoch 34 batch id 2451 loss 0.0016401445027440786 train acc 0.9782295491636067\n",
            "epoch 34 batch id 2461 loss 0.08603786677122116 train acc 0.9782227752945957\n",
            "epoch 34 batch id 2471 loss 0.09541723132133484 train acc 0.9781907628490489\n",
            "epoch 34 batch id 2481 loss 0.12024927884340286 train acc 0.9782282849657397\n",
            "epoch 34 batch id 2491 loss 0.08991318941116333 train acc 0.9782027800080288\n",
            "epoch 34 batch id 2501 loss 0.0685785636305809 train acc 0.9782212115153939\n",
            "epoch 34 batch id 2511 loss 0.03290070965886116 train acc 0.9782270509757068\n",
            "epoch 34 batch id 2521 loss 0.043375469744205475 train acc 0.9781956564855216\n",
            "epoch 34 batch id 2531 loss 0.10610754787921906 train acc 0.9782200711181351\n",
            "epoch 34 batch id 2541 loss 0.050527408719062805 train acc 0.9782012495080676\n",
            "epoch 34 batch id 2551 loss 0.04447939246892929 train acc 0.9781948255586045\n",
            "epoch 34 batch id 2561 loss 0.07482703775167465 train acc 0.9782189574385006\n",
            "epoch 34 batch id 2571 loss 0.12370907515287399 train acc 0.9782125145857643\n",
            "epoch 34 batch id 2581 loss 0.048800040036439896 train acc 0.9782121755133669\n",
            "epoch 34 batch id 2591 loss 0.018075037747621536 train acc 0.9782118390582787\n",
            "epoch 34 batch id 2601 loss 0.13836978375911713 train acc 0.9782355344098423\n",
            "epoch 34 batch id 2611 loss 0.001214505173265934 train acc 0.9782530639601685\n",
            "epoch 34 batch id 2621 loss 0.05898559093475342 train acc 0.9782466138878291\n",
            "epoch 34 batch id 2631 loss 0.14176486432552338 train acc 0.9782105188141391\n",
            "epoch 34 batch id 2641 loss 0.02880963124334812 train acc 0.9781865297235895\n",
            "epoch 34 batch id 2651 loss 0.0012561463518068194 train acc 0.9781862976235383\n",
            "epoch 34 batch id 2661 loss 0.044238656759262085 train acc 0.9781919391206313\n",
            "epoch 34 batch id 2671 loss 0.02569624036550522 train acc 0.9782209378509922\n",
            "epoch 34 batch id 2681 loss 0.07089415937662125 train acc 0.978173955613577\n",
            "epoch 34 batch id 2691 loss 0.032990917563438416 train acc 0.978167967298402\n",
            "epoch 34 batch id 2701 loss 0.04020772874355316 train acc 0.9781620233246946\n",
            "epoch 34 batch id 2711 loss 0.0005691694095730782 train acc 0.9781561232017706\n",
            "epoch 34 batch id 2721 loss 0.015399759635329247 train acc 0.9781560088202866\n",
            "epoch 34 batch id 2731 loss 0.0018668677657842636 train acc 0.9781616166239473\n",
            "epoch 34 batch id 2741 loss 0.11423490941524506 train acc 0.9781785844582269\n",
            "epoch 34 batch id 2751 loss 0.12855800986289978 train acc 0.9781613504180298\n",
            "epoch 34 batch id 2761 loss 0.01104961708188057 train acc 0.9782008330315103\n",
            "epoch 34 batch id 2771 loss 0.11049874126911163 train acc 0.9781949206062793\n",
            "epoch 34 batch id 2781 loss 0.04588822275400162 train acc 0.9782396170442287\n",
            "epoch 34 batch id 2791 loss 0.14532916247844696 train acc 0.9782615997850233\n",
            "epoch 34 batch id 2801 loss 0.13326746225357056 train acc 0.9782276419136022\n",
            "epoch 34 batch id 2811 loss 0.08942662179470062 train acc 0.9782550693703308\n",
            "epoch 34 batch id 2821 loss 0.1712532490491867 train acc 0.9782601471109535\n",
            "epoch 34 batch id 2831 loss 0.011589106172323227 train acc 0.9782596697280113\n",
            "epoch 34 batch id 2841 loss 0.20209448039531708 train acc 0.9782536958817318\n",
            "epoch 34 batch id 2851 loss 0.00139170465990901 train acc 0.9782587250087689\n",
            "epoch 34 batch id 2861 loss 0.06695906817913055 train acc 0.9782309507165327\n",
            "epoch 34 batch id 2871 loss 0.0947270393371582 train acc 0.978257793451759\n",
            "epoch 34 batch id 2881 loss 0.03776542469859123 train acc 0.9782573325234294\n",
            "epoch 34 batch id 2891 loss 0.20937615633010864 train acc 0.9782460653753027\n",
            "epoch 34 batch id 2901 loss 0.02617150917649269 train acc 0.9782618062736987\n",
            "epoch 34 batch id 2911 loss 0.019787399098277092 train acc 0.9782667038818276\n",
            "epoch 34 batch id 2921 loss 0.004513452760875225 train acc 0.9782127268058884\n",
            "epoch 34 batch id 2931 loss 0.04809923097491264 train acc 0.9781964346639372\n",
            "epoch 34 batch id 2941 loss 0.004979805089533329 train acc 0.9781802533151989\n",
            "epoch 34 batch id 2951 loss 0.04260009899735451 train acc 0.9782065401558794\n",
            "epoch 34 batch id 2961 loss 0.022061284631490707 train acc 0.9782168186423505\n",
            "epoch 34 batch id 2971 loss 0.06016407534480095 train acc 0.9782112504207338\n",
            "epoch 34 batch id 2981 loss 0.03453664854168892 train acc 0.9781742703790675\n",
            "epoch 34 batch id 2991 loss 0.03326123207807541 train acc 0.978184553660983\n",
            "epoch 34 batch id 3001 loss 0.0018422816647216678 train acc 0.978210388203932\n",
            "epoch 34 batch id 3011 loss 0.007964110933244228 train acc 0.9782049153105281\n",
            "epoch 34 batch id 3021 loss 0.13348381221294403 train acc 0.9781891343925853\n",
            "epoch 34 batch id 3031 loss 0.10349074751138687 train acc 0.9781837677334213\n",
            "epoch 34 batch id 3041 loss 0.12155001610517502 train acc 0.9781527458073003\n",
            "epoch 34 batch id 3051 loss 0.03730902448296547 train acc 0.9781577761389708\n",
            "epoch 34 batch id 3061 loss 0.0870809406042099 train acc 0.9781576690623979\n",
            "epoch 34 batch id 3071 loss 0.003421167377382517 train acc 0.978147386844676\n",
            "epoch 34 batch id 3081 loss 0.1709761619567871 train acc 0.9781118143459916\n",
            "epoch 34 batch id 3091 loss 0.03961103409528732 train acc 0.9781118570042058\n",
            "epoch 34 batch id 3101 loss 0.04578341916203499 train acc 0.9781018219929055\n",
            "epoch 34 batch id 3111 loss 0.25516653060913086 train acc 0.978106918997107\n",
            "epoch 34 batch id 3121 loss 0.025240857154130936 train acc 0.978127002563281\n",
            "epoch 34 batch id 3131 loss 0.0043971948325634 train acc 0.978136977004152\n",
            "epoch 34 batch id 3141 loss 0.07886506617069244 train acc 0.9781369388729704\n",
            "epoch 34 batch id 3151 loss 0.009832474403083324 train acc 0.9781170660107902\n",
            "epoch 34 batch id 3161 loss 0.09566774219274521 train acc 0.9781220341664031\n",
            "epoch 34 batch id 3171 loss 0.01688249595463276 train acc 0.9781171160517187\n",
            "epoch 34 batch id 3181 loss 0.03742875158786774 train acc 0.9781220528135807\n",
            "epoch 34 batch id 3191 loss 0.037736352533102036 train acc 0.9781269586336572\n",
            "epoch 34 batch id 3201 loss 0.0910046249628067 train acc 0.9780976647922525\n",
            "epoch 34 batch id 3211 loss 0.14717862010002136 train acc 0.9780831516661476\n",
            "epoch 34 batch id 3221 loss 0.08372179418802261 train acc 0.9780687286556969\n",
            "epoch 34 batch id 3231 loss 0.03562011942267418 train acc 0.9780495589600743\n",
            "epoch 34 batch id 3241 loss 0.027834799140691757 train acc 0.9780835390311632\n",
            "epoch 34 batch id 3251 loss 0.10223446786403656 train acc 0.9780500230698247\n",
            "epoch 34 batch id 3261 loss 0.1725386381149292 train acc 0.9780071297148114\n",
            "epoch 34 batch id 3271 loss 0.042865339666604996 train acc 0.9779931595842251\n",
            "epoch 34 batch id 3281 loss 0.13196519017219543 train acc 0.9779745123437976\n",
            "epoch 34 batch id 3291 loss 0.040375784039497375 train acc 0.9779654740200547\n",
            "epoch 34 batch id 3301 loss 0.025395188480615616 train acc 0.9779943577703726\n",
            "epoch 34 batch id 3311 loss 0.011444943025708199 train acc 0.9780089096949562\n",
            "epoch 34 batch id 3321 loss 0.010360926389694214 train acc 0.9780092592592593\n",
            "epoch 34 batch id 3331 loss 0.04818187654018402 train acc 0.977958008105674\n",
            "epoch 34 batch id 3341 loss 0.13781918585300446 train acc 0.9779631846752469\n",
            "epoch 34 batch id 3351 loss 0.01045102160423994 train acc 0.9779543419874664\n",
            "epoch 34 batch id 3361 loss 0.06305940449237823 train acc 0.9779687964891401\n",
            "epoch 34 batch id 3371 loss 0.1282256543636322 train acc 0.9779878003559774\n",
            "epoch 34 batch id 3381 loss 0.0006647490663453937 train acc 0.9779974489795918\n",
            "epoch 34 batch id 3391 loss 0.12293194234371185 train acc 0.978011648481274\n",
            "epoch 34 batch id 3401 loss 0.08171209692955017 train acc 0.9780349529550132\n",
            "epoch 34 batch id 3411 loss 0.08028955012559891 train acc 0.9780535400175902\n",
            "epoch 34 batch id 3421 loss 0.01502217911183834 train acc 0.9780491815258696\n",
            "epoch 34 batch id 3431 loss 0.13774137198925018 train acc 0.9780357403089478\n",
            "epoch 34 batch id 3441 loss 0.027290241792798042 train acc 0.9780314588782331\n",
            "epoch 34 batch id 3451 loss 0.011354723945260048 train acc 0.9780362576064908\n",
            "epoch 34 batch id 3461 loss 0.004996882285922766 train acc 0.9780455431956082\n",
            "epoch 34 batch id 3471 loss 0.07187628000974655 train acc 0.9780592768654567\n",
            "epoch 34 batch id 3481 loss 0.08493641763925552 train acc 0.9780504883654122\n",
            "epoch 34 batch id 3491 loss 0.05732262507081032 train acc 0.9780507018046405\n",
            "epoch 34 batch id 3501 loss 0.028762545436620712 train acc 0.9780598400457012\n",
            "epoch 34 batch id 3511 loss 0.07215946167707443 train acc 0.9780466747365423\n",
            "epoch 34 batch id 3521 loss 0.12009894847869873 train acc 0.9780557725078103\n",
            "epoch 34 batch id 3531 loss 0.07821681350469589 train acc 0.9780559685641461\n",
            "epoch 34 batch id 3541 loss 0.07282046228647232 train acc 0.9780473383225078\n",
            "epoch 34 batch id 3551 loss 0.02199060097336769 train acc 0.9780211560123909\n",
            "epoch 34 batch id 3561 loss 0.00742595549672842 train acc 0.9780433866891323\n",
            "epoch 34 batch id 3571 loss 0.08711311966180801 train acc 0.978039239708765\n",
            "epoch 34 batch id 3581 loss 0.0539720319211483 train acc 0.9780525691147725\n",
            "epoch 34 batch id 3591 loss 0.1221749484539032 train acc 0.9780484196602618\n",
            "epoch 34 batch id 3601 loss 0.1865798383951187 train acc 0.9780486323243544\n",
            "epoch 34 batch id 3611 loss 0.00362696242518723 train acc 0.9780488438105788\n",
            "epoch 34 batch id 3621 loss 0.06227031350135803 train acc 0.9780490541286937\n",
            "epoch 34 batch id 3631 loss 0.10269568860530853 train acc 0.9780363536215918\n",
            "epoch 34 batch id 3641 loss 0.18878953158855438 train acc 0.9780494712990937\n",
            "epoch 34 batch id 3651 loss 0.06301439553499222 train acc 0.978041118871542\n",
            "epoch 34 batch id 3661 loss 0.010394684970378876 train acc 0.978032812073204\n",
            "epoch 34 batch id 3671 loss 0.06944496929645538 train acc 0.9780543448651594\n",
            "epoch 34 batch id 3681 loss 0.019678104668855667 train acc 0.9780927397446346\n",
            "epoch 34 batch id 3691 loss 0.016285065561532974 train acc 0.9780716607965321\n",
            "epoch 34 batch id 3701 loss 0.04333513602614403 train acc 0.9780549175898405\n",
            "epoch 34 batch id 3711 loss 0.005706335883587599 train acc 0.9780593168957155\n",
            "epoch 34 batch id 3721 loss 0.06240326538681984 train acc 0.9780594934157485\n",
            "epoch 34 batch id 3731 loss 0.031883884221315384 train acc 0.9780638568748324\n",
            "epoch 34 batch id 3741 loss 0.11708827316761017 train acc 0.9780431368618017\n",
            "epoch 34 batch id 3751 loss 0.009722446091473103 train acc 0.9780600173287124\n",
            "epoch 34 batch id 3761 loss 0.1543106883764267 train acc 0.9780477266684392\n",
            "epoch 34 batch id 3771 loss 0.023267574608325958 train acc 0.9780189273402281\n",
            "epoch 34 batch id 3781 loss 0.10806673020124435 train acc 0.978010942872256\n",
            "epoch 34 batch id 3791 loss 0.0009105131612159312 train acc 0.9780194869427592\n",
            "epoch 34 batch id 3801 loss 0.06028123199939728 train acc 0.978011543014996\n",
            "epoch 34 batch id 3811 loss 0.0210392028093338 train acc 0.9780282406192601\n",
            "epoch 34 batch id 3821 loss 0.05357883498072624 train acc 0.978057118555352\n",
            "epoch 34 batch id 3831 loss 0.0009586287196725607 train acc 0.9780572957452363\n",
            "epoch 34 batch id 3841 loss 0.02390550635755062 train acc 0.9780615399635512\n",
            "epoch 34 batch id 3851 loss 0.0007036624010652304 train acc 0.9780779343027785\n",
            "epoch 34 batch id 3861 loss 0.03130730614066124 train acc 0.9780740093240093\n",
            "epoch 34 batch id 3871 loss 0.10729295015335083 train acc 0.9780781774735211\n",
            "epoch 34 batch id 3881 loss 0.05088072270154953 train acc 0.9780944022159237\n",
            "epoch 34 batch id 3891 loss 0.08907149732112885 train acc 0.9780784181444359\n",
            "epoch 34 batch id 3901 loss 0.009642579592764378 train acc 0.978066521404768\n",
            "epoch 34 batch id 3911 loss 0.031028779223561287 train acc 0.9780506903605216\n",
            "epoch 34 batch id 3921 loss 0.11451585590839386 train acc 0.9780628347360367\n",
            "epoch 34 batch id 3931 loss 0.0802702084183693 train acc 0.978023244721445\n",
            "epoch 34 batch id 3941 loss 0.019289420917630196 train acc 0.9780274676478051\n",
            "epoch 34 batch id 3951 loss 0.125703826546669 train acc 0.9780079410275879\n",
            "epoch 34 batch id 3961 loss 0.09964627772569656 train acc 0.9780161259782884\n",
            "epoch 34 batch id 3971 loss 0.05941367894411087 train acc 0.9780045958196928\n",
            "epoch 34 batch id 3981 loss 0.06214261054992676 train acc 0.9780009733735242\n",
            "epoch 34 batch id 3991 loss 0.11591367423534393 train acc 0.9780012841393134\n",
            "epoch 34 batch id 4001 loss 0.08454371988773346 train acc 0.9779898775306174\n",
            "epoch 34 batch id 4011 loss 0.017138991504907608 train acc 0.9779590501121914\n",
            "epoch 34 batch id 4021 loss 0.019086506217718124 train acc 0.977959462820194\n",
            "epoch 34 batch id 4031 loss 0.0039444477297365665 train acc 0.9779831307367899\n",
            "epoch 34 batch id 4041 loss 0.06137118861079216 train acc 0.9779718819599109\n",
            "epoch 34 batch id 4051 loss 0.020607270300388336 train acc 0.9779684028634905\n",
            "epoch 34 batch id 4061 loss 0.0593780055642128 train acc 0.9779610933267668\n",
            "epoch 34 batch id 4071 loss 0.05142996087670326 train acc 0.977965334070253\n",
            "epoch 34 batch id 4081 loss 0.15600700676441193 train acc 0.977961896593972\n",
            "epoch 34 batch id 4091 loss 0.03723525628447533 train acc 0.9779317404057688\n",
            "epoch 34 batch id 4101 loss 0.0954696536064148 train acc 0.9779398317483541\n",
            "epoch 34 batch id 4111 loss 0.013969413936138153 train acc 0.9779060751641936\n",
            "epoch 34 batch id 4121 loss 0.009236380457878113 train acc 0.9779028148507644\n",
            "epoch 34 batch id 4131 loss 0.12906068563461304 train acc 0.9779033526991043\n",
            "epoch 34 batch id 4141 loss 0.0022001939360052347 train acc 0.9779152076793045\n",
            "epoch 34 batch id 4151 loss 0.08146078884601593 train acc 0.9779157130811853\n",
            "epoch 34 batch id 4161 loss 0.0935383141040802 train acc 0.9779011956260514\n",
            "epoch 34 batch id 4171 loss 0.1808178871870041 train acc 0.9778979860944618\n",
            "epoch 34 batch id 4181 loss 0.01306481845676899 train acc 0.9778873176273619\n",
            "epoch 34 batch id 4191 loss 0.07590409368276596 train acc 0.9778841565258888\n",
            "epoch 34 batch id 4201 loss 0.03018859215080738 train acc 0.9779033265889074\n",
            "epoch 34 batch id 4211 loss 0.08186956495046616 train acc 0.9778964319639041\n",
            "epoch 34 batch id 4221 loss 0.10360252857208252 train acc 0.9778784648187633\n",
            "epoch 34 batch id 4231 loss 0.05298247188329697 train acc 0.9778790475064997\n",
            "epoch 34 batch id 4241 loss 0.12189704924821854 train acc 0.9778869959915114\n",
            "epoch 34 batch id 4251 loss 0.30174869298934937 train acc 0.977872853446248\n",
            "epoch 34 batch id 4261 loss 0.03604578226804733 train acc 0.977884446139404\n",
            "epoch 34 batch id 4271 loss 0.04091359302401543 train acc 0.9778996429407633\n",
            "epoch 34 batch id 4281 loss 0.036262478679418564 train acc 0.9778965195047886\n",
            "epoch 34 batch id 4291 loss 0.1003226563334465 train acc 0.9778788452575158\n",
            "epoch 34 batch id 4301 loss 0.04317905008792877 train acc 0.977850354568705\n",
            "epoch 34 batch id 4311 loss 0.04083104431629181 train acc 0.977869113894688\n",
            "epoch 34 batch id 4321 loss 0.02318384312093258 train acc 0.9778407775977783\n",
            "epoch 34 batch id 4331 loss 0.10470660775899887 train acc 0.9778522569845302\n",
            "epoch 34 batch id 4341 loss 0.03997116535902023 train acc 0.9778456864777701\n",
            "epoch 34 batch id 4351 loss 0.02748655527830124 train acc 0.9778427373017697\n",
            "epoch 34 batch id 4361 loss 0.04219873249530792 train acc 0.9778541332263242\n",
            "epoch 34 batch id 4371 loss 0.05774170532822609 train acc 0.977861902310684\n",
            "epoch 34 batch id 4381 loss 0.11808653175830841 train acc 0.9778553697785893\n",
            "epoch 34 batch id 4391 loss 0.001137408777140081 train acc 0.9778453085857436\n",
            "epoch 34 batch id 4401 loss 0.02627318911254406 train acc 0.9778352931152011\n",
            "epoch 34 batch id 4411 loss 0.024401823058724403 train acc 0.9778324076173204\n",
            "epoch 34 batch id 4421 loss 0.07804694771766663 train acc 0.9778401379778331\n",
            "epoch 34 batch id 4431 loss 0.02247578650712967 train acc 0.9778407808621079\n",
            "epoch 34 batch id 4441 loss 0.02375926822423935 train acc 0.9778484575546048\n",
            "epoch 34 batch id 4451 loss 0.01576097682118416 train acc 0.9778490788586834\n",
            "epoch 34 batch id 4461 loss 0.0009042893652804196 train acc 0.9778567025330643\n",
            "epoch 34 batch id 4471 loss 0.07963544130325317 train acc 0.9778468183851488\n",
            "epoch 34 batch id 4481 loss 0.01033557578921318 train acc 0.9778300044632895\n",
            "epoch 34 batch id 4491 loss 0.13803595304489136 train acc 0.9778306613226453\n",
            "epoch 34 batch id 4501 loss 0.022838642820715904 train acc 0.9778209009109087\n",
            "epoch 34 batch id 4511 loss 0.07728420943021774 train acc 0.9778181112835291\n",
            "epoch 34 batch id 4521 loss 0.11397025734186172 train acc 0.9778084218093342\n",
            "epoch 34 batch id 4531 loss 0.016657261177897453 train acc 0.977819465901567\n",
            "epoch 34 batch id 4541 loss 0.0647638738155365 train acc 0.977830461352125\n",
            "epoch 34 batch id 4551 loss 0.12030141055583954 train acc 0.9778276752362118\n",
            "epoch 34 batch id 4561 loss 0.061348456889390945 train acc 0.9778180497697874\n",
            "epoch 34 batch id 4571 loss 0.07322517782449722 train acc 0.9778255578647999\n",
            "epoch 34 batch id 4581 loss 0.030088763684034348 train acc 0.9778262115258677\n",
            "epoch 34 batch id 4591 loss 0.09527775645256042 train acc 0.9778302657373121\n",
            "epoch 34 batch id 4601 loss 0.02747976966202259 train acc 0.977830906324712\n",
            "epoch 34 batch id 4611 loss 0.03759525343775749 train acc 0.9778315441335936\n",
            "epoch 34 batch id 4621 loss 0.01768716424703598 train acc 0.9778423230902402\n",
            "epoch 34 batch id 4631 loss 0.10349137336015701 train acc 0.9778429334916865\n",
            "epoch 34 batch id 4641 loss 0.1274413913488388 train acc 0.9778637416505064\n",
            "epoch 34 batch id 4651 loss 0.10998798161745071 train acc 0.9778676628682004\n",
            "epoch 34 batch id 4661 loss 0.031416598707437515 train acc 0.9778816241149968\n",
            "epoch 34 batch id 4671 loss 0.015048783272504807 train acc 0.9778854902590451\n",
            "epoch 34 batch id 4681 loss 0.2515546381473541 train acc 0.9778860019226661\n",
            "epoch 34 batch id 4691 loss 0.05280660465359688 train acc 0.9778831805585163\n",
            "epoch 34 batch id 4701 loss 0.037604812532663345 train acc 0.9778803711976175\n",
            "epoch 34 batch id 4711 loss 0.07123437523841858 train acc 0.9778808904691149\n",
            "epoch 34 batch id 4721 loss 0.005089945625513792 train acc 0.9778780978606227\n",
            "epoch 34 batch id 4731 loss 0.0792301744222641 train acc 0.9778819224265483\n",
            "epoch 34 batch id 4741 loss 0.0798439010977745 train acc 0.9778791394220628\n",
            "epoch 34 batch id 4751 loss 0.1272825002670288 train acc 0.9778730793517154\n",
            "epoch 34 batch id 4761 loss 0.13455721735954285 train acc 0.9778670447385003\n",
            "epoch 34 batch id 4771 loss 0.06624800711870193 train acc 0.9778774103961434\n",
            "epoch 34 batch id 4781 loss 0.06631256639957428 train acc 0.9778648556787283\n",
            "epoch 34 batch id 4791 loss 0.2949489653110504 train acc 0.977836046754331\n",
            "epoch 34 batch id 4801 loss 0.16451610624790192 train acc 0.9778268850239533\n",
            "epoch 34 batch id 4811 loss 0.3198457360267639 train acc 0.9778275046767824\n",
            "epoch 34 batch id 4821 loss 0.09453341364860535 train acc 0.9778346038166356\n",
            "epoch 34 batch id 4831 loss 0.07186729460954666 train acc 0.977844907886566\n",
            "epoch 34 batch id 4841 loss 0.07622813433408737 train acc 0.9778422588308201\n",
            "epoch 34 batch id 4851 loss 0.025607995688915253 train acc 0.9778460626674912\n",
            "epoch 34 batch id 4861 loss 0.03399021923542023 train acc 0.9778369934169924\n",
            "epoch 34 batch id 4871 loss 0.03898615017533302 train acc 0.9778632467665777\n",
            "epoch 34 batch id 4881 loss 0.14290840923786163 train acc 0.9778669842245441\n",
            "epoch 34 batch id 4891 loss 0.04326605424284935 train acc 0.9778547331833981\n",
            "epoch 34 batch id 4901 loss 0.036788005381822586 train acc 0.977864849010406\n",
            "epoch 34 batch id 4911 loss 0.1557411253452301 train acc 0.977852652209326\n",
            "epoch 34 batch id 4921 loss 0.22415964305400848 train acc 0.9778436801463117\n",
            "epoch 34 batch id 4931 loss 0.08547903597354889 train acc 0.9778315757452849\n",
            "epoch 34 batch id 4941 loss 0.056255437433719635 train acc 0.9778195203400122\n",
            "epoch 34 batch id 4951 loss 0.08956808596849442 train acc 0.977823293274086\n",
            "epoch 34 train acc 0.9778249193060319\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9776517af5a847cf9a2b6c83cbdb1c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 34 loss 0.5231664180755615 test acc 0.8479609604105572\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b8823599dca498eac5254bc4127758f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 35 batch id 1 loss 0.04066484048962593 train acc 0.984375\n",
            "epoch 35 batch id 11 loss 0.03852681443095207 train acc 0.9801136363636364\n",
            "epoch 35 batch id 21 loss 0.005239850841462612 train acc 0.9784226190476191\n",
            "epoch 35 batch id 31 loss 0.08474279195070267 train acc 0.9788306451612904\n",
            "epoch 35 batch id 41 loss 0.1040550023317337 train acc 0.9805640243902439\n",
            "epoch 35 batch id 51 loss 0.022522928193211555 train acc 0.9797794117647058\n",
            "epoch 35 batch id 61 loss 0.21363036334514618 train acc 0.9787397540983607\n",
            "epoch 35 batch id 71 loss 0.10292506217956543 train acc 0.9786531690140845\n",
            "epoch 35 batch id 81 loss 0.023563846945762634 train acc 0.9776234567901234\n",
            "epoch 35 batch id 91 loss 0.10729968547821045 train acc 0.9783653846153846\n",
            "epoch 35 batch id 101 loss 0.05063468962907791 train acc 0.9771039603960396\n",
            "epoch 35 batch id 111 loss 0.013148603029549122 train acc 0.977759009009009\n",
            "epoch 35 batch id 121 loss 0.17102055251598358 train acc 0.9783057851239669\n",
            "epoch 35 batch id 131 loss 0.04276518151164055 train acc 0.9782919847328244\n",
            "epoch 35 batch id 141 loss 0.0008440131787210703 train acc 0.9782801418439716\n",
            "epoch 35 batch id 151 loss 0.07333683967590332 train acc 0.9781663907284768\n",
            "epoch 35 batch id 161 loss 0.0015990273095667362 train acc 0.9786490683229814\n",
            "epoch 35 batch id 171 loss 0.08350438624620438 train acc 0.9782529239766082\n",
            "epoch 35 batch id 181 loss 0.08897867798805237 train acc 0.9784185082872928\n",
            "epoch 35 batch id 191 loss 0.050026144832372665 train acc 0.9782395287958116\n",
            "epoch 35 batch id 201 loss 0.18586693704128265 train acc 0.9783115671641791\n",
            "epoch 35 batch id 211 loss 0.03294903039932251 train acc 0.9783767772511849\n",
            "epoch 35 batch id 221 loss 0.026472333818674088 train acc 0.9785067873303167\n",
            "epoch 35 batch id 231 loss 0.02864140458405018 train acc 0.9786931818181818\n",
            "epoch 35 batch id 241 loss 0.11764971911907196 train acc 0.9786047717842323\n",
            "epoch 35 batch id 251 loss 0.08871418237686157 train acc 0.9781499003984063\n",
            "epoch 35 batch id 261 loss 0.13119466602802277 train acc 0.9782686781609196\n",
            "epoch 35 batch id 271 loss 0.16450023651123047 train acc 0.9784940036900369\n",
            "epoch 35 batch id 281 loss 0.0006850374047644436 train acc 0.9787032918149466\n",
            "epoch 35 batch id 291 loss 0.07049155980348587 train acc 0.9790055841924399\n",
            "epoch 35 batch id 301 loss 0.10274286568164825 train acc 0.9787686877076412\n",
            "epoch 35 batch id 311 loss 0.019139030948281288 train acc 0.9788484726688103\n",
            "epoch 35 batch id 321 loss 0.13741014897823334 train acc 0.9787285825545171\n",
            "epoch 35 batch id 331 loss 0.011643354780972004 train acc 0.9785687311178247\n",
            "epoch 35 batch id 341 loss 0.2670818269252777 train acc 0.9785557184750733\n",
            "epoch 35 batch id 351 loss 0.04982585087418556 train acc 0.9786324786324786\n",
            "epoch 35 batch id 361 loss 0.3058880865573883 train acc 0.9784885734072022\n",
            "epoch 35 batch id 371 loss 0.028121724724769592 train acc 0.9786472371967655\n",
            "epoch 35 batch id 381 loss 0.015074558556079865 train acc 0.9785925196850394\n",
            "epoch 35 batch id 391 loss 0.07527081668376923 train acc 0.9787803708439897\n",
            "epoch 35 batch id 401 loss 0.00023731502005830407 train acc 0.9786860972568578\n",
            "epoch 35 batch id 411 loss 0.0012410469353199005 train acc 0.9789005474452555\n",
            "epoch 35 batch id 421 loss 0.01221450325101614 train acc 0.9791419239904988\n",
            "epoch 35 batch id 431 loss 0.07653695344924927 train acc 0.9790820765661253\n",
            "epoch 35 batch id 441 loss 0.13452684879302979 train acc 0.9789540816326531\n",
            "epoch 35 batch id 451 loss 0.022335361689329147 train acc 0.9791435698447893\n",
            "epoch 35 batch id 461 loss 0.018298819661140442 train acc 0.9787825379609545\n",
            "epoch 35 batch id 471 loss 0.014933785423636436 train acc 0.9789676220806794\n",
            "epoch 35 batch id 481 loss 0.14152683317661285 train acc 0.9790475571725572\n",
            "epoch 35 batch id 491 loss 0.005767464637756348 train acc 0.9789014765784114\n",
            "epoch 35 batch id 501 loss 0.28934672474861145 train acc 0.9789483532934131\n",
            "epoch 35 batch id 511 loss 0.10356829315423965 train acc 0.9788405088062623\n",
            "epoch 35 batch id 521 loss 0.10515033453702927 train acc 0.9789467370441459\n",
            "epoch 35 batch id 531 loss 0.031239263713359833 train acc 0.978695856873823\n",
            "epoch 35 batch id 541 loss 0.02581663616001606 train acc 0.9784542513863216\n",
            "epoch 35 batch id 551 loss 0.02438880130648613 train acc 0.9785333484573503\n",
            "epoch 35 batch id 561 loss 0.0512714609503746 train acc 0.9784146613190731\n",
            "epoch 35 batch id 571 loss 0.18274922668933868 train acc 0.9784643169877408\n",
            "epoch 35 batch id 581 loss 0.06377633661031723 train acc 0.9784584767641996\n",
            "epoch 35 batch id 591 loss 0.046973876655101776 train acc 0.97855858714044\n",
            "epoch 35 batch id 601 loss 0.03753892332315445 train acc 0.9786553660565723\n",
            "epoch 35 batch id 611 loss 0.05939968675374985 train acc 0.9786211129296236\n",
            "epoch 35 batch id 621 loss 0.30031895637512207 train acc 0.9786634460547504\n",
            "epoch 35 batch id 631 loss 0.045587897300720215 train acc 0.9786053882725833\n",
            "epoch 35 batch id 641 loss 0.023859485983848572 train acc 0.9786466458658346\n",
            "epoch 35 batch id 651 loss 0.018302034586668015 train acc 0.9785906298003072\n",
            "epoch 35 batch id 661 loss 0.0016740941209718585 train acc 0.9787490544629349\n",
            "epoch 35 batch id 671 loss 0.04487380012869835 train acc 0.9788328986587184\n",
            "epoch 35 batch id 681 loss 0.05707351863384247 train acc 0.9788683920704846\n",
            "epoch 35 batch id 691 loss 0.06554774194955826 train acc 0.97883502170767\n",
            "epoch 35 batch id 701 loss 0.13053138554096222 train acc 0.9786688659058488\n",
            "epoch 35 batch id 711 loss 0.09141156077384949 train acc 0.9787051687763713\n",
            "epoch 35 batch id 721 loss 0.07702101767063141 train acc 0.9785887656033287\n",
            "epoch 35 batch id 731 loss 0.010497490875422955 train acc 0.9786037961696307\n",
            "epoch 35 batch id 741 loss 0.009096840396523476 train acc 0.978597334682861\n",
            "epoch 35 batch id 751 loss 0.1014581099152565 train acc 0.9785286284953395\n",
            "epoch 35 batch id 761 loss 0.02929430641233921 train acc 0.9785643889618922\n",
            "epoch 35 batch id 771 loss 0.055546943098306656 train acc 0.9784573605706874\n",
            "epoch 35 batch id 781 loss 0.07200821489095688 train acc 0.9784531049935979\n",
            "epoch 35 batch id 791 loss 0.0540505088865757 train acc 0.9784489570164349\n",
            "epoch 35 batch id 801 loss 0.00487077422440052 train acc 0.9785034332084894\n",
            "epoch 35 batch id 811 loss 0.06680957227945328 train acc 0.9784795006165228\n",
            "epoch 35 batch id 821 loss 0.014711908996105194 train acc 0.9785322777101096\n",
            "epoch 35 batch id 831 loss 0.22247670590877533 train acc 0.9784897713598074\n",
            "epoch 35 batch id 841 loss 0.0014850833686068654 train acc 0.9785225921521997\n",
            "epoch 35 batch id 851 loss 0.16705083847045898 train acc 0.9783893948296122\n",
            "epoch 35 batch id 861 loss 0.18755201995372772 train acc 0.9784044715447154\n",
            "epoch 35 batch id 871 loss 0.014618397690355778 train acc 0.9783474454649828\n",
            "epoch 35 batch id 881 loss 0.020504998043179512 train acc 0.9783094494892168\n",
            "epoch 35 batch id 891 loss 0.00044579640962183475 train acc 0.9783599887766554\n",
            "epoch 35 batch id 901 loss 0.06534085422754288 train acc 0.9784094062153164\n",
            "epoch 35 batch id 911 loss 0.010534374974668026 train acc 0.9784577387486278\n",
            "epoch 35 batch id 921 loss 0.0035741927567869425 train acc 0.9783353691639523\n",
            "epoch 35 batch id 931 loss 0.0010384233901277184 train acc 0.9783834586466166\n",
            "epoch 35 batch id 941 loss 0.013842868618667126 train acc 0.9784803400637619\n",
            "epoch 35 batch id 951 loss 0.07324961572885513 train acc 0.9784108832807571\n",
            "epoch 35 batch id 961 loss 0.08234158903360367 train acc 0.9784241675338189\n",
            "epoch 35 batch id 971 loss 0.12377176433801651 train acc 0.978388903192585\n",
            "epoch 35 batch id 981 loss 0.17349277436733246 train acc 0.9784977064220184\n",
            "epoch 35 batch id 991 loss 0.07677667587995529 train acc 0.9785412462159435\n",
            "epoch 35 batch id 1001 loss 0.1069054827094078 train acc 0.978583916083916\n",
            "epoch 35 batch id 1011 loss 0.18262970447540283 train acc 0.9786102868447082\n",
            "epoch 35 batch id 1021 loss 0.039226628839969635 train acc 0.9786973555337904\n",
            "epoch 35 batch id 1031 loss 0.006752197165042162 train acc 0.9787372696411252\n",
            "epoch 35 batch id 1041 loss 0.12304993718862534 train acc 0.9787764169068204\n",
            "epoch 35 batch id 1051 loss 0.02949453331530094 train acc 0.978636417697431\n",
            "epoch 35 batch id 1061 loss 0.27948129177093506 train acc 0.9786021442035815\n",
            "epoch 35 batch id 1071 loss 0.007461239118129015 train acc 0.978656045751634\n",
            "epoch 35 batch id 1081 loss 0.03452790156006813 train acc 0.978593316373728\n",
            "epoch 35 batch id 1091 loss 0.13525816798210144 train acc 0.9786033455545371\n",
            "epoch 35 batch id 1101 loss 0.10970404744148254 train acc 0.9786131925522252\n",
            "epoch 35 batch id 1111 loss 0.06216934323310852 train acc 0.9785806705670567\n",
            "epoch 35 batch id 1121 loss 0.0375550203025341 train acc 0.9786741748438894\n",
            "epoch 35 batch id 1131 loss 0.09473937004804611 train acc 0.9786002431476569\n",
            "epoch 35 batch id 1141 loss 0.18344233930110931 train acc 0.9786234662576687\n",
            "epoch 35 batch id 1151 loss 0.03420319780707359 train acc 0.9785512597741095\n",
            "epoch 35 batch id 1161 loss 0.033080555498600006 train acc 0.9785610465116279\n",
            "epoch 35 batch id 1171 loss 0.014163129031658173 train acc 0.9785706660973527\n",
            "epoch 35 batch id 1181 loss 0.0364333912730217 train acc 0.9786198137171889\n",
            "epoch 35 batch id 1191 loss 0.020366014912724495 train acc 0.9786025398824517\n",
            "epoch 35 batch id 1201 loss 0.10609862208366394 train acc 0.9786506036636137\n",
            "epoch 35 batch id 1211 loss 0.004460466094315052 train acc 0.9786978736581338\n",
            "epoch 35 batch id 1221 loss 0.09516040235757828 train acc 0.9787443693693694\n",
            "epoch 35 batch id 1231 loss 0.1089075580239296 train acc 0.9787012591389115\n",
            "epoch 35 batch id 1241 loss 0.015118071809411049 train acc 0.9787595688960515\n",
            "epoch 35 batch id 1251 loss 0.1177060678601265 train acc 0.9787544964028777\n",
            "epoch 35 batch id 1261 loss 0.02614857628941536 train acc 0.9787742862807296\n",
            "epoch 35 batch id 1271 loss 0.05162349343299866 train acc 0.9787445908733281\n",
            "epoch 35 batch id 1281 loss 0.011055225506424904 train acc 0.9788007416081187\n",
            "epoch 35 batch id 1291 loss 0.18743740022182465 train acc 0.9787713013168087\n",
            "epoch 35 batch id 1301 loss 0.016909165307879448 train acc 0.9787303036126057\n",
            "epoch 35 batch id 1311 loss 0.018108142539858818 train acc 0.9787256864988558\n",
            "epoch 35 batch id 1321 loss 0.02384667657315731 train acc 0.9787329674489024\n",
            "epoch 35 batch id 1331 loss 0.23752203583717346 train acc 0.9786579639368895\n",
            "epoch 35 batch id 1341 loss 0.16268251836299896 train acc 0.9786772930648769\n",
            "epoch 35 batch id 1351 loss 0.0011110292980447412 train acc 0.9786616395262768\n",
            "epoch 35 batch id 1361 loss 0.006657288409769535 train acc 0.9786691770756797\n",
            "epoch 35 batch id 1371 loss 0.006313218269497156 train acc 0.9786880014587892\n",
            "epoch 35 batch id 1381 loss 0.14431807398796082 train acc 0.9786952389572773\n",
            "epoch 35 batch id 1391 loss 0.011667417362332344 train acc 0.9786574406901509\n",
            "epoch 35 batch id 1401 loss 0.01854972168803215 train acc 0.9786759457530335\n",
            "epoch 35 batch id 1411 loss 0.08784961700439453 train acc 0.9787384833451452\n",
            "epoch 35 batch id 1421 loss 0.058673594146966934 train acc 0.9787341660802252\n",
            "epoch 35 batch id 1431 loss 0.012804674915969372 train acc 0.978751747030049\n",
            "epoch 35 batch id 1441 loss 0.021366801112890244 train acc 0.978704024982651\n",
            "epoch 35 batch id 1451 loss 0.015317989513278008 train acc 0.9787646450723639\n",
            "epoch 35 batch id 1461 loss 0.004876445978879929 train acc 0.978770961670089\n",
            "epoch 35 batch id 1471 loss 0.04649008810520172 train acc 0.9788409245411285\n",
            "epoch 35 batch id 1481 loss 0.061930421739816666 train acc 0.9788149898717083\n",
            "epoch 35 batch id 1491 loss 0.09838662296533585 train acc 0.9788208417169685\n",
            "epoch 35 batch id 1501 loss 0.1361546665430069 train acc 0.9788370253164557\n",
            "epoch 35 batch id 1511 loss 0.015977611765265465 train acc 0.9788012905360688\n",
            "epoch 35 batch id 1521 loss 0.025844940915703773 train acc 0.9788071170282708\n",
            "epoch 35 batch id 1531 loss 0.0508948490023613 train acc 0.9787924559111691\n",
            "epoch 35 batch id 1541 loss 0.011010301299393177 train acc 0.9788084036340039\n",
            "epoch 35 batch id 1551 loss 0.03483964130282402 train acc 0.9788442940038685\n",
            "epoch 35 batch id 1561 loss 0.0623333677649498 train acc 0.9787996476617553\n",
            "epoch 35 batch id 1571 loss 0.049484677612781525 train acc 0.9787655155951623\n",
            "epoch 35 batch id 1581 loss 0.048498738557100296 train acc 0.9787219323213157\n",
            "epoch 35 batch id 1591 loss 0.1285131275653839 train acc 0.978688717787555\n",
            "epoch 35 batch id 1601 loss 0.20546957850456238 train acc 0.9787632729544035\n",
            "epoch 35 batch id 1611 loss 0.058538854122161865 train acc 0.9786623215394165\n",
            "epoch 35 batch id 1621 loss 0.039701469242572784 train acc 0.9786590067859346\n",
            "epoch 35 batch id 1631 loss 0.00042077043326571584 train acc 0.9786557326793378\n",
            "epoch 35 batch id 1641 loss 0.10021831095218658 train acc 0.9786334552102377\n",
            "epoch 35 batch id 1651 loss 0.03412449359893799 train acc 0.9785641278013325\n",
            "epoch 35 batch id 1661 loss 0.1614319533109665 train acc 0.9786085189644792\n",
            "epoch 35 batch id 1671 loss 0.1385275423526764 train acc 0.9785588719329743\n",
            "epoch 35 batch id 1681 loss 0.2795279324054718 train acc 0.9784633402736467\n",
            "epoch 35 batch id 1691 loss 0.0493745431303978 train acc 0.9784705795387345\n",
            "epoch 35 batch id 1701 loss 0.07801692187786102 train acc 0.9784318048206937\n",
            "epoch 35 batch id 1711 loss 0.05422624200582504 train acc 0.9784391437755698\n",
            "epoch 35 batch id 1721 loss 0.05246660113334656 train acc 0.9784463974433469\n",
            "epoch 35 batch id 1731 loss 0.0006147988606244326 train acc 0.9784355141536684\n",
            "epoch 35 batch id 1741 loss 0.03595976531505585 train acc 0.9784157811602527\n",
            "epoch 35 batch id 1751 loss 0.04095359519124031 train acc 0.9784230439748715\n",
            "epoch 35 batch id 1761 loss 0.03475380316376686 train acc 0.9784479699034639\n",
            "epoch 35 batch id 1771 loss 0.025223903357982635 train acc 0.97850790513834\n",
            "epoch 35 batch id 1781 loss 0.05144864320755005 train acc 0.9785408478382931\n",
            "epoch 35 batch id 1791 loss 0.11968488991260529 train acc 0.9785036292573981\n",
            "epoch 35 batch id 1801 loss 0.025596776977181435 train acc 0.9785449056079956\n",
            "epoch 35 batch id 1811 loss 0.030721619725227356 train acc 0.9785598426283821\n",
            "epoch 35 batch id 1821 loss 0.10873391479253769 train acc 0.9785660351455244\n",
            "epoch 35 batch id 1831 loss 0.019828837364912033 train acc 0.978572160021846\n",
            "epoch 35 batch id 1841 loss 0.0013280842686071992 train acc 0.9785357821835958\n",
            "epoch 35 batch id 1851 loss 0.1085791066288948 train acc 0.9785757698541329\n",
            "epoch 35 batch id 1861 loss 0.019770653918385506 train acc 0.9785397635679742\n",
            "epoch 35 batch id 1871 loss 0.10182933509349823 train acc 0.9785876536611438\n",
            "epoch 35 batch id 1881 loss 0.017732979729771614 train acc 0.9785935007974481\n",
            "epoch 35 batch id 1891 loss 0.02825913578271866 train acc 0.9786323373876256\n",
            "epoch 35 batch id 1901 loss 0.10506676882505417 train acc 0.9786296685954761\n",
            "epoch 35 batch id 1911 loss 0.1609315127134323 train acc 0.9786024986917844\n",
            "epoch 35 batch id 1921 loss 0.024880271404981613 train acc 0.9785837454450806\n",
            "epoch 35 batch id 1931 loss 0.05806322395801544 train acc 0.9786056447436562\n",
            "epoch 35 batch id 1941 loss 0.05615857616066933 train acc 0.978619268418341\n",
            "epoch 35 batch id 1951 loss 0.038538552820682526 train acc 0.9786487698616094\n",
            "epoch 35 batch id 1961 loss 0.04440351575613022 train acc 0.97863016318205\n",
            "epoch 35 batch id 1971 loss 0.02287544682621956 train acc 0.9786355276509386\n",
            "epoch 35 batch id 1981 loss 0.03572935611009598 train acc 0.9785856259464917\n",
            "epoch 35 batch id 1991 loss 0.10256071388721466 train acc 0.9785911602209945\n",
            "epoch 35 batch id 2001 loss 0.08101914823055267 train acc 0.9785888305847077\n",
            "epoch 35 batch id 2011 loss 0.15312494337558746 train acc 0.9786176031824962\n",
            "epoch 35 batch id 2021 loss 0.14723552763462067 train acc 0.978615165759525\n",
            "epoch 35 batch id 2031 loss 0.01810033805668354 train acc 0.9786050590841949\n",
            "epoch 35 batch id 2041 loss 0.0014578676782548428 train acc 0.9785873958843704\n",
            "epoch 35 batch id 2051 loss 0.31504765152931213 train acc 0.9785775231594345\n",
            "epoch 35 batch id 2061 loss 0.0033908546902239323 train acc 0.9785980713245997\n",
            "epoch 35 batch id 2071 loss 0.1701766848564148 train acc 0.9785882423949783\n",
            "epoch 35 batch id 2081 loss 0.044380128383636475 train acc 0.9785559827006247\n",
            "epoch 35 batch id 2091 loss 0.05058059096336365 train acc 0.9785539215686274\n",
            "epoch 35 batch id 2101 loss 0.0030958745628595352 train acc 0.9785667539267016\n",
            "epoch 35 batch id 2111 loss 0.020851891487836838 train acc 0.9785794647086689\n",
            "epoch 35 batch id 2121 loss 0.02530542016029358 train acc 0.9785994224422442\n",
            "epoch 35 batch id 2131 loss 0.0007644261932000518 train acc 0.9786411895823557\n",
            "epoch 35 batch id 2141 loss 0.03389771282672882 train acc 0.978667970574498\n",
            "epoch 35 batch id 2151 loss 0.026948343962430954 train acc 0.978672710367271\n",
            "epoch 35 batch id 2161 loss 0.12221947312355042 train acc 0.9786701758445164\n",
            "epoch 35 batch id 2171 loss 0.01569220796227455 train acc 0.9786964532473514\n",
            "epoch 35 batch id 2181 loss 0.08715011179447174 train acc 0.9786938331040806\n",
            "epoch 35 batch id 2191 loss 0.046108268201351166 train acc 0.9786983683249658\n",
            "epoch 35 batch id 2201 loss 0.09224935621023178 train acc 0.9787241594729669\n",
            "epoch 35 batch id 2211 loss 0.04358212277293205 train acc 0.9787426503844414\n",
            "epoch 35 batch id 2221 loss 0.06380072236061096 train acc 0.9787257991895543\n",
            "epoch 35 batch id 2231 loss 0.012235360220074654 train acc 0.9787371134020618\n",
            "epoch 35 batch id 2241 loss 0.07369515299797058 train acc 0.9787413543061133\n",
            "epoch 35 batch id 2251 loss 0.14456842839717865 train acc 0.9787247334517992\n",
            "epoch 35 batch id 2261 loss 0.03573068231344223 train acc 0.9787013489606369\n",
            "epoch 35 batch id 2271 loss 0.0036467022728174925 train acc 0.9787194517833554\n",
            "epoch 35 batch id 2281 loss 0.039367299526929855 train acc 0.9787305458132398\n",
            "epoch 35 batch id 2291 loss 0.020916730165481567 train acc 0.9787483631601921\n",
            "epoch 35 batch id 2301 loss 0.021759379655122757 train acc 0.9787592351151673\n",
            "epoch 35 batch id 2311 loss 0.057385511696338654 train acc 0.9787700129813933\n",
            "epoch 35 batch id 2321 loss 0.11399319767951965 train acc 0.978747037914692\n",
            "epoch 35 batch id 2331 loss 0.0721510648727417 train acc 0.9787577756327757\n",
            "epoch 35 batch id 2341 loss 0.005288322456181049 train acc 0.9788017941050833\n",
            "epoch 35 batch id 2351 loss 0.07320035248994827 train acc 0.9787723309230115\n",
            "epoch 35 batch id 2361 loss 0.10829198360443115 train acc 0.9788092969080898\n",
            "epoch 35 batch id 2371 loss 0.0009897304698824883 train acc 0.9788393610291016\n",
            "epoch 35 batch id 2381 loss 0.024712271988391876 train acc 0.9788626102477951\n",
            "epoch 35 batch id 2391 loss 0.022212116047739983 train acc 0.9788595253032204\n",
            "epoch 35 batch id 2401 loss 0.017967797815799713 train acc 0.9788564660558101\n",
            "epoch 35 batch id 2411 loss 0.027862710878252983 train acc 0.9788275093322273\n",
            "epoch 35 batch id 2421 loss 0.1346980780363083 train acc 0.9788439694341181\n",
            "epoch 35 batch id 2431 loss 0.048345860093832016 train acc 0.9788602941176471\n",
            "epoch 35 batch id 2441 loss 0.02575933001935482 train acc 0.9788700839819746\n",
            "epoch 35 batch id 2451 loss 0.011110927909612656 train acc 0.9788797939616483\n",
            "epoch 35 batch id 2461 loss 0.024394968524575233 train acc 0.978864028850061\n",
            "epoch 35 batch id 2471 loss 0.11844827979803085 train acc 0.9788230979360583\n",
            "epoch 35 batch id 2481 loss 0.10888976603746414 train acc 0.9788328798871423\n",
            "epoch 35 batch id 2491 loss 0.07865555584430695 train acc 0.9787861300682457\n",
            "epoch 35 batch id 2501 loss 0.002967141568660736 train acc 0.9787834866053579\n",
            "epoch 35 batch id 2511 loss 0.013391958549618721 train acc 0.9787995320589407\n",
            "epoch 35 batch id 2521 loss 0.020379113033413887 train acc 0.9787782625942086\n",
            "epoch 35 batch id 2531 loss 0.016103344038128853 train acc 0.9787756815487949\n",
            "epoch 35 batch id 2541 loss 0.02586025558412075 train acc 0.9787731208185754\n",
            "epoch 35 batch id 2551 loss 0.03804398328065872 train acc 0.9788073304586437\n",
            "epoch 35 batch id 2561 loss 0.01717083714902401 train acc 0.9788290706755174\n",
            "epoch 35 batch id 2571 loss 0.16055002808570862 train acc 0.978820254764683\n",
            "epoch 35 batch id 2581 loss 0.047483157366514206 train acc 0.9787993994575745\n",
            "epoch 35 batch id 2591 loss 0.11351561546325684 train acc 0.978796796603628\n",
            "epoch 35 batch id 2601 loss 0.047546014189720154 train acc 0.9788242502883506\n",
            "epoch 35 batch id 2611 loss 0.001776121906004846 train acc 0.9788395250861739\n",
            "epoch 35 batch id 2621 loss 0.04636406525969505 train acc 0.9788546833269745\n",
            "epoch 35 batch id 2631 loss 0.16916461288928986 train acc 0.9788103382744203\n",
            "epoch 35 batch id 2641 loss 0.046170614659786224 train acc 0.9787781616811814\n",
            "epoch 35 batch id 2651 loss 0.0030363649129867554 train acc 0.9787874858543946\n",
            "epoch 35 batch id 2661 loss 0.012356659397482872 train acc 0.9787615088312664\n",
            "epoch 35 batch id 2671 loss 0.097634457051754 train acc 0.9787532759266192\n",
            "epoch 35 batch id 2681 loss 0.03298474848270416 train acc 0.9787392763894069\n",
            "epoch 35 batch id 2691 loss 0.010461767204105854 train acc 0.978719574507618\n",
            "epoch 35 batch id 2701 loss 0.035269595682621 train acc 0.9787058034061459\n",
            "epoch 35 batch id 2711 loss 0.003453419776633382 train acc 0.9786921338989303\n",
            "epoch 35 batch id 2721 loss 0.009870549663901329 train acc 0.9786785648658581\n",
            "epoch 35 batch id 2731 loss 0.0017214297549799085 train acc 0.978670816550714\n",
            "epoch 35 batch id 2741 loss 0.21774153411388397 train acc 0.9786403228748631\n",
            "epoch 35 batch id 2751 loss 0.07384980469942093 train acc 0.9786270901490367\n",
            "epoch 35 batch id 2761 loss 0.038246721029281616 train acc 0.9786422491850779\n",
            "epoch 35 batch id 2771 loss 0.0007036477909423411 train acc 0.9786460212919523\n",
            "epoch 35 batch id 2781 loss 0.038931261748075485 train acc 0.9786722402013665\n",
            "epoch 35 batch id 2791 loss 0.020865485072135925 train acc 0.9786926728771049\n",
            "epoch 35 batch id 2801 loss 0.15489891171455383 train acc 0.9786906461977865\n",
            "epoch 35 batch id 2811 loss 0.06771404296159744 train acc 0.978721985058698\n",
            "epoch 35 batch id 2821 loss 0.012406833469867706 train acc 0.9787586405529954\n",
            "epoch 35 batch id 2831 loss 0.016407372429966927 train acc 0.9787674408336277\n",
            "epoch 35 batch id 2841 loss 0.05162812024354935 train acc 0.9787816789862724\n",
            "epoch 35 batch id 2851 loss 0.003984763287007809 train acc 0.9787903367239565\n",
            "epoch 35 batch id 2861 loss 0.0652192160487175 train acc 0.9787989339391822\n",
            "epoch 35 batch id 2871 loss 0.1255837082862854 train acc 0.9788020289097875\n",
            "epoch 35 batch id 2881 loss 0.04894646629691124 train acc 0.9787834085387018\n",
            "epoch 35 batch id 2891 loss 0.0033826499711722136 train acc 0.9787811310965064\n",
            "epoch 35 batch id 2901 loss 0.014475177973508835 train acc 0.9787788693553947\n",
            "epoch 35 batch id 2911 loss 0.016815995797514915 train acc 0.9787819907248368\n",
            "epoch 35 batch id 2921 loss 0.03421654552221298 train acc 0.9787636939404314\n",
            "epoch 35 batch id 2931 loss 0.02015809528529644 train acc 0.9787508529512112\n",
            "epoch 35 batch id 2941 loss 0.003306785598397255 train acc 0.9787593505610337\n",
            "epoch 35 batch id 2951 loss 0.03636173531413078 train acc 0.9787836750254151\n",
            "epoch 35 batch id 2961 loss 0.08423526585102081 train acc 0.9787814505234718\n",
            "epoch 35 batch id 2971 loss 0.00694704195484519 train acc 0.9787739818243015\n",
            "epoch 35 batch id 2981 loss 0.05384901538491249 train acc 0.9787560801744382\n",
            "epoch 35 batch id 2991 loss 0.032196007668972015 train acc 0.9787539702440655\n",
            "epoch 35 batch id 3001 loss 0.1544743925333023 train acc 0.9787883205598134\n",
            "epoch 35 batch id 3011 loss 0.08789779990911484 train acc 0.9787809282630355\n",
            "epoch 35 batch id 3021 loss 0.03253403678536415 train acc 0.9787735849056604\n",
            "epoch 35 batch id 3031 loss 0.11941304057836533 train acc 0.9787508248102936\n",
            "epoch 35 batch id 3041 loss 0.10467566549777985 train acc 0.9787076619533048\n",
            "epoch 35 batch id 3051 loss 0.0469554103910923 train acc 0.978721116027532\n",
            "epoch 35 batch id 3061 loss 0.10697522759437561 train acc 0.9786987504083633\n",
            "epoch 35 batch id 3071 loss 0.02344568632543087 train acc 0.9787223217193096\n",
            "epoch 35 batch id 3081 loss 0.17651502788066864 train acc 0.9787051687763713\n",
            "epoch 35 batch id 3091 loss 0.0577741302549839 train acc 0.9787184568100938\n",
            "epoch 35 batch id 3101 loss 0.10198397189378738 train acc 0.9787014269590455\n",
            "epoch 35 batch id 3111 loss 0.06678179651498795 train acc 0.9786995740919319\n",
            "epoch 35 batch id 3121 loss 0.031191609799861908 train acc 0.9787027395065684\n",
            "epoch 35 batch id 3131 loss 0.003956057131290436 train acc 0.9787208559565634\n",
            "epoch 35 batch id 3141 loss 0.07371701300144196 train acc 0.9787189589302769\n",
            "epoch 35 batch id 3151 loss 0.016194548457860947 train acc 0.9787021977150111\n",
            "epoch 35 batch id 3161 loss 0.1143667995929718 train acc 0.9787003717178108\n",
            "epoch 35 batch id 3171 loss 0.01908593624830246 train acc 0.9786837748344371\n",
            "epoch 35 batch id 3181 loss 0.03820227459073067 train acc 0.9786967541653568\n",
            "epoch 35 batch id 3191 loss 0.01720435731112957 train acc 0.9787047555625196\n",
            "epoch 35 batch id 3201 loss 0.09991097450256348 train acc 0.9786931818181818\n",
            "epoch 35 batch id 3211 loss 0.1492006778717041 train acc 0.9786476175646216\n",
            "epoch 35 batch id 3221 loss 0.21608997881412506 train acc 0.9786071872089414\n",
            "epoch 35 batch id 3231 loss 0.10013362765312195 train acc 0.9785621711544413\n",
            "epoch 35 batch id 3241 loss 0.033343605697155 train acc 0.9785801064486269\n",
            "epoch 35 batch id 3251 loss 0.0589648112654686 train acc 0.9785739003383574\n",
            "epoch 35 batch id 3261 loss 0.12499038130044937 train acc 0.9785341919656547\n",
            "epoch 35 batch id 3271 loss 0.03127370402216911 train acc 0.9785186105166616\n",
            "epoch 35 batch id 3281 loss 0.0007937613409012556 train acc 0.9785221731179519\n",
            "epoch 35 batch id 3291 loss 0.03400960564613342 train acc 0.9785114706776056\n",
            "epoch 35 batch id 3301 loss 0.015098167583346367 train acc 0.9785245001514693\n",
            "epoch 35 batch id 3311 loss 0.055472176522016525 train acc 0.9785232935668983\n",
            "epoch 35 batch id 3321 loss 0.011111432686448097 train acc 0.9785315040650406\n",
            "epoch 35 batch id 3331 loss 0.026577211916446686 train acc 0.9785068297808466\n",
            "epoch 35 batch id 3341 loss 0.15298891067504883 train acc 0.9785103636635738\n",
            "epoch 35 batch id 3351 loss 0.03328422084450722 train acc 0.9784998880931065\n",
            "epoch 35 batch id 3361 loss 0.02539217099547386 train acc 0.9785266661707825\n",
            "epoch 35 batch id 3371 loss 0.1943923532962799 train acc 0.9785069341441709\n",
            "epoch 35 batch id 3381 loss 0.00047927885316312313 train acc 0.9785150473232771\n",
            "epoch 35 batch id 3391 loss 0.026625683531165123 train acc 0.9785185048658213\n",
            "epoch 35 batch id 3401 loss 0.10648389160633087 train acc 0.9784989708909144\n",
            "epoch 35 batch id 3411 loss 0.0558658167719841 train acc 0.9785253591322193\n",
            "epoch 35 batch id 3421 loss 0.06343650817871094 train acc 0.9785333235895937\n",
            "epoch 35 batch id 3431 loss 0.11679234355688095 train acc 0.9785184712911688\n",
            "epoch 35 batch id 3441 loss 0.0027686695102602243 train acc 0.9785264094739902\n",
            "epoch 35 batch id 3451 loss 0.00789479911327362 train acc 0.9785297739785569\n",
            "epoch 35 batch id 3461 loss 0.0071144127286970615 train acc 0.978542148223057\n",
            "epoch 35 batch id 3471 loss 0.0935017392039299 train acc 0.9785319432440219\n",
            "epoch 35 batch id 3481 loss 0.07544443011283875 train acc 0.9785173082447572\n",
            "epoch 35 batch id 3491 loss 0.0002500585687812418 train acc 0.9785161844743626\n",
            "epoch 35 batch id 3501 loss 0.024202505126595497 train acc 0.978537382176521\n",
            "epoch 35 batch id 3511 loss 0.022360078990459442 train acc 0.9785184064369126\n",
            "epoch 35 batch id 3521 loss 0.09937308728694916 train acc 0.9785306021016756\n",
            "epoch 35 batch id 3531 loss 0.05508895218372345 train acc 0.9785427286887567\n",
            "epoch 35 batch id 3541 loss 0.05485488101840019 train acc 0.9785283112115222\n",
            "epoch 35 batch id 3551 loss 0.15382134914398193 train acc 0.9784919740918051\n",
            "epoch 35 batch id 3561 loss 0.002900016028434038 train acc 0.9785128826172423\n",
            "epoch 35 batch id 3571 loss 0.01792268455028534 train acc 0.9785205474656958\n",
            "epoch 35 batch id 3581 loss 0.066609226167202 train acc 0.9785281695057246\n",
            "epoch 35 batch id 3591 loss 0.20869553089141846 train acc 0.9785313979392927\n",
            "epoch 35 batch id 3601 loss 0.055269692093133926 train acc 0.9785346084420994\n",
            "epoch 35 batch id 3611 loss 0.00973159447312355 train acc 0.9785378011631127\n",
            "epoch 35 batch id 3621 loss 0.06251701712608337 train acc 0.9785409762496547\n",
            "epoch 35 batch id 3631 loss 0.028598235920071602 train acc 0.978544133847425\n",
            "epoch 35 batch id 3641 loss 0.022594638168811798 train acc 0.9785601483109035\n",
            "epoch 35 batch id 3651 loss 0.049354396760463715 train acc 0.9785332785538209\n",
            "epoch 35 batch id 3661 loss 0.001672696671448648 train acc 0.9785150915050532\n",
            "epoch 35 batch id 3671 loss 0.0531340017914772 train acc 0.978514028874966\n",
            "epoch 35 batch id 3681 loss 0.0075707146897912025 train acc 0.9785469301820158\n",
            "epoch 35 batch id 3691 loss 0.02602241560816765 train acc 0.9785542535898131\n",
            "epoch 35 batch id 3701 loss 0.11150771379470825 train acc 0.9785066536071332\n",
            "epoch 35 batch id 3711 loss 0.007371791638433933 train acc 0.9785182565346268\n",
            "epoch 35 batch id 3721 loss 0.0798763707280159 train acc 0.9785046022574577\n",
            "epoch 35 batch id 3731 loss 0.10379958152770996 train acc 0.9785119606003753\n",
            "epoch 35 batch id 3741 loss 0.0453588105738163 train acc 0.9784900427693131\n",
            "epoch 35 batch id 3751 loss 0.0015767948934808373 train acc 0.9785015662490003\n",
            "epoch 35 batch id 3761 loss 0.19923508167266846 train acc 0.9784839470885403\n",
            "epoch 35 batch id 3771 loss 0.015698866918683052 train acc 0.9784871386900027\n",
            "epoch 35 batch id 3781 loss 0.1307905912399292 train acc 0.9784779158952658\n",
            "epoch 35 batch id 3791 loss 0.0005486320587806404 train acc 0.9785058361909786\n",
            "epoch 35 batch id 3801 loss 0.06491238623857498 train acc 0.9784883912128387\n",
            "epoch 35 batch id 3811 loss 0.04662449657917023 train acc 0.9784997376016793\n",
            "epoch 35 batch id 3821 loss 0.03244900330901146 train acc 0.9785028461135828\n",
            "epoch 35 batch id 3831 loss 0.0007097443449310958 train acc 0.9785059383972853\n",
            "epoch 35 batch id 3841 loss 0.005235400982201099 train acc 0.9785171504816454\n",
            "epoch 35 batch id 3851 loss 0.013452911749482155 train acc 0.9785283043365359\n",
            "epoch 35 batch id 3861 loss 0.01723623275756836 train acc 0.978519166019166\n",
            "epoch 35 batch id 3871 loss 0.11556391417980194 train acc 0.9785181477654353\n",
            "epoch 35 batch id 3881 loss 0.04144829511642456 train acc 0.978533238855965\n",
            "epoch 35 batch id 3891 loss 0.01157754473388195 train acc 0.9785402210228733\n",
            "epoch 35 batch id 3901 loss 0.009977423585951328 train acc 0.9785231350935657\n",
            "epoch 35 batch id 3911 loss 0.03793366625905037 train acc 0.9785181219636921\n",
            "epoch 35 batch id 3921 loss 0.015048820525407791 train acc 0.9785330591685795\n",
            "epoch 35 batch id 3931 loss 0.0877128541469574 train acc 0.9785200966675146\n",
            "epoch 35 batch id 3941 loss 0.006667688023298979 train acc 0.9785468472468917\n",
            "epoch 35 batch id 3951 loss 0.015460231341421604 train acc 0.9785260060744115\n",
            "epoch 35 batch id 3961 loss 0.013236609287559986 train acc 0.9785407725321889\n",
            "epoch 35 batch id 3971 loss 0.12081346660852432 train acc 0.9785318559556787\n",
            "epoch 35 batch id 3981 loss 0.012124218977987766 train acc 0.9785347588545592\n",
            "epoch 35 batch id 3991 loss 0.11521190404891968 train acc 0.9785337321473315\n",
            "epoch 35 batch id 4001 loss 0.09279689192771912 train acc 0.9785327105723569\n",
            "epoch 35 batch id 4011 loss 0.03933200612664223 train acc 0.978500529793069\n",
            "epoch 35 batch id 4021 loss 0.06253017485141754 train acc 0.9785151392688386\n",
            "epoch 35 batch id 4031 loss 0.00864716712385416 train acc 0.9785451810965021\n",
            "epoch 35 batch id 4041 loss 0.17093946039676666 train acc 0.978544141301658\n",
            "epoch 35 batch id 4051 loss 0.028154859319329262 train acc 0.9785546778573192\n",
            "epoch 35 batch id 4061 loss 0.1596241444349289 train acc 0.9785767052450135\n",
            "epoch 35 batch id 4071 loss 0.05949320271611214 train acc 0.9785986244166053\n",
            "epoch 35 batch id 4081 loss 0.015181407332420349 train acc 0.9785974638568978\n",
            "epoch 35 batch id 4091 loss 0.06426557898521423 train acc 0.9785886702517722\n",
            "epoch 35 batch id 4101 loss 0.0007174768252298236 train acc 0.9785799195318216\n",
            "epoch 35 batch id 4111 loss 0.012906767427921295 train acc 0.9785674106056921\n",
            "epoch 35 batch id 4121 loss 0.021043686196208 train acc 0.9785777117204562\n",
            "epoch 35 batch id 4131 loss 0.07057315856218338 train acc 0.9785766158315178\n",
            "epoch 35 batch id 4141 loss 0.004937436431646347 train acc 0.9786019379376962\n",
            "epoch 35 batch id 4151 loss 0.06909403204917908 train acc 0.9786007889665141\n",
            "epoch 35 batch id 4161 loss 0.16548939049243927 train acc 0.9785696046623408\n",
            "epoch 35 batch id 4171 loss 0.04008578509092331 train acc 0.9785835231359387\n",
            "epoch 35 batch id 4181 loss 0.007544154766947031 train acc 0.9785861635972255\n",
            "epoch 35 batch id 4191 loss 0.08639305084943771 train acc 0.9785738785492722\n",
            "epoch 35 batch id 4201 loss 0.030234122648835182 train acc 0.9785802487502976\n",
            "epoch 35 batch id 4211 loss 0.06771919876337051 train acc 0.9785717466160057\n",
            "epoch 35 batch id 4221 loss 0.11269810050725937 train acc 0.978555881307747\n",
            "epoch 35 batch id 4231 loss 0.033779747784137726 train acc 0.9785733278184826\n",
            "epoch 35 batch id 4241 loss 0.18789109587669373 train acc 0.9785759549634521\n",
            "epoch 35 batch id 4251 loss 0.08681468665599823 train acc 0.9785822453540344\n",
            "epoch 35 batch id 4261 loss 0.006125116255134344 train acc 0.9785958401783619\n",
            "epoch 35 batch id 4271 loss 0.005919876974076033 train acc 0.9786093713416062\n",
            "epoch 35 batch id 4281 loss 0.02427762746810913 train acc 0.9786264891380518\n",
            "epoch 35 batch id 4291 loss 0.022321561351418495 train acc 0.9786216790957819\n",
            "epoch 35 batch id 4301 loss 0.0719638541340828 train acc 0.9785805626598465\n",
            "epoch 35 batch id 4311 loss 0.016198230907320976 train acc 0.9786085015077708\n",
            "epoch 35 batch id 4321 loss 0.018496517091989517 train acc 0.9785893022448507\n",
            "epoch 35 batch id 4331 loss 0.13793811202049255 train acc 0.978599053336412\n",
            "epoch 35 batch id 4341 loss 0.04663638770580292 train acc 0.9786015607002995\n",
            "epoch 35 batch id 4351 loss 0.06906318664550781 train acc 0.9785645541254884\n",
            "epoch 35 batch id 4361 loss 0.11506224423646927 train acc 0.9785563804173355\n",
            "epoch 35 batch id 4371 loss 0.06582482159137726 train acc 0.9785732669869595\n",
            "epoch 35 batch id 4381 loss 0.30120649933815 train acc 0.9785651107053184\n",
            "epoch 35 batch id 4391 loss 0.0018700656946748495 train acc 0.9785641084035527\n",
            "epoch 35 batch id 4401 loss 0.033279530704021454 train acc 0.9785737616450807\n",
            "epoch 35 batch id 4411 loss 0.0023092508781701326 train acc 0.9785692019950125\n",
            "epoch 35 batch id 4421 loss 0.08315961807966232 train acc 0.9785611287039131\n",
            "epoch 35 batch id 4431 loss 0.010985777713358402 train acc 0.9785566181448883\n",
            "epoch 35 batch id 4441 loss 0.011925598606467247 train acc 0.9785626829542896\n",
            "epoch 35 batch id 4451 loss 0.014611536636948586 train acc 0.978579251853516\n",
            "epoch 35 batch id 4461 loss 0.0037023054901510477 train acc 0.9785887413136068\n",
            "epoch 35 batch id 4471 loss 0.09000945091247559 train acc 0.9785702303735182\n",
            "epoch 35 batch id 4481 loss 0.020123735070228577 train acc 0.9785657498326267\n",
            "epoch 35 batch id 4491 loss 0.044611793011426926 train acc 0.9785821643286573\n",
            "epoch 35 batch id 4501 loss 0.0885695144534111 train acc 0.978581148633637\n",
            "epoch 35 batch id 4511 loss 0.07043145596981049 train acc 0.9785905287076037\n",
            "epoch 35 batch id 4521 loss 0.07643640786409378 train acc 0.9785929550984296\n",
            "epoch 35 batch id 4531 loss 0.0005129450000822544 train acc 0.9785884738468329\n",
            "epoch 35 batch id 4541 loss 0.06439248472452164 train acc 0.9786012166923586\n",
            "epoch 35 batch id 4551 loss 0.054723791778087616 train acc 0.9786070369149638\n",
            "epoch 35 batch id 4561 loss 0.10959739238023758 train acc 0.9786025542644157\n",
            "epoch 35 batch id 4571 loss 0.03947779908776283 train acc 0.9785980912273026\n",
            "epoch 35 batch id 4581 loss 0.022543199360370636 train acc 0.978617523466492\n",
            "epoch 35 batch id 4591 loss 0.05361761152744293 train acc 0.9786198540622958\n",
            "epoch 35 batch id 4601 loss 0.020895840600132942 train acc 0.9786119865246685\n",
            "epoch 35 batch id 4611 loss 0.02845122665166855 train acc 0.9786210962914769\n",
            "epoch 35 batch id 4621 loss 0.010061442852020264 train acc 0.9786301666305994\n",
            "epoch 35 batch id 4631 loss 0.11305425316095352 train acc 0.9786324497948607\n",
            "epoch 35 batch id 4641 loss 0.22674007713794708 train acc 0.9786380898513252\n",
            "epoch 35 batch id 4651 loss 0.12360820919275284 train acc 0.9786571436250269\n",
            "epoch 35 batch id 4661 loss 0.045022547245025635 train acc 0.9786593542158335\n",
            "epoch 35 batch id 4671 loss 0.0056625851429998875 train acc 0.9786849710982659\n",
            "epoch 35 batch id 4681 loss 0.17676910758018494 train acc 0.9786971266823329\n",
            "epoch 35 batch id 4691 loss 0.01150714885443449 train acc 0.9786992379023662\n",
            "epoch 35 batch id 4701 loss 0.024160966277122498 train acc 0.9787079876621996\n",
            "epoch 35 batch id 4711 loss 0.05599222332239151 train acc 0.9787067501592018\n",
            "epoch 35 batch id 4721 loss 0.005135064944624901 train acc 0.9787121372590553\n",
            "epoch 35 batch id 4731 loss 0.02217121794819832 train acc 0.9787108962164447\n",
            "epoch 35 batch id 4741 loss 0.09822915494441986 train acc 0.9787129561273993\n",
            "epoch 35 batch id 4751 loss 0.2506663501262665 train acc 0.9787117185855609\n",
            "epoch 35 batch id 4761 loss 0.048181913793087006 train acc 0.978717049989498\n",
            "epoch 35 batch id 4771 loss 0.08606157451868057 train acc 0.9787223590442256\n",
            "epoch 35 batch id 4781 loss 0.11982086300849915 train acc 0.9787015007320644\n",
            "epoch 35 batch id 4791 loss 0.1742544025182724 train acc 0.9786839908161136\n",
            "epoch 35 batch id 4801 loss 0.055401578545570374 train acc 0.9786730629035617\n",
            "epoch 35 batch id 4811 loss 0.004331718664616346 train acc 0.9786849147786323\n",
            "epoch 35 batch id 4821 loss 0.13891489803791046 train acc 0.9786999585148309\n",
            "epoch 35 batch id 4831 loss 0.03314348682761192 train acc 0.9787052370109708\n",
            "epoch 35 batch id 4841 loss 0.0006993681308813393 train acc 0.9786975831439785\n",
            "epoch 35 batch id 4851 loss 0.025251053273677826 train acc 0.9787125077303649\n",
            "epoch 35 batch id 4861 loss 0.0975695475935936 train acc 0.9787112991154083\n",
            "epoch 35 batch id 4871 loss 0.007780549116432667 train acc 0.978716510983371\n",
            "epoch 35 batch id 4881 loss 0.03076871857047081 train acc 0.9787345062487195\n",
            "epoch 35 batch id 4891 loss 0.023304667323827744 train acc 0.9787204814966265\n",
            "epoch 35 batch id 4901 loss 0.0008765293750911951 train acc 0.9787128902264844\n",
            "epoch 35 batch id 4911 loss 0.15755435824394226 train acc 0.9787148747709224\n",
            "epoch 35 batch id 4921 loss 0.139574334025383 train acc 0.978716851249746\n",
            "epoch 35 batch id 4931 loss 0.05811671167612076 train acc 0.9786871324274995\n",
            "epoch 35 batch id 4941 loss 0.10991273820400238 train acc 0.9786701831613034\n",
            "epoch 35 batch id 4951 loss 0.10314042121171951 train acc 0.9786753938598263\n",
            "epoch 35 train acc 0.9786791406092394\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0775048c6527444181c2ed65690960bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 35 loss 0.4767377972602844 test acc 0.8475348240469208\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50432633740a499db49d89d2e9be9528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 36 batch id 1 loss 0.07059118151664734 train acc 0.96875\n",
            "epoch 36 batch id 11 loss 0.016413796693086624 train acc 0.9815340909090909\n",
            "epoch 36 batch id 21 loss 0.009561954997479916 train acc 0.9836309523809523\n",
            "epoch 36 batch id 31 loss 0.12932179868221283 train acc 0.9808467741935484\n",
            "epoch 36 batch id 41 loss 0.0360378623008728 train acc 0.9824695121951219\n",
            "epoch 36 batch id 51 loss 0.041694674640893936 train acc 0.9828431372549019\n",
            "epoch 36 batch id 61 loss 0.03811066970229149 train acc 0.9800204918032787\n",
            "epoch 36 batch id 71 loss 0.10678049176931381 train acc 0.9797535211267606\n",
            "epoch 36 batch id 81 loss 0.013875038363039494 train acc 0.9801311728395061\n",
            "epoch 36 batch id 91 loss 0.06933438032865524 train acc 0.9811126373626373\n",
            "epoch 36 batch id 101 loss 0.0421333871781826 train acc 0.9803527227722773\n",
            "epoch 36 batch id 111 loss 0.002836662344634533 train acc 0.9805743243243243\n",
            "epoch 36 batch id 121 loss 0.06753982603549957 train acc 0.981146694214876\n",
            "epoch 36 batch id 131 loss 0.022866955026984215 train acc 0.9812738549618321\n",
            "epoch 36 batch id 141 loss 0.0009302442194893956 train acc 0.981604609929078\n",
            "epoch 36 batch id 151 loss 0.048221077769994736 train acc 0.9818915562913907\n",
            "epoch 36 batch id 161 loss 0.0022706256713718176 train acc 0.9820458074534162\n",
            "epoch 36 batch id 171 loss 0.010083182714879513 train acc 0.981999269005848\n",
            "epoch 36 batch id 181 loss 0.02260216325521469 train acc 0.9824758287292817\n",
            "epoch 36 batch id 191 loss 0.04723735898733139 train acc 0.9822480366492147\n",
            "epoch 36 batch id 201 loss 0.21398045122623444 train acc 0.9819651741293532\n",
            "epoch 36 batch id 211 loss 0.002115444280207157 train acc 0.9820053317535545\n",
            "epoch 36 batch id 221 loss 0.08857935667037964 train acc 0.9816176470588235\n",
            "epoch 36 batch id 231 loss 0.032755762338638306 train acc 0.9821428571428571\n",
            "epoch 36 batch id 241 loss 0.17223697900772095 train acc 0.9819761410788381\n",
            "epoch 36 batch id 251 loss 0.18885473906993866 train acc 0.9817604581673307\n",
            "epoch 36 batch id 261 loss 0.05438750982284546 train acc 0.9815613026819924\n",
            "epoch 36 batch id 271 loss 0.025491105392575264 train acc 0.9818380996309963\n",
            "epoch 36 batch id 281 loss 0.13298767805099487 train acc 0.9820395907473309\n",
            "epoch 36 batch id 291 loss 0.20614373683929443 train acc 0.982227233676976\n",
            "epoch 36 batch id 301 loss 0.09362170100212097 train acc 0.9820390365448505\n",
            "epoch 36 batch id 311 loss 0.025069473311305046 train acc 0.9820639067524116\n",
            "epoch 36 batch id 321 loss 0.2191995084285736 train acc 0.9819898753894081\n",
            "epoch 36 batch id 331 loss 0.021142860874533653 train acc 0.9818731117824774\n",
            "epoch 36 batch id 341 loss 0.1782870590686798 train acc 0.9819464809384164\n",
            "epoch 36 batch id 351 loss 0.06153629347681999 train acc 0.9821492165242165\n",
            "epoch 36 batch id 361 loss 0.021238792687654495 train acc 0.9819511772853186\n",
            "epoch 36 batch id 371 loss 0.02351812832057476 train acc 0.9821428571428571\n",
            "epoch 36 batch id 381 loss 0.020413361489772797 train acc 0.9822834645669292\n",
            "epoch 36 batch id 391 loss 0.09877654910087585 train acc 0.9821771099744245\n",
            "epoch 36 batch id 401 loss 0.0002485796285327524 train acc 0.9822708852867831\n",
            "epoch 36 batch id 411 loss 0.03396658971905708 train acc 0.9822840632603407\n",
            "epoch 36 batch id 421 loss 0.01001847442239523 train acc 0.9824821852731591\n",
            "epoch 36 batch id 431 loss 0.057635705918073654 train acc 0.982417343387471\n",
            "epoch 36 batch id 441 loss 0.1545814573764801 train acc 0.982249149659864\n",
            "epoch 36 batch id 451 loss 0.016243716701865196 train acc 0.9822962860310421\n",
            "epoch 36 batch id 461 loss 0.004577585030347109 train acc 0.9821380151843818\n",
            "epoch 36 batch id 471 loss 0.010120502673089504 train acc 0.9822518577494692\n",
            "epoch 36 batch id 481 loss 0.061871856451034546 train acc 0.9822635135135135\n",
            "epoch 36 batch id 491 loss 0.031445134431123734 train acc 0.9821474032586558\n",
            "epoch 36 batch id 501 loss 0.10898978263139725 train acc 0.982191866267465\n",
            "epoch 36 batch id 511 loss 0.07745079696178436 train acc 0.9821428571428571\n",
            "epoch 36 batch id 521 loss 0.08658354729413986 train acc 0.9822156909788867\n",
            "epoch 36 batch id 531 loss 0.026394056156277657 train acc 0.9821386534839924\n",
            "epoch 36 batch id 541 loss 0.01583336479961872 train acc 0.9820355822550831\n",
            "epoch 36 batch id 551 loss 0.09997769445180893 train acc 0.9819078947368421\n",
            "epoch 36 batch id 561 loss 0.04119054228067398 train acc 0.9818126114081996\n",
            "epoch 36 batch id 571 loss 0.06324969977140427 train acc 0.9817753940455342\n",
            "epoch 36 batch id 581 loss 0.05275798588991165 train acc 0.9817394578313253\n",
            "epoch 36 batch id 591 loss 0.03978114202618599 train acc 0.981757614213198\n",
            "epoch 36 batch id 601 loss 0.02380138635635376 train acc 0.9816971713810316\n",
            "epoch 36 batch id 611 loss 0.04957795888185501 train acc 0.9816131342062193\n",
            "epoch 36 batch id 621 loss 0.17945723235607147 train acc 0.9816576086956522\n",
            "epoch 36 batch id 631 loss 0.05410333722829819 train acc 0.9815273375594294\n",
            "epoch 36 batch id 641 loss 0.08639266341924667 train acc 0.9814498829953198\n",
            "epoch 36 batch id 651 loss 0.021380804479122162 train acc 0.9813028033794163\n",
            "epoch 36 batch id 661 loss 0.003994385711848736 train acc 0.9814674735249622\n",
            "epoch 36 batch id 671 loss 0.010048838332295418 train acc 0.9813478017883756\n",
            "epoch 36 batch id 681 loss 0.17005348205566406 train acc 0.9813004772393539\n",
            "epoch 36 batch id 691 loss 0.06001298502087593 train acc 0.9812771345875543\n",
            "epoch 36 batch id 701 loss 0.09382252395153046 train acc 0.9812321683309557\n",
            "epoch 36 batch id 711 loss 0.11342772841453552 train acc 0.9812763713080169\n",
            "epoch 36 batch id 721 loss 0.048944223672151566 train acc 0.9811459778085991\n",
            "epoch 36 batch id 731 loss 0.012850351631641388 train acc 0.9811046511627907\n",
            "epoch 36 batch id 741 loss 0.045141078531742096 train acc 0.9810855263157895\n",
            "epoch 36 batch id 751 loss 0.03527383133769035 train acc 0.9810669107856191\n",
            "epoch 36 batch id 761 loss 0.05699116364121437 train acc 0.9810693166885677\n",
            "epoch 36 batch id 771 loss 0.030785780400037766 train acc 0.9809095330739299\n",
            "epoch 36 batch id 781 loss 0.023129045963287354 train acc 0.9809138924455826\n",
            "epoch 36 batch id 791 loss 0.011268803849816322 train acc 0.980977402022756\n",
            "epoch 36 batch id 801 loss 0.0034432534594088793 train acc 0.9809808052434457\n",
            "epoch 36 batch id 811 loss 0.06579858809709549 train acc 0.9810611898890259\n",
            "epoch 36 batch id 821 loss 0.004808333236724138 train acc 0.9810634896467723\n",
            "epoch 36 batch id 831 loss 0.11280614137649536 train acc 0.9810281287605295\n",
            "epoch 36 batch id 841 loss 0.004111540038138628 train acc 0.9809750297265161\n",
            "epoch 36 batch id 851 loss 0.05577169358730316 train acc 0.9808497356051704\n",
            "epoch 36 batch id 861 loss 0.026091907173395157 train acc 0.9807999419279907\n",
            "epoch 36 batch id 871 loss 0.037707649171352386 train acc 0.9807871699196326\n",
            "epoch 36 batch id 881 loss 0.021234413608908653 train acc 0.9807037457434733\n",
            "epoch 36 batch id 891 loss 0.07300221920013428 train acc 0.9807274130190797\n",
            "epoch 36 batch id 901 loss 0.10615114122629166 train acc 0.9807505549389567\n",
            "epoch 36 batch id 911 loss 0.009996464475989342 train acc 0.9807560373216246\n",
            "epoch 36 batch id 921 loss 0.0004270183271728456 train acc 0.9807274701411509\n",
            "epoch 36 batch id 931 loss 0.0046932268887758255 train acc 0.9806995166487648\n",
            "epoch 36 batch id 941 loss 0.011218072846531868 train acc 0.980788390010627\n",
            "epoch 36 batch id 951 loss 0.2412843257188797 train acc 0.9806618033648791\n",
            "epoch 36 batch id 961 loss 0.05121366307139397 train acc 0.9806354058272633\n",
            "epoch 36 batch id 971 loss 0.009760981425642967 train acc 0.9806256436663234\n",
            "epoch 36 batch id 981 loss 0.010610750876367092 train acc 0.9806957186544343\n",
            "epoch 36 batch id 991 loss 0.0891355350613594 train acc 0.9807486125126135\n",
            "epoch 36 batch id 1001 loss 0.011534673161804676 train acc 0.9808160589410589\n",
            "epoch 36 batch id 1011 loss 0.07665584981441498 train acc 0.9807739861523245\n",
            "epoch 36 batch id 1021 loss 0.044192954897880554 train acc 0.9808092556317336\n",
            "epoch 36 batch id 1031 loss 0.02883881703019142 train acc 0.9808135305528612\n",
            "epoch 36 batch id 1041 loss 0.026327326893806458 train acc 0.9807877041306436\n",
            "epoch 36 batch id 1051 loss 0.021321700885891914 train acc 0.9807029019980971\n",
            "epoch 36 batch id 1061 loss 0.022409269586205482 train acc 0.9806491517436381\n",
            "epoch 36 batch id 1071 loss 0.002159322611987591 train acc 0.9805964052287581\n",
            "epoch 36 batch id 1081 loss 0.02648116648197174 train acc 0.9804723635522664\n",
            "epoch 36 batch id 1091 loss 0.17459966242313385 train acc 0.9805081347387717\n",
            "epoch 36 batch id 1101 loss 0.052072830498218536 train acc 0.98047229791099\n",
            "epoch 36 batch id 1111 loss 0.06619200110435486 train acc 0.9804230423042304\n",
            "epoch 36 batch id 1121 loss 0.02856789156794548 train acc 0.9805001115075825\n",
            "epoch 36 batch id 1131 loss 0.044541697949171066 train acc 0.9803962201591512\n",
            "epoch 36 batch id 1141 loss 0.03143061697483063 train acc 0.9804447852760736\n",
            "epoch 36 batch id 1151 loss 0.07691855728626251 train acc 0.9803296046915726\n",
            "epoch 36 batch id 1161 loss 0.18395279347896576 train acc 0.9802971576227391\n",
            "epoch 36 batch id 1171 loss 0.015639320015907288 train acc 0.9802519214346712\n",
            "epoch 36 batch id 1181 loss 0.0003116103762295097 train acc 0.9803000635055038\n",
            "epoch 36 batch id 1191 loss 0.021718578413128853 train acc 0.9803342779177162\n",
            "epoch 36 batch id 1201 loss 0.0223577618598938 train acc 0.980419962531224\n",
            "epoch 36 batch id 1211 loss 0.0007536964258179069 train acc 0.9804010115606936\n",
            "epoch 36 batch id 1221 loss 0.12227180600166321 train acc 0.9804463554463555\n",
            "epoch 36 batch id 1231 loss 0.07481110841035843 train acc 0.9803640333062551\n",
            "epoch 36 batch id 1241 loss 0.009489721618592739 train acc 0.9803459911361805\n",
            "epoch 36 batch id 1251 loss 0.0922141894698143 train acc 0.9803407274180655\n",
            "epoch 36 batch id 1261 loss 0.017090128734707832 train acc 0.980323156225218\n",
            "epoch 36 batch id 1271 loss 0.026931073516607285 train acc 0.9802812745869394\n",
            "epoch 36 batch id 1281 loss 0.08169125765562057 train acc 0.9803010343481655\n",
            "epoch 36 batch id 1291 loss 0.07534071803092957 train acc 0.9802962819519753\n",
            "epoch 36 batch id 1301 loss 0.29379796981811523 train acc 0.9802315526518063\n",
            "epoch 36 batch id 1311 loss 0.0370284840464592 train acc 0.9802154843630816\n",
            "epoch 36 batch id 1321 loss 0.01673148199915886 train acc 0.9801405185465556\n",
            "epoch 36 batch id 1331 loss 0.0310626532882452 train acc 0.9801253756574004\n",
            "epoch 36 batch id 1341 loss 0.20369966328144073 train acc 0.980122110365399\n",
            "epoch 36 batch id 1351 loss 0.00963450875133276 train acc 0.9801188934122872\n",
            "epoch 36 batch id 1361 loss 0.03789754584431648 train acc 0.9801157237325496\n",
            "epoch 36 batch id 1371 loss 0.017106177285313606 train acc 0.9801353938730853\n",
            "epoch 36 batch id 1381 loss 0.04563780874013901 train acc 0.98009820782042\n",
            "epoch 36 batch id 1391 loss 0.06982515752315521 train acc 0.9800503235082675\n",
            "epoch 36 batch id 1401 loss 0.14364518225193024 train acc 0.9800477337615988\n",
            "epoch 36 batch id 1411 loss 0.04963311180472374 train acc 0.9801005492558469\n",
            "epoch 36 batch id 1421 loss 0.0075635951943695545 train acc 0.9801196340605207\n",
            "epoch 36 batch id 1431 loss 0.2292761504650116 train acc 0.9801166142557652\n",
            "epoch 36 batch id 1441 loss 0.021039538085460663 train acc 0.9801136363636364\n",
            "epoch 36 batch id 1451 loss 0.014364599250257015 train acc 0.9801430048242591\n",
            "epoch 36 batch id 1461 loss 0.009597359225153923 train acc 0.9801398870636551\n",
            "epoch 36 batch id 1471 loss 0.01998232863843441 train acc 0.9801899218218899\n",
            "epoch 36 batch id 1481 loss 0.054305970668792725 train acc 0.9801759790681972\n",
            "epoch 36 batch id 1491 loss 0.07310386002063751 train acc 0.9801831824279007\n",
            "epoch 36 batch id 1501 loss 0.0643509179353714 train acc 0.9802006995336442\n",
            "epoch 36 batch id 1511 loss 0.10641958564519882 train acc 0.9801973031105228\n",
            "epoch 36 batch id 1521 loss 0.07410170137882233 train acc 0.980245315581854\n",
            "epoch 36 batch id 1531 loss 0.027074871584773064 train acc 0.9802620836054866\n",
            "epoch 36 batch id 1541 loss 0.009014065377414227 train acc 0.9802684944841012\n",
            "epoch 36 batch id 1551 loss 0.052409827709198 train acc 0.9803050451321728\n",
            "epoch 36 batch id 1561 loss 0.1501166671514511 train acc 0.9802210121716848\n",
            "epoch 36 batch id 1571 loss 0.008000237867236137 train acc 0.9801977243793762\n",
            "epoch 36 batch id 1581 loss 0.03428024798631668 train acc 0.9801944971537002\n",
            "epoch 36 batch id 1591 loss 0.14053063094615936 train acc 0.9802011313639221\n",
            "epoch 36 batch id 1601 loss 0.05579042062163353 train acc 0.980256480324797\n",
            "epoch 36 batch id 1611 loss 0.06305311620235443 train acc 0.9802335505896959\n",
            "epoch 36 batch id 1621 loss 0.03238990902900696 train acc 0.9802205428747687\n",
            "epoch 36 batch id 1631 loss 0.013410861603915691 train acc 0.9802555947271613\n",
            "epoch 36 batch id 1641 loss 0.06001932546496391 train acc 0.9802426112126752\n",
            "epoch 36 batch id 1651 loss 0.07937048375606537 train acc 0.9801730012113871\n",
            "epoch 36 batch id 1661 loss 0.057299304753541946 train acc 0.9801888922335942\n",
            "epoch 36 batch id 1671 loss 0.019119057804346085 train acc 0.9801765409934171\n",
            "epoch 36 batch id 1681 loss 0.018412135541439056 train acc 0.9801550416418798\n",
            "epoch 36 batch id 1691 loss 0.15622441470623016 train acc 0.9801984772324068\n",
            "epoch 36 batch id 1701 loss 0.043370552361011505 train acc 0.9801954732510288\n",
            "epoch 36 batch id 1711 loss 0.03395000100135803 train acc 0.9801925043834016\n",
            "epoch 36 batch id 1721 loss 0.045736584812402725 train acc 0.9802077280650784\n",
            "epoch 36 batch id 1731 loss 0.000178096117451787 train acc 0.9802318024263431\n",
            "epoch 36 batch id 1741 loss 0.01874377951025963 train acc 0.9802017518667433\n",
            "epoch 36 batch id 1751 loss 0.0471155010163784 train acc 0.9801541976013707\n",
            "epoch 36 batch id 1761 loss 0.07624489068984985 train acc 0.9801781658148779\n",
            "epoch 36 batch id 1771 loss 0.002662424696609378 train acc 0.9802106860530774\n",
            "epoch 36 batch id 1781 loss 0.0002797628112602979 train acc 0.98023406793936\n",
            "epoch 36 batch id 1791 loss 0.04225483164191246 train acc 0.9802571887213847\n",
            "epoch 36 batch id 1801 loss 0.014023805968463421 train acc 0.9802540255413659\n",
            "epoch 36 batch id 1811 loss 0.043861888349056244 train acc 0.9802681529541689\n",
            "epoch 36 batch id 1821 loss 0.09948092699050903 train acc 0.9802478034047227\n",
            "epoch 36 batch id 1831 loss 0.014496231451630592 train acc 0.98025327689787\n",
            "epoch 36 batch id 1841 loss 0.03168376162648201 train acc 0.9802077675176535\n",
            "epoch 36 batch id 1851 loss 0.017695855349302292 train acc 0.9802218395461912\n",
            "epoch 36 batch id 1861 loss 0.017034249380230904 train acc 0.9802021762493284\n",
            "epoch 36 batch id 1871 loss 0.07978899031877518 train acc 0.9802662346338856\n",
            "epoch 36 batch id 1881 loss 0.02078208513557911 train acc 0.9802797713981924\n",
            "epoch 36 batch id 1891 loss 0.03383030742406845 train acc 0.9802931649920676\n",
            "epoch 36 batch id 1901 loss 0.094681017100811 train acc 0.9802817596002105\n",
            "epoch 36 batch id 1911 loss 0.027277132496237755 train acc 0.980262297226583\n",
            "epoch 36 batch id 1921 loss 0.0083630895242095 train acc 0.9801942347735555\n",
            "epoch 36 batch id 1931 loss 0.11467637121677399 train acc 0.9801592439150699\n",
            "epoch 36 batch id 1941 loss 0.02080419473350048 train acc 0.9801568134981968\n",
            "epoch 36 batch id 1951 loss 0.031374070793390274 train acc 0.9801544079958996\n",
            "epoch 36 batch id 1961 loss 0.03811677545309067 train acc 0.9801838985211627\n",
            "epoch 36 batch id 1971 loss 0.0659983679652214 train acc 0.9801813800101471\n",
            "epoch 36 batch id 1981 loss 0.041871048510074615 train acc 0.9801552246340233\n",
            "epoch 36 batch id 1991 loss 0.16437643766403198 train acc 0.9800900929181315\n",
            "epoch 36 batch id 2001 loss 0.0007148456643335521 train acc 0.9801036981509246\n",
            "epoch 36 batch id 2011 loss 0.12434382736682892 train acc 0.9801016285430134\n",
            "epoch 36 batch id 2021 loss 0.029600918292999268 train acc 0.9801227733795151\n",
            "epoch 36 batch id 2031 loss 0.03921566531062126 train acc 0.9800821639586411\n",
            "epoch 36 batch id 2041 loss 0.01037592999637127 train acc 0.9800802302792748\n",
            "epoch 36 batch id 2051 loss 0.1707475185394287 train acc 0.9800630789858605\n",
            "epoch 36 batch id 2061 loss 0.10005097091197968 train acc 0.9800460941290635\n",
            "epoch 36 batch id 2071 loss 0.08169728517532349 train acc 0.9800066393046837\n",
            "epoch 36 batch id 2081 loss 0.04755929112434387 train acc 0.9800126141278231\n",
            "epoch 36 batch id 2091 loss 0.054442740976810455 train acc 0.9800035868005739\n",
            "epoch 36 batch id 2101 loss 0.003229558002203703 train acc 0.9800243931461209\n",
            "epoch 36 batch id 2111 loss 0.008617784827947617 train acc 0.9800005921364282\n",
            "epoch 36 batch id 2121 loss 0.027082454413175583 train acc 0.9800285832154644\n",
            "epoch 36 batch id 2131 loss 0.015966739505529404 train acc 0.9800783083059597\n",
            "epoch 36 batch id 2141 loss 0.03057178668677807 train acc 0.9800764829518916\n",
            "epoch 36 batch id 2151 loss 0.06515955924987793 train acc 0.9801109948860995\n",
            "epoch 36 batch id 2161 loss 0.06527140736579895 train acc 0.9800873438223044\n",
            "epoch 36 batch id 2171 loss 0.03134666755795479 train acc 0.9800855020727776\n",
            "epoch 36 batch id 2181 loss 0.049947552382946014 train acc 0.9800765130674003\n",
            "epoch 36 batch id 2191 loss 0.09911636263132095 train acc 0.9800604746691008\n",
            "epoch 36 batch id 2201 loss 0.09091626107692719 train acc 0.9800658791458428\n",
            "epoch 36 batch id 2211 loss 0.005662261042743921 train acc 0.9800712347354138\n",
            "epoch 36 batch id 2221 loss 0.01707671396434307 train acc 0.9800554367402071\n",
            "epoch 36 batch id 2231 loss 0.013817030005156994 train acc 0.9800888054683998\n",
            "epoch 36 batch id 2241 loss 0.06324689835309982 train acc 0.9800660977242303\n",
            "epoch 36 batch id 2251 loss 0.0015363069251179695 train acc 0.9800435917370057\n",
            "epoch 36 batch id 2261 loss 0.00026658305432647467 train acc 0.9800420168067226\n",
            "epoch 36 batch id 2271 loss 0.08660876750946045 train acc 0.9800473359753412\n",
            "epoch 36 batch id 2281 loss 0.0323546826839447 train acc 0.9800526085050416\n",
            "epoch 36 batch id 2291 loss 0.08372608572244644 train acc 0.9800510148406809\n",
            "epoch 36 batch id 2301 loss 0.04503827914595604 train acc 0.9800494350282486\n",
            "epoch 36 batch id 2311 loss 0.046600162982940674 train acc 0.9800681523150151\n",
            "epoch 36 batch id 2321 loss 0.10401882976293564 train acc 0.9800530482550625\n",
            "epoch 36 batch id 2331 loss 0.17353662848472595 train acc 0.979997854997855\n",
            "epoch 36 batch id 2341 loss 0.008601781912147999 train acc 0.980036576249466\n",
            "epoch 36 batch id 2351 loss 0.05913545563817024 train acc 0.9800217992343684\n",
            "epoch 36 batch id 2361 loss 0.07629907131195068 train acc 0.9800600910631089\n",
            "epoch 36 batch id 2371 loss 0.09067335724830627 train acc 0.9800782897511598\n",
            "epoch 36 batch id 2381 loss 0.03820324316620827 train acc 0.9801028979420412\n",
            "epoch 36 batch id 2391 loss 0.035252202302217484 train acc 0.980081555834379\n",
            "epoch 36 batch id 2401 loss 0.0232821274548769 train acc 0.9800473760932945\n",
            "epoch 36 batch id 2411 loss 0.02437973953783512 train acc 0.9800069991704687\n",
            "epoch 36 batch id 2421 loss 0.008029180578887463 train acc 0.9800508570838496\n",
            "epoch 36 batch id 2431 loss 0.055906686931848526 train acc 0.98008149938297\n",
            "epoch 36 batch id 2441 loss 0.0016827343497425318 train acc 0.9800926874231872\n",
            "epoch 36 batch id 2451 loss 0.0003936585853807628 train acc 0.9800591595267237\n",
            "epoch 36 batch id 2461 loss 0.14149445295333862 train acc 0.9800703474197481\n",
            "epoch 36 batch id 2471 loss 0.1993928998708725 train acc 0.9800435046539863\n",
            "epoch 36 batch id 2481 loss 0.1761622130870819 train acc 0.9800609633212415\n",
            "epoch 36 batch id 2491 loss 0.12235883623361588 train acc 0.9800218285828984\n",
            "epoch 36 batch id 2501 loss 0.05527225509285927 train acc 0.9800392343062775\n",
            "epoch 36 batch id 2511 loss 0.014018154703080654 train acc 0.980056501393867\n",
            "epoch 36 batch id 2521 loss 0.033187177032232285 train acc 0.9799992562475208\n",
            "epoch 36 batch id 2531 loss 0.03325716778635979 train acc 0.9799980244962465\n",
            "epoch 36 batch id 2541 loss 0.039306677877902985 train acc 0.9799968024399842\n",
            "epoch 36 batch id 2551 loss 0.05408106744289398 train acc 0.979952714621717\n",
            "epoch 36 batch id 2561 loss 0.016052061691880226 train acc 0.9799272745021476\n",
            "epoch 36 batch id 2571 loss 0.10051091015338898 train acc 0.9799202644885259\n",
            "epoch 36 batch id 2581 loss 0.03717103600502014 train acc 0.979883039519566\n",
            "epoch 36 batch id 2591 loss 0.012552102096378803 train acc 0.9798883153222694\n",
            "epoch 36 batch id 2601 loss 0.06612737476825714 train acc 0.9799295943867743\n",
            "epoch 36 batch id 2611 loss 0.0009309641318395734 train acc 0.9799466200689391\n",
            "epoch 36 batch id 2621 loss 0.04571061581373215 train acc 0.9799635158336513\n",
            "epoch 36 batch id 2631 loss 0.07803256809711456 train acc 0.9799327727099962\n",
            "epoch 36 batch id 2641 loss 0.030355235561728477 train acc 0.9799377603180613\n",
            "epoch 36 batch id 2651 loss 0.0004192761261947453 train acc 0.9799309222934741\n",
            "epoch 36 batch id 2661 loss 0.0014180168509483337 train acc 0.9799300075159715\n",
            "epoch 36 batch id 2671 loss 0.10556519776582718 train acc 0.9799407993260951\n",
            "epoch 36 batch id 2681 loss 0.03981414809823036 train acc 0.9799340264826557\n",
            "epoch 36 batch id 2691 loss 0.04090776666998863 train acc 0.979927303976217\n",
            "epoch 36 batch id 2701 loss 0.1227358803153038 train acc 0.9799148463532025\n",
            "epoch 36 batch id 2711 loss 0.00521846953779459 train acc 0.9798794264109185\n",
            "epoch 36 batch id 2721 loss 0.016795454546809196 train acc 0.9798500091877986\n",
            "epoch 36 batch id 2731 loss 0.0007381304749287665 train acc 0.9798665781764921\n",
            "epoch 36 batch id 2741 loss 0.08547480404376984 train acc 0.9798716253192266\n",
            "epoch 36 batch id 2751 loss 0.17936445772647858 train acc 0.9798425572519084\n",
            "epoch 36 batch id 2761 loss 0.023419247940182686 train acc 0.9798533140166606\n",
            "epoch 36 batch id 2771 loss 0.005918447859585285 train acc 0.9798527156261277\n",
            "epoch 36 batch id 2781 loss 0.03609011694788933 train acc 0.9798858324343761\n",
            "epoch 36 batch id 2791 loss 0.021572541445493698 train acc 0.9798851218201362\n",
            "epoch 36 batch id 2801 loss 0.08425205200910568 train acc 0.979867681185291\n",
            "epoch 36 batch id 2811 loss 0.06555568426847458 train acc 0.9798892742796158\n",
            "epoch 36 batch id 2821 loss 0.011834741570055485 train acc 0.9799273307337824\n",
            "epoch 36 batch id 2831 loss 0.01402443740516901 train acc 0.9799320028258566\n",
            "epoch 36 batch id 2841 loss 0.03207104653120041 train acc 0.9799311422034495\n",
            "epoch 36 batch id 2851 loss 0.005921583157032728 train acc 0.979941248684672\n",
            "epoch 36 batch id 2861 loss 0.056562040001153946 train acc 0.9799512845159035\n",
            "epoch 36 batch id 2871 loss 0.35372936725616455 train acc 0.9799285963079066\n",
            "epoch 36 batch id 2881 loss 0.04086006060242653 train acc 0.9799223359944463\n",
            "epoch 36 batch id 2891 loss 0.00048478785902261734 train acc 0.9799215236942235\n",
            "epoch 36 batch id 2901 loss 0.007190702483057976 train acc 0.9799153309203723\n",
            "epoch 36 batch id 2911 loss 0.03299607336521149 train acc 0.979898445551357\n",
            "epoch 36 batch id 2921 loss 0.00037641864037141204 train acc 0.9798816757959603\n",
            "epoch 36 batch id 2931 loss 0.021676957607269287 train acc 0.979865020470829\n",
            "epoch 36 batch id 2941 loss 0.0664171501994133 train acc 0.9798537912274736\n",
            "epoch 36 batch id 2951 loss 0.08512099087238312 train acc 0.9798955862419518\n",
            "epoch 36 batch id 2961 loss 0.08836337178945541 train acc 0.9799054373522459\n",
            "epoch 36 batch id 2971 loss 0.013405226171016693 train acc 0.9798784079434534\n",
            "epoch 36 batch id 2981 loss 0.013443553820252419 train acc 0.9798672844682992\n",
            "epoch 36 batch id 2991 loss 0.017129061743617058 train acc 0.9798666833834838\n",
            "epoch 36 batch id 3001 loss 0.027084531262516975 train acc 0.9798921192935688\n",
            "epoch 36 batch id 3011 loss 0.03764443099498749 train acc 0.979881061109266\n",
            "epoch 36 batch id 3021 loss 0.03057214803993702 train acc 0.9798907646474677\n",
            "epoch 36 batch id 3031 loss 0.1264905333518982 train acc 0.9798746288353679\n",
            "epoch 36 batch id 3041 loss 0.07345660775899887 train acc 0.9798431848076291\n",
            "epoch 36 batch id 3051 loss 0.013290653936564922 train acc 0.9798529170763685\n",
            "epoch 36 batch id 3061 loss 0.10094141960144043 train acc 0.9798472721332898\n",
            "epoch 36 batch id 3071 loss 0.0003864321915898472 train acc 0.9798518397915988\n",
            "epoch 36 batch id 3081 loss 0.1117113009095192 train acc 0.9798614492048037\n",
            "epoch 36 batch id 3091 loss 0.02800571732223034 train acc 0.9798709964412812\n",
            "epoch 36 batch id 3101 loss 0.06850800663232803 train acc 0.9798502499193809\n",
            "epoch 36 batch id 3111 loss 0.10001959651708603 train acc 0.9798597717775634\n",
            "epoch 36 batch id 3121 loss 0.09245327115058899 train acc 0.9798592198013457\n",
            "epoch 36 batch id 3131 loss 0.04113902151584625 train acc 0.9798886138613861\n",
            "epoch 36 batch id 3141 loss 0.0757804661989212 train acc 0.9798780245144858\n",
            "epoch 36 batch id 3151 loss 0.045265063643455505 train acc 0.9798972548397334\n",
            "epoch 36 batch id 3161 loss 0.12664270401000977 train acc 0.9798768190446061\n",
            "epoch 36 batch id 3171 loss 0.047417256981134415 train acc 0.9798565121412803\n",
            "epoch 36 batch id 3181 loss 0.0497993640601635 train acc 0.9798265089594467\n",
            "epoch 36 batch id 3191 loss 0.018416905775666237 train acc 0.9798505562519586\n",
            "epoch 36 batch id 3201 loss 0.11181574314832687 train acc 0.9798402842861605\n",
            "epoch 36 batch id 3211 loss 0.18083783984184265 train acc 0.979815478044223\n",
            "epoch 36 batch id 3221 loss 0.14926356077194214 train acc 0.9797714219186588\n",
            "epoch 36 batch id 3231 loss 0.03209567815065384 train acc 0.979746982358403\n",
            "epoch 36 batch id 3241 loss 0.06061626970767975 train acc 0.9797564409132984\n",
            "epoch 36 batch id 3251 loss 0.05231884866952896 train acc 0.9797514226391879\n",
            "epoch 36 batch id 3261 loss 0.10745910555124283 train acc 0.9797128948175406\n",
            "epoch 36 batch id 3271 loss 0.042094636708498 train acc 0.9797032635279731\n",
            "epoch 36 batch id 3281 loss 0.06008965149521828 train acc 0.9797032154830845\n",
            "epoch 36 batch id 3291 loss 0.015830116346478462 train acc 0.9797031677301732\n",
            "epoch 36 batch id 3301 loss 0.010681964457035065 train acc 0.9797220539230537\n",
            "epoch 36 batch id 3311 loss 0.1381148397922516 train acc 0.9797361069163395\n",
            "epoch 36 batch id 3321 loss 0.017768504098057747 train acc 0.9797312556458898\n",
            "epoch 36 batch id 3331 loss 0.03418784588575363 train acc 0.97970297958571\n",
            "epoch 36 batch id 3341 loss 0.18621277809143066 train acc 0.9797169634839868\n",
            "epoch 36 batch id 3351 loss 0.013027864508330822 train acc 0.9797122127723068\n",
            "epoch 36 batch id 3361 loss 0.03211815282702446 train acc 0.9797121392442726\n",
            "epoch 36 batch id 3371 loss 0.0912911668419838 train acc 0.9796888905369326\n",
            "epoch 36 batch id 3381 loss 0.0018800910329446197 train acc 0.9797027506654836\n",
            "epoch 36 batch id 3391 loss 0.12588012218475342 train acc 0.9797073134768505\n",
            "epoch 36 batch id 3401 loss 0.09300003200769424 train acc 0.979698066745075\n",
            "epoch 36 batch id 3411 loss 0.05868922173976898 train acc 0.9797117780709469\n",
            "epoch 36 batch id 3421 loss 0.09908200800418854 train acc 0.9797071397252265\n",
            "epoch 36 batch id 3431 loss 0.12415073066949844 train acc 0.979720744680851\n",
            "epoch 36 batch id 3441 loss 0.05061066523194313 train acc 0.9797206480674222\n",
            "epoch 36 batch id 3451 loss 0.012719796970486641 train acc 0.9797296073601854\n",
            "epoch 36 batch id 3461 loss 0.13138674199581146 train acc 0.9797294856977752\n",
            "epoch 36 batch id 3471 loss 0.03062540292739868 train acc 0.9797428694900605\n",
            "epoch 36 batch id 3481 loss 0.09657694399356842 train acc 0.9797247558172939\n",
            "epoch 36 batch id 3491 loss 0.0002550342760514468 train acc 0.9797425522772845\n",
            "epoch 36 batch id 3501 loss 0.04859520122408867 train acc 0.9797468580405598\n",
            "epoch 36 batch id 3511 loss 0.10013550519943237 train acc 0.9797422386784392\n",
            "epoch 36 batch id 3521 loss 0.0668354332447052 train acc 0.979755396194263\n",
            "epoch 36 batch id 3531 loss 0.15254484117031097 train acc 0.9797773293684509\n",
            "epoch 36 batch id 3541 loss 0.043091196566820145 train acc 0.9797506001129624\n",
            "epoch 36 batch id 3551 loss 0.13184486329555511 train acc 0.9797064207265559\n",
            "epoch 36 batch id 3561 loss 0.008065353147685528 train acc 0.9797239188430216\n",
            "epoch 36 batch id 3571 loss 0.01500949077308178 train acc 0.9797369434332119\n",
            "epoch 36 batch id 3581 loss 0.04916282743215561 train acc 0.9797411686679698\n",
            "epoch 36 batch id 3591 loss 0.11527417600154877 train acc 0.9797410192147035\n",
            "epoch 36 batch id 3601 loss 0.10025578737258911 train acc 0.9797321924465426\n",
            "epoch 36 batch id 3611 loss 0.00409617368131876 train acc 0.9797450498476876\n",
            "epoch 36 batch id 3621 loss 0.16360075771808624 train acc 0.979740575807788\n",
            "epoch 36 batch id 3631 loss 0.015533092431724072 train acc 0.9797404296337098\n",
            "epoch 36 batch id 3641 loss 0.01760709472000599 train acc 0.9797617412798681\n",
            "epoch 36 batch id 3651 loss 0.04040590301156044 train acc 0.979748698986579\n",
            "epoch 36 batch id 3661 loss 0.0011599746067076921 train acc 0.9797399959027588\n",
            "epoch 36 batch id 3671 loss 0.13645131886005402 train acc 0.9797483655679652\n",
            "epoch 36 batch id 3681 loss 0.03787732496857643 train acc 0.9797694240695464\n",
            "epoch 36 batch id 3691 loss 0.00619338545948267 train acc 0.9797946017339474\n",
            "epoch 36 batch id 3701 loss 0.06563188135623932 train acc 0.9797689813563901\n",
            "epoch 36 batch id 3711 loss 0.049459319561719894 train acc 0.9797771827000809\n",
            "epoch 36 batch id 3721 loss 0.0628533884882927 train acc 0.9797769416823434\n",
            "epoch 36 batch id 3731 loss 0.02416980266571045 train acc 0.9797808898418654\n",
            "epoch 36 batch id 3741 loss 0.06327170878648758 train acc 0.9797806402031543\n",
            "epoch 36 batch id 3751 loss 0.02922440692782402 train acc 0.9797887230071981\n",
            "epoch 36 batch id 3761 loss 0.15747211873531342 train acc 0.979751063546929\n",
            "epoch 36 batch id 3771 loss 0.137681245803833 train acc 0.9797384645982498\n",
            "epoch 36 batch id 3781 loss 0.23679792881011963 train acc 0.9797217997884158\n",
            "epoch 36 batch id 3791 loss 0.0015129715902730823 train acc 0.9797464389343181\n",
            "epoch 36 batch id 3801 loss 0.078504279255867 train acc 0.9797298408313602\n",
            "epoch 36 batch id 3811 loss 0.016650738194584846 train acc 0.97973792967725\n",
            "epoch 36 batch id 3821 loss 0.058767180889844894 train acc 0.979745976184245\n",
            "epoch 36 batch id 3831 loss 0.11147043853998184 train acc 0.9797417449752023\n",
            "epoch 36 batch id 3841 loss 0.006498309317976236 train acc 0.9797293998958605\n",
            "epoch 36 batch id 3851 loss 0.03247806057333946 train acc 0.9797333484809141\n",
            "epoch 36 batch id 3861 loss 0.024928707629442215 train acc 0.9797210890960891\n",
            "epoch 36 batch id 3871 loss 0.2788403630256653 train acc 0.9797210023249806\n",
            "epoch 36 batch id 3881 loss 0.04249531030654907 train acc 0.9797128639525895\n",
            "epoch 36 batch id 3891 loss 0.009734169580042362 train acc 0.9796806733487535\n",
            "epoch 36 batch id 3901 loss 0.01157464925199747 train acc 0.9796766854652653\n",
            "epoch 36 batch id 3911 loss 0.01895439438521862 train acc 0.9796807082587573\n",
            "epoch 36 batch id 3921 loss 0.01336459256708622 train acc 0.9796847105330273\n",
            "epoch 36 batch id 3931 loss 0.06226632744073868 train acc 0.9796727931823963\n",
            "epoch 36 batch id 3941 loss 0.11831840872764587 train acc 0.9796847246891652\n",
            "epoch 36 batch id 3951 loss 0.10403409600257874 train acc 0.9796570488483928\n",
            "epoch 36 batch id 3961 loss 0.033674467355012894 train acc 0.97966501514769\n",
            "epoch 36 batch id 3971 loss 0.17443490028381348 train acc 0.9796729413246034\n",
            "epoch 36 batch id 3981 loss 0.0011733147548511624 train acc 0.9796651281085155\n",
            "epoch 36 batch id 3991 loss 0.10436411947011948 train acc 0.9796730142821348\n",
            "epoch 36 batch id 4001 loss 0.07395141571760178 train acc 0.9796535241189702\n",
            "epoch 36 batch id 4011 loss 0.06705206632614136 train acc 0.9796380266766392\n",
            "epoch 36 batch id 4021 loss 0.016370899975299835 train acc 0.9796303780154191\n",
            "epoch 36 batch id 4031 loss 0.03978588059544563 train acc 0.9796460245596627\n",
            "epoch 36 batch id 4041 loss 0.036655738949775696 train acc 0.9796383939618907\n",
            "epoch 36 batch id 4051 loss 0.023485884070396423 train acc 0.9796385151814366\n",
            "epoch 36 batch id 4061 loss 0.04110924154520035 train acc 0.9796347882295001\n",
            "epoch 36 batch id 4071 loss 0.04404817894101143 train acc 0.9796387558339474\n",
            "epoch 36 batch id 4081 loss 0.09947775304317474 train acc 0.9796580188679245\n",
            "epoch 36 batch id 4091 loss 0.06873396039009094 train acc 0.9796504522121731\n",
            "epoch 36 batch id 4101 loss 0.017822813242673874 train acc 0.9796619726895879\n",
            "epoch 36 batch id 4111 loss 0.01742856204509735 train acc 0.9796620347847239\n",
            "epoch 36 batch id 4121 loss 0.019943546503782272 train acc 0.9796545134676049\n",
            "epoch 36 batch id 4131 loss 0.09889626502990723 train acc 0.9796432461873639\n",
            "epoch 36 batch id 4141 loss 0.028258152306079865 train acc 0.9796433530548176\n",
            "epoch 36 batch id 4151 loss 0.0512610524892807 train acc 0.9796284027945074\n",
            "epoch 36 batch id 4161 loss 0.02959565632045269 train acc 0.9796135243931747\n",
            "epoch 36 batch id 4171 loss 0.056626059114933014 train acc 0.9796099556461281\n",
            "epoch 36 batch id 4181 loss 0.005778794642537832 train acc 0.9796138782587898\n",
            "epoch 36 batch id 4191 loss 0.160711407661438 train acc 0.9795991410164638\n",
            "epoch 36 batch id 4201 loss 0.055167056620121 train acc 0.979603070697453\n",
            "epoch 36 batch id 4211 loss 0.08038316667079926 train acc 0.9796032711944906\n",
            "epoch 36 batch id 4221 loss 0.08038482815027237 train acc 0.9796034707415304\n",
            "epoch 36 batch id 4231 loss 0.056150369346141815 train acc 0.9795888974237769\n",
            "epoch 36 batch id 4241 loss 0.09968782216310501 train acc 0.979603867012497\n",
            "epoch 36 batch id 4251 loss 0.17109984159469604 train acc 0.979604063749706\n",
            "epoch 36 batch id 4261 loss 0.003594479989260435 train acc 0.9796299284205585\n",
            "epoch 36 batch id 4271 loss 0.024352964013814926 train acc 0.9796337216108639\n",
            "epoch 36 batch id 4281 loss 0.08672600984573364 train acc 0.9796374970801215\n",
            "epoch 36 batch id 4291 loss 0.008272946812212467 train acc 0.9796376136098811\n",
            "epoch 36 batch id 4301 loss 0.12880577147006989 train acc 0.9796122994652406\n",
            "epoch 36 batch id 4311 loss 0.044273342937231064 train acc 0.9796269717003016\n",
            "epoch 36 batch id 4321 loss 0.08743903785943985 train acc 0.9796162635963898\n",
            "epoch 36 batch id 4331 loss 0.11579181998968124 train acc 0.9795983895174325\n",
            "epoch 36 batch id 4341 loss 0.03318946808576584 train acc 0.979591395991707\n",
            "epoch 36 batch id 4351 loss 0.03997131809592247 train acc 0.9795628878418754\n",
            "epoch 36 batch id 4361 loss 0.01915603317320347 train acc 0.9795703393717038\n",
            "epoch 36 batch id 4371 loss 0.02058236300945282 train acc 0.9795813315030886\n",
            "epoch 36 batch id 4381 loss 0.11622092127799988 train acc 0.9795673076923077\n",
            "epoch 36 batch id 4391 loss 0.0021958048455417156 train acc 0.9795604645866545\n",
            "epoch 36 batch id 4401 loss 0.01875351555645466 train acc 0.9795714042263122\n",
            "epoch 36 batch id 4411 loss 0.0014049377059563994 train acc 0.9795681251416912\n",
            "epoch 36 batch id 4421 loss 0.07090625911951065 train acc 0.9795719294277313\n",
            "epoch 36 batch id 4431 loss 0.018650099635124207 train acc 0.9795686639584744\n",
            "epoch 36 batch id 4441 loss 0.049070388078689575 train acc 0.9795618948435038\n",
            "epoch 36 batch id 4451 loss 0.017704762518405914 train acc 0.9795797292743204\n",
            "epoch 36 batch id 4461 loss 0.0006146552041172981 train acc 0.9795834734364492\n",
            "epoch 36 batch id 4471 loss 0.05216662585735321 train acc 0.9795802113621114\n",
            "epoch 36 batch id 4481 loss 0.00983787328004837 train acc 0.9795734769024771\n",
            "epoch 36 batch id 4491 loss 0.0617104135453701 train acc 0.9795737307949232\n",
            "epoch 36 batch id 4501 loss 0.011922962963581085 train acc 0.9795739835592091\n",
            "epoch 36 batch id 4511 loss 0.07567497342824936 train acc 0.9795673076923077\n",
            "epoch 36 batch id 4521 loss 0.012075221166014671 train acc 0.9795710296394603\n",
            "epoch 36 batch id 4531 loss 0.00045039341785013676 train acc 0.9795747351578018\n",
            "epoch 36 batch id 4541 loss 0.06767470389604568 train acc 0.979585306099978\n",
            "epoch 36 batch id 4551 loss 0.05668787285685539 train acc 0.9795923972753241\n",
            "epoch 36 batch id 4561 loss 0.10006202757358551 train acc 0.9795857542205657\n",
            "epoch 36 batch id 4571 loss 0.025993313640356064 train acc 0.9795928133887551\n",
            "epoch 36 batch id 4581 loss 0.031389810144901276 train acc 0.9795759659462999\n",
            "epoch 36 batch id 4591 loss 0.03255948796868324 train acc 0.9795762088869527\n",
            "epoch 36 batch id 4601 loss 0.0252515971660614 train acc 0.9795560747663551\n",
            "epoch 36 batch id 4611 loss 0.05895061045885086 train acc 0.9795597484276729\n",
            "epoch 36 batch id 4621 loss 0.009421280585229397 train acc 0.9795701687946332\n",
            "epoch 36 batch id 4631 loss 0.08484461903572083 train acc 0.9795805441589289\n",
            "epoch 36 batch id 4641 loss 0.07857353985309601 train acc 0.9795976082740788\n",
            "epoch 36 batch id 4651 loss 0.10779545456171036 train acc 0.9796112395183831\n",
            "epoch 36 batch id 4661 loss 0.08247087895870209 train acc 0.9796214599871272\n",
            "epoch 36 batch id 4671 loss 0.004452120047062635 train acc 0.9796282915863841\n",
            "epoch 36 batch id 4681 loss 0.19600968062877655 train acc 0.9796250801110874\n",
            "epoch 36 batch id 4691 loss 0.00585773354396224 train acc 0.9796218823278618\n",
            "epoch 36 batch id 4701 loss 0.0906936526298523 train acc 0.9796186981493299\n",
            "epoch 36 batch id 4711 loss 0.037364840507507324 train acc 0.9796088940776905\n",
            "epoch 36 batch id 4721 loss 0.00230749836191535 train acc 0.9796090605803855\n",
            "epoch 36 batch id 4731 loss 0.026477834209799767 train acc 0.9796059236947792\n",
            "epoch 36 batch id 4741 loss 0.07466050982475281 train acc 0.9796159829149969\n",
            "epoch 36 batch id 4751 loss 0.22219762206077576 train acc 0.9796062671016628\n",
            "epoch 36 batch id 4761 loss 0.07362138479948044 train acc 0.9796064377231674\n",
            "epoch 36 batch id 4771 loss 0.07638908177614212 train acc 0.9796098826241878\n",
            "epoch 36 batch id 4781 loss 0.09871687740087509 train acc 0.9795937042459737\n",
            "epoch 36 batch id 4791 loss 0.2507975101470947 train acc 0.9795710707576706\n",
            "epoch 36 batch id 4801 loss 0.06055739149451256 train acc 0.979568058737763\n",
            "epoch 36 batch id 4811 loss 0.018469376489520073 train acc 0.9795748025358554\n",
            "epoch 36 batch id 4821 loss 0.060943908989429474 train acc 0.9795782773283551\n",
            "epoch 36 batch id 4831 loss 0.02738010324537754 train acc 0.9795946750155248\n",
            "epoch 36 batch id 4841 loss 0.0035588587634265423 train acc 0.9795980944019831\n",
            "epoch 36 batch id 4851 loss 0.1801510453224182 train acc 0.9796079416615131\n",
            "epoch 36 batch id 4861 loss 0.11751116812229156 train acc 0.9795984622505657\n",
            "epoch 36 batch id 4871 loss 0.03607526049017906 train acc 0.9795986450420858\n",
            "epoch 36 batch id 4881 loss 0.04335087910294533 train acc 0.9796148330260193\n",
            "epoch 36 batch id 4891 loss 0.02123871073126793 train acc 0.9796117869556328\n",
            "epoch 36 batch id 4901 loss 0.05370214208960533 train acc 0.9796119414405223\n",
            "epoch 36 batch id 4911 loss 0.13258695602416992 train acc 0.9796152769293422\n",
            "epoch 36 batch id 4921 loss 0.08821103721857071 train acc 0.9796122485267222\n",
            "epoch 36 batch id 4931 loss 0.05485590547323227 train acc 0.9795902200365038\n",
            "epoch 36 batch id 4941 loss 0.04600813239812851 train acc 0.9795872546043312\n",
            "epoch 36 batch id 4951 loss 0.10853826254606247 train acc 0.9795653655827106\n",
            "epoch 36 train acc 0.9795539938929338\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4e6c89312a14a809944f7ac4f4bafc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 36 loss 1.1224424839019775 test acc 0.8478876466275659\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6f4bd90891047e6bfd0c88d1e1d8a4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 37 batch id 1 loss 0.0664721205830574 train acc 0.984375\n",
            "epoch 37 batch id 11 loss 0.01979561150074005 train acc 0.9815340909090909\n",
            "epoch 37 batch id 21 loss 0.05601418763399124 train acc 0.9821428571428571\n",
            "epoch 37 batch id 31 loss 0.10554756969213486 train acc 0.9808467741935484\n",
            "epoch 37 batch id 41 loss 0.1269967406988144 train acc 0.9813262195121951\n",
            "epoch 37 batch id 51 loss 0.07135900855064392 train acc 0.9819240196078431\n",
            "epoch 37 batch id 61 loss 0.05527878552675247 train acc 0.9782274590163934\n",
            "epoch 37 batch id 71 loss 0.10726084560155869 train acc 0.9786531690140845\n",
            "epoch 37 batch id 81 loss 0.11001650989055634 train acc 0.9789737654320988\n",
            "epoch 37 batch id 91 loss 0.05737561360001564 train acc 0.9787087912087912\n",
            "epoch 37 batch id 101 loss 0.02792668342590332 train acc 0.978805693069307\n",
            "epoch 37 batch id 111 loss 0.0004027628747280687 train acc 0.9790259009009009\n",
            "epoch 37 batch id 121 loss 0.05218123644590378 train acc 0.9797262396694215\n",
            "epoch 37 batch id 131 loss 0.018904931843280792 train acc 0.9799618320610687\n",
            "epoch 37 batch id 141 loss 0.10407309979200363 train acc 0.9804964539007093\n",
            "epoch 37 batch id 151 loss 0.056502681225538254 train acc 0.9809602649006622\n",
            "epoch 37 batch id 161 loss 0.038101743906736374 train acc 0.9811723602484472\n",
            "epoch 37 batch id 171 loss 0.008118477649986744 train acc 0.9809027777777778\n",
            "epoch 37 batch id 181 loss 0.017499784007668495 train acc 0.9814399171270718\n",
            "epoch 37 batch id 191 loss 0.09421468526124954 train acc 0.980857329842932\n",
            "epoch 37 batch id 201 loss 0.013271673582494259 train acc 0.9807213930348259\n",
            "epoch 37 batch id 211 loss 0.15403063595294952 train acc 0.981042654028436\n",
            "epoch 37 batch id 221 loss 0.17581436038017273 train acc 0.9805571266968326\n",
            "epoch 37 batch id 231 loss 0.09781651198863983 train acc 0.9805194805194806\n",
            "epoch 37 batch id 241 loss 0.12358242273330688 train acc 0.9806146265560166\n",
            "epoch 37 batch id 251 loss 0.11941064894199371 train acc 0.9805776892430279\n",
            "epoch 37 batch id 261 loss 0.05309581011533737 train acc 0.9807231800766284\n",
            "epoch 37 batch id 271 loss 0.0001849133550422266 train acc 0.9807426199261993\n",
            "epoch 37 batch id 281 loss 0.005339129827916622 train acc 0.9809274911032029\n",
            "epoch 37 batch id 291 loss 0.07517196238040924 train acc 0.9810996563573883\n",
            "epoch 37 batch id 301 loss 0.1028534546494484 train acc 0.9807931893687708\n",
            "epoch 37 batch id 311 loss 0.02011691778898239 train acc 0.9807576366559485\n",
            "epoch 37 batch id 321 loss 0.1516791582107544 train acc 0.9807242990654206\n",
            "epoch 37 batch id 331 loss 0.002666937420144677 train acc 0.9807401812688822\n",
            "epoch 37 batch id 341 loss 0.2236844003200531 train acc 0.9807093108504399\n",
            "epoch 37 batch id 351 loss 0.061285655945539474 train acc 0.9807247150997151\n",
            "epoch 37 batch id 361 loss 0.025932887569069862 train acc 0.980652700831025\n",
            "epoch 37 batch id 371 loss 0.06069270521402359 train acc 0.9807951482479784\n",
            "epoch 37 batch id 381 loss 0.02309446968138218 train acc 0.9808891076115486\n",
            "epoch 37 batch id 391 loss 0.21914100646972656 train acc 0.9809782608695652\n",
            "epoch 37 batch id 401 loss 0.0019446684746071696 train acc 0.9810629675810474\n",
            "epoch 37 batch id 411 loss 0.0030269671697169542 train acc 0.9810675182481752\n",
            "epoch 37 batch id 421 loss 0.010982845909893513 train acc 0.981146080760095\n",
            "epoch 37 batch id 431 loss 0.06192384660243988 train acc 0.9809309744779582\n",
            "epoch 37 batch id 441 loss 0.1262008398771286 train acc 0.9808673469387755\n",
            "epoch 37 batch id 451 loss 0.022281967103481293 train acc 0.9810490576496674\n",
            "epoch 37 batch id 461 loss 0.001785992062650621 train acc 0.9808839479392625\n",
            "epoch 37 batch id 471 loss 0.07957880944013596 train acc 0.9810244161358811\n",
            "epoch 37 batch id 481 loss 0.18275336921215057 train acc 0.9809641372141372\n",
            "epoch 37 batch id 491 loss 0.0059449393302202225 train acc 0.9808426680244399\n",
            "epoch 37 batch id 501 loss 0.08468419313430786 train acc 0.9809443612774451\n",
            "epoch 37 batch id 511 loss 0.08228026330471039 train acc 0.9809197651663405\n",
            "epoch 37 batch id 521 loss 0.11304352432489395 train acc 0.980896113243762\n",
            "epoch 37 batch id 531 loss 0.14133386313915253 train acc 0.9807556497175142\n",
            "epoch 37 batch id 541 loss 0.011681259609758854 train acc 0.9806781423290203\n",
            "epoch 37 batch id 551 loss 0.036981064826250076 train acc 0.9807735934664247\n",
            "epoch 37 batch id 561 loss 0.026775438338518143 train acc 0.9805314171122995\n",
            "epoch 37 batch id 571 loss 0.11305047571659088 train acc 0.9806534588441331\n",
            "epoch 37 batch id 581 loss 0.18186450004577637 train acc 0.9805561531841652\n",
            "epoch 37 batch id 591 loss 0.023663513362407684 train acc 0.9806736463620981\n",
            "epoch 37 batch id 601 loss 0.01813179813325405 train acc 0.9807092346089851\n",
            "epoch 37 batch id 611 loss 0.04521045833826065 train acc 0.9807948036006546\n",
            "epoch 37 batch id 621 loss 0.1715572625398636 train acc 0.9807518115942029\n",
            "epoch 37 batch id 631 loss 0.07571226358413696 train acc 0.9807349445324881\n",
            "epoch 37 batch id 641 loss 0.012241440825164318 train acc 0.9807673556942278\n",
            "epoch 37 batch id 651 loss 0.04404332488775253 train acc 0.9807267665130568\n",
            "epoch 37 batch id 661 loss 0.0005246854270808399 train acc 0.9808055975794251\n",
            "epoch 37 batch id 671 loss 0.018525704741477966 train acc 0.9808587928464978\n",
            "epoch 37 batch id 681 loss 0.062330588698387146 train acc 0.980795704845815\n",
            "epoch 37 batch id 691 loss 0.024516679346561432 train acc 0.9807570549927641\n",
            "epoch 37 batch id 701 loss 0.07947300374507904 train acc 0.9806749286733238\n",
            "epoch 37 batch id 711 loss 0.17778824269771576 train acc 0.9805731364275668\n",
            "epoch 37 batch id 721 loss 0.05532608926296234 train acc 0.9805825242718447\n",
            "epoch 37 batch id 731 loss 0.01621221750974655 train acc 0.9806130300957593\n",
            "epoch 37 batch id 741 loss 0.09652789682149887 train acc 0.9805794534412956\n",
            "epoch 37 batch id 751 loss 0.06360103189945221 train acc 0.9803595206391478\n",
            "epoch 37 batch id 761 loss 0.0067358603700995445 train acc 0.9804328186596584\n",
            "epoch 37 batch id 771 loss 0.08678626269102097 train acc 0.98032182230869\n",
            "epoch 37 batch id 781 loss 0.03621567785739899 train acc 0.9803337067861716\n",
            "epoch 37 batch id 791 loss 0.009283219464123249 train acc 0.9804440581542352\n",
            "epoch 37 batch id 801 loss 0.0028166016563773155 train acc 0.9804151061173533\n",
            "epoch 37 batch id 811 loss 0.23559819161891937 train acc 0.9803290690505548\n",
            "epoch 37 batch id 821 loss 0.044662099331617355 train acc 0.9803022228989038\n",
            "epoch 37 batch id 831 loss 0.1396680772304535 train acc 0.9803512334536703\n",
            "epoch 37 batch id 841 loss 0.0018678311025723815 train acc 0.9803433412604042\n",
            "epoch 37 batch id 851 loss 0.045134782791137695 train acc 0.9803356345475911\n",
            "epoch 37 batch id 861 loss 0.06056544929742813 train acc 0.9803099593495935\n",
            "epoch 37 batch id 871 loss 0.03842325136065483 train acc 0.980302812858783\n",
            "epoch 37 batch id 881 loss 0.02806605026125908 train acc 0.9802426220204313\n",
            "epoch 37 batch id 891 loss 0.002890445990487933 train acc 0.9802714646464646\n",
            "epoch 37 batch id 901 loss 0.0658951997756958 train acc 0.980247641509434\n",
            "epoch 37 batch id 911 loss 0.08635243773460388 train acc 0.9802757958287596\n",
            "epoch 37 batch id 921 loss 0.029615292325615883 train acc 0.9801845819761129\n",
            "epoch 37 batch id 931 loss 0.0016119721112772822 train acc 0.9801456766917294\n",
            "epoch 37 batch id 941 loss 0.03448137640953064 train acc 0.9801906216790648\n",
            "epoch 37 batch id 951 loss 0.0458686426281929 train acc 0.9801689011566772\n",
            "epoch 37 batch id 961 loss 0.15498274564743042 train acc 0.9801313735691988\n",
            "epoch 37 batch id 971 loss 0.009849000722169876 train acc 0.98014289392379\n",
            "epoch 37 batch id 981 loss 0.03360811993479729 train acc 0.9802178899082569\n",
            "epoch 37 batch id 991 loss 0.16775953769683838 train acc 0.9802913723511605\n",
            "epoch 37 batch id 1001 loss 0.07100178301334381 train acc 0.9803633866133866\n",
            "epoch 37 batch id 1011 loss 0.012004060670733452 train acc 0.98037215628091\n",
            "epoch 37 batch id 1021 loss 0.0809992253780365 train acc 0.9804572722820764\n",
            "epoch 37 batch id 1031 loss 0.012680002488195896 train acc 0.9804952715809894\n",
            "epoch 37 batch id 1041 loss 0.051896482706069946 train acc 0.9805475504322767\n",
            "epoch 37 batch id 1051 loss 0.019190317019820213 train acc 0.9804650333016175\n",
            "epoch 37 batch id 1061 loss 0.01653728075325489 train acc 0.9804282516493874\n",
            "epoch 37 batch id 1071 loss 0.0003666783159133047 train acc 0.980406746031746\n",
            "epoch 37 batch id 1081 loss 0.10790108889341354 train acc 0.9803133672525439\n",
            "epoch 37 batch id 1091 loss 0.16911432147026062 train acc 0.9802646654445463\n",
            "epoch 37 batch id 1101 loss 0.06742121279239655 train acc 0.9802736148955495\n",
            "epoch 37 batch id 1111 loss 0.055196817964315414 train acc 0.9801698919891989\n",
            "epoch 37 batch id 1121 loss 0.06364383548498154 train acc 0.9802213425512935\n",
            "epoch 37 batch id 1131 loss 0.028239436447620392 train acc 0.9801613616268788\n",
            "epoch 37 batch id 1141 loss 0.05695454403758049 train acc 0.9801161262050833\n",
            "epoch 37 batch id 1151 loss 0.04044893756508827 train acc 0.9800716768027802\n",
            "epoch 37 batch id 1161 loss 0.047822847962379456 train acc 0.9800549095607235\n",
            "epoch 37 batch id 1171 loss 0.04467985779047012 train acc 0.9799850555081128\n",
            "epoch 37 batch id 1181 loss 0.0007938400958664715 train acc 0.9799957662997459\n",
            "epoch 37 batch id 1191 loss 0.01533759105950594 train acc 0.9800587741393787\n",
            "epoch 37 batch id 1201 loss 0.008552590385079384 train acc 0.9801337427144047\n",
            "epoch 37 batch id 1211 loss 0.0003533389826770872 train acc 0.9801687654830719\n",
            "epoch 37 batch id 1221 loss 0.07524725049734116 train acc 0.9801776208026208\n",
            "epoch 37 batch id 1231 loss 0.07955002039670944 train acc 0.9801228675873274\n",
            "epoch 37 batch id 1241 loss 0.006424780935049057 train acc 0.9801319500402901\n",
            "epoch 37 batch id 1251 loss 0.09956568479537964 train acc 0.9801408872901679\n",
            "epoch 37 batch id 1261 loss 0.021820439025759697 train acc 0.9801620737509913\n",
            "epoch 37 batch id 1271 loss 0.026353666558861732 train acc 0.9801706333595595\n",
            "epoch 37 batch id 1281 loss 0.0028842384926974773 train acc 0.9802522443403591\n",
            "epoch 37 batch id 1291 loss 0.08129968494176865 train acc 0.9802720759101472\n",
            "epoch 37 batch id 1301 loss 0.0013727392069995403 train acc 0.9802675826287471\n",
            "epoch 37 batch id 1311 loss 0.04080435261130333 train acc 0.980251239511823\n",
            "epoch 37 batch id 1321 loss 0.03332012891769409 train acc 0.980199659348978\n",
            "epoch 37 batch id 1331 loss 0.01667434349656105 train acc 0.9801840721262209\n",
            "epoch 37 batch id 1341 loss 0.10891406238079071 train acc 0.9801337621178225\n",
            "epoch 37 batch id 1351 loss 0.03560534492135048 train acc 0.980130458919319\n",
            "epoch 37 batch id 1361 loss 0.0010386018548160791 train acc 0.9800698016164585\n",
            "epoch 37 batch id 1371 loss 0.005170287564396858 train acc 0.9800442195477753\n",
            "epoch 37 batch id 1381 loss 0.051278650760650635 train acc 0.98009820782042\n",
            "epoch 37 batch id 1391 loss 0.09453539550304413 train acc 0.9800278576563624\n",
            "epoch 37 batch id 1401 loss 0.013836334459483624 train acc 0.9800477337615988\n",
            "epoch 37 batch id 1411 loss 0.016260884702205658 train acc 0.9801337703756201\n",
            "epoch 37 batch id 1421 loss 0.00785205326974392 train acc 0.9801306298381421\n",
            "epoch 37 batch id 1431 loss 0.10861501097679138 train acc 0.9800729385045422\n",
            "epoch 37 batch id 1441 loss 0.1792972981929779 train acc 0.9799943615544761\n",
            "epoch 37 batch id 1451 loss 0.0087892459705472 train acc 0.9800245520330806\n",
            "epoch 37 batch id 1461 loss 0.013319107703864574 train acc 0.9800329397672827\n",
            "epoch 37 batch id 1471 loss 0.04132002592086792 train acc 0.980094323589395\n",
            "epoch 37 batch id 1481 loss 0.0907292366027832 train acc 0.9800599257258609\n",
            "epoch 37 batch id 1491 loss 0.033004239201545715 train acc 0.980067907444668\n",
            "epoch 37 batch id 1501 loss 0.05572298914194107 train acc 0.9800966022651566\n",
            "epoch 37 batch id 1511 loss 0.014985041692852974 train acc 0.9801145764394441\n",
            "epoch 37 batch id 1521 loss 0.0034713135100901127 train acc 0.9801631328073636\n",
            "epoch 37 batch id 1531 loss 0.09574069827795029 train acc 0.9801702318745917\n",
            "epoch 37 batch id 1541 loss 0.009862098842859268 train acc 0.9801569597663855\n",
            "epoch 37 batch id 1551 loss 0.026724234223365784 train acc 0.9801841553836235\n",
            "epoch 37 batch id 1561 loss 0.07487466931343079 train acc 0.9801809737347854\n",
            "epoch 37 batch id 1571 loss 0.008856282569468021 train acc 0.9801181572246976\n",
            "epoch 37 batch id 1581 loss 0.033057332038879395 train acc 0.98010555028463\n",
            "epoch 37 batch id 1591 loss 0.07587354630231857 train acc 0.980142206159648\n",
            "epoch 37 batch id 1601 loss 0.03193485736846924 train acc 0.9801979231730169\n",
            "epoch 37 batch id 1611 loss 0.08021814376115799 train acc 0.9801462600869025\n",
            "epoch 37 batch id 1621 loss 0.030119752511382103 train acc 0.9801723473164713\n",
            "epoch 37 batch id 1631 loss 0.0006856769905425608 train acc 0.9801981146535867\n",
            "epoch 37 batch id 1641 loss 0.15655136108398438 train acc 0.98014739488117\n",
            "epoch 37 batch id 1651 loss 0.08611741662025452 train acc 0.9801067534827377\n",
            "epoch 37 batch id 1661 loss 0.10651261359453201 train acc 0.9800948223961469\n",
            "epoch 37 batch id 1671 loss 0.044029027223587036 train acc 0.9800830341113106\n",
            "epoch 37 batch id 1681 loss 0.007441334426403046 train acc 0.9800527959547888\n",
            "epoch 37 batch id 1691 loss 0.024734128266572952 train acc 0.9800691159077469\n",
            "epoch 37 batch id 1701 loss 0.04569438099861145 train acc 0.9800117577895355\n",
            "epoch 37 batch id 1711 loss 0.0775163471698761 train acc 0.980009862653419\n",
            "epoch 37 batch id 1721 loss 0.009254704229533672 train acc 0.9800079895409646\n",
            "epoch 37 batch id 1731 loss 0.00013373597175814211 train acc 0.980015164644714\n",
            "epoch 37 batch id 1741 loss 0.13839790225028992 train acc 0.9800043078690408\n",
            "epoch 37 batch id 1751 loss 0.04164353758096695 train acc 0.9800292689891491\n",
            "epoch 37 batch id 1761 loss 0.021081408485770226 train acc 0.9800805650198751\n",
            "epoch 37 batch id 1771 loss 0.0005937150563113391 train acc 0.9801312817617166\n",
            "epoch 37 batch id 1781 loss 0.0003414601378608495 train acc 0.9801638826501965\n",
            "epoch 37 batch id 1791 loss 0.03879152610898018 train acc 0.9801524986041318\n",
            "epoch 37 batch id 1801 loss 0.02425961196422577 train acc 0.9801585924486397\n",
            "epoch 37 batch id 1811 loss 0.03456989303231239 train acc 0.9801559911651021\n",
            "epoch 37 batch id 1821 loss 0.08617070317268372 train acc 0.9801190966501923\n",
            "epoch 37 batch id 1831 loss 0.01481591910123825 train acc 0.9801082058984162\n",
            "epoch 37 batch id 1841 loss 0.034712761640548706 train acc 0.980097433460076\n",
            "epoch 37 batch id 1851 loss 0.020182296633720398 train acc 0.9801121015667207\n",
            "epoch 37 batch id 1861 loss 0.024256732314825058 train acc 0.9800930279419667\n",
            "epoch 37 batch id 1871 loss 0.2838764190673828 train acc 0.9801159139497595\n",
            "epoch 37 batch id 1881 loss 0.017320949584245682 train acc 0.9801219431153642\n",
            "epoch 37 batch id 1891 loss 0.0255277156829834 train acc 0.9801526969857218\n",
            "epoch 37 batch id 1901 loss 0.0897127017378807 train acc 0.9801749079431878\n",
            "epoch 37 batch id 1911 loss 0.11207408457994461 train acc 0.9800987702773417\n",
            "epoch 37 batch id 1921 loss 0.0005882119294255972 train acc 0.9800640942217596\n",
            "epoch 37 batch id 1931 loss 0.026303188875317574 train acc 0.9800702356292077\n",
            "epoch 37 batch id 1941 loss 0.04243366792798042 train acc 0.980076313755796\n",
            "epoch 37 batch id 1951 loss 0.09963846206665039 train acc 0.9800583034341364\n",
            "epoch 37 batch id 1961 loss 0.05182952806353569 train acc 0.9800962519122897\n",
            "epoch 37 batch id 1971 loss 0.01119034644216299 train acc 0.9801338153221715\n",
            "epoch 37 batch id 1981 loss 0.048302266746759415 train acc 0.980100012619889\n",
            "epoch 37 batch id 1991 loss 0.12508605420589447 train acc 0.9800900929181315\n",
            "epoch 37 batch id 2001 loss 0.033000607043504715 train acc 0.9800958895552224\n",
            "epoch 37 batch id 2011 loss 0.11581607908010483 train acc 0.9801093983092989\n",
            "epoch 37 batch id 2021 loss 0.07947796583175659 train acc 0.9801073107372588\n",
            "epoch 37 batch id 2031 loss 0.0012674846220761538 train acc 0.9801052437223042\n",
            "epoch 37 batch id 2041 loss 0.007898002862930298 train acc 0.9801108525232729\n",
            "epoch 37 batch id 2051 loss 0.19982407987117767 train acc 0.9800935519258898\n",
            "epoch 37 batch id 2061 loss 0.003077021101489663 train acc 0.9801143255701116\n",
            "epoch 37 batch id 2071 loss 0.06449216604232788 train acc 0.9801198092708836\n",
            "epoch 37 batch id 2081 loss 0.041134677827358246 train acc 0.9801177318596829\n",
            "epoch 37 batch id 2091 loss 0.028164580464363098 train acc 0.9801007293161167\n",
            "epoch 37 batch id 2101 loss 0.028130099177360535 train acc 0.9801656949071871\n",
            "epoch 37 batch id 2111 loss 0.05847737193107605 train acc 0.9801708313595452\n",
            "epoch 37 batch id 2121 loss 0.03982090577483177 train acc 0.9801980198019802\n",
            "epoch 37 batch id 2131 loss 0.01238200906664133 train acc 0.9802396175504458\n",
            "epoch 37 batch id 2141 loss 0.03732248395681381 train acc 0.9802589327417095\n",
            "epoch 37 batch id 2151 loss 0.009935038164258003 train acc 0.9803071245932125\n",
            "epoch 37 batch id 2161 loss 0.17660802602767944 train acc 0.9802753354928274\n",
            "epoch 37 batch id 2171 loss 0.029311122372746468 train acc 0.9802798249654537\n",
            "epoch 37 batch id 2181 loss 0.10477063804864883 train acc 0.9802627808344796\n",
            "epoch 37 batch id 2191 loss 0.08390823006629944 train acc 0.9802886809675947\n",
            "epoch 37 batch id 2201 loss 0.00664353184401989 train acc 0.9803001476601545\n",
            "epoch 37 batch id 2211 loss 0.001187062356621027 train acc 0.9803115106286748\n",
            "epoch 37 batch id 2221 loss 0.05530165135860443 train acc 0.9803016659162539\n",
            "epoch 37 batch id 2231 loss 0.00873546116054058 train acc 0.9802919094576423\n",
            "epoch 37 batch id 2241 loss 0.05017659440636635 train acc 0.9803240740740741\n",
            "epoch 37 batch id 2251 loss 0.005060835741460323 train acc 0.9803420701910263\n",
            "epoch 37 batch id 2261 loss 0.00745196221396327 train acc 0.9803391751437417\n",
            "epoch 37 batch id 2271 loss 0.01123880222439766 train acc 0.9803707067371202\n",
            "epoch 37 batch id 2281 loss 0.02996138483285904 train acc 0.9803882617273126\n",
            "epoch 37 batch id 2291 loss 0.018059322610497475 train acc 0.9803715626364033\n",
            "epoch 37 batch id 2301 loss 0.013962819240987301 train acc 0.9803821707953064\n",
            "epoch 37 batch id 2311 loss 0.053443919867277145 train acc 0.9803791648636954\n",
            "epoch 37 batch id 2321 loss 0.13124388456344604 train acc 0.9803290607496769\n",
            "epoch 37 batch id 2331 loss 0.14144323766231537 train acc 0.9802927927927928\n",
            "epoch 37 batch id 2341 loss 0.002775519620627165 train acc 0.9803169051687313\n",
            "epoch 37 batch id 2351 loss 0.13647957146167755 train acc 0.9803075818800511\n",
            "epoch 37 batch id 2361 loss 0.11584500968456268 train acc 0.9802983375688268\n",
            "epoch 37 batch id 2371 loss 0.001846556318923831 train acc 0.9803221214677351\n",
            "epoch 37 batch id 2381 loss 0.003582688281312585 train acc 0.9803653926921462\n",
            "epoch 37 batch id 2391 loss 0.02472407929599285 train acc 0.9803821622751987\n",
            "epoch 37 batch id 2401 loss 0.02217678911983967 train acc 0.9803792690545606\n",
            "epoch 37 batch id 2411 loss 0.026721589267253876 train acc 0.9803699191206968\n",
            "epoch 37 batch id 2421 loss 0.020370662212371826 train acc 0.980405824039653\n",
            "epoch 37 batch id 2431 loss 0.05536659434437752 train acc 0.9804092965857671\n",
            "epoch 37 batch id 2441 loss 0.00045873710769228637 train acc 0.9804127406800491\n",
            "epoch 37 batch id 2451 loss 0.00020084838615730405 train acc 0.9804289065687475\n",
            "epoch 37 batch id 2461 loss 0.10593903064727783 train acc 0.9804322429906542\n",
            "epoch 37 batch id 2471 loss 0.10590516775846481 train acc 0.9804165823553217\n",
            "epoch 37 batch id 2481 loss 0.01574941724538803 train acc 0.980457728738412\n",
            "epoch 37 batch id 2491 loss 0.09583134204149246 train acc 0.9804546366920915\n",
            "epoch 37 batch id 2501 loss 0.0008320237975567579 train acc 0.9804328268692523\n",
            "epoch 37 batch id 2511 loss 0.014735452830791473 train acc 0.980423636001593\n",
            "epoch 37 batch id 2521 loss 0.020782992243766785 train acc 0.9803587366124554\n",
            "epoch 37 batch id 2531 loss 0.029542041942477226 train acc 0.9803684314500197\n",
            "epoch 37 batch id 2541 loss 0.025193285197019577 train acc 0.9803288567493113\n",
            "epoch 37 batch id 2551 loss 0.025258449837565422 train acc 0.9803140925127402\n",
            "epoch 37 batch id 2561 loss 0.07241838425397873 train acc 0.980305544709098\n",
            "epoch 37 batch id 2571 loss 0.09190790355205536 train acc 0.9803092182030338\n",
            "epoch 37 batch id 2581 loss 0.03499995917081833 train acc 0.9803128632313057\n",
            "epoch 37 batch id 2591 loss 0.0341871976852417 train acc 0.9803104496333462\n",
            "epoch 37 batch id 2601 loss 0.06935174018144608 train acc 0.9803621203383314\n",
            "epoch 37 batch id 2611 loss 0.03094722516834736 train acc 0.9803715051704328\n",
            "epoch 37 batch id 2621 loss 0.05379696562886238 train acc 0.9803748569248378\n",
            "epoch 37 batch id 2631 loss 0.13054604828357697 train acc 0.9803187951349297\n",
            "epoch 37 batch id 2641 loss 0.09387074410915375 train acc 0.9802749905338887\n",
            "epoch 37 batch id 2651 loss 0.006104632280766964 train acc 0.980284562429272\n",
            "epoch 37 batch id 2661 loss 0.005813405383378267 train acc 0.9802705749718151\n",
            "epoch 37 batch id 2671 loss 0.03740397095680237 train acc 0.9802625421190565\n",
            "epoch 37 batch id 2681 loss 0.01939232647418976 train acc 0.9802429130921299\n",
            "epoch 37 batch id 2691 loss 0.054552651941776276 train acc 0.9802234299516909\n",
            "epoch 37 batch id 2701 loss 0.03283166140317917 train acc 0.9802445853387635\n",
            "epoch 37 batch id 2711 loss 0.07408282160758972 train acc 0.980242530431575\n",
            "epoch 37 batch id 2721 loss 0.05884517729282379 train acc 0.9802290058801911\n",
            "epoch 37 batch id 2731 loss 0.013851635158061981 train acc 0.9802155803734895\n",
            "epoch 37 batch id 2741 loss 0.1427384912967682 train acc 0.9802136537759941\n",
            "epoch 37 batch id 2751 loss 0.08162502944469452 train acc 0.9802060614322065\n",
            "epoch 37 batch id 2761 loss 0.08821054548025131 train acc 0.9802381383556682\n",
            "epoch 37 batch id 2771 loss 0.007346183527261019 train acc 0.9802530674846626\n",
            "epoch 37 batch id 2781 loss 0.056081097573041916 train acc 0.9802847446961525\n",
            "epoch 37 batch id 2791 loss 0.01857755146920681 train acc 0.9802993998566822\n",
            "epoch 37 batch id 2801 loss 0.07984303683042526 train acc 0.9803027936451267\n",
            "epoch 37 batch id 2811 loss 0.01654686965048313 train acc 0.9802950462468872\n",
            "epoch 37 batch id 2821 loss 0.118065245449543 train acc 0.9803095090393478\n",
            "epoch 37 batch id 2831 loss 0.011992470361292362 train acc 0.9803238696573648\n",
            "epoch 37 batch id 2841 loss 0.06962721049785614 train acc 0.9803436290038718\n",
            "epoch 37 batch id 2851 loss 0.0016625069547444582 train acc 0.9803303665380568\n",
            "epoch 37 batch id 2861 loss 0.07008746266365051 train acc 0.9803171967843411\n",
            "epoch 37 batch id 2871 loss 0.16371487081050873 train acc 0.9803204458376872\n",
            "epoch 37 batch id 2881 loss 0.04102712124586105 train acc 0.9803019784796946\n",
            "epoch 37 batch id 2891 loss 0.000510577519889921 train acc 0.9803052576962988\n",
            "epoch 37 batch id 2901 loss 0.010528932325541973 train acc 0.980324672526715\n",
            "epoch 37 batch id 2911 loss 0.02132599987089634 train acc 0.9803063809687392\n",
            "epoch 37 batch id 2921 loss 0.0010280921123921871 train acc 0.9803042622389593\n",
            "epoch 37 batch id 2931 loss 0.011552544310688972 train acc 0.9803021579665643\n",
            "epoch 37 batch id 2941 loss 0.0007527777925133705 train acc 0.9802841295477729\n",
            "epoch 37 batch id 2951 loss 0.04109008610248566 train acc 0.9803138766519823\n",
            "epoch 37 batch id 2961 loss 0.025505008175969124 train acc 0.9803117612293144\n",
            "epoch 37 batch id 2971 loss 0.02744666486978531 train acc 0.9803201783911141\n",
            "epoch 37 batch id 2981 loss 0.012302562594413757 train acc 0.9803075729620933\n",
            "epoch 37 batch id 2991 loss 0.017408257350325584 train acc 0.98032117184888\n",
            "epoch 37 batch id 3001 loss 0.0008530872291885316 train acc 0.9803450933022326\n",
            "epoch 37 batch id 3011 loss 0.005619140807539225 train acc 0.9803429093324477\n",
            "epoch 37 batch id 3021 loss 0.02833736315369606 train acc 0.9803355676928169\n",
            "epoch 37 batch id 3031 loss 0.24588680267333984 train acc 0.9803076542395249\n",
            "epoch 37 batch id 3041 loss 0.08626651018857956 train acc 0.9803004768168365\n",
            "epoch 37 batch id 3051 loss 0.06934041529893875 train acc 0.9803189528023599\n",
            "epoch 37 batch id 3061 loss 0.07113072276115417 train acc 0.9803219944462593\n",
            "epoch 37 batch id 3071 loss 0.0006485700141638517 train acc 0.9803453679583197\n",
            "epoch 37 batch id 3081 loss 0.021808966994285583 train acc 0.9803330899058748\n",
            "epoch 37 batch id 3091 loss 0.01370926108211279 train acc 0.980336056292462\n",
            "epoch 37 batch id 3101 loss 0.10081077367067337 train acc 0.9803238874556595\n",
            "epoch 37 batch id 3111 loss 0.0725981593132019 train acc 0.9803117968498875\n",
            "epoch 37 batch id 3121 loss 0.03314027562737465 train acc 0.9803248157641782\n",
            "epoch 37 batch id 3131 loss 0.003633133601397276 train acc 0.9803577131906739\n",
            "epoch 37 batch id 3141 loss 0.06303738802671432 train acc 0.9803356813116841\n",
            "epoch 37 batch id 3151 loss 0.0027526712510734797 train acc 0.9803385829895271\n",
            "epoch 37 batch id 3161 loss 0.08755837380886078 train acc 0.9803068649161658\n",
            "epoch 37 batch id 3171 loss 0.014898056164383888 train acc 0.9802999842321034\n",
            "epoch 37 batch id 3181 loss 0.03249318152666092 train acc 0.9802931468091794\n",
            "epoch 37 batch id 3191 loss 0.01967683620750904 train acc 0.980281455656534\n",
            "epoch 37 batch id 3201 loss 0.07121342420578003 train acc 0.9802600749765699\n",
            "epoch 37 batch id 3211 loss 0.2727372646331787 train acc 0.9802193631267518\n",
            "epoch 37 batch id 3221 loss 0.27172139286994934 train acc 0.9801400962434027\n",
            "epoch 37 batch id 3231 loss 0.05437689274549484 train acc 0.9801338594862272\n",
            "epoch 37 batch id 3241 loss 0.012463336810469627 train acc 0.9801276612156742\n",
            "epoch 37 batch id 3251 loss 0.03275712952017784 train acc 0.9801118886496463\n",
            "epoch 37 batch id 3261 loss 0.09562128782272339 train acc 0.980096212818154\n",
            "epoch 37 batch id 3271 loss 0.023507528007030487 train acc 0.9800710791806787\n",
            "epoch 37 batch id 3281 loss 0.054638102650642395 train acc 0.9800699100883877\n",
            "epoch 37 batch id 3291 loss 0.024645091965794563 train acc 0.9800687481008812\n",
            "epoch 37 batch id 3301 loss 0.010953409597277641 train acc 0.9800817933959406\n",
            "epoch 37 batch id 3311 loss 0.01674281433224678 train acc 0.9800947598912715\n",
            "epoch 37 batch id 3321 loss 0.10066379606723785 train acc 0.9801123532068654\n",
            "epoch 37 batch id 3331 loss 0.038017638027668 train acc 0.9800735514860403\n",
            "epoch 37 batch id 3341 loss 0.12254227697849274 train acc 0.9800911029631847\n",
            "epoch 37 batch id 3351 loss 0.014580276794731617 train acc 0.9800619218143838\n",
            "epoch 37 batch id 3361 loss 0.04372171312570572 train acc 0.98007475453734\n",
            "epoch 37 batch id 3371 loss 0.12070292979478836 train acc 0.9800828760011866\n",
            "epoch 37 batch id 3381 loss 0.0004692736838478595 train acc 0.9800909494232476\n",
            "epoch 37 batch id 3391 loss 0.07628822326660156 train acc 0.980089759657918\n",
            "epoch 37 batch id 3401 loss 0.057750094681978226 train acc 0.9800701999411938\n",
            "epoch 37 batch id 3411 loss 0.017305759713053703 train acc 0.9800782395192026\n",
            "epoch 37 batch id 3421 loss 0.01916833408176899 train acc 0.9800770973399591\n",
            "epoch 37 batch id 3431 loss 0.17884594202041626 train acc 0.9800531914893617\n",
            "epoch 37 batch id 3441 loss 0.024210987612605095 train acc 0.9800657512351061\n",
            "epoch 37 batch id 3451 loss 0.01555323414504528 train acc 0.9800737105186902\n",
            "epoch 37 batch id 3461 loss 0.011846079491078854 train acc 0.9800725946258307\n",
            "epoch 37 batch id 3471 loss 0.026078205555677414 train acc 0.9800669835782195\n",
            "epoch 37 batch id 3481 loss 0.09476767480373383 train acc 0.9800479388106866\n",
            "epoch 37 batch id 3491 loss 0.001173035241663456 train acc 0.9800603337152678\n",
            "epoch 37 batch id 3501 loss 0.10189015418291092 train acc 0.9800771208226221\n",
            "epoch 37 batch id 3511 loss 0.04256607964634895 train acc 0.9800671105098263\n",
            "epoch 37 batch id 3521 loss 0.09897776693105698 train acc 0.980074907696677\n",
            "epoch 37 batch id 3531 loss 0.09951389580965042 train acc 0.9800826607193429\n",
            "epoch 37 batch id 3541 loss 0.046534836292266846 train acc 0.9800771321660547\n",
            "epoch 37 batch id 3551 loss 0.01877373829483986 train acc 0.980058434243875\n",
            "epoch 37 batch id 3561 loss 0.02675754763185978 train acc 0.9800617803987643\n",
            "epoch 37 batch id 3571 loss 0.02801879681646824 train acc 0.9800782343881266\n",
            "epoch 37 batch id 3581 loss 0.13004054129123688 train acc 0.9800858698687518\n",
            "epoch 37 batch id 3591 loss 0.1336159110069275 train acc 0.9800847605123921\n",
            "epoch 37 batch id 3601 loss 0.032982222735881805 train acc 0.980079318244932\n",
            "epoch 37 batch id 3611 loss 0.0014195626135915518 train acc 0.9800825602326225\n",
            "epoch 37 batch id 3621 loss 0.06964094936847687 train acc 0.9800814692074012\n",
            "epoch 37 batch id 3631 loss 0.01387463416904211 train acc 0.9800846874139355\n",
            "epoch 37 batch id 3641 loss 0.01657339558005333 train acc 0.9801093449601758\n",
            "epoch 37 batch id 3651 loss 0.08557837456464767 train acc 0.9800782319912352\n",
            "epoch 37 batch id 3661 loss 0.05340332165360451 train acc 0.9800600928708003\n",
            "epoch 37 batch id 3671 loss 0.04354127123951912 train acc 0.9800675905747752\n",
            "epoch 37 batch id 3681 loss 0.2752538025379181 train acc 0.9800750475414289\n",
            "epoch 37 batch id 3691 loss 0.0667167529463768 train acc 0.9800824641018694\n",
            "epoch 37 batch id 3701 loss 0.04534195363521576 train acc 0.9800771750878141\n",
            "epoch 37 batch id 3711 loss 0.004761047195643187 train acc 0.9800887563998922\n",
            "epoch 37 batch id 3721 loss 0.0696532279253006 train acc 0.9800792797635044\n",
            "epoch 37 batch id 3731 loss 0.0382733978331089 train acc 0.9800656660412758\n",
            "epoch 37 batch id 3741 loss 0.064588762819767 train acc 0.9800479484095161\n",
            "epoch 37 batch id 3751 loss 0.10122247040271759 train acc 0.9800553185817116\n",
            "epoch 37 batch id 3761 loss 0.16542695462703705 train acc 0.980025259239564\n",
            "epoch 37 batch id 3771 loss 0.048065342009067535 train acc 0.980007789710952\n",
            "epoch 37 batch id 3781 loss 0.1058201938867569 train acc 0.9799904125892621\n",
            "epoch 37 batch id 3791 loss 0.00014024921983946115 train acc 0.9799937351622263\n",
            "epoch 37 batch id 3801 loss 0.07740658521652222 train acc 0.9799929294922389\n",
            "epoch 37 batch id 3811 loss 0.013566656969487667 train acc 0.9800003279979008\n",
            "epoch 37 batch id 3821 loss 0.057343948632478714 train acc 0.9800158662653755\n",
            "epoch 37 batch id 3831 loss 0.00040321567212231457 train acc 0.9800190877055599\n",
            "epoch 37 batch id 3841 loss 0.0011165885953232646 train acc 0.9799897487633429\n",
            "epoch 37 batch id 3851 loss 0.001191321644000709 train acc 0.9799970786808622\n",
            "epoch 37 batch id 3861 loss 0.041891589760780334 train acc 0.9800084175084175\n",
            "epoch 37 batch id 3871 loss 0.11598847061395645 train acc 0.9800156613278223\n",
            "epoch 37 batch id 3881 loss 0.08784331381320953 train acc 0.9800268938417933\n",
            "epoch 37 batch id 3891 loss 0.006991136819124222 train acc 0.9800340529426883\n",
            "epoch 37 batch id 3901 loss 0.007066358812153339 train acc 0.9800331645731863\n",
            "epoch 37 batch id 3911 loss 0.06635759025812149 train acc 0.9800322807466121\n",
            "epoch 37 batch id 3921 loss 0.013707301579415798 train acc 0.9800393713338434\n",
            "epoch 37 batch id 3931 loss 0.046464547514915466 train acc 0.9800225769524294\n",
            "epoch 37 batch id 3941 loss 0.013020521961152554 train acc 0.9800455150976909\n",
            "epoch 37 batch id 3951 loss 0.021872084587812424 train acc 0.9800525183497849\n",
            "epoch 37 batch id 3961 loss 0.002592329867184162 train acc 0.9800634309517798\n",
            "epoch 37 batch id 3971 loss 0.15037497878074646 train acc 0.9800585494837573\n",
            "epoch 37 batch id 3981 loss 0.00864983256906271 train acc 0.9800536925395629\n",
            "epoch 37 batch id 3991 loss 0.2092435657978058 train acc 0.9800488599348535\n",
            "epoch 37 batch id 4001 loss 0.07713834941387177 train acc 0.9800518620344914\n",
            "epoch 37 batch id 4011 loss 0.013174375519156456 train acc 0.9800081027175268\n",
            "epoch 37 batch id 4021 loss 0.03250221908092499 train acc 0.9800267346431236\n",
            "epoch 37 batch id 4031 loss 0.008773467503488064 train acc 0.9800646551724138\n",
            "epoch 37 batch id 4041 loss 0.03129967674612999 train acc 0.9800714550853749\n",
            "epoch 37 batch id 4051 loss 0.04094235599040985 train acc 0.9800666502098248\n",
            "epoch 37 batch id 4061 loss 0.028712967410683632 train acc 0.9800657165722728\n",
            "epoch 37 batch id 4071 loss 0.11230219900608063 train acc 0.980072463768116\n",
            "epoch 37 batch id 4081 loss 0.022358303889632225 train acc 0.9800906640529282\n",
            "epoch 37 batch id 4091 loss 0.03521758317947388 train acc 0.9800858592031289\n",
            "epoch 37 batch id 4101 loss 0.06588435918092728 train acc 0.9800963179712265\n",
            "epoch 37 batch id 4111 loss 0.026340186595916748 train acc 0.9800877219654586\n",
            "epoch 37 batch id 4121 loss 0.04171408340334892 train acc 0.9800867507886435\n",
            "epoch 37 batch id 4131 loss 0.07385954260826111 train acc 0.9800820019365771\n",
            "epoch 37 batch id 4141 loss 0.07497070729732513 train acc 0.9800961422361748\n",
            "epoch 37 batch id 4151 loss 0.049838144332170486 train acc 0.9800989219465189\n",
            "epoch 37 batch id 4161 loss 0.13477256894111633 train acc 0.9800716474405191\n",
            "epoch 37 batch id 4171 loss 0.15048088133335114 train acc 0.9800594881323423\n",
            "epoch 37 batch id 4181 loss 0.010399263352155685 train acc 0.9800399127003109\n",
            "epoch 37 batch id 4191 loss 0.14180266857147217 train acc 0.9800167024576474\n",
            "epoch 37 batch id 4201 loss 0.03172953799366951 train acc 0.980038234944061\n",
            "epoch 37 batch id 4211 loss 0.06827471405267715 train acc 0.9800374020422702\n",
            "epoch 37 batch id 4221 loss 0.08970958739519119 train acc 0.9800476782752903\n",
            "epoch 37 batch id 4231 loss 0.0400124192237854 train acc 0.980050519971638\n",
            "epoch 37 batch id 4241 loss 0.11648135632276535 train acc 0.9800607168120726\n",
            "epoch 37 batch id 4251 loss 0.13592422008514404 train acc 0.980067190072924\n",
            "epoch 37 batch id 4261 loss 0.009020156227052212 train acc 0.9800956348275053\n",
            "epoch 37 batch id 4271 loss 0.010972769930958748 train acc 0.9801166295949426\n",
            "epoch 37 batch id 4281 loss 0.12035245448350906 train acc 0.9801302265825742\n",
            "epoch 37 batch id 4291 loss 0.010323891416192055 train acc 0.980114629457003\n",
            "epoch 37 batch id 4301 loss 0.04439139366149902 train acc 0.9800918391071843\n",
            "epoch 37 batch id 4311 loss 0.042031534016132355 train acc 0.9801017745302714\n",
            "epoch 37 batch id 4321 loss 0.019503692165017128 train acc 0.9800935836611896\n",
            "epoch 37 batch id 4331 loss 0.09416195750236511 train acc 0.980081822904641\n",
            "epoch 37 batch id 4341 loss 0.05123298615217209 train acc 0.9800773151347616\n",
            "epoch 37 batch id 4351 loss 0.039779335260391235 train acc 0.9800476901861641\n",
            "epoch 37 batch id 4361 loss 0.033215709030628204 train acc 0.9800576129328136\n",
            "epoch 37 batch id 4371 loss 0.13676753640174866 train acc 0.9800674902768245\n",
            "epoch 37 batch id 4381 loss 0.10452240705490112 train acc 0.9800559233051814\n",
            "epoch 37 batch id 4391 loss 0.0004422116617206484 train acc 0.9800444090184468\n",
            "epoch 37 batch id 4401 loss 0.019292552024126053 train acc 0.9800435980458987\n",
            "epoch 37 batch id 4411 loss 0.0003834953822661191 train acc 0.9800392484697348\n",
            "epoch 37 batch id 4421 loss 0.20019708573818207 train acc 0.9800525899117847\n",
            "epoch 37 batch id 4431 loss 0.011465740390121937 train acc 0.9800517659670503\n",
            "epoch 37 batch id 4441 loss 0.012559403665363789 train acc 0.9800650191398333\n",
            "epoch 37 batch id 4451 loss 0.0014190010260790586 train acc 0.9800711918669962\n",
            "epoch 37 batch id 4461 loss 0.0018837398383766413 train acc 0.9800668291862811\n",
            "epoch 37 batch id 4471 loss 0.06025262922048569 train acc 0.9800624860210244\n",
            "epoch 37 batch id 4481 loss 0.009117758832871914 train acc 0.9800442144610578\n",
            "epoch 37 batch id 4491 loss 0.05393755063414574 train acc 0.9800364618125139\n",
            "epoch 37 batch id 4501 loss 0.00842390675097704 train acc 0.9800322150633193\n",
            "epoch 37 batch id 4511 loss 0.08311983197927475 train acc 0.9800210596320107\n",
            "epoch 37 batch id 4521 loss 0.07909290492534637 train acc 0.9800376023003761\n",
            "epoch 37 batch id 4531 loss 0.022913549095392227 train acc 0.980040278084308\n",
            "epoch 37 batch id 4541 loss 0.0637836679816246 train acc 0.9800429420832416\n",
            "epoch 37 batch id 4551 loss 0.046200335025787354 train acc 0.9800558943089431\n",
            "epoch 37 batch id 4561 loss 0.13063926994800568 train acc 0.9800448092523569\n",
            "epoch 37 batch id 4571 loss 0.03232374042272568 train acc 0.9800508641435135\n",
            "epoch 37 batch id 4581 loss 0.02851884998381138 train acc 0.9800534817725387\n",
            "epoch 37 batch id 4591 loss 0.1311614215373993 train acc 0.9800390710084949\n",
            "epoch 37 batch id 4601 loss 0.014688410796225071 train acc 0.9800281188871984\n",
            "epoch 37 batch id 4611 loss 0.030916418880224228 train acc 0.9800578779006723\n",
            "epoch 37 batch id 4621 loss 0.009113740175962448 train acc 0.9800570763903916\n",
            "epoch 37 batch id 4631 loss 0.07800055295228958 train acc 0.9800664003454977\n",
            "epoch 37 batch id 4641 loss 0.0918174535036087 train acc 0.9800891510450334\n",
            "epoch 37 batch id 4651 loss 0.11665232479572296 train acc 0.9800950064502257\n",
            "epoch 37 batch id 4661 loss 0.047776732593774796 train acc 0.9801041890152328\n",
            "epoch 37 batch id 4671 loss 0.0061706542037427425 train acc 0.9801233675872404\n",
            "epoch 37 batch id 4681 loss 0.21196770668029785 train acc 0.9801391262550737\n",
            "epoch 37 batch id 4691 loss 0.011623726226389408 train acc 0.9801115167341719\n",
            "epoch 37 batch id 4701 loss 0.010974385775625706 train acc 0.9801139385237183\n",
            "epoch 37 batch id 4711 loss 0.07948371022939682 train acc 0.9801130333262577\n",
            "epoch 37 batch id 4721 loss 0.002492972882464528 train acc 0.9801121319635671\n",
            "epoch 37 batch id 4731 loss 0.02605273388326168 train acc 0.9801112344113295\n",
            "epoch 37 batch id 4741 loss 0.07874813675880432 train acc 0.9801136363636364\n",
            "epoch 37 batch id 4751 loss 0.20702356100082397 train acc 0.9800995842980426\n",
            "epoch 37 batch id 4761 loss 0.05782053992152214 train acc 0.9801020006301198\n",
            "epoch 37 batch id 4771 loss 0.0724787712097168 train acc 0.980107681827709\n",
            "epoch 37 batch id 4781 loss 0.07895007729530334 train acc 0.9800937303911316\n",
            "epoch 37 batch id 4791 loss 0.1446717530488968 train acc 0.9800733145481111\n",
            "epoch 37 batch id 4801 loss 0.037019990384578705 train acc 0.9800725109352219\n",
            "epoch 37 batch id 4811 loss 0.049177344888448715 train acc 0.9800911972562877\n",
            "epoch 37 batch id 4821 loss 0.06241584196686745 train acc 0.9801098060568347\n",
            "epoch 37 batch id 4831 loss 0.0966731533408165 train acc 0.9801121662181743\n",
            "epoch 37 batch id 4841 loss 0.045768022537231445 train acc 0.9801016060731254\n",
            "epoch 37 batch id 4851 loss 0.04066901281476021 train acc 0.9801071943929087\n",
            "epoch 37 batch id 4861 loss 0.0004992851172573864 train acc 0.9800966879242954\n",
            "epoch 37 batch id 4871 loss 0.0039060802664607763 train acc 0.9801054711558201\n",
            "epoch 37 batch id 4881 loss 0.12188456952571869 train acc 0.9801238219627125\n",
            "epoch 37 batch id 4891 loss 0.02343815378844738 train acc 0.9801133459415252\n",
            "epoch 37 batch id 4901 loss 0.0031953039579093456 train acc 0.9801029126708835\n",
            "epoch 37 batch id 4911 loss 0.05165960267186165 train acc 0.9801116116880473\n",
            "epoch 37 batch id 4921 loss 0.16130594909191132 train acc 0.9800980491769965\n",
            "epoch 37 batch id 4931 loss 0.09083902835845947 train acc 0.9800845416751166\n",
            "epoch 37 batch id 4941 loss 0.05853952467441559 train acc 0.9800679265330905\n",
            "epoch 37 batch id 4951 loss 0.09300996363162994 train acc 0.9800703140779641\n",
            "epoch 37 train acc 0.9800646354099805\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a95f3cf8746f46349938517e5f0ffb4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 37 loss 0.2701036334037781 test acc 0.8494123442082111\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89a70d9439914ad89cfa2029f1e87b30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 38 batch id 1 loss 0.029823610559105873 train acc 0.984375\n",
            "epoch 38 batch id 11 loss 0.016951577737927437 train acc 0.9829545454545454\n",
            "epoch 38 batch id 21 loss 0.005886203143745661 train acc 0.9813988095238095\n",
            "epoch 38 batch id 31 loss 0.1011018380522728 train acc 0.9768145161290323\n",
            "epoch 38 batch id 41 loss 0.08408746123313904 train acc 0.9790396341463414\n",
            "epoch 38 batch id 51 loss 0.03259457275271416 train acc 0.9785539215686274\n",
            "epoch 38 batch id 61 loss 0.0325351245701313 train acc 0.9777151639344263\n",
            "epoch 38 batch id 71 loss 0.08067920058965683 train acc 0.9788732394366197\n",
            "epoch 38 batch id 81 loss 0.0079592140391469 train acc 0.9791666666666666\n",
            "epoch 38 batch id 91 loss 0.08668532967567444 train acc 0.9795673076923077\n",
            "epoch 38 batch id 101 loss 0.04599395766854286 train acc 0.978805693069307\n",
            "epoch 38 batch id 111 loss 0.0030857569072395563 train acc 0.9794481981981982\n",
            "epoch 38 batch id 121 loss 0.06170738488435745 train acc 0.9805010330578512\n",
            "epoch 38 batch id 131 loss 0.06254032999277115 train acc 0.980081106870229\n",
            "epoch 38 batch id 141 loss 0.002802392467856407 train acc 0.9802748226950354\n",
            "epoch 38 batch id 151 loss 0.054251089692115784 train acc 0.980546357615894\n",
            "epoch 38 batch id 161 loss 0.00022101975628174841 train acc 0.9806871118012422\n",
            "epoch 38 batch id 171 loss 0.012436039745807648 train acc 0.9808114035087719\n",
            "epoch 38 batch id 181 loss 0.017457209527492523 train acc 0.9812672651933702\n",
            "epoch 38 batch id 191 loss 0.05373992398381233 train acc 0.9811027486910995\n",
            "epoch 38 batch id 201 loss 0.10930100083351135 train acc 0.9813432835820896\n",
            "epoch 38 batch id 211 loss 0.010530874133110046 train acc 0.9817831753554502\n",
            "epoch 38 batch id 221 loss 0.04843790456652641 train acc 0.9817590497737556\n",
            "epoch 38 batch id 231 loss 0.02686741203069687 train acc 0.9822781385281385\n",
            "epoch 38 batch id 241 loss 0.11376616358757019 train acc 0.9821058091286307\n",
            "epoch 38 batch id 251 loss 0.09473130106925964 train acc 0.9821962151394422\n",
            "epoch 38 batch id 261 loss 0.12237735837697983 train acc 0.9821599616858238\n",
            "epoch 38 batch id 271 loss 0.002055167220532894 train acc 0.9822993542435424\n",
            "epoch 38 batch id 281 loss 0.004004818852990866 train acc 0.982317615658363\n",
            "epoch 38 batch id 291 loss 0.06687232851982117 train acc 0.9823346219931272\n",
            "epoch 38 batch id 301 loss 0.11889539659023285 train acc 0.9818313953488372\n",
            "epoch 38 batch id 311 loss 0.07196689397096634 train acc 0.981963424437299\n",
            "epoch 38 batch id 321 loss 0.08341871201992035 train acc 0.981941199376947\n",
            "epoch 38 batch id 331 loss 0.0001880145282484591 train acc 0.9820147280966768\n",
            "epoch 38 batch id 341 loss 0.21503013372421265 train acc 0.9818548387096774\n",
            "epoch 38 batch id 351 loss 0.17746394872665405 train acc 0.9819266381766382\n",
            "epoch 38 batch id 361 loss 0.18187713623046875 train acc 0.9818646121883656\n",
            "epoch 38 batch id 371 loss 0.013519886881113052 train acc 0.9820586253369272\n",
            "epoch 38 batch id 381 loss 0.015948928892612457 train acc 0.9821604330708661\n",
            "epoch 38 batch id 391 loss 0.0717063769698143 train acc 0.9822570332480819\n",
            "epoch 38 batch id 401 loss 0.0004737728741019964 train acc 0.9819591645885287\n",
            "epoch 38 batch id 411 loss 0.00019801271264441311 train acc 0.9820939781021898\n",
            "epoch 38 batch id 421 loss 0.04790814220905304 train acc 0.9822223871733967\n",
            "epoch 38 batch id 431 loss 0.15207792818546295 train acc 0.9821273201856149\n",
            "epoch 38 batch id 441 loss 0.11981309205293655 train acc 0.9820365646258503\n",
            "epoch 38 batch id 451 loss 0.02480590157210827 train acc 0.9822269955654102\n",
            "epoch 38 batch id 461 loss 0.0060472646728158 train acc 0.9821041214750542\n",
            "epoch 38 batch id 471 loss 0.013273751363158226 train acc 0.9821523354564756\n",
            "epoch 38 batch id 481 loss 0.1400432586669922 train acc 0.9821335758835759\n",
            "epoch 38 batch id 491 loss 0.0024625244550406933 train acc 0.9820837576374746\n",
            "epoch 38 batch id 501 loss 0.2980809211730957 train acc 0.9820671157684631\n",
            "epoch 38 batch id 511 loss 0.10677867382764816 train acc 0.9819593933463796\n",
            "epoch 38 batch id 521 loss 0.09774129837751389 train acc 0.9819757677543186\n",
            "epoch 38 batch id 531 loss 0.10399121791124344 train acc 0.9816678436911488\n",
            "epoch 38 batch id 541 loss 0.008333458565175533 train acc 0.9815734750462107\n",
            "epoch 38 batch id 551 loss 0.031372763216495514 train acc 0.981510889292196\n",
            "epoch 38 batch id 561 loss 0.025176556780934334 train acc 0.98150623885918\n",
            "epoch 38 batch id 571 loss 0.03034159168601036 train acc 0.98152911558669\n",
            "epoch 38 batch id 581 loss 0.13396170735359192 train acc 0.9814436316695353\n",
            "epoch 38 batch id 591 loss 0.02584248036146164 train acc 0.9814932318104906\n",
            "epoch 38 batch id 601 loss 0.013341831974685192 train acc 0.9814111896838602\n",
            "epoch 38 batch id 611 loss 0.05778106302022934 train acc 0.9812295417348609\n",
            "epoch 38 batch id 621 loss 0.18460027873516083 train acc 0.9811795491143317\n",
            "epoch 38 batch id 631 loss 0.03906877711415291 train acc 0.9812054278922345\n",
            "epoch 38 batch id 641 loss 0.041457246989011765 train acc 0.9812061232449298\n",
            "epoch 38 batch id 651 loss 0.04590855538845062 train acc 0.981206797235023\n",
            "epoch 38 batch id 661 loss 0.0030590747483074665 train acc 0.9812547276853253\n",
            "epoch 38 batch id 671 loss 0.011573359370231628 train acc 0.9812779433681073\n",
            "epoch 38 batch id 681 loss 0.1474236249923706 train acc 0.9812545888399412\n",
            "epoch 38 batch id 691 loss 0.10967236012220383 train acc 0.9811866859623734\n",
            "epoch 38 batch id 701 loss 0.05811486393213272 train acc 0.9811430099857347\n",
            "epoch 38 batch id 711 loss 0.11246749013662338 train acc 0.9810785864978903\n",
            "epoch 38 batch id 721 loss 0.052265848964452744 train acc 0.980994278779473\n",
            "epoch 38 batch id 731 loss 0.014605346135795116 train acc 0.981061901504788\n",
            "epoch 38 batch id 741 loss 0.002110508969053626 train acc 0.9810433535762483\n",
            "epoch 38 batch id 751 loss 0.0174283254891634 train acc 0.9809212716378163\n",
            "epoch 38 batch id 761 loss 0.02992246299982071 train acc 0.980946123521682\n",
            "epoch 38 batch id 771 loss 0.052991993725299835 train acc 0.9808082036316472\n",
            "epoch 38 batch id 781 loss 0.035632308572530746 train acc 0.9808538732394366\n",
            "epoch 38 batch id 791 loss 0.01591954194009304 train acc 0.9809576485461441\n",
            "epoch 38 batch id 801 loss 0.04365712031722069 train acc 0.9809027777777778\n",
            "epoch 38 batch id 811 loss 0.10510626435279846 train acc 0.9808685265104808\n",
            "epoch 38 batch id 821 loss 0.03355912119150162 train acc 0.9809873629719854\n",
            "epoch 38 batch id 831 loss 0.08598671108484268 train acc 0.9809905234657039\n",
            "epoch 38 batch id 841 loss 0.09097030758857727 train acc 0.9809564506539834\n",
            "epoch 38 batch id 851 loss 0.19529008865356445 train acc 0.9808497356051704\n",
            "epoch 38 batch id 861 loss 0.05344819277524948 train acc 0.9807636469221835\n",
            "epoch 38 batch id 871 loss 0.003521483391523361 train acc 0.9807692307692307\n",
            "epoch 38 batch id 881 loss 0.06959091126918793 train acc 0.9806505391600454\n",
            "epoch 38 batch id 891 loss 0.0016712829237803817 train acc 0.9806397306397306\n",
            "epoch 38 batch id 901 loss 0.0889502614736557 train acc 0.9806465038845728\n",
            "epoch 38 batch id 911 loss 0.01198304072022438 train acc 0.9807045828759605\n",
            "epoch 38 batch id 921 loss 0.0013743812451139092 train acc 0.9806256786102063\n",
            "epoch 38 batch id 931 loss 0.009137489832937717 train acc 0.9806827336197637\n",
            "epoch 38 batch id 941 loss 0.08521190285682678 train acc 0.9807385759829969\n",
            "epoch 38 batch id 951 loss 0.03641946241259575 train acc 0.9806289432176656\n",
            "epoch 38 batch id 961 loss 0.12875749170780182 train acc 0.9806191467221644\n",
            "epoch 38 batch id 971 loss 0.009244534187018871 train acc 0.9806256436663234\n",
            "epoch 38 batch id 981 loss 0.00949038378894329 train acc 0.9807912844036697\n",
            "epoch 38 batch id 991 loss 0.08653463423252106 train acc 0.980858980827447\n",
            "epoch 38 batch id 1001 loss 0.025583690032362938 train acc 0.9807848401598401\n",
            "epoch 38 batch id 1011 loss 0.0009414392407052219 train acc 0.9808203511374877\n",
            "epoch 38 batch id 1021 loss 0.028846092522144318 train acc 0.9809010773751224\n",
            "epoch 38 batch id 1031 loss 0.012361297383904457 train acc 0.9809650824442289\n",
            "epoch 38 batch id 1041 loss 0.07134407758712769 train acc 0.9810278578290106\n",
            "epoch 38 batch id 1051 loss 0.023043422028422356 train acc 0.9809407706945766\n",
            "epoch 38 batch id 1061 loss 0.0383407324552536 train acc 0.9808553251649388\n",
            "epoch 38 batch id 1071 loss 0.0003941396716982126 train acc 0.9808590102707749\n",
            "epoch 38 batch id 1081 loss 0.03814644366502762 train acc 0.980804810360777\n",
            "epoch 38 batch id 1091 loss 0.20552179217338562 train acc 0.9807802474793768\n",
            "epoch 38 batch id 1101 loss 0.0677151158452034 train acc 0.9807561307901907\n",
            "epoch 38 batch id 1111 loss 0.08589327335357666 train acc 0.9807465121512151\n",
            "epoch 38 batch id 1121 loss 0.012479865923523903 train acc 0.9807788804638715\n",
            "epoch 38 batch id 1131 loss 0.11617118120193481 train acc 0.9807139699381079\n",
            "epoch 38 batch id 1141 loss 0.09983645379543304 train acc 0.9807049737072743\n",
            "epoch 38 batch id 1151 loss 0.07166182994842529 train acc 0.9806011077324066\n",
            "epoch 38 batch id 1161 loss 0.11306776106357574 train acc 0.9805932385874246\n",
            "epoch 38 batch id 1171 loss 0.01075507141649723 train acc 0.9805321306575576\n",
            "epoch 38 batch id 1181 loss 0.013150008395314217 train acc 0.9805382091447925\n",
            "epoch 38 batch id 1191 loss 0.012087886221706867 train acc 0.9805441855583543\n",
            "epoch 38 batch id 1201 loss 0.007665536366403103 train acc 0.9806541423813488\n",
            "epoch 38 batch id 1211 loss 0.00032886702683754265 train acc 0.9807364781172585\n",
            "epoch 38 batch id 1221 loss 0.08155892789363861 train acc 0.9807022932022932\n",
            "epoch 38 batch id 1231 loss 0.044811543077230453 train acc 0.9806559707554834\n",
            "epoch 38 batch id 1241 loss 0.034441009163856506 train acc 0.9806733481063659\n",
            "epoch 38 batch id 1251 loss 0.07353921234607697 train acc 0.9806904476418865\n",
            "epoch 38 batch id 1261 loss 0.06028455123305321 train acc 0.9806824940523394\n",
            "epoch 38 batch id 1271 loss 0.03991561755537987 train acc 0.9806500786782061\n",
            "epoch 38 batch id 1281 loss 0.001691935583949089 train acc 0.9806669594067136\n",
            "epoch 38 batch id 1291 loss 0.14670409262180328 train acc 0.9806835786212239\n",
            "epoch 38 batch id 1301 loss 0.0012517088325694203 train acc 0.9806639123750961\n",
            "epoch 38 batch id 1311 loss 0.019575055688619614 train acc 0.9806922196796338\n",
            "epoch 38 batch id 1321 loss 0.019817106425762177 train acc 0.9806846139288418\n",
            "epoch 38 batch id 1331 loss 0.025020001456141472 train acc 0.9806653831705484\n",
            "epoch 38 batch id 1341 loss 0.10289531201124191 train acc 0.9806930462341537\n",
            "epoch 38 batch id 1351 loss 0.0019093080190941691 train acc 0.9806624722427831\n",
            "epoch 38 batch id 1361 loss 0.003490795847028494 train acc 0.9805290227773695\n",
            "epoch 38 batch id 1371 loss 0.02117750234901905 train acc 0.9805684719183078\n",
            "epoch 38 batch id 1381 loss 0.03398878872394562 train acc 0.9805960354815351\n",
            "epoch 38 batch id 1391 loss 0.008654829114675522 train acc 0.9806007368799425\n",
            "epoch 38 batch id 1401 loss 0.009605970233678818 train acc 0.9806276766595289\n",
            "epoch 38 batch id 1411 loss 0.018594415858387947 train acc 0.9806985294117647\n",
            "epoch 38 batch id 1421 loss 0.010633723810315132 train acc 0.9806804187192119\n",
            "epoch 38 batch id 1431 loss 0.12023021280765533 train acc 0.9806953179594688\n",
            "epoch 38 batch id 1441 loss 0.01397836022078991 train acc 0.9806557945870923\n",
            "epoch 38 batch id 1451 loss 0.007421903312206268 train acc 0.9806814266023433\n",
            "epoch 38 batch id 1461 loss 0.01134563609957695 train acc 0.9806960130047913\n",
            "epoch 38 batch id 1471 loss 0.018938802182674408 train acc 0.980731645139361\n",
            "epoch 38 batch id 1481 loss 0.09612125903367996 train acc 0.980692943956786\n",
            "epoch 38 batch id 1491 loss 0.12918445467948914 train acc 0.9807281187122736\n",
            "epoch 38 batch id 1501 loss 0.05675583332777023 train acc 0.9807524150566289\n",
            "epoch 38 batch id 1511 loss 0.011926654726266861 train acc 0.9808074123097287\n",
            "epoch 38 batch id 1521 loss 0.001222833408974111 train acc 0.9808000493096647\n",
            "epoch 38 batch id 1531 loss 0.029419025406241417 train acc 0.9807723709993469\n",
            "epoch 38 batch id 1541 loss 0.00990868080407381 train acc 0.9807856099935107\n",
            "epoch 38 batch id 1551 loss 0.03547682985663414 train acc 0.98078860412637\n",
            "epoch 38 batch id 1561 loss 0.03389062359929085 train acc 0.980761531069827\n",
            "epoch 38 batch id 1571 loss 0.010513156652450562 train acc 0.9807845321451305\n",
            "epoch 38 batch id 1581 loss 0.04268485680222511 train acc 0.9807775932953827\n",
            "epoch 38 batch id 1591 loss 0.03028123825788498 train acc 0.9808198460087995\n",
            "epoch 38 batch id 1601 loss 0.025950497016310692 train acc 0.980851811367895\n",
            "epoch 38 batch id 1611 loss 0.05831492319703102 train acc 0.9808057883302297\n",
            "epoch 38 batch id 1621 loss 0.03653182461857796 train acc 0.9808278069093153\n",
            "epoch 38 batch id 1631 loss 0.0002444860292598605 train acc 0.9808687155119559\n",
            "epoch 38 batch id 1641 loss 0.06643012166023254 train acc 0.980918647166362\n",
            "epoch 38 batch id 1651 loss 0.038344986736774445 train acc 0.9808922622652938\n",
            "epoch 38 batch id 1661 loss 0.09691762179136276 train acc 0.9808944160144492\n",
            "epoch 38 batch id 1671 loss 0.18428181111812592 train acc 0.9808310891681628\n",
            "epoch 38 batch id 1681 loss 0.06259427219629288 train acc 0.9807592207019631\n",
            "epoch 38 batch id 1691 loss 0.07388359308242798 train acc 0.9807898432879952\n",
            "epoch 38 batch id 1701 loss 0.03866128623485565 train acc 0.9807649911816578\n",
            "epoch 38 batch id 1711 loss 0.028761621564626694 train acc 0.9807769579193454\n",
            "epoch 38 batch id 1721 loss 0.00994160957634449 train acc 0.9807615485183033\n",
            "epoch 38 batch id 1731 loss 0.00012007407349301502 train acc 0.9807553437319468\n",
            "epoch 38 batch id 1741 loss 0.02958293817937374 train acc 0.9807581849511775\n",
            "epoch 38 batch id 1751 loss 0.03730387985706329 train acc 0.9806896059394632\n",
            "epoch 38 batch id 1761 loss 0.019178492948412895 train acc 0.9807282793867121\n",
            "epoch 38 batch id 1771 loss 0.02138160727918148 train acc 0.980766516092603\n",
            "epoch 38 batch id 1781 loss 0.025164255872368813 train acc 0.9807867770915216\n",
            "epoch 38 batch id 1791 loss 0.06034974753856659 train acc 0.9807631909547738\n",
            "epoch 38 batch id 1801 loss 0.0220805536955595 train acc 0.9808005968906163\n",
            "epoch 38 batch id 1811 loss 0.044357892125844955 train acc 0.9808030784097184\n",
            "epoch 38 batch id 1821 loss 0.07213350385427475 train acc 0.9807969522240527\n",
            "epoch 38 batch id 1831 loss 0.015841832384467125 train acc 0.9807738257782632\n",
            "epoch 38 batch id 1841 loss 0.004537404049187899 train acc 0.980776412275937\n",
            "epoch 38 batch id 1851 loss 0.036959122866392136 train acc 0.9807874122096164\n",
            "epoch 38 batch id 1861 loss 0.020746104419231415 train acc 0.9807898979043524\n",
            "epoch 38 batch id 1871 loss 0.09090924263000488 train acc 0.9808341127739177\n",
            "epoch 38 batch id 1881 loss 0.01878361776471138 train acc 0.9808612440191388\n",
            "epoch 38 batch id 1891 loss 0.024592498317360878 train acc 0.9809211396086727\n",
            "epoch 38 batch id 1901 loss 0.09617798030376434 train acc 0.9809146501841136\n",
            "epoch 38 batch id 1911 loss 0.03238556534051895 train acc 0.9808918759811617\n",
            "epoch 38 batch id 1921 loss 0.001161261578090489 train acc 0.9808368037480479\n",
            "epoch 38 batch id 1931 loss 0.08631041646003723 train acc 0.9808146685655101\n",
            "epoch 38 batch id 1941 loss 0.010868004523217678 train acc 0.9807766615146831\n",
            "epoch 38 batch id 1951 loss 0.05989164486527443 train acc 0.9807951050743209\n",
            "epoch 38 batch id 1961 loss 0.03667045384645462 train acc 0.9807974247832738\n",
            "epoch 38 batch id 1971 loss 0.011895984411239624 train acc 0.9808235032978183\n",
            "epoch 38 batch id 1981 loss 0.0505736842751503 train acc 0.9807941065118627\n",
            "epoch 38 batch id 1991 loss 0.1206812709569931 train acc 0.9807728528377699\n",
            "epoch 38 batch id 2001 loss 0.164531409740448 train acc 0.9807908545727136\n",
            "epoch 38 batch id 2011 loss 0.19185391068458557 train acc 0.9807853679761313\n",
            "epoch 38 batch id 2021 loss 0.1252412647008896 train acc 0.9807567417120238\n",
            "epoch 38 batch id 2031 loss 0.0003095184511039406 train acc 0.9807591703594288\n",
            "epoch 38 batch id 2041 loss 0.023373370990157127 train acc 0.9807462640862322\n",
            "epoch 38 batch id 2051 loss 0.284284383058548 train acc 0.9807106289614822\n",
            "epoch 38 batch id 2061 loss 0.003838323289528489 train acc 0.9807511523532266\n",
            "epoch 38 batch id 2071 loss 0.10364525765180588 train acc 0.980753561081603\n",
            "epoch 38 batch id 2081 loss 0.047944363206624985 train acc 0.9807484382508409\n",
            "epoch 38 batch id 2091 loss 0.035705823451280594 train acc 0.9807358919177427\n",
            "epoch 38 batch id 2101 loss 0.006944479886442423 train acc 0.9807606496906235\n",
            "epoch 38 batch id 2111 loss 0.009295963682234287 train acc 0.9807629677877783\n",
            "epoch 38 batch id 2121 loss 0.014608429744839668 train acc 0.9807873644507308\n",
            "epoch 38 batch id 2131 loss 0.0019331868970766664 train acc 0.9808335288596903\n",
            "epoch 38 batch id 2141 loss 0.04243790730834007 train acc 0.9808354740775339\n",
            "epoch 38 batch id 2151 loss 0.01038164459168911 train acc 0.9808737215248722\n",
            "epoch 38 batch id 2161 loss 0.2321172058582306 train acc 0.9808682322998612\n",
            "epoch 38 batch id 2171 loss 0.0456010065972805 train acc 0.9808340050667895\n",
            "epoch 38 batch id 2181 loss 0.09088530391454697 train acc 0.9808287482806052\n",
            "epoch 38 batch id 2191 loss 0.025998055934906006 train acc 0.9808663281606572\n",
            "epoch 38 batch id 2201 loss 0.01244053989648819 train acc 0.9808680713312131\n",
            "epoch 38 batch id 2211 loss 0.012279940769076347 train acc 0.9808697987336047\n",
            "epoch 38 batch id 2221 loss 0.03243173658847809 train acc 0.9808504052228726\n",
            "epoch 38 batch id 2231 loss 0.07800168544054031 train acc 0.9808591999103541\n",
            "epoch 38 batch id 2241 loss 0.04346305876970291 train acc 0.9808400267737617\n",
            "epoch 38 batch id 2251 loss 0.0002698327007237822 train acc 0.9808418480675255\n",
            "epoch 38 batch id 2261 loss 0.00039778725476935506 train acc 0.9808436532507739\n",
            "epoch 38 batch id 2271 loss 0.00684895645827055 train acc 0.9808385623073536\n",
            "epoch 38 batch id 2281 loss 0.04552258551120758 train acc 0.9808540661990355\n",
            "epoch 38 batch id 2291 loss 0.036465976387262344 train acc 0.9808830750763858\n",
            "epoch 38 batch id 2301 loss 0.09913787990808487 train acc 0.9809118318122555\n",
            "epoch 38 batch id 2311 loss 0.032084960490465164 train acc 0.9809471008221549\n",
            "epoch 38 batch id 2321 loss 0.08345148712396622 train acc 0.9809349418354157\n",
            "epoch 38 batch id 2331 loss 0.050313595682382584 train acc 0.9809027777777778\n",
            "epoch 38 batch id 2341 loss 0.03390158340334892 train acc 0.9809042609995728\n",
            "epoch 38 batch id 2351 loss 0.03350601717829704 train acc 0.9808459166312208\n",
            "epoch 38 batch id 2361 loss 0.07208646833896637 train acc 0.9808608640406608\n",
            "epoch 38 batch id 2371 loss 0.16188378632068634 train acc 0.9808888654576128\n",
            "epoch 38 batch id 2381 loss 0.0897904485464096 train acc 0.9808772574548509\n",
            "epoch 38 batch id 2391 loss 0.020877931267023087 train acc 0.9808853513174404\n",
            "epoch 38 batch id 2401 loss 0.04327485337853432 train acc 0.9808868700541441\n",
            "epoch 38 batch id 2411 loss 0.023921750485897064 train acc 0.9808559726254666\n",
            "epoch 38 batch id 2421 loss 0.011749042198061943 train acc 0.9809027777777778\n",
            "epoch 38 batch id 2431 loss 0.053406596183776855 train acc 0.9809042060880296\n",
            "epoch 38 batch id 2441 loss 0.0005431052995845675 train acc 0.9808864195002048\n",
            "epoch 38 batch id 2451 loss 0.0014499237295240164 train acc 0.9808942778457772\n",
            "epoch 38 batch id 2461 loss 0.03435008227825165 train acc 0.9808893742381146\n",
            "epoch 38 batch id 2471 loss 0.08087120205163956 train acc 0.9808592169162282\n",
            "epoch 38 batch id 2481 loss 0.08399932831525803 train acc 0.9808985792019347\n",
            "epoch 38 batch id 2491 loss 0.09826131165027618 train acc 0.9808937173825772\n",
            "epoch 38 batch id 2501 loss 0.0030760110821574926 train acc 0.9809138844462215\n",
            "epoch 38 batch id 2511 loss 0.04925379529595375 train acc 0.9808903325368379\n",
            "epoch 38 batch id 2521 loss 0.16694483160972595 train acc 0.9808483736612456\n",
            "epoch 38 batch id 2531 loss 0.016146086156368256 train acc 0.9808808277360727\n",
            "epoch 38 batch id 2541 loss 0.037043582648038864 train acc 0.9808884297520661\n",
            "epoch 38 batch id 2551 loss 0.03805336356163025 train acc 0.9809082222657781\n",
            "epoch 38 batch id 2561 loss 0.14853055775165558 train acc 0.9808729500195236\n",
            "epoch 38 batch id 2571 loss 0.08711255341768265 train acc 0.980868339167639\n",
            "epoch 38 batch id 2581 loss 0.05133562162518501 train acc 0.9808395486245641\n",
            "epoch 38 batch id 2591 loss 0.008580966852605343 train acc 0.9808471632574296\n",
            "epoch 38 batch id 2601 loss 0.12209830433130264 train acc 0.9808907631680123\n",
            "epoch 38 batch id 2611 loss 0.0006511498941108584 train acc 0.9809160762160092\n",
            "epoch 38 batch id 2621 loss 0.04662008583545685 train acc 0.9809292731781762\n",
            "epoch 38 batch id 2631 loss 0.09489092975854874 train acc 0.980906736982136\n",
            "epoch 38 batch id 2641 loss 0.047484204173088074 train acc 0.9808429572131768\n",
            "epoch 38 batch id 2651 loss 0.00032297795405611396 train acc 0.9808268106374953\n",
            "epoch 38 batch id 2661 loss 0.004755022004246712 train acc 0.9808225291243893\n",
            "epoch 38 batch id 2671 loss 0.06021667644381523 train acc 0.9808124298015725\n",
            "epoch 38 batch id 2681 loss 0.019518503919243813 train acc 0.9807849216710183\n",
            "epoch 38 batch id 2691 loss 0.07832800596952438 train acc 0.9807634243775548\n",
            "epoch 38 batch id 2701 loss 0.11766044795513153 train acc 0.9807536560533135\n",
            "epoch 38 batch id 2711 loss 0.0075176795944571495 train acc 0.9807209055699004\n",
            "epoch 38 batch id 2721 loss 0.010728384368121624 train acc 0.9807228500551268\n",
            "epoch 38 batch id 2731 loss 0.01025470718741417 train acc 0.9807190589527646\n",
            "epoch 38 batch id 2741 loss 0.14828543365001678 train acc 0.980732396935425\n",
            "epoch 38 batch id 2751 loss 0.16570442914962769 train acc 0.9807172391857506\n",
            "epoch 38 batch id 2761 loss 0.011850751005113125 train acc 0.9807474646867077\n",
            "epoch 38 batch id 2771 loss 0.0006331131444312632 train acc 0.9807718332731866\n",
            "epoch 38 batch id 2781 loss 0.030508000403642654 train acc 0.9808016450916937\n",
            "epoch 38 batch id 2791 loss 0.052505891770124435 train acc 0.9807976531709065\n",
            "epoch 38 batch id 2801 loss 0.13612046837806702 train acc 0.9807881113887897\n",
            "epoch 38 batch id 2811 loss 0.036803603172302246 train acc 0.9808119886161508\n",
            "epoch 38 batch id 2821 loss 0.048717375844717026 train acc 0.9808246189294576\n",
            "epoch 38 batch id 2831 loss 0.014775138348340988 train acc 0.9808261215118332\n",
            "epoch 38 batch id 2841 loss 0.058707352727651596 train acc 0.9808331133403732\n",
            "epoch 38 batch id 2851 loss 0.002755114808678627 train acc 0.9808290950543669\n",
            "epoch 38 batch id 2861 loss 0.049193643033504486 train acc 0.9808251048584411\n",
            "epoch 38 batch id 2871 loss 0.08407001197338104 train acc 0.9808265848136538\n",
            "epoch 38 batch id 2881 loss 0.04931727796792984 train acc 0.9808226310308921\n",
            "epoch 38 batch id 2891 loss 0.0016303505981341004 train acc 0.9808403234175026\n",
            "epoch 38 batch id 2901 loss 0.08379223197698593 train acc 0.9808255773871078\n",
            "epoch 38 batch id 2911 loss 0.0238084327429533 train acc 0.9808216678117485\n",
            "epoch 38 batch id 2921 loss 0.04189389571547508 train acc 0.9807910390277302\n",
            "epoch 38 batch id 2931 loss 0.05161377042531967 train acc 0.980781943022859\n",
            "epoch 38 batch id 2941 loss 0.03717688471078873 train acc 0.9807675960557634\n",
            "epoch 38 batch id 2951 loss 0.02169637568295002 train acc 0.9807904100304982\n",
            "epoch 38 batch id 2961 loss 0.04555521532893181 train acc 0.9808025160418777\n",
            "epoch 38 batch id 2971 loss 0.010296523571014404 train acc 0.9807829855267587\n",
            "epoch 38 batch id 2981 loss 0.0790516808629036 train acc 0.9807688275746393\n",
            "epoch 38 batch id 2991 loss 0.011954812332987785 train acc 0.9807861083249749\n",
            "epoch 38 batch id 3001 loss 0.00021219412155915052 train acc 0.9808188937020993\n",
            "epoch 38 batch id 3011 loss 0.004870781674981117 train acc 0.9808255147791431\n",
            "epoch 38 batch id 3021 loss 0.03588653355836868 train acc 0.9808217477656405\n",
            "epoch 38 batch id 3031 loss 0.15850137174129486 train acc 0.9808076954800395\n",
            "epoch 38 batch id 3041 loss 0.07915715128183365 train acc 0.9807834593883591\n",
            "epoch 38 batch id 3051 loss 0.0050356630235910416 train acc 0.9808003523434939\n",
            "epoch 38 batch id 3061 loss 0.08999249339103699 train acc 0.9807762985952303\n",
            "epoch 38 batch id 3071 loss 0.05329522117972374 train acc 0.9807931048518398\n",
            "epoch 38 batch id 3081 loss 0.06412357836961746 train acc 0.9808047306069458\n",
            "epoch 38 batch id 3091 loss 0.026930758729577065 train acc 0.9808314461339372\n",
            "epoch 38 batch id 3101 loss 0.05536249652504921 train acc 0.9808327958722992\n",
            "epoch 38 batch id 3111 loss 0.08120659738779068 train acc 0.9808341369334619\n",
            "epoch 38 batch id 3121 loss 0.015053369104862213 train acc 0.9808605014418456\n",
            "epoch 38 batch id 3131 loss 0.0413471944630146 train acc 0.9808817071223251\n",
            "epoch 38 batch id 3141 loss 0.07390139997005463 train acc 0.9808779051257561\n",
            "epoch 38 batch id 3151 loss 0.012602640315890312 train acc 0.980874127261187\n",
            "epoch 38 batch id 3161 loss 0.07839863002300262 train acc 0.980850601075609\n",
            "epoch 38 batch id 3171 loss 0.02375091426074505 train acc 0.9808469331441185\n",
            "epoch 38 batch id 3181 loss 0.034821007400751114 train acc 0.9808285523420308\n",
            "epoch 38 batch id 3191 loss 0.01590570993721485 train acc 0.980829873080539\n",
            "epoch 38 batch id 3201 loss 0.09589105099439621 train acc 0.9808311855670103\n",
            "epoch 38 batch id 3211 loss 0.12763315439224243 train acc 0.9807838290252258\n",
            "epoch 38 batch id 3221 loss 0.07653144001960754 train acc 0.9807610214219187\n",
            "epoch 38 batch id 3231 loss 0.08135932683944702 train acc 0.9807431909625502\n",
            "epoch 38 batch id 3241 loss 0.012818104587495327 train acc 0.9807543967911139\n",
            "epoch 38 batch id 3251 loss 0.022729311138391495 train acc 0.9807463088280529\n",
            "epoch 38 batch id 3261 loss 0.10059431940317154 train acc 0.9807191045691506\n",
            "epoch 38 batch id 3271 loss 0.027888964861631393 train acc 0.9806920666462855\n",
            "epoch 38 batch id 3281 loss 0.006796037778258324 train acc 0.9807080539469674\n",
            "epoch 38 batch id 3291 loss 0.0159167293459177 train acc 0.980714448495898\n",
            "epoch 38 batch id 3301 loss 0.011655225418508053 train acc 0.9807208043017267\n",
            "epoch 38 batch id 3311 loss 0.027050193399190903 train acc 0.9807271217154938\n",
            "epoch 38 batch id 3321 loss 0.016107838600873947 train acc 0.9807569256248118\n",
            "epoch 38 batch id 3331 loss 0.05454997345805168 train acc 0.9807208796157311\n",
            "epoch 38 batch id 3341 loss 0.08402541279792786 train acc 0.9807318168213109\n",
            "epoch 38 batch id 3351 loss 0.01943379081785679 train acc 0.9807100492390332\n",
            "epoch 38 batch id 3361 loss 0.01802925579249859 train acc 0.9807209535852425\n",
            "epoch 38 batch id 3371 loss 0.0733461007475853 train acc 0.9807225229902106\n",
            "epoch 38 batch id 3381 loss 0.027763351798057556 train acc 0.9807287045252884\n",
            "epoch 38 batch id 3391 loss 0.05901440232992172 train acc 0.9807348496018874\n",
            "epoch 38 batch id 3401 loss 0.08744893223047256 train acc 0.9807363643046163\n",
            "epoch 38 batch id 3411 loss 0.03734849765896797 train acc 0.9807332893579596\n",
            "epoch 38 batch id 3421 loss 0.0636512041091919 train acc 0.9807622040339082\n",
            "epoch 38 batch id 3431 loss 0.13039584457874298 train acc 0.9807681798309531\n",
            "epoch 38 batch id 3441 loss 0.007490781135857105 train acc 0.9807832025573961\n",
            "epoch 38 batch id 3451 loss 0.01505213137716055 train acc 0.9807754998551145\n",
            "epoch 38 batch id 3461 loss 0.008090099319815636 train acc 0.9807723562554175\n",
            "epoch 38 batch id 3471 loss 0.0689954161643982 train acc 0.9807782339383463\n",
            "epoch 38 batch id 3481 loss 0.06574293971061707 train acc 0.9807751005458202\n",
            "epoch 38 batch id 3491 loss 0.0017700662137940526 train acc 0.9807764608994557\n",
            "epoch 38 batch id 3501 loss 0.020104175433516502 train acc 0.980795665524136\n",
            "epoch 38 batch id 3511 loss 0.10198300331830978 train acc 0.9807613571632013\n",
            "epoch 38 batch id 3521 loss 0.11957339942455292 train acc 0.9807804955978415\n",
            "epoch 38 batch id 3531 loss 0.05485929548740387 train acc 0.9807951005380912\n",
            "epoch 38 batch id 3541 loss 0.05896569415926933 train acc 0.9807787348206721\n",
            "epoch 38 batch id 3551 loss 0.01831146515905857 train acc 0.9807668614474796\n",
            "epoch 38 batch id 3561 loss 0.012528328225016594 train acc 0.9807857694467846\n",
            "epoch 38 batch id 3571 loss 0.09584105014801025 train acc 0.9807870694483338\n",
            "epoch 38 batch id 3581 loss 0.029640091583132744 train acc 0.9808014521083496\n",
            "epoch 38 batch id 3591 loss 0.07699871063232422 train acc 0.9807939988861042\n",
            "epoch 38 batch id 3601 loss 0.020111683756113052 train acc 0.9807865870591502\n",
            "epoch 38 batch id 3611 loss 0.014331855811178684 train acc 0.9808008515646636\n",
            "epoch 38 batch id 3621 loss 0.09377080947160721 train acc 0.9807891466445733\n",
            "epoch 38 batch id 3631 loss 0.012694545090198517 train acc 0.9807947190856513\n",
            "epoch 38 batch id 3641 loss 0.06953324377536774 train acc 0.9808088437242516\n",
            "epoch 38 batch id 3651 loss 0.018588442355394363 train acc 0.9808186113393591\n",
            "epoch 38 batch id 3661 loss 0.004486740566790104 train acc 0.9808240576345261\n",
            "epoch 38 batch id 3671 loss 0.08362748473882675 train acc 0.9808294742576954\n",
            "epoch 38 batch id 3681 loss 0.031721364706754684 train acc 0.9808433509915784\n",
            "epoch 38 batch id 3691 loss 0.00802444014698267 train acc 0.9808613858033053\n",
            "epoch 38 batch id 3701 loss 0.03627745807170868 train acc 0.9808455485004053\n",
            "epoch 38 batch id 3711 loss 0.027646955102682114 train acc 0.9808592697386149\n",
            "epoch 38 batch id 3721 loss 0.05784214660525322 train acc 0.9808519215264714\n",
            "epoch 38 batch id 3731 loss 0.03428776189684868 train acc 0.980836236933798\n",
            "epoch 38 batch id 3741 loss 0.138637512922287 train acc 0.9808331662657044\n",
            "epoch 38 batch id 3751 loss 0.043837450444698334 train acc 0.9808217808584377\n",
            "epoch 38 batch id 3761 loss 0.147144615650177 train acc 0.9808063015155544\n",
            "epoch 38 batch id 3771 loss 0.02231808379292488 train acc 0.9807950477326969\n",
            "epoch 38 batch id 3781 loss 0.07623811811208725 train acc 0.9807797209732875\n",
            "epoch 38 batch id 3791 loss 0.0002472166088409722 train acc 0.980797447902928\n",
            "epoch 38 batch id 3801 loss 0.06330013275146484 train acc 0.980782195474875\n",
            "epoch 38 batch id 3811 loss 0.0140453539788723 train acc 0.9808080228286539\n",
            "epoch 38 batch id 3821 loss 0.135541170835495 train acc 0.9808091795341534\n",
            "epoch 38 batch id 3831 loss 0.0003933423140551895 train acc 0.9808184873401201\n",
            "epoch 38 batch id 3841 loss 0.0012122699990868568 train acc 0.9808196107784432\n",
            "epoch 38 batch id 3851 loss 0.002442741533741355 train acc 0.9808329005453129\n",
            "epoch 38 batch id 3861 loss 0.027418190613389015 train acc 0.9808218401968402\n",
            "epoch 38 batch id 3871 loss 0.132346972823143 train acc 0.9808310191165074\n",
            "epoch 38 batch id 3881 loss 0.1305433064699173 train acc 0.9808320986859057\n",
            "epoch 38 batch id 3891 loss 0.02004929445683956 train acc 0.9808331727062451\n",
            "epoch 38 batch id 3901 loss 0.012553403154015541 train acc 0.9808182196872597\n",
            "epoch 38 batch id 3911 loss 0.023567192256450653 train acc 0.9808193237023779\n",
            "epoch 38 batch id 3921 loss 0.012828578241169453 train acc 0.9808283919918388\n",
            "epoch 38 batch id 3931 loss 0.04534261301159859 train acc 0.980817540066141\n",
            "epoch 38 batch id 3941 loss 0.005542846862226725 train acc 0.9808305315909668\n",
            "epoch 38 batch id 3951 loss 0.015157056972384453 train acc 0.9808039104024298\n",
            "epoch 38 batch id 3961 loss 0.003986129071563482 train acc 0.9808129260287806\n",
            "epoch 38 batch id 3971 loss 0.06792694330215454 train acc 0.9808140266935281\n",
            "epoch 38 batch id 3981 loss 0.008873865939676762 train acc 0.9808190467219292\n",
            "epoch 38 batch id 3991 loss 0.1396687924861908 train acc 0.9808162114758205\n",
            "epoch 38 batch id 4001 loss 0.06609112024307251 train acc 0.9808133904023995\n",
            "epoch 38 batch id 4011 loss 0.07749428600072861 train acc 0.9807716280229369\n",
            "epoch 38 batch id 4021 loss 0.02408299595117569 train acc 0.9807650460084556\n",
            "epoch 38 batch id 4031 loss 0.03531710430979729 train acc 0.9807778776978417\n",
            "epoch 38 batch id 4041 loss 0.04209167882800102 train acc 0.9807751794110369\n",
            "epoch 38 batch id 4051 loss 0.028953861445188522 train acc 0.9807609232288323\n",
            "epoch 38 batch id 4061 loss 0.02013537287712097 train acc 0.9807582799803004\n",
            "epoch 38 batch id 4071 loss 0.09478386491537094 train acc 0.9807518115942029\n",
            "epoch 38 batch id 4081 loss 0.010587567463517189 train acc 0.9807683472188189\n",
            "epoch 38 batch id 4091 loss 0.03247926011681557 train acc 0.9807733439256905\n",
            "epoch 38 batch id 4101 loss 0.004347353707998991 train acc 0.980782126310656\n",
            "epoch 38 batch id 4111 loss 0.0965019017457962 train acc 0.9807528581853564\n",
            "epoch 38 batch id 4121 loss 0.016819965094327927 train acc 0.98075406454744\n",
            "epoch 38 batch id 4131 loss 0.17236226797103882 train acc 0.9807363531832486\n",
            "epoch 38 batch id 4141 loss 0.0005788677372038364 train acc 0.9807413668196088\n",
            "epoch 38 batch id 4151 loss 0.053462959825992584 train acc 0.9807350638400385\n",
            "epoch 38 batch id 4161 loss 0.015822134912014008 train acc 0.9807287911559721\n",
            "epoch 38 batch id 4171 loss 0.17437782883644104 train acc 0.980730040757612\n",
            "epoch 38 batch id 4181 loss 0.006015199702233076 train acc 0.9807238100932791\n",
            "epoch 38 batch id 4191 loss 0.1968296766281128 train acc 0.9807176091624911\n",
            "epoch 38 batch id 4201 loss 0.04547944292426109 train acc 0.9807337538681267\n",
            "epoch 38 batch id 4211 loss 0.0585736520588398 train acc 0.9807238482545714\n",
            "epoch 38 batch id 4221 loss 0.0965862050652504 train acc 0.9807102878464818\n",
            "epoch 38 batch id 4231 loss 0.0479305237531662 train acc 0.9807152564405578\n",
            "epoch 38 batch id 4241 loss 0.057540412992239 train acc 0.9807349386937043\n",
            "epoch 38 batch id 4251 loss 0.2341851145029068 train acc 0.9807398259233122\n",
            "epoch 38 batch id 4261 loss 0.031201884150505066 train acc 0.9807666920910585\n",
            "epoch 38 batch id 4271 loss 0.002131053479388356 train acc 0.9807861156637789\n",
            "epoch 38 batch id 4281 loss 0.14158646762371063 train acc 0.9807871992525111\n",
            "epoch 38 batch id 4291 loss 0.04429088160395622 train acc 0.9807664297366582\n",
            "epoch 38 batch id 4301 loss 0.02377261407673359 train acc 0.9807203266682167\n",
            "epoch 38 batch id 4311 loss 0.03465081751346588 train acc 0.980746926467177\n",
            "epoch 38 batch id 4321 loss 0.029016736894845963 train acc 0.9807480907197408\n",
            "epoch 38 batch id 4331 loss 0.12595385313034058 train acc 0.9807528573077812\n",
            "epoch 38 batch id 4341 loss 0.11775068193674088 train acc 0.9807432043307993\n",
            "epoch 38 batch id 4351 loss 0.03913036361336708 train acc 0.9807264134681682\n",
            "epoch 38 batch id 4361 loss 0.026218265295028687 train acc 0.9807311969731712\n",
            "epoch 38 batch id 4371 loss 0.03317265957593918 train acc 0.9807359585907115\n",
            "epoch 38 batch id 4381 loss 0.12939618527889252 train acc 0.980729998858708\n",
            "epoch 38 batch id 4391 loss 0.0013553759781643748 train acc 0.9807205078569802\n",
            "epoch 38 batch id 4401 loss 0.014200115576386452 train acc 0.9807323619631901\n",
            "epoch 38 batch id 4411 loss 0.005684847943484783 train acc 0.9807264509181591\n",
            "epoch 38 batch id 4421 loss 0.05853286385536194 train acc 0.980748840760009\n",
            "epoch 38 batch id 4431 loss 0.014849803410470486 train acc 0.980742919205597\n",
            "epoch 38 batch id 4441 loss 0.015755709260702133 train acc 0.9807475793740149\n",
            "epoch 38 batch id 4451 loss 0.0006997151067480445 train acc 0.9807487081554707\n",
            "epoch 38 batch id 4461 loss 0.003706129500642419 train acc 0.9807603396099529\n",
            "epoch 38 batch id 4471 loss 0.06500355899333954 train acc 0.9807544453142474\n",
            "epoch 38 batch id 4481 loss 0.010263843461871147 train acc 0.9807555512162464\n",
            "epoch 38 batch id 4491 loss 0.06948825716972351 train acc 0.9807566521932755\n",
            "epoch 38 batch id 4501 loss 0.009878667071461678 train acc 0.9807542768273717\n",
            "epoch 38 batch id 4511 loss 0.06359464675188065 train acc 0.9807449844823765\n",
            "epoch 38 batch id 4521 loss 0.009558145888149738 train acc 0.9807495576199956\n",
            "epoch 38 batch id 4531 loss 0.0008203959441743791 train acc 0.9807472136393732\n",
            "epoch 38 batch id 4541 loss 0.0663343071937561 train acc 0.9807483208544373\n",
            "epoch 38 batch id 4551 loss 0.09070237725973129 train acc 0.9807391232696111\n",
            "epoch 38 batch id 4561 loss 0.08787856251001358 train acc 0.9807333918000438\n",
            "epoch 38 batch id 4571 loss 0.030100753530859947 train acc 0.9807345219864362\n",
            "epoch 38 batch id 4581 loss 0.03189319744706154 train acc 0.9807424688932548\n",
            "epoch 38 batch id 4591 loss 0.018291259184479713 train acc 0.9807435743846656\n",
            "epoch 38 batch id 4601 loss 0.016015030443668365 train acc 0.9807378830688981\n",
            "epoch 38 batch id 4611 loss 0.0802096500992775 train acc 0.9807423823465625\n",
            "epoch 38 batch id 4621 loss 0.01939520053565502 train acc 0.9807333369400563\n",
            "epoch 38 batch id 4631 loss 0.1864626258611679 train acc 0.9807445746059167\n",
            "epoch 38 batch id 4641 loss 0.13064919412136078 train acc 0.9807490303813833\n",
            "epoch 38 batch id 4651 loss 0.11451563984155655 train acc 0.9807501075037627\n",
            "epoch 38 batch id 4661 loss 0.04003984481096268 train acc 0.9807645891439605\n",
            "epoch 38 batch id 4671 loss 0.0038643034640699625 train acc 0.9807823538856776\n",
            "epoch 38 batch id 4681 loss 0.25495645403862 train acc 0.9808000427259133\n",
            "epoch 38 batch id 4691 loss 0.00504562770947814 train acc 0.9807943402259646\n",
            "epoch 38 batch id 4701 loss 0.02029254660010338 train acc 0.980805280791321\n",
            "epoch 38 batch id 4711 loss 0.03928585350513458 train acc 0.9808161749097856\n",
            "epoch 38 batch id 4721 loss 0.021933795884251595 train acc 0.9807972357551367\n",
            "epoch 38 batch id 4731 loss 0.019198328256607056 train acc 0.98080810082435\n",
            "epoch 38 batch id 4741 loss 0.20107798278331757 train acc 0.9808057371862476\n",
            "epoch 38 batch id 4751 loss 0.130157932639122 train acc 0.9808132498421385\n",
            "epoch 38 batch id 4761 loss 0.08820055425167084 train acc 0.9808174490653224\n",
            "epoch 38 batch id 4771 loss 0.06849952787160873 train acc 0.9808347306644309\n",
            "epoch 38 batch id 4781 loss 0.07496466487646103 train acc 0.9808290629575402\n",
            "epoch 38 batch id 4791 loss 0.27587613463401794 train acc 0.9808103736171989\n",
            "epoch 38 batch id 4801 loss 0.06360606104135513 train acc 0.9808015257238075\n",
            "epoch 38 batch id 4811 loss 0.0029537114314734936 train acc 0.980795962377884\n",
            "epoch 38 batch id 4821 loss 0.0490579754114151 train acc 0.9808098682845883\n",
            "epoch 38 batch id 4831 loss 0.025795137509703636 train acc 0.9808140136617678\n",
            "epoch 38 batch id 4841 loss 0.002667289460077882 train acc 0.9808052313571576\n",
            "epoch 38 batch id 4851 loss 0.028243709355592728 train acc 0.980815811172954\n",
            "epoch 38 batch id 4861 loss 0.011723575182259083 train acc 0.9808134900226291\n",
            "epoch 38 batch id 4871 loss 0.0007098215282894671 train acc 0.9808272172038596\n",
            "epoch 38 batch id 4881 loss 0.030332664027810097 train acc 0.9808280833845523\n",
            "epoch 38 batch id 4891 loss 0.025681471452116966 train acc 0.9808289460233082\n",
            "epoch 38 batch id 4901 loss 0.032732777297496796 train acc 0.9808266170169353\n",
            "epoch 38 batch id 4911 loss 0.10854435712099075 train acc 0.9808338423946243\n",
            "epoch 38 batch id 4921 loss 0.20977407693862915 train acc 0.980837863239179\n",
            "epoch 38 batch id 4931 loss 0.06173112988471985 train acc 0.9808291928614885\n",
            "epoch 38 batch id 4941 loss 0.14866094291210175 train acc 0.9808110706334751\n",
            "epoch 38 batch id 4951 loss 0.10625943541526794 train acc 0.9808024893960816\n",
            "epoch 38 train acc 0.9807881875034385\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca45b269f4c94d8c851a2a3404a315e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 38 loss 0.953676164150238 test acc 0.8495257514662756\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "739452c4125743eb9ff2e4b9bf12fd0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 39 batch id 1 loss 0.02543468587100506 train acc 1.0\n",
            "epoch 39 batch id 11 loss 0.020199740305542946 train acc 0.9900568181818182\n",
            "epoch 39 batch id 21 loss 0.00382624426856637 train acc 0.9888392857142857\n",
            "epoch 39 batch id 31 loss 0.10635959357023239 train acc 0.983366935483871\n",
            "epoch 39 batch id 41 loss 0.015174668282270432 train acc 0.984375\n",
            "epoch 39 batch id 51 loss 0.011126292869448662 train acc 0.9837622549019608\n",
            "epoch 39 batch id 61 loss 0.0460546612739563 train acc 0.9810450819672131\n",
            "epoch 39 batch id 71 loss 0.0845659077167511 train acc 0.981294014084507\n",
            "epoch 39 batch id 81 loss 0.11420799046754837 train acc 0.9812885802469136\n",
            "epoch 39 batch id 91 loss 0.06260823458433151 train acc 0.9812843406593407\n",
            "epoch 39 batch id 101 loss 0.0267043374478817 train acc 0.9798886138613861\n",
            "epoch 39 batch id 111 loss 0.00020100860274396837 train acc 0.9794481981981982\n",
            "epoch 39 batch id 121 loss 0.0985301062464714 train acc 0.9795971074380165\n",
            "epoch 39 batch id 131 loss 0.020534951239824295 train acc 0.9799618320610687\n",
            "epoch 39 batch id 141 loss 0.023426389321684837 train acc 0.9800531914893617\n",
            "epoch 39 batch id 151 loss 0.075206458568573 train acc 0.980546357615894\n",
            "epoch 39 batch id 161 loss 0.004105512518435717 train acc 0.9812694099378882\n",
            "epoch 39 batch id 171 loss 0.005144092254340649 train acc 0.9814510233918129\n",
            "epoch 39 batch id 181 loss 0.03371451422572136 train acc 0.9815262430939227\n",
            "epoch 39 batch id 191 loss 0.06353487074375153 train acc 0.9812663612565445\n",
            "epoch 39 batch id 201 loss 0.03827594220638275 train acc 0.9813432835820896\n",
            "epoch 39 batch id 211 loss 0.00038595491787418723 train acc 0.9816350710900474\n",
            "epoch 39 batch id 221 loss 0.15342725813388824 train acc 0.9813348416289592\n",
            "epoch 39 batch id 231 loss 0.027937645092606544 train acc 0.981939935064935\n",
            "epoch 39 batch id 241 loss 0.137751966714859 train acc 0.9819113070539419\n",
            "epoch 39 batch id 251 loss 0.07705742865800858 train acc 0.9819472111553785\n",
            "epoch 39 batch id 261 loss 0.07049490511417389 train acc 0.9818606321839081\n",
            "epoch 39 batch id 271 loss 0.0002999815042130649 train acc 0.9820110701107011\n",
            "epoch 39 batch id 281 loss 0.0032955000642687082 train acc 0.9820395907473309\n",
            "epoch 39 batch id 291 loss 0.14118541777133942 train acc 0.9820661512027491\n",
            "epoch 39 batch id 301 loss 0.0821806862950325 train acc 0.9818313953488372\n",
            "epoch 39 batch id 311 loss 0.05901867896318436 train acc 0.981963424437299\n",
            "epoch 39 batch id 321 loss 0.2373645305633545 train acc 0.9819898753894081\n",
            "epoch 39 batch id 331 loss 0.005774185061454773 train acc 0.9818259063444109\n",
            "epoch 39 batch id 341 loss 0.1577593833208084 train acc 0.9817631964809385\n",
            "epoch 39 batch id 351 loss 0.0061105405911803246 train acc 0.9821492165242165\n",
            "epoch 39 batch id 361 loss 0.05874188244342804 train acc 0.9820810249307479\n",
            "epoch 39 batch id 371 loss 0.1653987020254135 train acc 0.9823534366576819\n",
            "epoch 39 batch id 381 loss 0.018052155151963234 train acc 0.9825705380577427\n",
            "epoch 39 batch id 391 loss 0.01880398951470852 train acc 0.9826166879795396\n",
            "epoch 39 batch id 401 loss 0.0001696556864771992 train acc 0.9825046758104738\n",
            "epoch 39 batch id 411 loss 0.0007712994120083749 train acc 0.9825121654501217\n",
            "epoch 39 batch id 421 loss 0.09186237305402756 train acc 0.9826306413301663\n",
            "epoch 39 batch id 431 loss 0.0593428798019886 train acc 0.982526102088167\n",
            "epoch 39 batch id 441 loss 0.07633280754089355 train acc 0.9824617346938775\n",
            "epoch 39 batch id 451 loss 0.036466460675001144 train acc 0.982434866962306\n",
            "epoch 39 batch id 461 loss 0.08224830776453018 train acc 0.982273590021692\n",
            "epoch 39 batch id 471 loss 0.011706114746630192 train acc 0.982484076433121\n",
            "epoch 39 batch id 481 loss 0.12137533724308014 train acc 0.98245841995842\n",
            "epoch 39 batch id 491 loss 0.045158255845308304 train acc 0.9823701629327902\n",
            "epoch 39 batch id 501 loss 0.18501363694667816 train acc 0.982378992015968\n",
            "epoch 39 batch id 511 loss 0.0993376076221466 train acc 0.9822345890410958\n",
            "epoch 39 batch id 521 loss 0.10315841436386108 train acc 0.9823056621880998\n",
            "epoch 39 batch id 531 loss 0.030065098777413368 train acc 0.9821092278719398\n",
            "epoch 39 batch id 541 loss 0.017636172473430634 train acc 0.9819489371534196\n",
            "epoch 39 batch id 551 loss 0.03565554320812225 train acc 0.9819646098003629\n",
            "epoch 39 batch id 561 loss 0.036718204617500305 train acc 0.9818126114081996\n",
            "epoch 39 batch id 571 loss 0.020481517538428307 train acc 0.9818574868651488\n",
            "epoch 39 batch id 581 loss 0.0796283632516861 train acc 0.9817663511187608\n",
            "epoch 39 batch id 591 loss 0.13481268286705017 train acc 0.9818104906937394\n",
            "epoch 39 batch id 601 loss 0.006142264697700739 train acc 0.981827163061564\n",
            "epoch 39 batch id 611 loss 0.07040427625179291 train acc 0.9818177168576104\n",
            "epoch 39 batch id 621 loss 0.21027640998363495 train acc 0.9818337359098228\n",
            "epoch 39 batch id 631 loss 0.03434636443853378 train acc 0.9818740095087163\n",
            "epoch 39 batch id 641 loss 0.033247772604227066 train acc 0.9819130265210608\n",
            "epoch 39 batch id 651 loss 0.017881078645586967 train acc 0.981902841781874\n",
            "epoch 39 batch id 661 loss 0.07024233043193817 train acc 0.9820111573373677\n",
            "epoch 39 batch id 671 loss 0.009315104223787785 train acc 0.9819998137108793\n",
            "epoch 39 batch id 681 loss 0.08922915160655975 train acc 0.9820346916299559\n",
            "epoch 39 batch id 691 loss 0.03462757170200348 train acc 0.9820007235890015\n",
            "epoch 39 batch id 701 loss 0.07076708972454071 train acc 0.9817894079885877\n",
            "epoch 39 batch id 711 loss 0.1781739592552185 train acc 0.9817378691983122\n",
            "epoch 39 batch id 721 loss 0.05025424435734749 train acc 0.9817527739251041\n",
            "epoch 39 batch id 731 loss 0.008613415993750095 train acc 0.9817458960328317\n",
            "epoch 39 batch id 741 loss 0.05029638484120369 train acc 0.9817813765182186\n",
            "epoch 39 batch id 751 loss 0.02736804075539112 train acc 0.9816286617842876\n",
            "epoch 39 batch id 761 loss 0.0011618159478530288 train acc 0.9816236859395532\n",
            "epoch 39 batch id 771 loss 0.040219370275735855 train acc 0.9815783073929961\n",
            "epoch 39 batch id 781 loss 0.01838706061244011 train acc 0.9816341229193342\n",
            "epoch 39 batch id 791 loss 0.009609181433916092 train acc 0.9817477876106194\n",
            "epoch 39 batch id 801 loss 0.03566060587763786 train acc 0.9817415730337079\n",
            "epoch 39 batch id 811 loss 0.11149384826421738 train acc 0.9816969790382244\n",
            "epoch 39 batch id 821 loss 0.008171028457581997 train acc 0.9817866930572473\n",
            "epoch 39 batch id 831 loss 0.1534804105758667 train acc 0.9817050240673887\n",
            "epoch 39 batch id 841 loss 0.007577965036034584 train acc 0.9816624554102259\n",
            "epoch 39 batch id 851 loss 0.05332428216934204 train acc 0.9816208871915394\n",
            "epoch 39 batch id 861 loss 0.04030958563089371 train acc 0.9815802845528455\n",
            "epoch 39 batch id 871 loss 0.029056573286652565 train acc 0.9815226750861079\n",
            "epoch 39 batch id 881 loss 0.07372327148914337 train acc 0.9813954313280363\n",
            "epoch 39 batch id 891 loss 0.0017196759581565857 train acc 0.9814288720538721\n",
            "epoch 39 batch id 901 loss 0.0629877895116806 train acc 0.9814442286348501\n",
            "epoch 39 batch id 911 loss 0.038279276341199875 train acc 0.981476399560922\n",
            "epoch 39 batch id 921 loss 0.0035087396390736103 train acc 0.9814400108577633\n",
            "epoch 39 batch id 931 loss 0.0025129560381174088 train acc 0.9814715359828142\n",
            "epoch 39 batch id 941 loss 0.01572176069021225 train acc 0.9815356004250797\n",
            "epoch 39 batch id 951 loss 0.03888797014951706 train acc 0.9814340168243953\n",
            "epoch 39 batch id 961 loss 0.020101003348827362 train acc 0.9815133975026015\n",
            "epoch 39 batch id 971 loss 0.09052350372076035 train acc 0.9814785015447992\n",
            "epoch 39 batch id 981 loss 0.01324574463069439 train acc 0.9815876656472987\n",
            "epoch 39 batch id 991 loss 0.13397112488746643 train acc 0.9816157921291625\n",
            "epoch 39 batch id 1001 loss 0.08556123822927475 train acc 0.9816121378621379\n",
            "epoch 39 batch id 1011 loss 0.019548645243048668 train acc 0.9815776458951533\n",
            "epoch 39 batch id 1021 loss 0.04437053203582764 train acc 0.9816050440744368\n",
            "epoch 39 batch id 1031 loss 0.006477517541497946 train acc 0.9816925315227935\n",
            "epoch 39 batch id 1041 loss 0.09676156938076019 train acc 0.9817333093179635\n",
            "epoch 39 batch id 1051 loss 0.023927809670567513 train acc 0.9816246431969553\n",
            "epoch 39 batch id 1061 loss 0.17253877222537994 train acc 0.9815474787935909\n",
            "epoch 39 batch id 1071 loss 0.0003193911979906261 train acc 0.9815592903828197\n",
            "epoch 39 batch id 1081 loss 0.03544116020202637 train acc 0.981484158186864\n",
            "epoch 39 batch id 1091 loss 0.3112979531288147 train acc 0.9814676901924839\n",
            "epoch 39 batch id 1101 loss 0.15539254248142242 train acc 0.9814657129881925\n",
            "epoch 39 batch id 1111 loss 0.0578991174697876 train acc 0.9814215796579658\n",
            "epoch 39 batch id 1121 loss 0.005785584449768066 train acc 0.9814618644067796\n",
            "epoch 39 batch id 1131 loss 0.023187005892395973 train acc 0.9814461759504863\n",
            "epoch 39 batch id 1141 loss 0.02812352403998375 train acc 0.9814855390008764\n",
            "epoch 39 batch id 1151 loss 0.028589066118001938 train acc 0.981402041702867\n",
            "epoch 39 batch id 1161 loss 0.018407614901661873 train acc 0.9814141903531438\n",
            "epoch 39 batch id 1171 loss 0.00995742529630661 train acc 0.9813861016225448\n",
            "epoch 39 batch id 1181 loss 0.05201105400919914 train acc 0.9814114098221846\n",
            "epoch 39 batch id 1191 loss 0.04439525306224823 train acc 0.9813706968933669\n",
            "epoch 39 batch id 1201 loss 0.008003674447536469 train acc 0.9814347418817652\n",
            "epoch 39 batch id 1211 loss 0.0002494761429261416 train acc 0.9814461189099918\n",
            "epoch 39 batch id 1221 loss 0.10722202062606812 train acc 0.9813677313677314\n",
            "epoch 39 batch id 1231 loss 0.0936465933918953 train acc 0.9813286961819658\n",
            "epoch 39 batch id 1241 loss 0.007135407067835331 train acc 0.9813280620467365\n",
            "epoch 39 batch id 1251 loss 0.19764187932014465 train acc 0.9813024580335732\n",
            "epoch 39 batch id 1261 loss 0.02161503955721855 train acc 0.9813392149088025\n",
            "epoch 39 batch id 1271 loss 0.11889137327671051 train acc 0.9813508064516129\n",
            "epoch 39 batch id 1281 loss 0.010987387970089912 train acc 0.9813500195160031\n",
            "epoch 39 batch id 1291 loss 0.05970490351319313 train acc 0.981397656855151\n",
            "epoch 39 batch id 1301 loss 0.04420379549264908 train acc 0.9813244619523444\n",
            "epoch 39 batch id 1311 loss 0.01922232285141945 train acc 0.9813119755911518\n",
            "epoch 39 batch id 1321 loss 0.012755008414387703 train acc 0.9812996782740349\n",
            "epoch 39 batch id 1331 loss 0.018232231959700584 train acc 0.9812640871525169\n",
            "epoch 39 batch id 1341 loss 0.09474672377109528 train acc 0.9812756338553319\n",
            "epoch 39 batch id 1351 loss 0.002262225141748786 train acc 0.9813217061435974\n",
            "epoch 39 batch id 1361 loss 0.0010550911538302898 train acc 0.9813096987509184\n",
            "epoch 39 batch id 1371 loss 0.00819443166255951 train acc 0.9813206601021153\n",
            "epoch 39 batch id 1381 loss 0.18217533826828003 train acc 0.9813654055032585\n",
            "epoch 39 batch id 1391 loss 0.00903458334505558 train acc 0.9813533429187635\n",
            "epoch 39 batch id 1401 loss 0.014219149015843868 train acc 0.9813749107780158\n",
            "epoch 39 batch id 1411 loss 0.03488720953464508 train acc 0.9814072466335932\n",
            "epoch 39 batch id 1421 loss 0.005006329622119665 train acc 0.9814501231527094\n",
            "epoch 39 batch id 1431 loss 0.01705583557486534 train acc 0.9814487246680643\n",
            "epoch 39 batch id 1441 loss 0.01992006041109562 train acc 0.9814148160999306\n",
            "epoch 39 batch id 1451 loss 0.009513289667665958 train acc 0.981467522398346\n",
            "epoch 39 batch id 1461 loss 0.014184556901454926 train acc 0.9814660335386721\n",
            "epoch 39 batch id 1471 loss 0.01854071207344532 train acc 0.9815389191026512\n",
            "epoch 39 batch id 1481 loss 0.06118398904800415 train acc 0.9815475185685347\n",
            "epoch 39 batch id 1491 loss 0.05578488111495972 train acc 0.9815979208584842\n",
            "epoch 39 batch id 1501 loss 0.057160601019859314 train acc 0.981647651565623\n",
            "epoch 39 batch id 1511 loss 0.08955741673707962 train acc 0.9816657015221707\n",
            "epoch 39 batch id 1521 loss 0.021155457943677902 train acc 0.9816629684418146\n",
            "epoch 39 batch id 1531 loss 0.025529449805617332 train acc 0.9816806825604181\n",
            "epoch 39 batch id 1541 loss 0.00561522226780653 train acc 0.9816778877352369\n",
            "epoch 39 batch id 1551 loss 0.039296653121709824 train acc 0.9817254996776273\n",
            "epoch 39 batch id 1561 loss 0.033617276698350906 train acc 0.9817224535554132\n",
            "epoch 39 batch id 1571 loss 0.04005955159664154 train acc 0.9817194462126034\n",
            "epoch 39 batch id 1581 loss 0.025986991822719574 train acc 0.981676944971537\n",
            "epoch 39 batch id 1591 loss 0.16873612999916077 train acc 0.9816644406033941\n",
            "epoch 39 batch id 1601 loss 0.030496034771203995 train acc 0.9817204091193005\n",
            "epoch 39 batch id 1611 loss 0.05757643282413483 train acc 0.981659295468653\n",
            "epoch 39 batch id 1621 loss 0.07477463781833649 train acc 0.9816278531770513\n",
            "epoch 39 batch id 1631 loss 0.0035459513310343027 train acc 0.9816638565297363\n",
            "epoch 39 batch id 1641 loss 0.07107409089803696 train acc 0.9816042047531993\n",
            "epoch 39 batch id 1651 loss 0.046163614839315414 train acc 0.9815642035130224\n",
            "epoch 39 batch id 1661 loss 0.025263726711273193 train acc 0.9816093467790488\n",
            "epoch 39 batch id 1671 loss 0.09675776958465576 train acc 0.9815791442250149\n",
            "epoch 39 batch id 1681 loss 0.0008921806584112346 train acc 0.9815957763236169\n",
            "epoch 39 batch id 1691 loss 0.014686807058751583 train acc 0.9816029716144293\n",
            "epoch 39 batch id 1701 loss 0.04746522009372711 train acc 0.9815917107583775\n",
            "epoch 39 batch id 1711 loss 0.02325771003961563 train acc 0.98156231735827\n",
            "epoch 39 batch id 1721 loss 0.020881788805127144 train acc 0.9815605026147589\n",
            "epoch 39 batch id 1731 loss 0.0004003536014351994 train acc 0.9815857885615251\n",
            "epoch 39 batch id 1741 loss 0.013870522379875183 train acc 0.9816018093049971\n",
            "epoch 39 batch id 1751 loss 0.041658271104097366 train acc 0.9815998001142204\n",
            "epoch 39 batch id 1761 loss 0.17847424745559692 train acc 0.9815978137421919\n",
            "epoch 39 batch id 1771 loss 0.0009103468037210405 train acc 0.9816487859966121\n",
            "epoch 39 batch id 1781 loss 0.00016310144565068185 train acc 0.9816728663672094\n",
            "epoch 39 batch id 1791 loss 0.09973468631505966 train acc 0.9816443327749861\n",
            "epoch 39 batch id 1801 loss 0.030299128964543343 train acc 0.9816855219322599\n",
            "epoch 39 batch id 1811 loss 0.042862117290496826 train acc 0.9816744892324683\n",
            "epoch 39 batch id 1821 loss 0.08684857934713364 train acc 0.9816292559033498\n",
            "epoch 39 batch id 1831 loss 0.07674967497587204 train acc 0.9815930502457674\n",
            "epoch 39 batch id 1841 loss 0.0013237874954938889 train acc 0.9815487506789788\n",
            "epoch 39 batch id 1851 loss 0.047163888812065125 train acc 0.9815724608319827\n",
            "epoch 39 batch id 1861 loss 0.021278144791722298 train acc 0.9815875201504567\n",
            "epoch 39 batch id 1871 loss 0.11908450722694397 train acc 0.9816525253874934\n",
            "epoch 39 batch id 1881 loss 0.019852420315146446 train acc 0.9816669989367358\n",
            "epoch 39 batch id 1891 loss 0.026765605434775352 train acc 0.9816647937599154\n",
            "epoch 39 batch id 1901 loss 0.08114946633577347 train acc 0.9816872698579695\n",
            "epoch 39 batch id 1911 loss 0.034044522792100906 train acc 0.9816032182103611\n",
            "epoch 39 batch id 1921 loss 0.14936673641204834 train acc 0.9815607105674128\n",
            "epoch 39 batch id 1931 loss 0.017163483425974846 train acc 0.9815752848265148\n",
            "epoch 39 batch id 1941 loss 0.002328400732949376 train acc 0.981541409067491\n",
            "epoch 39 batch id 1951 loss 0.034605976194143295 train acc 0.9815238980010251\n",
            "epoch 39 batch id 1961 loss 0.03787263110280037 train acc 0.9815623406425293\n",
            "epoch 39 batch id 1971 loss 0.01141555979847908 train acc 0.9815766108574328\n",
            "epoch 39 batch id 1981 loss 0.07600533217191696 train acc 0.9815512998485614\n",
            "epoch 39 batch id 1991 loss 0.11430268734693527 train acc 0.9814634605725766\n",
            "epoch 39 batch id 2001 loss 0.0005212313262745738 train acc 0.9814858195902049\n",
            "epoch 39 batch id 2011 loss 0.19876955449581146 train acc 0.9814613376429637\n",
            "epoch 39 batch id 2021 loss 0.07496696710586548 train acc 0.981421635329045\n",
            "epoch 39 batch id 2031 loss 0.0002327472175238654 train acc 0.9814130969965534\n",
            "epoch 39 batch id 2041 loss 0.03772253915667534 train acc 0.9813816756491915\n",
            "epoch 39 batch id 2051 loss 0.19973383843898773 train acc 0.9813810336421258\n",
            "epoch 39 batch id 2061 loss 0.07849791646003723 train acc 0.9813424915089762\n",
            "epoch 39 batch id 2071 loss 0.07282832264900208 train acc 0.9813495895702559\n",
            "epoch 39 batch id 2081 loss 0.04562605917453766 train acc 0.9813716362325805\n",
            "epoch 39 batch id 2091 loss 0.036018963903188705 train acc 0.9813710545193687\n",
            "epoch 39 batch id 2101 loss 0.010386612266302109 train acc 0.9814225368871966\n",
            "epoch 39 batch id 2111 loss 0.012074325233697891 train acc 0.9814143178588347\n",
            "epoch 39 batch id 2121 loss 0.01362796314060688 train acc 0.9814356435643564\n",
            "epoch 39 batch id 2131 loss 0.0002357381017645821 train acc 0.9814860980760206\n",
            "epoch 39 batch id 2141 loss 0.04587266221642494 train acc 0.9814703993460999\n",
            "epoch 39 batch id 2151 loss 0.017150811851024628 train acc 0.9814911668991166\n",
            "epoch 39 batch id 2161 loss 0.08419981598854065 train acc 0.981461129106895\n",
            "epoch 39 batch id 2171 loss 0.001984004396945238 train acc 0.9814457623215108\n",
            "epoch 39 batch id 2181 loss 0.06986391544342041 train acc 0.9814806854653828\n",
            "epoch 39 batch id 2191 loss 0.05765535309910774 train acc 0.9814867640346874\n",
            "epoch 39 batch id 2201 loss 0.01632484421133995 train acc 0.981506985461154\n",
            "epoch 39 batch id 2211 loss 0.0001655949163250625 train acc 0.9815128900949797\n",
            "epoch 39 batch id 2221 loss 0.018873747438192368 train acc 0.9814976361999099\n",
            "epoch 39 batch id 2231 loss 0.009327991865575314 train acc 0.9814965262214254\n",
            "epoch 39 batch id 2241 loss 0.07063240557909012 train acc 0.9814884538152611\n",
            "epoch 39 batch id 2251 loss 0.003749686758965254 train acc 0.9814804531319413\n",
            "epoch 39 batch id 2261 loss 0.00014432544412557036 train acc 0.981500165855816\n",
            "epoch 39 batch id 2271 loss 0.004078835714608431 train acc 0.9815059445178336\n",
            "epoch 39 batch id 2281 loss 0.06618385016918182 train acc 0.9815048224462954\n",
            "epoch 39 batch id 2291 loss 0.03190691024065018 train acc 0.9814900698384985\n",
            "epoch 39 batch id 2301 loss 0.01885850541293621 train acc 0.9815297696653629\n",
            "epoch 39 batch id 2311 loss 0.05267126113176346 train acc 0.9815623647771528\n",
            "epoch 39 batch id 2321 loss 0.07394284754991531 train acc 0.9815273588970271\n",
            "epoch 39 batch id 2331 loss 0.1398543417453766 train acc 0.9815060596310596\n",
            "epoch 39 batch id 2341 loss 0.007848703302443027 train acc 0.9815316638188808\n",
            "epoch 39 batch id 2351 loss 0.034080080687999725 train acc 0.981510527435134\n",
            "epoch 39 batch id 2361 loss 0.07379721105098724 train acc 0.9815358958068615\n",
            "epoch 39 batch id 2371 loss 0.004240970592945814 train acc 0.9815742302825812\n",
            "epoch 39 batch id 2381 loss 0.000980960438027978 train acc 0.9816056803863923\n",
            "epoch 39 batch id 2391 loss 0.015739647671580315 train acc 0.9816172626516102\n",
            "epoch 39 batch id 2401 loss 0.01963759958744049 train acc 0.9816092253227822\n",
            "epoch 39 batch id 2411 loss 0.020153582096099854 train acc 0.9816012546661137\n",
            "epoch 39 batch id 2421 loss 0.03344212844967842 train acc 0.9816320735233375\n",
            "epoch 39 batch id 2431 loss 0.11991843581199646 train acc 0.981604792266557\n",
            "epoch 39 batch id 2441 loss 0.00020682568720076233 train acc 0.9816097398607129\n",
            "epoch 39 batch id 2451 loss 0.052423615008592606 train acc 0.9815827723378213\n",
            "epoch 39 batch id 2461 loss 0.08397470414638519 train acc 0.9815560239739943\n",
            "epoch 39 batch id 2471 loss 0.07680053263902664 train acc 0.9815547855119385\n",
            "epoch 39 batch id 2481 loss 0.09154194593429565 train acc 0.9815724506247481\n",
            "epoch 39 batch id 2491 loss 0.08053724467754364 train acc 0.9815523384183059\n",
            "epoch 39 batch id 2501 loss 0.001591572305187583 train acc 0.9815511295481807\n",
            "epoch 39 batch id 2511 loss 0.0247041042894125 train acc 0.9815312624452409\n",
            "epoch 39 batch id 2521 loss 0.06371337920427322 train acc 0.9815053550178501\n",
            "epoch 39 batch id 2531 loss 0.02354217693209648 train acc 0.9815166930067167\n",
            "epoch 39 batch id 2541 loss 0.01934117078781128 train acc 0.9815156434474617\n",
            "epoch 39 batch id 2551 loss 0.02938421256840229 train acc 0.9815574774598197\n",
            "epoch 39 batch id 2561 loss 0.02216002158820629 train acc 0.9815623779773526\n",
            "epoch 39 batch id 2571 loss 0.08467548340559006 train acc 0.9815672403733956\n",
            "epoch 39 batch id 2581 loss 0.06021334230899811 train acc 0.9815660112359551\n",
            "epoch 39 batch id 2591 loss 0.019414613023400307 train acc 0.9815708220764183\n",
            "epoch 39 batch id 2601 loss 0.09468049556016922 train acc 0.9815996251441753\n",
            "epoch 39 batch id 2611 loss 0.0007084247190505266 train acc 0.9815982860972807\n",
            "epoch 39 batch id 2621 loss 0.05858844518661499 train acc 0.9815969572682183\n",
            "epoch 39 batch id 2631 loss 0.19421489536762238 train acc 0.9815718833143292\n",
            "epoch 39 batch id 2641 loss 0.03039863333106041 train acc 0.9815884134797426\n",
            "epoch 39 batch id 2651 loss 0.013797062449157238 train acc 0.9815694549226707\n",
            "epoch 39 batch id 2661 loss 0.0031954178120940924 train acc 0.9815565107102593\n",
            "epoch 39 batch id 2671 loss 0.016219282522797585 train acc 0.9815729127667541\n",
            "epoch 39 batch id 2681 loss 0.022859932854771614 train acc 0.9815367400223797\n",
            "epoch 39 batch id 2691 loss 0.026812948286533356 train acc 0.9815298680787812\n",
            "epoch 39 batch id 2701 loss 0.03666945546865463 train acc 0.9815346168085894\n",
            "epoch 39 batch id 2711 loss 0.06164098531007767 train acc 0.9814989856141645\n",
            "epoch 39 batch id 2721 loss 0.011802238412201405 train acc 0.9814578739434031\n",
            "epoch 39 batch id 2731 loss 0.0022725469898432493 train acc 0.9814857195166605\n",
            "epoch 39 batch id 2741 loss 0.07654613256454468 train acc 0.9814848595403137\n",
            "epoch 39 batch id 2751 loss 0.0997026115655899 train acc 0.9814669665576155\n",
            "epoch 39 batch id 2761 loss 0.008838016539812088 train acc 0.981488817457443\n",
            "epoch 39 batch id 2771 loss 0.13780653476715088 train acc 0.9814766780945507\n",
            "epoch 39 batch id 2781 loss 0.03990088403224945 train acc 0.9814927184466019\n",
            "epoch 39 batch id 2791 loss 0.02197272703051567 train acc 0.9815254389107847\n",
            "epoch 39 batch id 2801 loss 0.1266193389892578 train acc 0.9815411906461978\n",
            "epoch 39 batch id 2811 loss 0.025512777268886566 train acc 0.9815568303094984\n",
            "epoch 39 batch id 2821 loss 0.07409289479255676 train acc 0.9815889755405884\n",
            "epoch 39 batch id 2831 loss 0.013117496855556965 train acc 0.9815877781702579\n",
            "epoch 39 batch id 2841 loss 0.09671337902545929 train acc 0.9815920890531503\n",
            "epoch 39 batch id 2851 loss 0.029711859300732613 train acc 0.9815963696948439\n",
            "epoch 39 batch id 2861 loss 0.0690668523311615 train acc 0.9815842362810207\n",
            "epoch 39 batch id 2871 loss 0.0694088265299797 train acc 0.981593956809474\n",
            "epoch 39 batch id 2881 loss 0.04495898261666298 train acc 0.9815656456091635\n",
            "epoch 39 batch id 2891 loss 0.0013673164648935199 train acc 0.9815699584918713\n",
            "epoch 39 batch id 2901 loss 0.018747679889202118 train acc 0.9815957859358841\n",
            "epoch 39 batch id 2911 loss 0.05502218008041382 train acc 0.9815892305049811\n",
            "epoch 39 batch id 2921 loss 0.002750293118879199 train acc 0.9815666723724752\n",
            "epoch 39 batch id 2931 loss 0.009619117714464664 train acc 0.9815495991129307\n",
            "epoch 39 batch id 2941 loss 0.007399220019578934 train acc 0.9815273291397484\n",
            "epoch 39 batch id 2951 loss 0.0288335420191288 train acc 0.9815422738054896\n",
            "epoch 39 batch id 2961 loss 0.018036697059869766 train acc 0.9815465636609254\n",
            "epoch 39 batch id 2971 loss 0.02025700733065605 train acc 0.981545565466173\n",
            "epoch 39 batch id 2981 loss 0.07583737373352051 train acc 0.9815288493794029\n",
            "epoch 39 batch id 2991 loss 0.06679556518793106 train acc 0.9815226930792377\n",
            "epoch 39 batch id 3001 loss 0.0005337128532119095 train acc 0.9815217844051982\n",
            "epoch 39 batch id 3011 loss 0.041574664413928986 train acc 0.9815156924609765\n",
            "epoch 39 batch id 3021 loss 0.04101664945483208 train acc 0.9815148129758359\n",
            "epoch 39 batch id 3031 loss 0.2732527256011963 train acc 0.9815087842296272\n",
            "epoch 39 batch id 3041 loss 0.06007677689194679 train acc 0.9814873807957909\n",
            "epoch 39 batch id 3051 loss 0.02603975310921669 train acc 0.9814917240249099\n",
            "epoch 39 batch id 3061 loss 0.08526519685983658 train acc 0.9814807252531852\n",
            "epoch 39 batch id 3071 loss 0.07687362283468246 train acc 0.9814901497883426\n",
            "epoch 39 batch id 3081 loss 0.012067153118550777 train acc 0.9815197987666342\n",
            "epoch 39 batch id 3091 loss 0.021419761702418327 train acc 0.9815593659010029\n",
            "epoch 39 batch id 3101 loss 0.05729692429304123 train acc 0.9815533295711061\n",
            "epoch 39 batch id 3111 loss 0.06831205636262894 train acc 0.9815523545483767\n",
            "epoch 39 batch id 3121 loss 0.11800803244113922 train acc 0.9815513857737904\n",
            "epoch 39 batch id 3131 loss 0.008192175067961216 train acc 0.9815554136058767\n",
            "epoch 39 batch id 3141 loss 0.17791840434074402 train acc 0.9815444921999363\n",
            "epoch 39 batch id 3151 loss 0.039158884435892105 train acc 0.9815385988575056\n",
            "epoch 39 batch id 3161 loss 0.06688994914293289 train acc 0.9815277997469155\n",
            "epoch 39 batch id 3171 loss 0.022584635764360428 train acc 0.9815072138126774\n",
            "epoch 39 batch id 3181 loss 0.03410176560282707 train acc 0.9814671093995598\n",
            "epoch 39 batch id 3191 loss 0.017078755423426628 train acc 0.9814958085239737\n",
            "epoch 39 batch id 3201 loss 0.0946216881275177 train acc 0.9814608716026242\n",
            "epoch 39 batch id 3211 loss 0.12671148777008057 train acc 0.9814407505450016\n",
            "epoch 39 batch id 3221 loss 0.1144263967871666 train acc 0.9813819466004347\n",
            "epoch 39 batch id 3231 loss 0.013484824448823929 train acc 0.9813573584029712\n",
            "epoch 39 batch id 3241 loss 0.016917262226343155 train acc 0.9813714902807775\n",
            "epoch 39 batch id 3251 loss 0.029769932851195335 train acc 0.9813615041525684\n",
            "epoch 39 batch id 3261 loss 0.11038729548454285 train acc 0.9813324133701319\n",
            "epoch 39 batch id 3271 loss 0.028305869549512863 train acc 0.981327384591868\n",
            "epoch 39 batch id 3281 loss 0.0010589273879304528 train acc 0.9813319110027431\n",
            "epoch 39 batch id 3291 loss 0.009233182296156883 train acc 0.9813269143117593\n",
            "epoch 39 batch id 3301 loss 0.023755604401230812 train acc 0.9813219478945774\n",
            "epoch 39 batch id 3311 loss 0.013086733408272266 train acc 0.9813264497130776\n",
            "epoch 39 batch id 3321 loss 0.015578296966850758 train acc 0.9813356293285155\n",
            "epoch 39 batch id 3331 loss 0.02204224094748497 train acc 0.9813166091263885\n",
            "epoch 39 batch id 3341 loss 0.05146636441349983 train acc 0.9813257632445376\n",
            "epoch 39 batch id 3351 loss 0.021748730912804604 train acc 0.9812928976424948\n",
            "epoch 39 batch id 3361 loss 0.021916396915912628 train acc 0.9812927700089259\n",
            "epoch 39 batch id 3371 loss 0.04277610033750534 train acc 0.9812880080094928\n",
            "epoch 39 batch id 3381 loss 0.00030997119029052556 train acc 0.9813017598343685\n",
            "epoch 39 batch id 3391 loss 0.1326800286769867 train acc 0.9813062149808316\n",
            "epoch 39 batch id 3401 loss 0.0566951222717762 train acc 0.981296861217289\n",
            "epoch 39 batch id 3411 loss 0.05225412920117378 train acc 0.9813013046027558\n",
            "epoch 39 batch id 3421 loss 0.00698016956448555 train acc 0.9813194241449869\n",
            "epoch 39 batch id 3431 loss 0.13368390500545502 train acc 0.9813419921305742\n",
            "epoch 39 batch id 3441 loss 0.0005031289183534682 train acc 0.9813553472827666\n",
            "epoch 39 batch id 3451 loss 0.01085086353123188 train acc 0.9813505143436685\n",
            "epoch 39 batch id 3461 loss 0.011082046665251255 train acc 0.9813457093325628\n",
            "epoch 39 batch id 3471 loss 0.054275572299957275 train acc 0.9813499351771824\n",
            "epoch 39 batch id 3481 loss 0.08693698048591614 train acc 0.9813406707842574\n",
            "epoch 39 batch id 3491 loss 0.0013240078696981072 train acc 0.9813404110570038\n",
            "epoch 39 batch id 3501 loss 0.020541833713650703 train acc 0.9813446158240503\n",
            "epoch 39 batch id 3511 loss 0.0628616139292717 train acc 0.9813309954428938\n",
            "epoch 39 batch id 3521 loss 0.14363394677639008 train acc 0.9813307654075547\n",
            "epoch 39 batch id 3531 loss 0.017213132232427597 train acc 0.9813438119512886\n",
            "epoch 39 batch id 3541 loss 0.05167918652296066 train acc 0.9813391344253036\n",
            "epoch 39 batch id 3551 loss 0.013558161444962025 train acc 0.9813080822303576\n",
            "epoch 39 batch id 3561 loss 0.03873785585165024 train acc 0.9813342459983151\n",
            "epoch 39 batch id 3571 loss 0.0019972599111497402 train acc 0.9813515121814618\n",
            "epoch 39 batch id 3581 loss 0.06350165605545044 train acc 0.981355592013404\n",
            "epoch 39 batch id 3591 loss 0.04637977480888367 train acc 0.981359649122807\n",
            "epoch 39 batch id 3601 loss 0.05988496541976929 train acc 0.9813376492640933\n",
            "epoch 39 batch id 3611 loss 0.0010118683567270637 train acc 0.9813374065355858\n",
            "epoch 39 batch id 3621 loss 0.04864497482776642 train acc 0.9813414802540734\n",
            "epoch 39 batch id 3631 loss 0.011262561194598675 train acc 0.9813283186450014\n",
            "epoch 39 batch id 3641 loss 0.020673347637057304 train acc 0.9813366863499039\n",
            "epoch 39 batch id 3651 loss 0.04769672453403473 train acc 0.9813150506710491\n",
            "epoch 39 batch id 3661 loss 0.015367371961474419 train acc 0.9813148729855231\n",
            "epoch 39 batch id 3671 loss 0.05141979455947876 train acc 0.9813359779351676\n",
            "epoch 39 batch id 3681 loss 0.00946752354502678 train acc 0.9813697025264874\n",
            "epoch 39 batch id 3691 loss 0.04917299374938011 train acc 0.9813693782172853\n",
            "epoch 39 batch id 3701 loss 0.037406131625175476 train acc 0.9813479465009457\n",
            "epoch 39 batch id 3711 loss 0.03539825230836868 train acc 0.9813518930207491\n",
            "epoch 39 batch id 3721 loss 0.05887502059340477 train acc 0.981347420048374\n",
            "epoch 39 batch id 3731 loss 0.10019898414611816 train acc 0.9813513468239078\n",
            "epoch 39 batch id 3741 loss 0.04483116418123245 train acc 0.9813385458433574\n",
            "epoch 39 batch id 3751 loss 0.003827227745205164 train acc 0.9813591375633165\n",
            "epoch 39 batch id 3761 loss 0.13640186190605164 train acc 0.981333920499867\n",
            "epoch 39 batch id 3771 loss 0.03905899450182915 train acc 0.9813005502519225\n",
            "epoch 39 batch id 3781 loss 0.08368092775344849 train acc 0.9813004165564665\n",
            "epoch 39 batch id 3791 loss 0.0001664177980273962 train acc 0.9813126483777368\n",
            "epoch 39 batch id 3801 loss 0.03841147944331169 train acc 0.9812919297553275\n",
            "epoch 39 batch id 3811 loss 0.013460365124046803 train acc 0.9813082196273943\n",
            "epoch 39 batch id 3821 loss 0.029397305101156235 train acc 0.9813080672598796\n",
            "epoch 39 batch id 3831 loss 0.007070345804095268 train acc 0.9812997585486818\n",
            "epoch 39 batch id 3841 loss 0.01887899823486805 train acc 0.9812792892475918\n",
            "epoch 39 batch id 3851 loss 0.00035916033084504306 train acc 0.9812751558036874\n",
            "epoch 39 batch id 3861 loss 0.14659063518047333 train acc 0.9812589031339032\n",
            "epoch 39 batch id 3871 loss 0.15796397626399994 train acc 0.9812629165590286\n",
            "epoch 39 batch id 3881 loss 0.04510103166103363 train acc 0.9812588572532852\n",
            "epoch 39 batch id 3891 loss 0.008137349970638752 train acc 0.9812467874582369\n",
            "epoch 39 batch id 3901 loss 0.011906185187399387 train acc 0.9812347795437067\n",
            "epoch 39 batch id 3911 loss 0.021734045818448067 train acc 0.9812268281769368\n",
            "epoch 39 batch id 3921 loss 0.009987913072109222 train acc 0.981234857179291\n",
            "epoch 39 batch id 3931 loss 0.06211712211370468 train acc 0.981238870516408\n",
            "epoch 39 batch id 3941 loss 0.0016541131772100925 train acc 0.9812587224054808\n",
            "epoch 39 batch id 3951 loss 0.02531050331890583 train acc 0.9812547456340167\n",
            "epoch 39 batch id 3961 loss 0.007703215815126896 train acc 0.981262623074981\n",
            "epoch 39 batch id 3971 loss 0.03950491547584534 train acc 0.9812389826240242\n",
            "epoch 39 batch id 3981 loss 0.013413919135928154 train acc 0.9812272356191911\n",
            "epoch 39 batch id 3991 loss 0.13660942018032074 train acc 0.9812429528940115\n",
            "epoch 39 batch id 4001 loss 0.06464266777038574 train acc 0.9812312546863284\n",
            "epoch 39 batch id 4011 loss 0.07872916013002396 train acc 0.981207928197457\n",
            "epoch 39 batch id 4021 loss 0.017915189266204834 train acc 0.981211918676946\n",
            "epoch 39 batch id 4031 loss 0.001324535347521305 train acc 0.9812391466137435\n",
            "epoch 39 batch id 4041 loss 0.031281325966119766 train acc 0.981227573620391\n",
            "epoch 39 batch id 4051 loss 0.02592608705163002 train acc 0.9812314860528265\n",
            "epoch 39 batch id 4061 loss 0.01567012630403042 train acc 0.9812430743659197\n",
            "epoch 39 batch id 4071 loss 0.07930357754230499 train acc 0.9812315770081061\n",
            "epoch 39 batch id 4081 loss 0.008013700135052204 train acc 0.981258423180593\n",
            "epoch 39 batch id 4091 loss 0.07971206307411194 train acc 0.9812469445123442\n",
            "epoch 39 batch id 4101 loss 0.10157093405723572 train acc 0.9812126615459644\n",
            "epoch 39 batch id 4111 loss 0.015343891456723213 train acc 0.9811823461444904\n",
            "epoch 39 batch id 4121 loss 0.011642235331237316 train acc 0.9811825103130308\n",
            "epoch 39 batch id 4131 loss 0.16584157943725586 train acc 0.9811675441781651\n",
            "epoch 39 batch id 4141 loss 0.0032182298600673676 train acc 0.9811639700555421\n",
            "epoch 39 batch id 4151 loss 0.0668540969491005 train acc 0.9811453565405927\n",
            "epoch 39 batch id 4161 loss 0.10190777480602264 train acc 0.9811268324921893\n",
            "epoch 39 batch id 4171 loss 0.0442872978746891 train acc 0.9811121433708943\n",
            "epoch 39 batch id 4181 loss 0.007641396950930357 train acc 0.9811087359483377\n",
            "epoch 39 batch id 4191 loss 0.09857749938964844 train acc 0.9811016165592937\n",
            "epoch 39 batch id 4201 loss 0.11712735146284103 train acc 0.9811205665317781\n",
            "epoch 39 batch id 4211 loss 0.04538875073194504 train acc 0.9811097423414866\n",
            "epoch 39 batch id 4221 loss 0.11178594827651978 train acc 0.9810878642501777\n",
            "epoch 39 batch id 4231 loss 0.05265830084681511 train acc 0.9810771685180808\n",
            "epoch 39 batch id 4241 loss 0.041568078100681305 train acc 0.9810628389530771\n",
            "epoch 39 batch id 4251 loss 0.14998948574066162 train acc 0.9810632792284169\n",
            "epoch 39 batch id 4261 loss 0.0044486504048109055 train acc 0.9810783853555504\n",
            "epoch 39 batch id 4271 loss 0.12525729835033417 train acc 0.9810824455631\n",
            "epoch 39 batch id 4281 loss 0.032041940838098526 train acc 0.9810828369539827\n",
            "epoch 39 batch id 4291 loss 0.010013765655457973 train acc 0.9810759438359357\n",
            "epoch 39 batch id 4301 loss 0.14277935028076172 train acc 0.9810400197628458\n",
            "epoch 39 batch id 4311 loss 0.04271445795893669 train acc 0.9810586290883786\n",
            "epoch 39 batch id 4321 loss 0.025805650278925896 train acc 0.9810373756074983\n",
            "epoch 39 batch id 4331 loss 0.19274553656578064 train acc 0.9810306511198338\n",
            "epoch 39 batch id 4341 loss 0.03252994641661644 train acc 0.9810275570145128\n",
            "epoch 39 batch id 4351 loss 0.04230106994509697 train acc 0.9809849747184556\n",
            "epoch 39 batch id 4361 loss 0.00759086711332202 train acc 0.9809855824352213\n",
            "epoch 39 batch id 4371 loss 0.02434081956744194 train acc 0.980996911461908\n",
            "epoch 39 batch id 4381 loss 0.20332789421081543 train acc 0.9809725233964848\n",
            "epoch 39 batch id 4391 loss 0.00031885586213320494 train acc 0.9809731553176952\n",
            "epoch 39 batch id 4401 loss 0.08461043238639832 train acc 0.9809666837082481\n",
            "epoch 39 batch id 4411 loss 0.0008067441522143781 train acc 0.9809673260031739\n",
            "epoch 39 batch id 4421 loss 0.0887143537402153 train acc 0.9809714996607103\n",
            "epoch 39 batch id 4431 loss 0.02156347595155239 train acc 0.9809756544798014\n",
            "epoch 39 batch id 4441 loss 0.09064280986785889 train acc 0.9809762722359829\n",
            "epoch 39 batch id 4451 loss 0.0009143583592958748 train acc 0.9809768872163559\n",
            "epoch 39 batch id 4461 loss 0.00029736608848907053 train acc 0.9809845045953822\n",
            "epoch 39 batch id 4471 loss 0.07641324400901794 train acc 0.9809920878997987\n",
            "epoch 39 batch id 4481 loss 0.023984061554074287 train acc 0.9809822026333408\n",
            "epoch 39 batch id 4491 loss 0.1064089834690094 train acc 0.9809793197506124\n",
            "epoch 39 batch id 4501 loss 0.011020033620297909 train acc 0.9809660353254832\n",
            "epoch 39 batch id 4511 loss 0.06676279008388519 train acc 0.9809735923298604\n",
            "epoch 39 batch id 4521 loss 0.02578374184668064 train acc 0.9809811159035612\n",
            "epoch 39 batch id 4531 loss 0.0042940848506987095 train acc 0.980988606267932\n",
            "epoch 39 batch id 4541 loss 0.08488074690103531 train acc 0.980992622770315\n",
            "epoch 39 batch id 4551 loss 0.029003459960222244 train acc 0.9810034882443419\n",
            "epoch 39 batch id 4561 loss 0.0859747901558876 train acc 0.9810006029379522\n",
            "epoch 39 batch id 4571 loss 0.02864247001707554 train acc 0.9809977302559615\n",
            "epoch 39 batch id 4581 loss 0.15506751835346222 train acc 0.9809948701156953\n",
            "epoch 39 batch id 4591 loss 0.030260084196925163 train acc 0.9810022326290568\n",
            "epoch 39 batch id 4601 loss 0.028748780488967896 train acc 0.9809891871332319\n",
            "epoch 39 batch id 4611 loss 0.04048662632703781 train acc 0.9810066959444806\n",
            "epoch 39 batch id 4621 loss 0.0171296838670969 train acc 0.9809970785544254\n",
            "epoch 39 batch id 4631 loss 0.09704948216676712 train acc 0.9809976247030879\n",
            "epoch 39 batch id 4641 loss 0.17549169063568115 train acc 0.9809880683042448\n",
            "epoch 39 batch id 4651 loss 0.1797264665365219 train acc 0.9809886314771017\n",
            "epoch 39 batch id 4661 loss 0.07157986611127853 train acc 0.9809858399485089\n",
            "epoch 39 batch id 4671 loss 0.004176706075668335 train acc 0.9809964408049668\n",
            "epoch 39 batch id 4681 loss 0.29345017671585083 train acc 0.9809769547105319\n",
            "epoch 39 batch id 4691 loss 0.011581812053918839 train acc 0.9809742059262417\n",
            "epoch 39 batch id 4701 loss 0.014636089093983173 train acc 0.9809781163582216\n",
            "epoch 39 batch id 4711 loss 0.040212541818618774 train acc 0.9809853268945022\n",
            "epoch 39 batch id 4721 loss 0.0024858508259058 train acc 0.9809925068841348\n",
            "epoch 39 batch id 4731 loss 0.015484385192394257 train acc 0.9809930511519763\n",
            "epoch 39 batch id 4741 loss 0.11537879705429077 train acc 0.9809870016874077\n",
            "epoch 39 batch id 4751 loss 0.21465538442134857 train acc 0.9809809776889076\n",
            "epoch 39 batch id 4761 loss 0.153158038854599 train acc 0.9809749789960093\n",
            "epoch 39 batch id 4771 loss 0.07111594825983047 train acc 0.9809821054286313\n",
            "epoch 39 batch id 4781 loss 0.08765920251607895 train acc 0.980976129470822\n",
            "epoch 39 batch id 4791 loss 0.13045816123485565 train acc 0.9809603944896681\n",
            "epoch 39 batch id 4801 loss 0.006857729516923428 train acc 0.9809609977088106\n",
            "epoch 39 batch id 4811 loss 0.004944045562297106 train acc 0.9809778372479734\n",
            "epoch 39 batch id 4821 loss 0.045585401356220245 train acc 0.9809946069280232\n",
            "epoch 39 batch id 4831 loss 0.07492303103208542 train acc 0.9809951355826951\n",
            "epoch 39 batch id 4841 loss 0.1290477216243744 train acc 0.9809892067754596\n",
            "epoch 39 batch id 4851 loss 0.07713757455348969 train acc 0.9809897443826016\n",
            "epoch 39 batch id 4861 loss 0.0403449572622776 train acc 0.9809806367002675\n",
            "epoch 39 batch id 4871 loss 0.10300251841545105 train acc 0.981000436255389\n",
            "epoch 39 batch id 4881 loss 0.07040795683860779 train acc 0.9810105511165744\n",
            "epoch 39 batch id 4891 loss 0.02684938535094261 train acc 0.9810078460437538\n",
            "epoch 39 batch id 4901 loss 0.0002579038846306503 train acc 0.9810051520097939\n",
            "epoch 39 batch id 4911 loss 0.10497995465993881 train acc 0.9809897424149867\n",
            "epoch 39 batch id 4921 loss 0.14864733815193176 train acc 0.9809775706157285\n",
            "epoch 39 batch id 4931 loss 0.0573338083922863 train acc 0.9809622794564997\n",
            "epoch 39 batch id 4941 loss 0.06080017238855362 train acc 0.9809502125075895\n",
            "epoch 39 batch id 4951 loss 0.09121482819318771 train acc 0.9809413502322764\n",
            "epoch 39 train acc 0.9809360500302602\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6e3d8902e874d32b47e13ea2941dfcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 39 loss 0.3221795856952667 test acc 0.8500171829178885\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1c340033743403e86453bd52609b760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 40 batch id 1 loss 0.051439546048641205 train acc 0.96875\n",
            "epoch 40 batch id 11 loss 0.014281699433922768 train acc 0.9829545454545454\n",
            "epoch 40 batch id 21 loss 0.020242800936102867 train acc 0.9821428571428571\n",
            "epoch 40 batch id 31 loss 0.10112056881189346 train acc 0.9803427419354839\n",
            "epoch 40 batch id 41 loss 0.01604980230331421 train acc 0.9824695121951219\n",
            "epoch 40 batch id 51 loss 0.011051638051867485 train acc 0.9831495098039216\n",
            "epoch 40 batch id 61 loss 0.02504759654402733 train acc 0.9805327868852459\n",
            "epoch 40 batch id 71 loss 0.11401548981666565 train acc 0.9806338028169014\n",
            "epoch 40 batch id 81 loss 0.01797422394156456 train acc 0.9797453703703703\n",
            "epoch 40 batch id 91 loss 0.06645578145980835 train acc 0.9800824175824175\n",
            "epoch 40 batch id 101 loss 0.057220879942178726 train acc 0.979115099009901\n",
            "epoch 40 batch id 111 loss 0.00010917813051491976 train acc 0.9791666666666666\n",
            "epoch 40 batch id 121 loss 0.041496194899082184 train acc 0.9801136363636364\n",
            "epoch 40 batch id 131 loss 0.024394281208515167 train acc 0.9799618320610687\n",
            "epoch 40 batch id 141 loss 0.0019132745219394565 train acc 0.9802748226950354\n",
            "epoch 40 batch id 151 loss 0.07405441254377365 train acc 0.9809602649006622\n",
            "epoch 40 batch id 161 loss 0.08184187859296799 train acc 0.9809782608695652\n",
            "epoch 40 batch id 171 loss 0.1773202270269394 train acc 0.9806286549707602\n",
            "epoch 40 batch id 181 loss 0.023632770404219627 train acc 0.9810946132596685\n",
            "epoch 40 batch id 191 loss 0.04685254767537117 train acc 0.9807755235602095\n",
            "epoch 40 batch id 201 loss 0.04214630648493767 train acc 0.9808768656716418\n",
            "epoch 40 batch id 211 loss 0.010506860911846161 train acc 0.9814129146919431\n",
            "epoch 40 batch id 221 loss 0.06775019317865372 train acc 0.9811227375565611\n",
            "epoch 40 batch id 231 loss 0.020091144368052483 train acc 0.9816693722943723\n",
            "epoch 40 batch id 241 loss 0.19654256105422974 train acc 0.9816519709543569\n",
            "epoch 40 batch id 251 loss 0.09067395329475403 train acc 0.9816982071713147\n",
            "epoch 40 batch id 261 loss 0.06074259430170059 train acc 0.9816810344827587\n",
            "epoch 40 batch id 271 loss 0.01566908322274685 train acc 0.9820110701107011\n",
            "epoch 40 batch id 281 loss 0.007657619658857584 train acc 0.982317615658363\n",
            "epoch 40 batch id 291 loss 0.051493555307388306 train acc 0.9827104810996563\n",
            "epoch 40 batch id 301 loss 0.10853049904108047 train acc 0.9825581395348837\n",
            "epoch 40 batch id 311 loss 0.14654524624347687 train acc 0.9823653536977492\n",
            "epoch 40 batch id 321 loss 0.24143841862678528 train acc 0.9823306074766355\n",
            "epoch 40 batch id 331 loss 0.0255279578268528 train acc 0.9822507552870091\n",
            "epoch 40 batch id 341 loss 0.19507622718811035 train acc 0.9821297653958945\n",
            "epoch 40 batch id 351 loss 0.051121097058057785 train acc 0.9822382478632479\n",
            "epoch 40 batch id 361 loss 0.012913665734231472 train acc 0.9822974376731302\n",
            "epoch 40 batch id 371 loss 0.0394776351749897 train acc 0.9825640161725068\n",
            "epoch 40 batch id 381 loss 0.013727154582738876 train acc 0.9826935695538058\n",
            "epoch 40 batch id 391 loss 0.0713600292801857 train acc 0.9827765345268542\n",
            "epoch 40 batch id 401 loss 0.0001507596462033689 train acc 0.9828163965087282\n",
            "epoch 40 batch id 411 loss 0.003045313525944948 train acc 0.983044403892944\n",
            "epoch 40 batch id 421 loss 0.007294217590242624 train acc 0.9831131235154394\n",
            "epoch 40 batch id 431 loss 0.06818689405918121 train acc 0.9830336426914154\n",
            "epoch 40 batch id 441 loss 0.17226111888885498 train acc 0.9827097505668935\n",
            "epoch 40 batch id 451 loss 0.05082426592707634 train acc 0.9828159645232816\n",
            "epoch 40 batch id 461 loss 0.004045069217681885 train acc 0.9825786334056399\n",
            "epoch 40 batch id 471 loss 0.0145800756290555 train acc 0.9827162951167728\n",
            "epoch 40 batch id 481 loss 0.10554774850606918 train acc 0.9827507796257796\n",
            "epoch 40 batch id 491 loss 0.004366863518953323 train acc 0.9827202138492872\n",
            "epoch 40 batch id 501 loss 0.12355164438486099 train acc 0.9827532435129741\n",
            "epoch 40 batch id 511 loss 0.09261885285377502 train acc 0.982570939334638\n",
            "epoch 40 batch id 521 loss 0.11642264574766159 train acc 0.982515595009597\n",
            "epoch 40 batch id 531 loss 0.03617187589406967 train acc 0.982403483992467\n",
            "epoch 40 batch id 541 loss 0.0433519221842289 train acc 0.9822088724584104\n",
            "epoch 40 batch id 551 loss 0.024994023144245148 train acc 0.9822198275862069\n",
            "epoch 40 batch id 561 loss 0.047461021691560745 train acc 0.9820911319073083\n",
            "epoch 40 batch id 571 loss 0.01834235154092312 train acc 0.9822679509632224\n",
            "epoch 40 batch id 581 loss 0.13421988487243652 train acc 0.9821966437177281\n",
            "epoch 40 batch id 591 loss 0.024698978289961815 train acc 0.9821806260575296\n",
            "epoch 40 batch id 601 loss 0.024399667978286743 train acc 0.9821911397670549\n",
            "epoch 40 batch id 611 loss 0.058576636016368866 train acc 0.9820990180032734\n",
            "epoch 40 batch id 621 loss 0.1813119649887085 train acc 0.9821105072463768\n",
            "epoch 40 batch id 631 loss 0.05659272521734238 train acc 0.981973058637084\n",
            "epoch 40 batch id 641 loss 0.09347919374704361 train acc 0.9819861544461779\n",
            "epoch 40 batch id 651 loss 0.013060769997537136 train acc 0.9819268433179723\n",
            "epoch 40 batch id 661 loss 0.001473390031605959 train acc 0.9819875189107413\n",
            "epoch 40 batch id 671 loss 0.011877952143549919 train acc 0.9820696721311475\n",
            "epoch 40 batch id 681 loss 0.07984957098960876 train acc 0.9820346916299559\n",
            "epoch 40 batch id 691 loss 0.08619724959135056 train acc 0.9818198263386396\n",
            "epoch 40 batch id 701 loss 0.05235905200242996 train acc 0.9817894079885877\n",
            "epoch 40 batch id 711 loss 0.11467243731021881 train acc 0.9817598452883263\n",
            "epoch 40 batch id 721 loss 0.06641970574855804 train acc 0.9816444174757282\n",
            "epoch 40 batch id 731 loss 0.012685484252870083 train acc 0.9816817715458276\n",
            "epoch 40 batch id 741 loss 0.0066365087404847145 train acc 0.9816970310391363\n",
            "epoch 40 batch id 751 loss 0.03257201239466667 train acc 0.9816078561917443\n",
            "epoch 40 batch id 761 loss 0.0022487572859972715 train acc 0.9815415571616294\n",
            "epoch 40 batch id 771 loss 0.07807023078203201 train acc 0.9814161802853437\n",
            "epoch 40 batch id 781 loss 0.026026470586657524 train acc 0.981414052496799\n",
            "epoch 40 batch id 791 loss 0.0526602603495121 train acc 0.9814712389380531\n",
            "epoch 40 batch id 801 loss 0.07374022901058197 train acc 0.9814684769038702\n",
            "epoch 40 batch id 811 loss 0.10075896233320236 train acc 0.9814657829839704\n",
            "epoch 40 batch id 821 loss 0.007759230677038431 train acc 0.9815773447015834\n",
            "epoch 40 batch id 831 loss 0.12230052798986435 train acc 0.9816298134777377\n",
            "epoch 40 batch id 841 loss 0.00220895791426301 train acc 0.9816624554102259\n",
            "epoch 40 batch id 851 loss 0.07040071487426758 train acc 0.9816392479435958\n",
            "epoch 40 batch id 861 loss 0.031859979033470154 train acc 0.9815802845528455\n",
            "epoch 40 batch id 871 loss 0.023733077570796013 train acc 0.9815764925373134\n",
            "epoch 40 batch id 881 loss 0.015526658855378628 train acc 0.9815018444948922\n",
            "epoch 40 batch id 891 loss 0.00028516678139567375 train acc 0.9815867003367004\n",
            "epoch 40 batch id 901 loss 0.18054816126823425 train acc 0.9816176470588235\n",
            "epoch 40 batch id 911 loss 0.08959431946277618 train acc 0.9816479143798024\n",
            "epoch 40 batch id 921 loss 0.05854123458266258 train acc 0.9815757328990228\n",
            "epoch 40 batch id 931 loss 0.0017462661489844322 train acc 0.9816058002148228\n",
            "epoch 40 batch id 941 loss 0.015672417357563972 train acc 0.9816850425079703\n",
            "epoch 40 batch id 951 loss 0.026846885681152344 train acc 0.9816640378548895\n",
            "epoch 40 batch id 961 loss 0.022399401292204857 train acc 0.9816597294484911\n",
            "epoch 40 batch id 971 loss 0.07144425064325333 train acc 0.9815911431513903\n",
            "epoch 40 batch id 981 loss 0.026294205337762833 train acc 0.9816673037716616\n",
            "epoch 40 batch id 991 loss 0.09330469369888306 train acc 0.9816946266397578\n",
            "epoch 40 batch id 1001 loss 0.008175392635166645 train acc 0.9818150599400599\n",
            "epoch 40 batch id 1011 loss 0.08955422788858414 train acc 0.9818094708209694\n",
            "epoch 40 batch id 1021 loss 0.04033535718917847 train acc 0.9818958129285015\n",
            "epoch 40 batch id 1031 loss 0.10286818444728851 train acc 0.9819350145489816\n",
            "epoch 40 batch id 1041 loss 0.029987970367074013 train acc 0.9819434438040345\n",
            "epoch 40 batch id 1051 loss 0.022959867492318153 train acc 0.9818922454804948\n",
            "epoch 40 batch id 1061 loss 0.02776939608156681 train acc 0.9817978322337417\n",
            "epoch 40 batch id 1071 loss 0.014157614670693874 train acc 0.9817781279178338\n",
            "epoch 40 batch id 1081 loss 0.023858388885855675 train acc 0.9817443339500462\n",
            "epoch 40 batch id 1091 loss 0.16702310740947723 train acc 0.9817827681026581\n",
            "epoch 40 batch id 1101 loss 0.05863498896360397 train acc 0.9818205040871935\n",
            "epoch 40 batch id 1111 loss 0.12838198244571686 train acc 0.9817872412241224\n",
            "epoch 40 batch id 1121 loss 0.04513974115252495 train acc 0.9818242640499554\n",
            "epoch 40 batch id 1131 loss 0.03290898725390434 train acc 0.9818053713527851\n",
            "epoch 40 batch id 1141 loss 0.07600226253271103 train acc 0.9818415863277826\n",
            "epoch 40 batch id 1151 loss 0.10776710510253906 train acc 0.9816735447437012\n",
            "epoch 40 batch id 1161 loss 0.036500439047813416 train acc 0.9816833548664944\n",
            "epoch 40 batch id 1171 loss 0.012168272398412228 train acc 0.9816929974380871\n",
            "epoch 40 batch id 1181 loss 0.0011048439191654325 train acc 0.9817686282811177\n",
            "epoch 40 batch id 1191 loss 0.27853038907051086 train acc 0.9817773929471033\n",
            "epoch 40 batch id 1201 loss 0.005216259974986315 train acc 0.9818510616153205\n",
            "epoch 40 batch id 1211 loss 0.012145145796239376 train acc 0.981846098265896\n",
            "epoch 40 batch id 1221 loss 0.11824845522642136 train acc 0.9818540131040131\n",
            "epoch 40 batch id 1231 loss 0.030840016901493073 train acc 0.9817602558895208\n",
            "epoch 40 batch id 1241 loss 0.006782947573810816 train acc 0.9817687348912167\n",
            "epoch 40 batch id 1251 loss 0.08632723242044449 train acc 0.9817770783373302\n",
            "epoch 40 batch id 1261 loss 0.05669775977730751 train acc 0.981810071371927\n",
            "epoch 40 batch id 1271 loss 0.07019460201263428 train acc 0.9818302517702596\n",
            "epoch 40 batch id 1281 loss 0.0024704558309167624 train acc 0.9818379195940672\n",
            "epoch 40 batch id 1291 loss 0.06471884995698929 train acc 0.9818454686289698\n",
            "epoch 40 batch id 1301 loss 0.015813764184713364 train acc 0.9818168716372021\n",
            "epoch 40 batch id 1311 loss 0.026508187875151634 train acc 0.9818244660564455\n",
            "epoch 40 batch id 1321 loss 0.06724582612514496 train acc 0.981796461014383\n",
            "epoch 40 batch id 1331 loss 0.021176626905798912 train acc 0.9817688767843726\n",
            "epoch 40 batch id 1341 loss 0.10000570118427277 train acc 0.981753355704698\n",
            "epoch 40 batch id 1351 loss 0.0010108700953423977 train acc 0.9817380643967432\n",
            "epoch 40 batch id 1361 loss 0.07136324048042297 train acc 0.9817115172667157\n",
            "epoch 40 batch id 1371 loss 0.007289585657417774 train acc 0.9816853574033552\n",
            "epoch 40 batch id 1381 loss 0.03220976144075394 train acc 0.9817161477190441\n",
            "epoch 40 batch id 1391 loss 0.013062830083072186 train acc 0.98169033069734\n",
            "epoch 40 batch id 1401 loss 0.05908343195915222 train acc 0.9816760349750179\n",
            "epoch 40 batch id 1411 loss 0.02179724909365177 train acc 0.9817283841247342\n",
            "epoch 40 batch id 1421 loss 0.01305419858545065 train acc 0.9817360133708656\n",
            "epoch 40 batch id 1431 loss 0.03681710734963417 train acc 0.9817216981132075\n",
            "epoch 40 batch id 1441 loss 0.021137943491339684 train acc 0.9816967383761277\n",
            "epoch 40 batch id 1451 loss 0.010761902667582035 train acc 0.9817475017229497\n",
            "epoch 40 batch id 1461 loss 0.011903420090675354 train acc 0.9817440965092402\n",
            "epoch 40 batch id 1471 loss 0.08095091581344604 train acc 0.9817726036709721\n",
            "epoch 40 batch id 1481 loss 0.06206035614013672 train acc 0.9817479743416611\n",
            "epoch 40 batch id 1491 loss 0.04218952730298042 train acc 0.981755114017438\n",
            "epoch 40 batch id 1501 loss 0.05240510404109955 train acc 0.9817933877415057\n",
            "epoch 40 batch id 1511 loss 0.026612911373376846 train acc 0.9818001323626737\n",
            "epoch 40 batch id 1521 loss 0.0018532713875174522 train acc 0.9817862426035503\n",
            "epoch 40 batch id 1531 loss 0.012736691161990166 train acc 0.9818031515349445\n",
            "epoch 40 batch id 1541 loss 0.007885000668466091 train acc 0.981799561972745\n",
            "epoch 40 batch id 1551 loss 0.09773281961679459 train acc 0.9817557221147647\n",
            "epoch 40 batch id 1561 loss 0.03758129104971886 train acc 0.981742472773863\n",
            "epoch 40 batch id 1571 loss 0.013140459544956684 train acc 0.9817592297899427\n",
            "epoch 40 batch id 1581 loss 0.07589588314294815 train acc 0.981756008855155\n",
            "epoch 40 batch id 1591 loss 0.06397851556539536 train acc 0.9817822910119421\n",
            "epoch 40 batch id 1601 loss 0.007639087736606598 train acc 0.9818570424734541\n",
            "epoch 40 batch id 1611 loss 0.05549623444676399 train acc 0.9818241775294848\n",
            "epoch 40 batch id 1621 loss 0.05055028572678566 train acc 0.9817724398519433\n",
            "epoch 40 batch id 1631 loss 0.004284325521439314 train acc 0.9817883966891477\n",
            "epoch 40 batch id 1641 loss 0.07070474326610565 train acc 0.9817755941499086\n",
            "epoch 40 batch id 1651 loss 0.03054966777563095 train acc 0.9817534827377347\n",
            "epoch 40 batch id 1661 loss 0.01276383362710476 train acc 0.9818257074051776\n",
            "epoch 40 batch id 1671 loss 0.05318335443735123 train acc 0.9817848593656493\n",
            "epoch 40 batch id 1681 loss 0.08182079344987869 train acc 0.9817630874479476\n",
            "epoch 40 batch id 1691 loss 0.022083504125475883 train acc 0.9817785334121821\n",
            "epoch 40 batch id 1701 loss 0.029901109635829926 train acc 0.9817570546737213\n",
            "epoch 40 batch id 1711 loss 0.022711357101798058 train acc 0.9817906195207481\n",
            "epoch 40 batch id 1721 loss 0.006930137984454632 train acc 0.9818147152818129\n",
            "epoch 40 batch id 1731 loss 0.0006986736552789807 train acc 0.9818385326400925\n",
            "epoch 40 batch id 1741 loss 0.0072947354055941105 train acc 0.9818351522113727\n",
            "epoch 40 batch id 1751 loss 0.03608432039618492 train acc 0.9818139634494575\n",
            "epoch 40 batch id 1761 loss 0.10566683113574982 train acc 0.9818551249290176\n",
            "epoch 40 batch id 1771 loss 0.00032453855965286493 train acc 0.9818869988706945\n",
            "epoch 40 batch id 1781 loss 0.0007516078185290098 train acc 0.9819097417181358\n",
            "epoch 40 batch id 1791 loss 0.10059484094381332 train acc 0.9818973338916807\n",
            "epoch 40 batch id 1801 loss 0.012289200909435749 train acc 0.9819371182676291\n",
            "epoch 40 batch id 1811 loss 0.031871549785137177 train acc 0.9819505797901712\n",
            "epoch 40 batch id 1821 loss 0.08387631177902222 train acc 0.9819209912136189\n",
            "epoch 40 batch id 1831 loss 0.0157935693860054 train acc 0.9818746586564718\n",
            "epoch 40 batch id 1841 loss 0.002544464310631156 train acc 0.9818373166757197\n",
            "epoch 40 batch id 1851 loss 0.009643809869885445 train acc 0.9818932333873582\n",
            "epoch 40 batch id 1861 loss 0.04611212760210037 train acc 0.9818561929070392\n",
            "epoch 40 batch id 1871 loss 0.06798183172941208 train acc 0.9819030598610369\n",
            "epoch 40 batch id 1881 loss 0.014991861768066883 train acc 0.9818829744816587\n",
            "epoch 40 batch id 1891 loss 0.020919468253850937 train acc 0.981920941300899\n",
            "epoch 40 batch id 1901 loss 0.21521799266338348 train acc 0.9818845344555497\n",
            "epoch 40 batch id 1911 loss 0.032763417810201645 train acc 0.9818321559392988\n",
            "epoch 40 batch id 1921 loss 0.10791581124067307 train acc 0.9817803227485684\n",
            "epoch 40 batch id 1931 loss 0.02289270982146263 train acc 0.9817856680476437\n",
            "epoch 40 batch id 1941 loss 0.003328704508021474 train acc 0.9817668083462133\n",
            "epoch 40 batch id 1951 loss 0.03961556777358055 train acc 0.9817561506919529\n",
            "epoch 40 batch id 1961 loss 0.03726833313703537 train acc 0.9818252804691484\n",
            "epoch 40 batch id 1971 loss 0.010160249657928944 train acc 0.9818382166412988\n",
            "epoch 40 batch id 1981 loss 0.08652526140213013 train acc 0.9818115850580514\n",
            "epoch 40 batch id 1991 loss 0.12007366120815277 train acc 0.9817381341034656\n",
            "epoch 40 batch id 2001 loss 0.0025862811598926783 train acc 0.9817747376311844\n",
            "epoch 40 batch id 2011 loss 0.11565455049276352 train acc 0.9818032073595226\n",
            "epoch 40 batch id 2021 loss 0.07548031210899353 train acc 0.9817850074220683\n",
            "epoch 40 batch id 2031 loss 0.024421226233243942 train acc 0.9817823732151649\n",
            "epoch 40 batch id 2041 loss 0.010050193406641483 train acc 0.9817414870161686\n",
            "epoch 40 batch id 2051 loss 0.3110511302947998 train acc 0.9817086177474402\n",
            "epoch 40 batch id 2061 loss 0.002951760543510318 train acc 0.9817215550703542\n",
            "epoch 40 batch id 2071 loss 0.17566454410552979 train acc 0.9817041887976823\n",
            "epoch 40 batch id 2081 loss 0.0470634363591671 train acc 0.9816944978375781\n",
            "epoch 40 batch id 2091 loss 0.036132872104644775 train acc 0.9816998445719751\n",
            "epoch 40 batch id 2101 loss 0.0035316876601427794 train acc 0.9817274512137077\n",
            "epoch 40 batch id 2111 loss 0.027550170198082924 train acc 0.9817177877783041\n",
            "epoch 40 batch id 2121 loss 0.013985592871904373 train acc 0.9817450495049505\n",
            "epoch 40 batch id 2131 loss 0.0014102597488090396 train acc 0.9817940520882215\n",
            "epoch 40 batch id 2141 loss 0.04382290318608284 train acc 0.981798808967772\n",
            "epoch 40 batch id 2151 loss 0.05687709152698517 train acc 0.9818325778707578\n",
            "epoch 40 batch id 2161 loss 0.09545698016881943 train acc 0.9818009602036094\n",
            "epoch 40 batch id 2171 loss 0.00961417704820633 train acc 0.981791225241824\n",
            "epoch 40 batch id 2181 loss 0.07047975808382034 train acc 0.9817887436955525\n",
            "epoch 40 batch id 2191 loss 0.03967084735631943 train acc 0.9818076791419443\n",
            "epoch 40 batch id 2201 loss 0.009365994483232498 train acc 0.9818193434802363\n",
            "epoch 40 batch id 2211 loss 0.0009613608126528561 train acc 0.9818379692446857\n",
            "epoch 40 batch id 2221 loss 0.07342895865440369 train acc 0.9818282868077443\n",
            "epoch 40 batch id 2231 loss 0.009304636158049107 train acc 0.9818326983415508\n",
            "epoch 40 batch id 2241 loss 0.09067517518997192 train acc 0.9818300981704596\n",
            "epoch 40 batch id 2251 loss 0.0014454333577305079 train acc 0.9818275211017325\n",
            "epoch 40 batch id 2261 loss 0.000210056736250408 train acc 0.9818318774878373\n",
            "epoch 40 batch id 2271 loss 0.01558336615562439 train acc 0.9818499559665346\n",
            "epoch 40 batch id 2281 loss 0.03565908595919609 train acc 0.9818541758000877\n",
            "epoch 40 batch id 2291 loss 0.09232070297002792 train acc 0.9818515386294194\n",
            "epoch 40 batch id 2301 loss 0.024370595812797546 train acc 0.9818760864841374\n",
            "epoch 40 batch id 2311 loss 0.0412120595574379 train acc 0.981907183037646\n",
            "epoch 40 batch id 2321 loss 0.10827761143445969 train acc 0.981904351572598\n",
            "epoch 40 batch id 2331 loss 0.059749752283096313 train acc 0.981881435006435\n",
            "epoch 40 batch id 2341 loss 0.009791937656700611 train acc 0.9819121102093122\n",
            "epoch 40 batch id 2351 loss 0.01862337440252304 train acc 0.9818760633772863\n",
            "epoch 40 batch id 2361 loss 0.0662362352013588 train acc 0.9819065014824228\n",
            "epoch 40 batch id 2371 loss 0.0011035845382139087 train acc 0.981949862927035\n",
            "epoch 40 batch id 2381 loss 0.0002656415745150298 train acc 0.9819862977740446\n",
            "epoch 40 batch id 2391 loss 0.03939986974000931 train acc 0.9819701484734421\n",
            "epoch 40 batch id 2401 loss 0.03221052139997482 train acc 0.9819606413994169\n",
            "epoch 40 batch id 2411 loss 0.023652663454413414 train acc 0.981944732476151\n",
            "epoch 40 batch id 2421 loss 0.012563660740852356 train acc 0.9819934944237918\n",
            "epoch 40 batch id 2431 loss 0.13137967884540558 train acc 0.9820097182229535\n",
            "epoch 40 batch id 2441 loss 0.00569802476093173 train acc 0.982019408029496\n",
            "epoch 40 batch id 2451 loss 0.026760568842291832 train acc 0.982016268869849\n",
            "epoch 40 batch id 2461 loss 0.03214539214968681 train acc 0.9820131552214547\n",
            "epoch 40 batch id 2471 loss 0.09443183243274689 train acc 0.981997420072845\n",
            "epoch 40 batch id 2481 loss 0.2537543475627899 train acc 0.9820070032245063\n",
            "epoch 40 batch id 2491 loss 0.17189890146255493 train acc 0.9820039642713769\n",
            "epoch 40 batch id 2501 loss 0.0055342609994113445 train acc 0.9820009496201519\n",
            "epoch 40 batch id 2511 loss 0.012275367975234985 train acc 0.9819917363600159\n",
            "epoch 40 batch id 2521 loss 0.06971964985132217 train acc 0.9819702003173344\n",
            "epoch 40 batch id 2531 loss 0.030950231477618217 train acc 0.9819673548004741\n",
            "epoch 40 batch id 2541 loss 0.018070247024297714 train acc 0.9819460842188115\n",
            "epoch 40 batch id 2551 loss 0.023578209802508354 train acc 0.981967855742846\n",
            "epoch 40 batch id 2561 loss 0.03358705714344978 train acc 0.9819589515814136\n",
            "epoch 40 batch id 2571 loss 0.09587101638317108 train acc 0.9819561940879036\n",
            "epoch 40 batch id 2581 loss 0.047948818653821945 train acc 0.9819171348314607\n",
            "epoch 40 batch id 2591 loss 0.006058105267584324 train acc 0.9819326514859128\n",
            "epoch 40 batch id 2601 loss 0.027835065498948097 train acc 0.9819540561322568\n",
            "epoch 40 batch id 2611 loss 0.13343451917171478 train acc 0.9819693125239372\n",
            "epoch 40 batch id 2621 loss 0.04321274906396866 train acc 0.9819665681037771\n",
            "epoch 40 batch id 2631 loss 0.08443281799554825 train acc 0.9819460281261878\n",
            "epoch 40 batch id 2641 loss 0.033400844782590866 train acc 0.981919727375994\n",
            "epoch 40 batch id 2651 loss 0.0011479512322694063 train acc 0.9819230950584685\n",
            "epoch 40 batch id 2661 loss 0.024263905361294746 train acc 0.9819264374295378\n",
            "epoch 40 batch id 2671 loss 0.08818463981151581 train acc 0.9819239049045302\n",
            "epoch 40 batch id 2681 loss 0.02998758852481842 train acc 0.9819272193211488\n",
            "epoch 40 batch id 2691 loss 0.017630768939852715 train acc 0.9819014771460424\n",
            "epoch 40 batch id 2701 loss 0.0288715660572052 train acc 0.9819106349500185\n",
            "epoch 40 batch id 2711 loss 0.04677615687251091 train acc 0.9818851438583549\n",
            "epoch 40 batch id 2721 loss 0.010752816684544086 train acc 0.9818540977581771\n",
            "epoch 40 batch id 2731 loss 0.0012904573231935501 train acc 0.9818518857561332\n",
            "epoch 40 batch id 2741 loss 0.035362571477890015 train acc 0.9818610908427581\n",
            "epoch 40 batch id 2751 loss 0.11348580569028854 train acc 0.9818191112322792\n",
            "epoch 40 batch id 2761 loss 0.009270792827010155 train acc 0.9818453458891706\n",
            "epoch 40 batch id 2771 loss 0.011963018216192722 train acc 0.9818262811259473\n",
            "epoch 40 batch id 2781 loss 0.030902395024895668 train acc 0.9818466828478964\n",
            "epoch 40 batch id 2791 loss 0.01764737069606781 train acc 0.9818613400214977\n",
            "epoch 40 batch id 2801 loss 0.23795300722122192 train acc 0.9818591574437701\n",
            "epoch 40 batch id 2811 loss 0.09031309187412262 train acc 0.9818792244752758\n",
            "epoch 40 batch id 2821 loss 0.048511311411857605 train acc 0.9818936104218362\n",
            "epoch 40 batch id 2831 loss 0.02232924848794937 train acc 0.9818968562345461\n",
            "epoch 40 batch id 2841 loss 0.05118212103843689 train acc 0.9818835797254488\n",
            "epoch 40 batch id 2851 loss 0.0036177420988678932 train acc 0.9818813574184496\n",
            "epoch 40 batch id 2861 loss 0.05818774923682213 train acc 0.9818736892694861\n",
            "epoch 40 batch id 2871 loss 0.06570813059806824 train acc 0.981876959247649\n",
            "epoch 40 batch id 2881 loss 0.05090636387467384 train acc 0.981874783061437\n",
            "epoch 40 batch id 2891 loss 0.002727596089243889 train acc 0.9818888360428918\n",
            "epoch 40 batch id 2901 loss 0.010405482724308968 train acc 0.9819135642881764\n",
            "epoch 40 batch id 2911 loss 0.024772683158516884 train acc 0.9819166523531433\n",
            "epoch 40 batch id 2921 loss 0.0002081624261336401 train acc 0.9819250684697022\n",
            "epoch 40 batch id 2931 loss 0.023304186761379242 train acc 0.9819067724326168\n",
            "epoch 40 batch id 2941 loss 0.0021757115609943867 train acc 0.9818779751785107\n",
            "epoch 40 batch id 2951 loss 0.02837444096803665 train acc 0.9819182056929854\n",
            "epoch 40 batch id 2961 loss 0.0192507766187191 train acc 0.9819370567375887\n",
            "epoch 40 batch id 2971 loss 0.0384480282664299 train acc 0.9819294850218782\n",
            "epoch 40 batch id 2981 loss 0.03636668249964714 train acc 0.9819062395169407\n",
            "epoch 40 batch id 2991 loss 0.023346805945038795 train acc 0.9819197174857907\n",
            "epoch 40 batch id 3001 loss 0.000457512098364532 train acc 0.9819643452182606\n",
            "epoch 40 batch id 3011 loss 0.006674010772258043 train acc 0.9819671620724012\n",
            "epoch 40 batch id 3021 loss 0.07621653378009796 train acc 0.9819596160211851\n",
            "epoch 40 batch id 3031 loss 0.1878626048564911 train acc 0.9819521197624547\n",
            "epoch 40 batch id 3041 loss 0.12584862112998962 train acc 0.9819292584676094\n",
            "epoch 40 batch id 3051 loss 0.001591461943462491 train acc 0.9819526384791871\n",
            "epoch 40 batch id 3061 loss 0.07786324620246887 train acc 0.9819401339431558\n",
            "epoch 40 batch id 3071 loss 0.0076329056173563 train acc 0.9819633262780854\n",
            "epoch 40 batch id 3081 loss 0.028208360075950623 train acc 0.9819711538461539\n",
            "epoch 40 batch id 3091 loss 0.056302234530448914 train acc 0.9819738757683597\n",
            "epoch 40 batch id 3101 loss 0.17078746855258942 train acc 0.981941309255079\n",
            "epoch 40 batch id 3111 loss 0.09627192467451096 train acc 0.9819441096110575\n",
            "epoch 40 batch id 3121 loss 0.02178155817091465 train acc 0.9819418856135854\n",
            "epoch 40 batch id 3131 loss 0.006843073293566704 train acc 0.981969618332801\n",
            "epoch 40 batch id 3141 loss 0.09953735768795013 train acc 0.9819474291626871\n",
            "epoch 40 batch id 3151 loss 0.0015691925073042512 train acc 0.9819402570612504\n",
            "epoch 40 batch id 3161 loss 0.23135776817798615 train acc 0.9819183011705157\n",
            "epoch 40 batch id 3171 loss 0.051558513194322586 train acc 0.9818964837590666\n",
            "epoch 40 batch id 3181 loss 0.031096894294023514 train acc 0.9818649795661741\n",
            "epoch 40 batch id 3191 loss 0.018614569678902626 train acc 0.9818924318395488\n",
            "epoch 40 batch id 3201 loss 0.1016976609826088 train acc 0.9818806622930334\n",
            "epoch 40 batch id 3211 loss 0.10728221386671066 train acc 0.981864099968857\n",
            "epoch 40 batch id 3221 loss 0.09696464985609055 train acc 0.9818185346165788\n",
            "epoch 40 batch id 3231 loss 0.05399441719055176 train acc 0.9818071030640668\n",
            "epoch 40 batch id 3241 loss 0.22415944933891296 train acc 0.9818053841406973\n",
            "epoch 40 batch id 3251 loss 0.04117477312684059 train acc 0.9817844509381729\n",
            "epoch 40 batch id 3261 loss 0.10564166307449341 train acc 0.9817540631708065\n",
            "epoch 40 batch id 3271 loss 0.13247302174568176 train acc 0.9817429685111587\n",
            "epoch 40 batch id 3281 loss 0.1166277751326561 train acc 0.981736703748857\n",
            "epoch 40 batch id 3291 loss 0.013151779770851135 train acc 0.9817447204497113\n",
            "epoch 40 batch id 3301 loss 0.008606022223830223 train acc 0.9817432217509845\n",
            "epoch 40 batch id 3311 loss 0.011884388513863087 train acc 0.9817511703412867\n",
            "epoch 40 batch id 3321 loss 0.009323725476861 train acc 0.9817684808792533\n",
            "epoch 40 batch id 3331 loss 0.028598248958587646 train acc 0.9817387796457521\n",
            "epoch 40 batch id 3341 loss 0.09891316294670105 train acc 0.9817466701586351\n",
            "epoch 40 batch id 3351 loss 0.10191933065652847 train acc 0.9817172112802148\n",
            "epoch 40 batch id 3361 loss 0.020510921254754066 train acc 0.9817437146682535\n",
            "epoch 40 batch id 3371 loss 0.07104562968015671 train acc 0.9817515203203797\n",
            "epoch 40 batch id 3381 loss 0.00015366011939477175 train acc 0.9817823868677906\n",
            "epoch 40 batch id 3391 loss 0.05036899074912071 train acc 0.9817854246534945\n",
            "epoch 40 batch id 3401 loss 0.07017014920711517 train acc 0.9817838503381359\n",
            "epoch 40 batch id 3411 loss 0.02208000048995018 train acc 0.9817914467897977\n",
            "epoch 40 batch id 3421 loss 0.010965378023684025 train acc 0.9818218357205496\n",
            "epoch 40 batch id 3431 loss 0.12487665563821793 train acc 0.9818292771786651\n",
            "epoch 40 batch id 3441 loss 0.017627539113163948 train acc 0.9818502978785237\n",
            "epoch 40 batch id 3451 loss 0.013453193940222263 train acc 0.9818485583888727\n",
            "epoch 40 batch id 3461 loss 0.003644290380179882 train acc 0.9818513435423288\n",
            "epoch 40 batch id 3471 loss 0.035385169088840485 train acc 0.981854112647652\n",
            "epoch 40 batch id 3481 loss 0.06855019181966782 train acc 0.9818209566216605\n",
            "epoch 40 batch id 3491 loss 0.01367204450070858 train acc 0.981837224291034\n",
            "epoch 40 batch id 3501 loss 0.015982329845428467 train acc 0.9818623250499857\n",
            "epoch 40 batch id 3511 loss 0.07257096469402313 train acc 0.9818650313301054\n",
            "epoch 40 batch id 3521 loss 0.04818642884492874 train acc 0.9818899105367793\n",
            "epoch 40 batch id 3531 loss 0.02951042540371418 train acc 0.9819057986406117\n",
            "epoch 40 batch id 3541 loss 0.10335209220647812 train acc 0.9819083592205592\n",
            "epoch 40 batch id 3551 loss 0.019544847309589386 train acc 0.9818493030132357\n",
            "epoch 40 batch id 3561 loss 0.00822257436811924 train acc 0.9818607834877844\n",
            "epoch 40 batch id 3571 loss 0.009926951490342617 train acc 0.9818853262391487\n",
            "epoch 40 batch id 3581 loss 0.03913441300392151 train acc 0.9818922786931025\n",
            "epoch 40 batch id 3591 loss 0.07926788926124573 train acc 0.9818817878028404\n",
            "epoch 40 batch id 3601 loss 0.020501744002103806 train acc 0.9818583379616773\n",
            "epoch 40 batch id 3611 loss 0.09330517053604126 train acc 0.9818523262254223\n",
            "epoch 40 batch id 3621 loss 0.09417851269245148 train acc 0.9818290872687103\n",
            "epoch 40 batch id 3631 loss 0.011529610492289066 train acc 0.9818274924263288\n",
            "epoch 40 batch id 3641 loss 0.07831478118896484 train acc 0.9818387805547927\n",
            "epoch 40 batch id 3651 loss 0.035513270646333694 train acc 0.9818157696521501\n",
            "epoch 40 batch id 3661 loss 0.0007790024392306805 train acc 0.9817928844577984\n",
            "epoch 40 batch id 3671 loss 0.21905605494976044 train acc 0.98179140561155\n",
            "epoch 40 batch id 3681 loss 0.0029930351302027702 train acc 0.9818238929638685\n",
            "epoch 40 batch id 3691 loss 0.0218000840395689 train acc 0.9818308046599837\n",
            "epoch 40 batch id 3701 loss 0.047128576785326004 train acc 0.9817996825182383\n",
            "epoch 40 batch id 3711 loss 0.011236616410315037 train acc 0.9818108326596605\n",
            "epoch 40 batch id 3721 loss 0.07282216846942902 train acc 0.9818051263101317\n",
            "epoch 40 batch id 3731 loss 0.028167923912405968 train acc 0.9818120142053068\n",
            "epoch 40 batch id 3741 loss 0.04370380938053131 train acc 0.981797981823042\n",
            "epoch 40 batch id 3751 loss 0.0406327024102211 train acc 0.9818006864836044\n",
            "epoch 40 batch id 3761 loss 0.12847848236560822 train acc 0.981778449880351\n",
            "epoch 40 batch id 3771 loss 0.017990900203585625 train acc 0.9817811919915141\n",
            "epoch 40 batch id 3781 loss 0.06789585202932358 train acc 0.9817963171118752\n",
            "epoch 40 batch id 3791 loss 0.0015805709408596158 train acc 0.9818072408335532\n",
            "epoch 40 batch id 3801 loss 0.0349988117814064 train acc 0.9817893317548013\n",
            "epoch 40 batch id 3811 loss 0.011536689475178719 train acc 0.9818084164261349\n",
            "epoch 40 batch id 3821 loss 0.14291954040527344 train acc 0.9817987764982988\n",
            "epoch 40 batch id 3831 loss 0.01630123145878315 train acc 0.9817932654659358\n",
            "epoch 40 batch id 3841 loss 0.0008945129811763763 train acc 0.9817959190315022\n",
            "epoch 40 batch id 3851 loss 0.007847187109291553 train acc 0.9818026162035834\n",
            "epoch 40 batch id 3861 loss 0.022221054881811142 train acc 0.9817809505309505\n",
            "epoch 40 batch id 3871 loss 0.07121097296476364 train acc 0.9817836153448721\n",
            "epoch 40 batch id 3881 loss 0.045132771134376526 train acc 0.9817741883535172\n",
            "epoch 40 batch id 3891 loss 0.012192894704639912 train acc 0.9817728411719352\n",
            "epoch 40 batch id 3901 loss 0.013888370245695114 train acc 0.9817594847475006\n",
            "epoch 40 batch id 3911 loss 0.04434889554977417 train acc 0.981754186908719\n",
            "epoch 40 batch id 3921 loss 0.010768423788249493 train acc 0.9817648559041061\n",
            "epoch 40 batch id 3931 loss 0.04238281026482582 train acc 0.9817396972780463\n",
            "epoch 40 batch id 3941 loss 0.04059503972530365 train acc 0.9817503488962193\n",
            "epoch 40 batch id 3951 loss 0.19286800920963287 train acc 0.981737218425715\n",
            "epoch 40 batch id 3961 loss 0.017445746809244156 train acc 0.9817478225195657\n",
            "epoch 40 batch id 3971 loss 0.009769320487976074 train acc 0.981746568874339\n",
            "epoch 40 batch id 3981 loss 0.014268559403717518 train acc 0.9817335468475258\n",
            "epoch 40 batch id 3991 loss 0.1411246359348297 train acc 0.9817401653720872\n",
            "epoch 40 batch id 4001 loss 0.1346706748008728 train acc 0.9817467508122969\n",
            "epoch 40 batch id 4011 loss 0.014879067428410053 train acc 0.9817221391174271\n",
            "epoch 40 batch id 4021 loss 0.020608915016055107 train acc 0.9817287366326785\n",
            "epoch 40 batch id 4031 loss 0.10138159245252609 train acc 0.9817508062515505\n",
            "epoch 40 batch id 4041 loss 0.04081887751817703 train acc 0.9817147673843108\n",
            "epoch 40 batch id 4051 loss 0.022277535870671272 train acc 0.981713620093804\n",
            "epoch 40 batch id 4061 loss 0.019352450966835022 train acc 0.9817163260280719\n",
            "epoch 40 batch id 4071 loss 0.040548019111156464 train acc 0.981722856791943\n",
            "epoch 40 batch id 4081 loss 0.007889626547694206 train acc 0.9817293555501103\n",
            "epoch 40 batch id 4091 loss 0.04433893412351608 train acc 0.9817243644585676\n",
            "epoch 40 batch id 4101 loss 0.004131498280912638 train acc 0.9817346378931968\n",
            "epoch 40 batch id 4111 loss 0.015910381451249123 train acc 0.9817030527852104\n",
            "epoch 40 batch id 4121 loss 0.014391419477760792 train acc 0.981698161853919\n",
            "epoch 40 batch id 4131 loss 0.08718512952327728 train acc 0.9816857298474946\n",
            "epoch 40 batch id 4141 loss 0.00017278073937632143 train acc 0.9816922241004589\n",
            "epoch 40 batch id 4151 loss 0.0768653079867363 train acc 0.981691158756926\n",
            "epoch 40 batch id 4161 loss 0.03396356850862503 train acc 0.9816900985340062\n",
            "epoch 40 batch id 4171 loss 0.0964542105793953 train acc 0.9816852972908175\n",
            "epoch 40 batch id 4181 loss 0.003452074946835637 train acc 0.9816954675914853\n",
            "epoch 40 batch id 4191 loss 0.05048944428563118 train acc 0.9816869482223813\n",
            "epoch 40 batch id 4201 loss 0.03120655007660389 train acc 0.9817119435848608\n",
            "epoch 40 batch id 4211 loss 0.0609893798828125 train acc 0.9817108465922584\n",
            "epoch 40 batch id 4221 loss 0.08949580788612366 train acc 0.9817097547974414\n",
            "epoch 40 batch id 4231 loss 0.09255290776491165 train acc 0.9816902032616402\n",
            "epoch 40 batch id 4241 loss 0.06866809725761414 train acc 0.981707586654091\n",
            "epoch 40 batch id 4251 loss 0.07638636976480484 train acc 0.9817101858386262\n",
            "epoch 40 batch id 4261 loss 0.000538430642336607 train acc 0.9817237737620277\n",
            "epoch 40 batch id 4271 loss 0.005983888637274504 train acc 0.9817263228752049\n",
            "epoch 40 batch id 4281 loss 0.11552553623914719 train acc 0.981732509927587\n",
            "epoch 40 batch id 4291 loss 0.16050203144550323 train acc 0.9817022547191797\n",
            "epoch 40 batch id 4301 loss 0.04919400438666344 train acc 0.9816903045803301\n",
            "epoch 40 batch id 4311 loss 0.1832365244626999 train acc 0.9817001565762005\n",
            "epoch 40 batch id 4321 loss 0.02577272243797779 train acc 0.9816882666049526\n",
            "epoch 40 batch id 4331 loss 0.1378084272146225 train acc 0.9816800392519048\n",
            "epoch 40 batch id 4341 loss 0.03934049606323242 train acc 0.9816862474084312\n",
            "epoch 40 batch id 4351 loss 0.03274572268128395 train acc 0.9816565157435072\n",
            "epoch 40 batch id 4361 loss 0.012407686561346054 train acc 0.9816663322632424\n",
            "epoch 40 batch id 4371 loss 0.10382147878408432 train acc 0.9816761038663921\n",
            "epoch 40 batch id 4381 loss 0.1015704944729805 train acc 0.9816679981739329\n",
            "epoch 40 batch id 4391 loss 0.001305249286815524 train acc 0.9816706046458665\n",
            "epoch 40 batch id 4401 loss 0.07250741124153137 train acc 0.9816696489434219\n",
            "epoch 40 batch id 4411 loss 0.0017397587653249502 train acc 0.9816580707322603\n",
            "epoch 40 batch id 4421 loss 0.04873323068022728 train acc 0.9816642162406696\n",
            "epoch 40 batch id 4431 loss 0.014752059243619442 train acc 0.9816597551342812\n",
            "epoch 40 batch id 4441 loss 0.039241984486579895 train acc 0.9816658691736095\n",
            "epoch 40 batch id 4451 loss 0.00021255901083350182 train acc 0.9816684452931925\n",
            "epoch 40 batch id 4461 loss 0.00035693024983629584 train acc 0.9816745124411567\n",
            "epoch 40 batch id 4471 loss 0.06013704091310501 train acc 0.9816770577052114\n",
            "epoch 40 batch id 4481 loss 0.0148230055347085 train acc 0.9816691307743807\n",
            "epoch 40 batch id 4491 loss 0.06211288645863533 train acc 0.9816681975061233\n",
            "epoch 40 batch id 4501 loss 0.0089348079636693 train acc 0.9816533825816485\n",
            "epoch 40 batch id 4511 loss 0.060003094375133514 train acc 0.9816524883617823\n",
            "epoch 40 batch id 4521 loss 0.021974783390760422 train acc 0.9816619663791196\n",
            "epoch 40 batch id 4531 loss 0.006472656968981028 train acc 0.9816541602295299\n",
            "epoch 40 batch id 4541 loss 0.05265280604362488 train acc 0.9816670336930191\n",
            "epoch 40 batch id 4551 loss 0.08414527028799057 train acc 0.981666117336849\n",
            "epoch 40 batch id 4561 loss 0.08408807218074799 train acc 0.9816515018636264\n",
            "epoch 40 batch id 4571 loss 0.029840640723705292 train acc 0.9816540417851674\n",
            "epoch 40 batch id 4581 loss 0.03347282111644745 train acc 0.9816668030997598\n",
            "epoch 40 batch id 4591 loss 0.12520207464694977 train acc 0.9816590884338924\n",
            "epoch 40 batch id 4601 loss 0.014976298436522484 train acc 0.9816514073027602\n",
            "epoch 40 batch id 4611 loss 0.04837493225932121 train acc 0.9816640913034049\n",
            "epoch 40 batch id 4621 loss 0.015426219440996647 train acc 0.9816699578013417\n",
            "epoch 40 batch id 4631 loss 0.1449446827173233 train acc 0.9816656769596199\n",
            "epoch 40 batch id 4641 loss 0.03364521265029907 train acc 0.981674881491058\n",
            "epoch 40 batch id 4651 loss 0.1343994140625 train acc 0.981677327456461\n",
            "epoch 40 batch id 4661 loss 0.031561966985464096 train acc 0.981683115211328\n",
            "epoch 40 batch id 4671 loss 0.004458967596292496 train acc 0.9817022586169984\n",
            "epoch 40 batch id 4681 loss 0.18215903639793396 train acc 0.981717982268746\n",
            "epoch 40 batch id 4691 loss 0.00859435647726059 train acc 0.981706992112556\n",
            "epoch 40 batch id 4701 loss 0.0217434074729681 train acc 0.9817159912784514\n",
            "epoch 40 batch id 4711 loss 0.03485659137368202 train acc 0.9817183188282742\n",
            "epoch 40 batch id 4721 loss 0.002442987635731697 train acc 0.9817239461978394\n",
            "epoch 40 batch id 4731 loss 0.019094660878181458 train acc 0.9817328524624815\n",
            "epoch 40 batch id 4741 loss 0.16353993117809296 train acc 0.9817351297194684\n",
            "epoch 40 batch id 4751 loss 0.15223939716815948 train acc 0.9817406861713324\n",
            "epoch 40 batch id 4761 loss 0.1045074388384819 train acc 0.9817330917874396\n",
            "epoch 40 batch id 4771 loss 0.0689220279455185 train acc 0.9817451792077133\n",
            "epoch 40 batch id 4781 loss 0.0976475402712822 train acc 0.9817245346161891\n",
            "epoch 40 batch id 4791 loss 0.18868127465248108 train acc 0.9817039762053851\n",
            "epoch 40 batch id 4801 loss 0.06665018945932388 train acc 0.9816867579670902\n",
            "epoch 40 batch id 4811 loss 0.008258769288659096 train acc 0.9816988411972563\n",
            "epoch 40 batch id 4821 loss 0.06197885051369667 train acc 0.9817043922422733\n",
            "epoch 40 batch id 4831 loss 0.015734579414129257 train acc 0.9817066859863383\n",
            "epoch 40 batch id 4841 loss 0.15746720135211945 train acc 0.9817154255319149\n",
            "epoch 40 batch id 4851 loss 0.045454733073711395 train acc 0.9817273500309215\n",
            "epoch 40 batch id 4861 loss 0.00908796675503254 train acc 0.9817135105945278\n",
            "epoch 40 batch id 4871 loss 0.004477667156606913 train acc 0.9817285978238555\n",
            "epoch 40 batch id 4881 loss 0.033478278666734695 train acc 0.9817212149149764\n",
            "epoch 40 batch id 4891 loss 0.020264724269509315 train acc 0.9817170568390922\n",
            "epoch 40 batch id 4901 loss 0.00180230347905308 train acc 0.9817224801061007\n",
            "epoch 40 batch id 4911 loss 0.13876354694366455 train acc 0.9817183363877011\n",
            "epoch 40 batch id 4921 loss 0.11361733824014664 train acc 0.981717384677911\n",
            "epoch 40 batch id 4931 loss 0.06048398092389107 train acc 0.9816974244575137\n",
            "epoch 40 batch id 4941 loss 0.04546300694346428 train acc 0.981680707346691\n",
            "epoch 40 batch id 4951 loss 0.07560832798480988 train acc 0.9816766814784892\n",
            "epoch 40 train acc 0.9816767954407908\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9450c841b47a41dfafd536cfd12469fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 40 loss 0.47169530391693115 test acc 0.8514261821847507\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c018da5ebda14ab6ba4ba626bbdca529",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 41 batch id 1 loss 0.07275312393903732 train acc 0.96875\n",
            "epoch 41 batch id 11 loss 0.013000622391700745 train acc 0.9886363636363636\n",
            "epoch 41 batch id 21 loss 0.030610764399170876 train acc 0.9873511904761905\n",
            "epoch 41 batch id 31 loss 0.13755503296852112 train acc 0.9868951612903226\n",
            "epoch 41 batch id 41 loss 0.039454177021980286 train acc 0.9862804878048781\n",
            "epoch 41 batch id 51 loss 0.04257197305560112 train acc 0.9846813725490197\n",
            "epoch 41 batch id 61 loss 0.04884352535009384 train acc 0.9818135245901639\n",
            "epoch 41 batch id 71 loss 0.08599741011857986 train acc 0.9817341549295775\n",
            "epoch 41 batch id 81 loss 0.10869801789522171 train acc 0.9814814814814815\n",
            "epoch 41 batch id 91 loss 0.07017459720373154 train acc 0.9821428571428571\n",
            "epoch 41 batch id 101 loss 0.04030478373169899 train acc 0.9806621287128713\n",
            "epoch 41 batch id 111 loss 0.00038153937202878296 train acc 0.9807150900900901\n",
            "epoch 41 batch id 121 loss 0.041405390948057175 train acc 0.9808884297520661\n",
            "epoch 41 batch id 131 loss 0.12546205520629883 train acc 0.9811545801526718\n",
            "epoch 41 batch id 141 loss 0.03792805224657059 train acc 0.9817154255319149\n",
            "epoch 41 batch id 151 loss 0.052386574447155 train acc 0.9822019867549668\n",
            "epoch 41 batch id 161 loss 0.002780355280265212 train acc 0.9823369565217391\n",
            "epoch 41 batch id 171 loss 0.011781599372625351 train acc 0.982547514619883\n",
            "epoch 41 batch id 181 loss 0.02316143363714218 train acc 0.9827348066298343\n",
            "epoch 41 batch id 191 loss 0.08012788742780685 train acc 0.9823298429319371\n",
            "epoch 41 batch id 201 loss 0.07367034256458282 train acc 0.9825093283582089\n",
            "epoch 41 batch id 211 loss 0.0027885371819138527 train acc 0.9828199052132701\n",
            "epoch 41 batch id 221 loss 0.06457103788852692 train acc 0.9826781674208145\n",
            "epoch 41 batch id 231 loss 0.021046772599220276 train acc 0.9831574675324676\n",
            "epoch 41 batch id 241 loss 0.05406644195318222 train acc 0.9832079875518672\n",
            "epoch 41 batch id 251 loss 0.08496279269456863 train acc 0.9831299800796812\n",
            "epoch 41 batch id 261 loss 0.03944888710975647 train acc 0.9828783524904214\n",
            "epoch 41 batch id 271 loss 0.00020658786525018513 train acc 0.9832218634686347\n",
            "epoch 41 batch id 281 loss 0.015420804731547832 train acc 0.9834853202846975\n",
            "epoch 41 batch id 291 loss 0.06549995392560959 train acc 0.9834621993127147\n",
            "epoch 41 batch id 301 loss 0.09098295122385025 train acc 0.9829734219269103\n",
            "epoch 41 batch id 311 loss 0.02186252921819687 train acc 0.9832194533762058\n",
            "epoch 41 batch id 321 loss 0.19234998524188995 train acc 0.9831580996884736\n",
            "epoch 41 batch id 331 loss 0.0005877765943296254 train acc 0.9832420694864048\n",
            "epoch 41 batch id 341 loss 0.24000658094882965 train acc 0.9830920087976539\n",
            "epoch 41 batch id 351 loss 0.05324120819568634 train acc 0.9832621082621082\n",
            "epoch 41 batch id 361 loss 0.017411230131983757 train acc 0.9832496537396122\n",
            "epoch 41 batch id 371 loss 0.20776677131652832 train acc 0.983279986522911\n",
            "epoch 41 batch id 381 loss 0.02372051030397415 train acc 0.9833087270341208\n",
            "epoch 41 batch id 391 loss 0.07179667800664902 train acc 0.9833759590792839\n",
            "epoch 41 batch id 401 loss 0.06401273608207703 train acc 0.9833619077306733\n",
            "epoch 41 batch id 411 loss 0.02959657646715641 train acc 0.9833485401459854\n",
            "epoch 41 batch id 421 loss 0.02849876508116722 train acc 0.9833358076009501\n",
            "epoch 41 batch id 431 loss 0.07433976233005524 train acc 0.9832149071925754\n",
            "epoch 41 batch id 441 loss 0.10686129331588745 train acc 0.9830640589569161\n",
            "epoch 41 batch id 451 loss 0.014893450774252415 train acc 0.9831277716186253\n",
            "epoch 41 batch id 461 loss 0.04610729590058327 train acc 0.9829853579175705\n",
            "epoch 41 batch id 471 loss 0.012018936686217785 train acc 0.9831475583864119\n",
            "epoch 41 batch id 481 loss 0.05154851824045181 train acc 0.9832055613305614\n",
            "epoch 41 batch id 491 loss 0.15487325191497803 train acc 0.983165733197556\n",
            "epoch 41 batch id 501 loss 0.3157901167869568 train acc 0.9831586826347305\n",
            "epoch 41 batch id 511 loss 0.09683496505022049 train acc 0.9830295988258317\n",
            "epoch 41 batch id 521 loss 0.12554888427257538 train acc 0.9829354606525912\n",
            "epoch 41 batch id 531 loss 0.020100390538573265 train acc 0.9827565913370998\n",
            "epoch 41 batch id 541 loss 0.014878650195896626 train acc 0.9825843345656192\n",
            "epoch 41 batch id 551 loss 0.025838101282715797 train acc 0.9825884754990926\n",
            "epoch 41 batch id 561 loss 0.040450144559144974 train acc 0.9824532085561497\n",
            "epoch 41 batch id 571 loss 0.014539564959704876 train acc 0.9825142294220666\n",
            "epoch 41 batch id 581 loss 0.1075388565659523 train acc 0.9824117900172117\n",
            "epoch 41 batch id 591 loss 0.029360052198171616 train acc 0.9824714467005076\n",
            "epoch 41 batch id 601 loss 0.02798212692141533 train acc 0.9824771214642263\n",
            "epoch 41 batch id 611 loss 0.07764460146427155 train acc 0.9824058919803601\n",
            "epoch 41 batch id 621 loss 0.17792190611362457 train acc 0.9824376006441223\n",
            "epoch 41 batch id 631 loss 0.18475496768951416 train acc 0.9823692551505546\n",
            "epoch 41 batch id 641 loss 0.016659794375300407 train acc 0.9824492979719188\n",
            "epoch 41 batch id 651 loss 0.023971330374479294 train acc 0.9824308755760369\n",
            "epoch 41 batch id 661 loss 0.007677579764276743 train acc 0.9825075642965204\n",
            "epoch 41 batch id 671 loss 0.10216082632541656 train acc 0.982535394932936\n",
            "epoch 41 batch id 681 loss 0.05467433109879494 train acc 0.9825624082232012\n",
            "epoch 41 batch id 691 loss 0.04409922659397125 train acc 0.982566027496382\n",
            "epoch 41 batch id 701 loss 0.05632520094513893 train acc 0.9824803851640513\n",
            "epoch 41 batch id 711 loss 0.1748349964618683 train acc 0.9824630801687764\n",
            "epoch 41 batch id 721 loss 0.04649770259857178 train acc 0.9823812413314841\n",
            "epoch 41 batch id 731 loss 0.012827102094888687 train acc 0.9823016415868673\n",
            "epoch 41 batch id 741 loss 0.10150423645973206 train acc 0.9823085357624831\n",
            "epoch 41 batch id 751 loss 0.025233350694179535 train acc 0.9822528295605859\n",
            "epoch 41 batch id 761 loss 0.03659169003367424 train acc 0.9822807161629435\n",
            "epoch 41 batch id 771 loss 0.02858235128223896 train acc 0.9821862840466926\n",
            "epoch 41 batch id 781 loss 0.020551975816488266 train acc 0.9822543213828425\n",
            "epoch 41 batch id 791 loss 0.1041821539402008 train acc 0.9823206384323641\n",
            "epoch 41 batch id 801 loss 0.04117785766720772 train acc 0.9823657927590512\n",
            "epoch 41 batch id 811 loss 0.13146528601646423 train acc 0.9823520345252774\n",
            "epoch 41 batch id 821 loss 0.007814946584403515 train acc 0.9824147381242387\n",
            "epoch 41 batch id 831 loss 0.10346104204654694 train acc 0.9824383273164862\n",
            "epoch 41 batch id 841 loss 0.003332975320518017 train acc 0.9823870392390012\n",
            "epoch 41 batch id 851 loss 0.04372607544064522 train acc 0.9823369565217391\n",
            "epoch 41 batch id 861 loss 0.02289174124598503 train acc 0.982378774680604\n",
            "epoch 41 batch id 871 loss 0.014011776074767113 train acc 0.9823478760045924\n",
            "epoch 41 batch id 881 loss 0.02069409191608429 train acc 0.9823354143019296\n",
            "epoch 41 batch id 891 loss 0.0003908296930603683 train acc 0.9823407687991021\n",
            "epoch 41 batch id 901 loss 0.025702474638819695 train acc 0.9822939789123196\n",
            "epoch 41 batch id 911 loss 0.007656523957848549 train acc 0.9823339736553238\n",
            "epoch 41 batch id 921 loss 0.000634652329608798 train acc 0.9822543431053203\n",
            "epoch 41 batch id 931 loss 0.00197914382442832 train acc 0.9822939044038668\n",
            "epoch 41 batch id 941 loss 0.10571877658367157 train acc 0.9823160201912858\n",
            "epoch 41 batch id 951 loss 0.06201169267296791 train acc 0.9822390904311251\n",
            "epoch 41 batch id 961 loss 0.01563914306461811 train acc 0.9822287981269511\n",
            "epoch 41 batch id 971 loss 0.03543800115585327 train acc 0.982186534500515\n",
            "epoch 41 batch id 981 loss 0.01189220231026411 train acc 0.9822406982670744\n",
            "epoch 41 batch id 991 loss 0.06928342580795288 train acc 0.9822937689202825\n",
            "epoch 41 batch id 1001 loss 0.011359725147485733 train acc 0.9823145604395604\n",
            "epoch 41 batch id 1011 loss 0.0005893374909646809 train acc 0.9823040306627102\n",
            "epoch 41 batch id 1021 loss 0.02739868313074112 train acc 0.9823396180215475\n",
            "epoch 41 batch id 1031 loss 0.008999809622764587 train acc 0.9823745150339476\n",
            "epoch 41 batch id 1041 loss 0.03750653564929962 train acc 0.9824087415946205\n",
            "epoch 41 batch id 1051 loss 0.015800470486283302 train acc 0.982323382492864\n",
            "epoch 41 batch id 1061 loss 0.01827206462621689 train acc 0.9822396324222432\n",
            "epoch 41 batch id 1071 loss 0.0004968214780092239 train acc 0.9822595704948646\n",
            "epoch 41 batch id 1081 loss 0.11550826579332352 train acc 0.9821924144310823\n",
            "epoch 41 batch id 1091 loss 0.04544796422123909 train acc 0.9821837763519706\n",
            "epoch 41 batch id 1101 loss 0.03265950456261635 train acc 0.9822178701180745\n",
            "epoch 41 batch id 1111 loss 0.06224619224667549 train acc 0.982195094509451\n",
            "epoch 41 batch id 1121 loss 0.010023549199104309 train acc 0.9822702943800179\n",
            "epoch 41 batch id 1131 loss 0.030798790976405144 train acc 0.9822198275862069\n",
            "epoch 41 batch id 1141 loss 0.021259400993585587 train acc 0.9822661042944786\n",
            "epoch 41 batch id 1151 loss 0.04121667891740799 train acc 0.9822029756733276\n",
            "epoch 41 batch id 1161 loss 0.06283882260322571 train acc 0.9821947674418605\n",
            "epoch 41 batch id 1171 loss 0.03879430145025253 train acc 0.9821333262169086\n",
            "epoch 41 batch id 1181 loss 0.002186601283028722 train acc 0.9822184589331076\n",
            "epoch 41 batch id 1191 loss 0.06876245141029358 train acc 0.9821972082283795\n",
            "epoch 41 batch id 1201 loss 0.032235048711299896 train acc 0.9822803913405496\n",
            "epoch 41 batch id 1211 loss 0.00030045249150134623 train acc 0.9823234929810074\n",
            "epoch 41 batch id 1221 loss 0.0865626335144043 train acc 0.9823147010647011\n",
            "epoch 41 batch id 1231 loss 0.02618422731757164 train acc 0.9822045085296507\n",
            "epoch 41 batch id 1241 loss 0.008373299613595009 train acc 0.9821968170829976\n",
            "epoch 41 batch id 1251 loss 0.09841848909854889 train acc 0.9821892486011191\n",
            "epoch 41 batch id 1261 loss 0.023579655215144157 train acc 0.9822313639968279\n",
            "epoch 41 batch id 1271 loss 0.02313164621591568 train acc 0.9822236428009441\n",
            "epoch 41 batch id 1281 loss 0.04276185482740402 train acc 0.98224043715847\n",
            "epoch 41 batch id 1291 loss 0.1668538600206375 train acc 0.9822448683191325\n",
            "epoch 41 batch id 1301 loss 0.003629302838817239 train acc 0.9822011913912375\n",
            "epoch 41 batch id 1311 loss 0.012733008712530136 train acc 0.9821939359267735\n",
            "epoch 41 batch id 1321 loss 0.012118374928832054 train acc 0.982139477668433\n",
            "epoch 41 batch id 1331 loss 0.014523513615131378 train acc 0.9821210555972952\n",
            "epoch 41 batch id 1341 loss 0.13782444596290588 train acc 0.9821145600298284\n",
            "epoch 41 batch id 1351 loss 0.0024797716177999973 train acc 0.9821081606217616\n",
            "epoch 41 batch id 1361 loss 0.014195342548191547 train acc 0.9821362968405584\n",
            "epoch 41 batch id 1371 loss 0.006039769388735294 train acc 0.9821184354485777\n",
            "epoch 41 batch id 1381 loss 0.023844394832849503 train acc 0.9821234612599565\n",
            "epoch 41 batch id 1391 loss 0.13147741556167603 train acc 0.9820385514018691\n",
            "epoch 41 batch id 1401 loss 0.00884720403701067 train acc 0.9819883119200571\n",
            "epoch 41 batch id 1411 loss 0.015898628160357475 train acc 0.9820605953224664\n",
            "epoch 41 batch id 1421 loss 0.006380431819707155 train acc 0.9820988740323716\n",
            "epoch 41 batch id 1431 loss 0.07331221550703049 train acc 0.9820711041229909\n",
            "epoch 41 batch id 1441 loss 0.01878463849425316 train acc 0.9820220333102012\n",
            "epoch 41 batch id 1451 loss 0.11488465219736099 train acc 0.9820705547898001\n",
            "epoch 41 batch id 1461 loss 0.011385831981897354 train acc 0.9820649383983573\n",
            "epoch 41 batch id 1471 loss 0.0925571396946907 train acc 0.9821125084976207\n",
            "epoch 41 batch id 1481 loss 0.05497469753026962 train acc 0.9821172349763673\n",
            "epoch 41 batch id 1491 loss 0.05358690023422241 train acc 0.9821428571428571\n",
            "epoch 41 batch id 1501 loss 0.047091614454984665 train acc 0.9821160892738174\n",
            "epoch 41 batch id 1511 loss 0.016326112672686577 train acc 0.9821724023825281\n",
            "epoch 41 batch id 1521 loss 0.0009241952793672681 train acc 0.9822382478632479\n",
            "epoch 41 batch id 1531 loss 0.019081471487879753 train acc 0.9822522044415415\n",
            "epoch 41 batch id 1541 loss 0.01790992170572281 train acc 0.9822558403634004\n",
            "epoch 41 batch id 1551 loss 0.04789980500936508 train acc 0.9822695035460993\n",
            "epoch 41 batch id 1561 loss 0.02644345350563526 train acc 0.9822529628443306\n",
            "epoch 41 batch id 1571 loss 0.007811324205249548 train acc 0.9822565245066837\n",
            "epoch 41 batch id 1581 loss 0.03745619207620621 train acc 0.9822501581277673\n",
            "epoch 41 batch id 1591 loss 0.08909150958061218 train acc 0.9822733343808925\n",
            "epoch 41 batch id 1601 loss 0.011417588219046593 train acc 0.9823254996876952\n",
            "epoch 41 batch id 1611 loss 0.052856430411338806 train acc 0.9822800279329609\n",
            "epoch 41 batch id 1621 loss 0.12507782876491547 train acc 0.9822543954349168\n",
            "epoch 41 batch id 1631 loss 0.00015134949353523552 train acc 0.982276977314531\n",
            "epoch 41 batch id 1641 loss 0.07352090626955032 train acc 0.9822326325411335\n",
            "epoch 41 batch id 1651 loss 0.09731505066156387 train acc 0.9821793609933374\n",
            "epoch 41 batch id 1661 loss 0.06749185919761658 train acc 0.9822113937387116\n",
            "epoch 41 batch id 1671 loss 0.020162265747785568 train acc 0.9821869389587073\n",
            "epoch 41 batch id 1681 loss 0.013931517489254475 train acc 0.9821441850089233\n",
            "epoch 41 batch id 1691 loss 0.018012432381510735 train acc 0.9821481371969248\n",
            "epoch 41 batch id 1701 loss 0.027197731658816338 train acc 0.9821152998236331\n",
            "epoch 41 batch id 1711 loss 0.005453185178339481 train acc 0.9821193746347165\n",
            "epoch 41 batch id 1721 loss 0.01379819680005312 train acc 0.9821415601394538\n",
            "epoch 41 batch id 1731 loss 8.851495658745989e-05 train acc 0.982145436164067\n",
            "epoch 41 batch id 1741 loss 0.011653383262455463 train acc 0.9821313182079264\n",
            "epoch 41 batch id 1751 loss 0.03453867882490158 train acc 0.9821173615077099\n",
            "epoch 41 batch id 1761 loss 0.016686124727129936 train acc 0.9821568001135719\n",
            "epoch 41 batch id 1771 loss 0.0023358045145869255 train acc 0.9822046160361377\n",
            "epoch 41 batch id 1781 loss 0.06111256405711174 train acc 0.982243121841662\n",
            "epoch 41 batch id 1791 loss 0.04277299344539642 train acc 0.9822550251256281\n",
            "epoch 41 batch id 1801 loss 0.03972519934177399 train acc 0.9823014991671294\n",
            "epoch 41 batch id 1811 loss 0.03386688604950905 train acc 0.9823215764770845\n",
            "epoch 41 batch id 1821 loss 0.14479126036167145 train acc 0.9822642092257001\n",
            "epoch 41 batch id 1831 loss 0.0168460663408041 train acc 0.9822501365374112\n",
            "epoch 41 batch id 1841 loss 0.0035595193039625883 train acc 0.9822107550244432\n",
            "epoch 41 batch id 1851 loss 0.13282634317874908 train acc 0.9822224473257699\n",
            "epoch 41 batch id 1861 loss 0.04100862890481949 train acc 0.982217221923697\n",
            "epoch 41 batch id 1871 loss 0.07903549075126648 train acc 0.9822538081239979\n",
            "epoch 41 batch id 1881 loss 0.015843810513615608 train acc 0.9822484715576821\n",
            "epoch 41 batch id 1891 loss 0.02409699372947216 train acc 0.9822762427287149\n",
            "epoch 41 batch id 1901 loss 0.0885353833436966 train acc 0.9822708442924777\n",
            "epoch 41 batch id 1911 loss 0.12057840079069138 train acc 0.9822327969649398\n",
            "epoch 41 batch id 1921 loss 0.0002463546406943351 train acc 0.9821707444039562\n",
            "epoch 41 batch id 1931 loss 0.03673819825053215 train acc 0.9821740678404971\n",
            "epoch 41 batch id 1941 loss 0.006379242520779371 train acc 0.9821934569809376\n",
            "epoch 41 batch id 1951 loss 0.04068521410226822 train acc 0.9821966299333675\n",
            "epoch 41 batch id 1961 loss 0.04395022615790367 train acc 0.9822555456399796\n",
            "epoch 41 batch id 1971 loss 0.024118145927786827 train acc 0.9822504439370878\n",
            "epoch 41 batch id 1981 loss 0.05139432102441788 train acc 0.9822138440181727\n",
            "epoch 41 batch id 1991 loss 0.1554938554763794 train acc 0.9821854595680563\n",
            "epoch 41 batch id 2001 loss 0.0005846234853379428 train acc 0.9822198275862069\n",
            "epoch 41 batch id 2011 loss 0.12378136068582535 train acc 0.9822227747389358\n",
            "epoch 41 batch id 2021 loss 0.081597238779068 train acc 0.9822334240475012\n",
            "epoch 41 batch id 2031 loss 0.00016030091501306742 train acc 0.9822131954702117\n",
            "epoch 41 batch id 2041 loss 0.005879863630980253 train acc 0.9822161317981382\n",
            "epoch 41 batch id 2051 loss 0.28536275029182434 train acc 0.9822114212579229\n",
            "epoch 41 batch id 2061 loss 0.005924524273723364 train acc 0.9822219189713731\n",
            "epoch 41 batch id 2071 loss 0.0686410516500473 train acc 0.9822021366489618\n",
            "epoch 41 batch id 2081 loss 0.04132944345474243 train acc 0.9822050696780394\n",
            "epoch 41 batch id 2091 loss 0.037213485687971115 train acc 0.9822154471544715\n",
            "epoch 41 batch id 2101 loss 0.013755986467003822 train acc 0.9822331627796288\n",
            "epoch 41 batch id 2111 loss 0.00723670981824398 train acc 0.9822655139744197\n",
            "epoch 41 batch id 2121 loss 0.03345035761594772 train acc 0.9823049269212636\n",
            "epoch 41 batch id 2131 loss 0.0013937900075688958 train acc 0.9823439699671516\n",
            "epoch 41 batch id 2141 loss 0.03393946588039398 train acc 0.9823388603456329\n",
            "epoch 41 batch id 2151 loss 0.04901190847158432 train acc 0.9823555904230591\n",
            "epoch 41 batch id 2161 loss 0.23681272566318512 train acc 0.9823360134197131\n",
            "epoch 41 batch id 2171 loss 0.010826819576323032 train acc 0.9823597996315062\n",
            "epoch 41 batch id 2181 loss 0.11349982023239136 train acc 0.9823475469967905\n",
            "epoch 41 batch id 2191 loss 0.08415877819061279 train acc 0.9823354062072113\n",
            "epoch 41 batch id 2201 loss 0.015325226821005344 train acc 0.9823588709677419\n",
            "epoch 41 batch id 2211 loss 0.03403332829475403 train acc 0.9823397218453188\n",
            "epoch 41 batch id 2221 loss 0.013773694634437561 train acc 0.9823488856371004\n",
            "epoch 41 batch id 2231 loss 0.01294429786503315 train acc 0.9823439601075751\n",
            "epoch 41 batch id 2241 loss 0.0454426147043705 train acc 0.9823530232039268\n",
            "epoch 41 batch id 2251 loss 0.0018474900862202048 train acc 0.9823342403376277\n",
            "epoch 41 batch id 2261 loss 0.00016417622100561857 train acc 0.9823639982308713\n",
            "epoch 41 batch id 2271 loss 0.07066080719232559 train acc 0.9823797335975342\n",
            "epoch 41 batch id 2281 loss 0.026356015354394913 train acc 0.9823953309951775\n",
            "epoch 41 batch id 2291 loss 0.014558159746229649 train acc 0.9823971518987342\n",
            "epoch 41 batch id 2301 loss 0.020566271618008614 train acc 0.9824057475010864\n",
            "epoch 41 batch id 2311 loss 0.0848233699798584 train acc 0.9824075075724794\n",
            "epoch 41 batch id 2321 loss 0.10034163296222687 train acc 0.9823823244291254\n",
            "epoch 41 batch id 2331 loss 0.06477681547403336 train acc 0.9823171385671385\n",
            "epoch 41 batch id 2341 loss 0.0008110872586257756 train acc 0.9823259290901324\n",
            "epoch 41 batch id 2351 loss 0.026635346934199333 train acc 0.982314706507869\n",
            "epoch 41 batch id 2361 loss 0.08355401456356049 train acc 0.9823366687844134\n",
            "epoch 41 batch id 2371 loss 0.0038665826432406902 train acc 0.9823584458034584\n",
            "epoch 41 batch id 2381 loss 0.0003401433059480041 train acc 0.9823669151616967\n",
            "epoch 41 batch id 2391 loss 0.022180084139108658 train acc 0.9823557089084065\n",
            "epoch 41 batch id 2401 loss 0.05321573466062546 train acc 0.9823511037067888\n",
            "epoch 41 batch id 2411 loss 0.06472350656986237 train acc 0.9823270945665699\n",
            "epoch 41 batch id 2421 loss 0.05048944056034088 train acc 0.9823484613795952\n",
            "epoch 41 batch id 2431 loss 0.0459832102060318 train acc 0.9823696524064172\n",
            "epoch 41 batch id 2441 loss 0.0016842554323375225 train acc 0.9823842687423188\n",
            "epoch 41 batch id 2451 loss 0.00025676010409370065 train acc 0.9823413912688699\n",
            "epoch 41 batch id 2461 loss 0.029117155820131302 train acc 0.9823433055668428\n",
            "epoch 41 batch id 2471 loss 0.08085864037275314 train acc 0.9822882942128692\n",
            "epoch 41 batch id 2481 loss 0.06733193248510361 train acc 0.9823281942765014\n",
            "epoch 41 batch id 2491 loss 0.08353261649608612 train acc 0.9823050481734243\n",
            "epoch 41 batch id 2501 loss 0.0024218251928687096 train acc 0.9823320671731307\n",
            "epoch 41 batch id 2511 loss 0.055903736501932144 train acc 0.9823402031063322\n",
            "epoch 41 batch id 2521 loss 0.07525693625211716 train acc 0.9822924930583102\n",
            "epoch 41 batch id 2531 loss 0.0659082904458046 train acc 0.9823130679573291\n",
            "epoch 41 batch id 2541 loss 0.028192494064569473 train acc 0.9823088842975206\n",
            "epoch 41 batch id 2551 loss 0.028802325949072838 train acc 0.9823169835358683\n",
            "epoch 41 batch id 2561 loss 0.01453064102679491 train acc 0.9823372217883639\n",
            "epoch 41 batch id 2571 loss 0.1039346233010292 train acc 0.9823147607934656\n",
            "epoch 41 batch id 2581 loss 0.046985797584056854 train acc 0.9822864199922511\n",
            "epoch 41 batch id 2591 loss 0.011867783963680267 train acc 0.9822824199150907\n",
            "epoch 41 batch id 2601 loss 0.023291293531656265 train acc 0.9823325163398693\n",
            "epoch 41 batch id 2611 loss 0.0002815877669490874 train acc 0.9823702604366144\n",
            "epoch 41 batch id 2621 loss 0.049430910497903824 train acc 0.9823779091949637\n",
            "epoch 41 batch id 2631 loss 0.17893896996974945 train acc 0.9823261117445838\n",
            "epoch 41 batch id 2641 loss 0.04509345442056656 train acc 0.9823338697463082\n",
            "epoch 41 batch id 2651 loss 0.000392519636079669 train acc 0.9823356752168992\n",
            "epoch 41 batch id 2661 loss 0.00392649183049798 train acc 0.9823374671176249\n",
            "epoch 41 batch id 2671 loss 0.0006502197938971221 train acc 0.9823509453388244\n",
            "epoch 41 batch id 2681 loss 0.04229268431663513 train acc 0.982335182767624\n",
            "epoch 41 batch id 2691 loss 0.010217185132205486 train acc 0.9823079245633594\n",
            "epoch 41 batch id 2701 loss 0.027758989483118057 train acc 0.9823040077748982\n",
            "epoch 41 batch id 2711 loss 0.000658683420624584 train acc 0.9823001198819624\n",
            "epoch 41 batch id 2721 loss 0.01290038600564003 train acc 0.9823020029400955\n",
            "epoch 41 batch id 2731 loss 0.052756037563085556 train acc 0.9822924295129989\n",
            "epoch 41 batch id 2741 loss 0.12367521226406097 train acc 0.9822943268879971\n",
            "epoch 41 batch id 2751 loss 0.11318979412317276 train acc 0.9822678117048346\n",
            "epoch 41 batch id 2761 loss 0.050943076610565186 train acc 0.9822867620427381\n",
            "epoch 41 batch id 2771 loss 0.01746532879769802 train acc 0.982294298087333\n",
            "epoch 41 batch id 2781 loss 0.024107780307531357 train acc 0.9823298723480762\n",
            "epoch 41 batch id 2791 loss 0.019410550594329834 train acc 0.9823260032246507\n",
            "epoch 41 batch id 2801 loss 0.08149392902851105 train acc 0.982327740092824\n",
            "epoch 41 batch id 2811 loss 0.02938954345881939 train acc 0.9823461401636429\n",
            "epoch 41 batch id 2821 loss 0.05104059725999832 train acc 0.9823644097837646\n",
            "epoch 41 batch id 2831 loss 0.1336316019296646 train acc 0.9823659925821264\n",
            "epoch 41 batch id 2841 loss 0.06766168028116226 train acc 0.9823620644139388\n",
            "epoch 41 batch id 2851 loss 0.00020569803018588573 train acc 0.9823691248684672\n",
            "epoch 41 batch id 2861 loss 0.0868741124868393 train acc 0.9823542904578818\n",
            "epoch 41 batch id 2871 loss 0.07452195137739182 train acc 0.9823667711598746\n",
            "epoch 41 batch id 2881 loss 0.15031808614730835 train acc 0.9823520479000347\n",
            "epoch 41 batch id 2891 loss 0.009973200038075447 train acc 0.9823482359045314\n",
            "epoch 41 batch id 2901 loss 0.013163150288164616 train acc 0.9823552223371251\n",
            "epoch 41 batch id 2911 loss 0.030046656727790833 train acc 0.9823567931982137\n",
            "epoch 41 batch id 2921 loss 0.00017637301061768085 train acc 0.9823583533036632\n",
            "epoch 41 batch id 2931 loss 0.059185151010751724 train acc 0.9823332480382122\n",
            "epoch 41 batch id 2941 loss 0.0006988950772210956 train acc 0.98230831349881\n",
            "epoch 41 batch id 2951 loss 0.03760448470711708 train acc 0.9823417909183327\n",
            "epoch 41 batch id 2961 loss 0.02742791920900345 train acc 0.9823381036811888\n",
            "epoch 41 batch id 2971 loss 0.009686236269772053 train acc 0.9823291820935712\n",
            "epoch 41 batch id 2981 loss 0.021591654047369957 train acc 0.9823045957732305\n",
            "epoch 41 batch id 2991 loss 0.012003297917544842 train acc 0.9823115178869943\n",
            "epoch 41 batch id 3001 loss 0.007527060341089964 train acc 0.9823444268577141\n",
            "epoch 41 batch id 3011 loss 0.0031238042283803225 train acc 0.9823407920956493\n",
            "epoch 41 batch id 3021 loss 0.1255415976047516 train acc 0.9823216650115856\n",
            "epoch 41 batch id 3031 loss 0.17271524667739868 train acc 0.9822923540085781\n",
            "epoch 41 batch id 3041 loss 0.07593995332717896 train acc 0.9822580976652417\n",
            "epoch 41 batch id 3051 loss 0.0033331613522022963 train acc 0.9822855211406096\n",
            "epoch 41 batch id 3061 loss 0.08167416602373123 train acc 0.982282138190134\n",
            "epoch 41 batch id 3071 loss 0.002786908531561494 train acc 0.9823093047867144\n",
            "epoch 41 batch id 3081 loss 0.06595506519079208 train acc 0.9823160094125284\n",
            "epoch 41 batch id 3091 loss 0.02047637663781643 train acc 0.9823075056615982\n",
            "epoch 41 batch id 3101 loss 0.060249146074056625 train acc 0.9822990567558852\n",
            "epoch 41 batch id 3111 loss 0.031733475625514984 train acc 0.9823007071681131\n",
            "epoch 41 batch id 3121 loss 0.03354072570800781 train acc 0.9823023470041653\n",
            "epoch 41 batch id 3131 loss 0.0022180762607604265 train acc 0.9823239380389652\n",
            "epoch 41 batch id 3141 loss 0.0671190619468689 train acc 0.9823155444126075\n",
            "epoch 41 batch id 3151 loss 0.08120975643396378 train acc 0.9822972865756903\n",
            "epoch 41 batch id 3161 loss 0.05303112789988518 train acc 0.982293973426131\n",
            "epoch 41 batch id 3171 loss 0.12498278170824051 train acc 0.9822808262377799\n",
            "epoch 41 batch id 3181 loss 0.025885142385959625 train acc 0.9822775856648852\n",
            "epoch 41 batch id 3191 loss 0.012780746445059776 train acc 0.9822939517392667\n",
            "epoch 41 batch id 3201 loss 0.19629982113838196 train acc 0.982271165260856\n",
            "epoch 41 batch id 3211 loss 0.10971639305353165 train acc 0.9822290563687325\n",
            "epoch 41 batch id 3221 loss 0.08512315899133682 train acc 0.982201761875194\n",
            "epoch 41 batch id 3231 loss 0.0713101178407669 train acc 0.9821794722995977\n",
            "epoch 41 batch id 3241 loss 0.022899828851222992 train acc 0.9821862465288491\n",
            "epoch 41 batch id 3251 loss 0.044335998594760895 train acc 0.9821641418025223\n",
            "epoch 41 batch id 3261 loss 0.10322386026382446 train acc 0.9821182152713891\n",
            "epoch 41 batch id 3271 loss 0.03544782102108002 train acc 0.9820677927239376\n",
            "epoch 41 batch id 3281 loss 0.2466125786304474 train acc 0.982070062480951\n",
            "epoch 41 batch id 3291 loss 0.013908128254115582 train acc 0.9820438316621087\n",
            "epoch 41 batch id 3301 loss 0.02845620922744274 train acc 0.9820414268403515\n",
            "epoch 41 batch id 3311 loss 0.013183366507291794 train acc 0.9820437556629417\n",
            "epoch 41 batch id 3321 loss 0.09192570298910141 train acc 0.9820507753688648\n",
            "epoch 41 batch id 3331 loss 0.126933753490448 train acc 0.9820108450915641\n",
            "epoch 41 batch id 3341 loss 0.04609224200248718 train acc 0.9820038910505836\n",
            "epoch 41 batch id 3351 loss 0.020883195102214813 train acc 0.9819690017905103\n",
            "epoch 41 batch id 3361 loss 0.015503828413784504 train acc 0.9819901071109789\n",
            "epoch 41 batch id 3371 loss 0.07558739930391312 train acc 0.9819879115989321\n",
            "epoch 41 batch id 3381 loss 0.0010966164991259575 train acc 0.9819995933155871\n",
            "epoch 41 batch id 3391 loss 0.05876656621694565 train acc 0.982020421704512\n",
            "epoch 41 batch id 3401 loss 0.08820410072803497 train acc 0.9820135621875918\n",
            "epoch 41 batch id 3411 loss 0.087703175842762 train acc 0.9820204851949574\n",
            "epoch 41 batch id 3421 loss 0.014133731834590435 train acc 0.9820456372405729\n",
            "epoch 41 batch id 3431 loss 0.11621230840682983 train acc 0.9820524264062955\n",
            "epoch 41 batch id 3441 loss 0.02041441574692726 train acc 0.9820546352804417\n",
            "epoch 41 batch id 3451 loss 0.010343317873775959 train acc 0.9820432483338163\n",
            "epoch 41 batch id 3461 loss 0.001340773538686335 train acc 0.9820454709621497\n",
            "epoch 41 batch id 3471 loss 0.03682642802596092 train acc 0.9820566839527514\n",
            "epoch 41 batch id 3481 loss 0.07018747925758362 train acc 0.982045389255961\n",
            "epoch 41 batch id 3491 loss 0.07014384865760803 train acc 0.9820520624462905\n",
            "epoch 41 batch id 3501 loss 0.013038570992648602 train acc 0.9820542345044273\n",
            "epoch 41 batch id 3511 loss 0.016157137230038643 train acc 0.982029692395329\n",
            "epoch 41 batch id 3521 loss 0.06041385605931282 train acc 0.9820541039477422\n",
            "epoch 41 batch id 3531 loss 0.016888458281755447 train acc 0.9820739521382045\n",
            "epoch 41 batch id 3541 loss 0.059058185666799545 train acc 0.9820495622705451\n",
            "epoch 41 batch id 3551 loss 0.06095467507839203 train acc 0.9819813080822304\n",
            "epoch 41 batch id 3561 loss 0.009225718677043915 train acc 0.9819880300477394\n",
            "epoch 41 batch id 3571 loss 0.001827929401770234 train acc 0.9820122164659759\n",
            "epoch 41 batch id 3581 loss 0.050510894507169724 train acc 0.9820275411896119\n",
            "epoch 41 batch id 3591 loss 0.08494904637336731 train acc 0.9820297270955166\n",
            "epoch 41 batch id 3601 loss 0.03647395223379135 train acc 0.9820102054984726\n",
            "epoch 41 batch id 3611 loss 0.015880068764090538 train acc 0.9820254084741069\n",
            "epoch 41 batch id 3621 loss 0.041236214339733124 train acc 0.982018951946976\n",
            "epoch 41 batch id 3631 loss 0.016528518870472908 train acc 0.9820039245386946\n",
            "epoch 41 batch id 3641 loss 0.01925733871757984 train acc 0.9820318937105191\n",
            "epoch 41 batch id 3651 loss 0.029618430882692337 train acc 0.9820340317721172\n",
            "epoch 41 batch id 3661 loss 0.025666143745183945 train acc 0.9820233542747883\n",
            "epoch 41 batch id 3671 loss 0.0605810210108757 train acc 0.9820425292835739\n",
            "epoch 41 batch id 3681 loss 0.024661565199494362 train acc 0.9820700896495518\n",
            "epoch 41 batch id 3691 loss 0.0024440684355795383 train acc 0.9820678677865077\n",
            "epoch 41 batch id 3701 loss 0.03377951681613922 train acc 0.9820403269386653\n",
            "epoch 41 batch id 3711 loss 0.054699499160051346 train acc 0.9820466181622204\n",
            "epoch 41 batch id 3721 loss 0.055925190448760986 train acc 0.9820360790110185\n",
            "epoch 41 batch id 3731 loss 0.022792959585785866 train acc 0.9820255963548646\n",
            "epoch 41 batch id 3741 loss 0.037814851850271225 train acc 0.9820235231221599\n",
            "epoch 41 batch id 3751 loss 0.00037468221853487194 train acc 0.9820422887230073\n",
            "epoch 41 batch id 3761 loss 0.09139963239431381 train acc 0.9820235642116458\n",
            "epoch 41 batch id 3771 loss 0.02170538529753685 train acc 0.9820297997878547\n",
            "epoch 41 batch id 3781 loss 0.10218346118927002 train acc 0.9820029423432954\n",
            "epoch 41 batch id 3791 loss 0.0004961646045558155 train acc 0.9820298074386705\n",
            "epoch 41 batch id 3801 loss 0.03824090585112572 train acc 0.9820113128124178\n",
            "epoch 41 batch id 3811 loss 0.026539567857980728 train acc 0.9820257150354238\n",
            "epoch 41 batch id 3821 loss 0.15929192304611206 train acc 0.9820277741428945\n",
            "epoch 41 batch id 3831 loss 0.0005445888382382691 train acc 0.982005351083268\n",
            "epoch 41 batch id 3841 loss 0.00024410597688984126 train acc 0.9819871127310597\n",
            "epoch 41 batch id 3851 loss 0.0014260208699852228 train acc 0.9820095429758504\n",
            "epoch 41 batch id 3861 loss 0.0271579772233963 train acc 0.9819832944832945\n",
            "epoch 41 batch id 3871 loss 0.18791957199573517 train acc 0.9819733273056058\n",
            "epoch 41 batch id 3881 loss 0.05255144089460373 train acc 0.9819473073950012\n",
            "epoch 41 batch id 3891 loss 0.013347008265554905 train acc 0.9819495309689026\n",
            "epoch 41 batch id 3901 loss 0.009327743202447891 train acc 0.9819437323763137\n",
            "epoch 41 batch id 3911 loss 0.030622972175478935 train acc 0.9819179877269241\n",
            "epoch 41 batch id 3921 loss 0.010715640150010586 train acc 0.9819282389696506\n",
            "epoch 41 batch id 3931 loss 0.08329110592603683 train acc 0.9819106143474943\n",
            "epoch 41 batch id 3941 loss 0.011208032257854939 train acc 0.981912902816544\n",
            "epoch 41 batch id 3951 loss 0.1858282834291458 train acc 0.981887496836244\n",
            "epoch 41 batch id 3961 loss 0.07483717054128647 train acc 0.981905610956829\n",
            "epoch 41 batch id 3971 loss 0.05291169881820679 train acc 0.9818960904054395\n",
            "epoch 41 batch id 3981 loss 0.01135882269591093 train acc 0.9819023172569706\n",
            "epoch 41 batch id 3991 loss 0.09505657851696014 train acc 0.9819006827862691\n",
            "epoch 41 batch id 4001 loss 0.06445510685443878 train acc 0.9818873406648337\n",
            "epoch 41 batch id 4011 loss 0.011595395393669605 train acc 0.9818701695337821\n",
            "epoch 41 batch id 4021 loss 0.016662627458572388 train acc 0.9818725130564536\n",
            "epoch 41 batch id 4031 loss 0.03667778894305229 train acc 0.9818825973703795\n",
            "epoch 41 batch id 4041 loss 0.11175300180912018 train acc 0.9818616988369215\n",
            "epoch 41 batch id 4051 loss 0.025871453806757927 train acc 0.9818524746976055\n",
            "epoch 41 batch id 4061 loss 0.01623838022351265 train acc 0.9818663814331445\n",
            "epoch 41 batch id 4071 loss 0.2073037475347519 train acc 0.9818610292311472\n",
            "epoch 41 batch id 4081 loss 0.017331285402178764 train acc 0.9818786755697133\n",
            "epoch 41 batch id 4091 loss 0.03518405184149742 train acc 0.9818771388413591\n",
            "epoch 41 batch id 4101 loss 0.00035548265441320837 train acc 0.9818832297000731\n",
            "epoch 41 batch id 4111 loss 0.022509343922138214 train acc 0.981881689369983\n",
            "epoch 41 batch id 4121 loss 0.011709140613675117 train acc 0.9818953227371997\n",
            "epoch 41 batch id 4131 loss 0.0940992534160614 train acc 0.9818899782135077\n",
            "epoch 41 batch id 4141 loss 0.026868239045143127 train acc 0.9819035257184255\n",
            "epoch 41 batch id 4151 loss 0.05893607437610626 train acc 0.981894423030595\n",
            "epoch 41 batch id 4161 loss 0.028614848852157593 train acc 0.9818740987743331\n",
            "epoch 41 batch id 4171 loss 0.16831544041633606 train acc 0.9818651102853033\n",
            "epoch 41 batch id 4181 loss 0.012118984013795853 train acc 0.9818673762257834\n",
            "epoch 41 batch id 4191 loss 0.07500756531953812 train acc 0.981850990217132\n",
            "epoch 41 batch id 4201 loss 0.05890261009335518 train acc 0.9818607176862651\n",
            "epoch 41 batch id 4211 loss 0.127271369099617 train acc 0.9818481358347186\n",
            "epoch 41 batch id 4221 loss 0.09544201195240021 train acc 0.9818393153281213\n",
            "epoch 41 batch id 4231 loss 0.030629513785243034 train acc 0.9818268435358072\n",
            "epoch 41 batch id 4241 loss 0.0400502048432827 train acc 0.9818328519217165\n",
            "epoch 41 batch id 4251 loss 0.12799935042858124 train acc 0.9818388320395202\n",
            "epoch 41 batch id 4261 loss 0.06376510113477707 train acc 0.981855785026989\n",
            "epoch 41 batch id 4271 loss 0.005132013466209173 train acc 0.9818653418403184\n",
            "epoch 41 batch id 4281 loss 0.09797070920467377 train acc 0.9818675543097407\n",
            "epoch 41 batch id 4291 loss 0.009160966612398624 train acc 0.9818406257282685\n",
            "epoch 41 batch id 4301 loss 0.07616908103227615 train acc 0.9818210881190421\n",
            "epoch 41 batch id 4311 loss 0.022158000618219376 train acc 0.9818342611922988\n",
            "epoch 41 batch id 4321 loss 0.02769327163696289 train acc 0.9818184448044435\n",
            "epoch 41 batch id 4331 loss 0.07907529920339584 train acc 0.9818099168783191\n",
            "epoch 41 batch id 4341 loss 0.03475835546851158 train acc 0.9817870306381018\n",
            "epoch 41 batch id 4351 loss 0.040449075400829315 train acc 0.9817750229832223\n",
            "epoch 41 batch id 4361 loss 0.009005376137793064 train acc 0.9817774019720248\n",
            "epoch 41 batch id 4371 loss 0.033045437186956406 train acc 0.9817904941660948\n",
            "epoch 41 batch id 4381 loss 0.10537142306566238 train acc 0.9817856939055011\n",
            "epoch 41 batch id 4391 loss 0.0006760857068002224 train acc 0.9817737986791164\n",
            "epoch 41 batch id 4401 loss 0.02176501974463463 train acc 0.9817726084980686\n",
            "epoch 41 batch id 4411 loss 0.05116308107972145 train acc 0.9817749659941056\n",
            "epoch 41 batch id 4421 loss 0.06114961951971054 train acc 0.9817702442886225\n",
            "epoch 41 batch id 4431 loss 0.03880321979522705 train acc 0.9817584913112164\n",
            "epoch 41 batch id 4441 loss 0.10645639151334763 train acc 0.9817538279666742\n",
            "epoch 41 batch id 4451 loss 0.0006257870700210333 train acc 0.9817526960233656\n",
            "epoch 41 batch id 4461 loss 0.00022927949612494558 train acc 0.98176207688859\n",
            "epoch 41 batch id 4471 loss 0.07535266131162643 train acc 0.9817504473272198\n",
            "epoch 41 batch id 4481 loss 0.03391823545098305 train acc 0.981745843561705\n",
            "epoch 41 batch id 4491 loss 0.05270113795995712 train acc 0.9817482186595413\n",
            "epoch 41 batch id 4501 loss 0.010682804509997368 train acc 0.9817366974005777\n",
            "epoch 41 batch id 4511 loss 0.06913165003061295 train acc 0.9817252272223453\n",
            "epoch 41 batch id 4521 loss 0.158579483628273 train acc 0.9817138077858881\n",
            "epoch 41 batch id 4531 loss 0.005306694190949202 train acc 0.9817265780180976\n",
            "epoch 41 batch id 4541 loss 0.06850095838308334 train acc 0.9817289693900022\n",
            "epoch 41 batch id 4551 loss 0.052197717130184174 train acc 0.9817313502526918\n",
            "epoch 41 batch id 4561 loss 0.088917076587677 train acc 0.9817200175400131\n",
            "epoch 41 batch id 4571 loss 0.024909835308790207 train acc 0.9817258258586743\n",
            "epoch 41 batch id 4581 loss 0.06387678533792496 train acc 0.981717965509714\n",
            "epoch 41 batch id 4591 loss 0.12793998420238495 train acc 0.9817033326072752\n",
            "epoch 41 batch id 4601 loss 0.012878977693617344 train acc 0.9816853673114541\n",
            "epoch 41 batch id 4611 loss 0.042449671775102615 train acc 0.9817013662979831\n",
            "epoch 41 batch id 4621 loss 0.007664455566555262 train acc 0.9817071521315732\n",
            "epoch 41 batch id 4631 loss 0.09331390261650085 train acc 0.9817129129777585\n",
            "epoch 41 batch id 4641 loss 0.215758815407753 train acc 0.9817051820728291\n",
            "epoch 41 batch id 4651 loss 0.09953071177005768 train acc 0.9817176413674479\n",
            "epoch 41 batch id 4661 loss 0.07963503897190094 train acc 0.9817367517700064\n",
            "epoch 41 batch id 4671 loss 0.008156516589224339 train acc 0.9817591254549347\n",
            "epoch 41 batch id 4681 loss 0.19253624975681305 train acc 0.9817580378124332\n",
            "epoch 41 batch id 4691 loss 0.02858152985572815 train acc 0.9817569548070774\n",
            "epoch 41 batch id 4701 loss 0.016230517998337746 train acc 0.9817658476919804\n",
            "epoch 41 batch id 4711 loss 0.042801979929208755 train acc 0.9817713861175971\n",
            "epoch 41 batch id 4721 loss 0.0012314689811319113 train acc 0.9817702817199746\n",
            "epoch 41 batch id 4731 loss 0.04952674359083176 train acc 0.9817757873599662\n",
            "epoch 41 batch id 4741 loss 0.14830295741558075 train acc 0.9817746783379034\n",
            "epoch 41 batch id 4751 loss 0.1455768495798111 train acc 0.9817735739844243\n",
            "epoch 41 batch id 4761 loss 0.05951344221830368 train acc 0.9817856017643353\n",
            "epoch 41 batch id 4771 loss 0.06851271539926529 train acc 0.9817812041500733\n",
            "epoch 41 batch id 4781 loss 0.07491444051265717 train acc 0.9817637523530642\n",
            "epoch 41 batch id 4791 loss 0.17973056435585022 train acc 0.9817431120851596\n",
            "epoch 41 batch id 4801 loss 0.15372054278850555 train acc 0.9817323213913768\n",
            "epoch 41 batch id 4811 loss 0.07508252561092377 train acc 0.9817443099147787\n",
            "epoch 41 batch id 4821 loss 0.11732763797044754 train acc 0.9817497666459241\n",
            "epoch 41 batch id 4831 loss 0.03771422058343887 train acc 0.9817649037466363\n",
            "epoch 41 batch id 4841 loss 0.004749556537717581 train acc 0.9817670677545962\n",
            "epoch 41 batch id 4851 loss 0.12707142531871796 train acc 0.9817660018552876\n",
            "epoch 41 batch id 4861 loss 0.0028285628650337458 train acc 0.9817649403414935\n",
            "epoch 41 batch id 4871 loss 0.023773761466145515 train acc 0.9817767142270581\n",
            "epoch 41 batch id 4881 loss 0.03058631718158722 train acc 0.9817724339274739\n",
            "epoch 41 batch id 4891 loss 0.02257285639643669 train acc 0.9817681711306482\n",
            "epoch 41 batch id 4901 loss 0.0013092849403619766 train acc 0.9817734901040603\n",
            "epoch 41 batch id 4911 loss 0.11462543904781342 train acc 0.9817787874160049\n",
            "epoch 41 batch id 4921 loss 0.15724067389965057 train acc 0.981780888030888\n",
            "epoch 41 batch id 4931 loss 0.07628356665372849 train acc 0.9817766426688298\n",
            "epoch 41 batch id 4941 loss 0.05450471490621567 train acc 0.9817629275450314\n",
            "epoch 41 batch id 4951 loss 0.07656662166118622 train acc 0.9817587356089679\n",
            "epoch 41 train acc 0.9817619023602986\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5e39cf1d7894e76bfd06e5382b9f74c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 41 loss 0.7039852142333984 test acc 0.8516277950879766\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9063af92ace4bef8c2e3b9476ea0306",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 42 batch id 1 loss 0.03905059024691582 train acc 0.96875\n",
            "epoch 42 batch id 11 loss 0.08869334310293198 train acc 0.9829545454545454\n",
            "epoch 42 batch id 21 loss 0.01855677179992199 train acc 0.9858630952380952\n",
            "epoch 42 batch id 31 loss 0.10122516006231308 train acc 0.984375\n",
            "epoch 42 batch id 41 loss 0.09684380143880844 train acc 0.9858993902439024\n",
            "epoch 42 batch id 51 loss 0.013341891579329967 train acc 0.9862132352941176\n",
            "epoch 42 batch id 61 loss 0.03356495127081871 train acc 0.9836065573770492\n",
            "epoch 42 batch id 71 loss 0.1395992487668991 train acc 0.983274647887324\n",
            "epoch 42 batch id 81 loss 0.02551713027060032 train acc 0.9830246913580247\n",
            "epoch 42 batch id 91 loss 0.10288718342781067 train acc 0.9823145604395604\n",
            "epoch 42 batch id 101 loss 0.03238815814256668 train acc 0.9820544554455446\n",
            "epoch 42 batch id 111 loss 0.0003734748752322048 train acc 0.9821227477477478\n",
            "epoch 42 batch id 121 loss 0.006298033054918051 train acc 0.9829545454545454\n",
            "epoch 42 batch id 131 loss 0.03909911960363388 train acc 0.9829437022900763\n",
            "epoch 42 batch id 141 loss 0.06044018268585205 train acc 0.9831560283687943\n",
            "epoch 42 batch id 151 loss 0.0488617829978466 train acc 0.9833402317880795\n",
            "epoch 42 batch id 161 loss 0.025632768869400024 train acc 0.9833074534161491\n",
            "epoch 42 batch id 171 loss 0.011756040155887604 train acc 0.9834612573099415\n",
            "epoch 42 batch id 181 loss 0.018583085387945175 train acc 0.9838570441988951\n",
            "epoch 42 batch id 191 loss 0.05286474898457527 train acc 0.9834751308900523\n",
            "epoch 42 batch id 201 loss 0.09484284371137619 train acc 0.9835976368159204\n",
            "epoch 42 batch id 211 loss 0.0018951700767502189 train acc 0.9839306872037915\n",
            "epoch 42 batch id 221 loss 0.0570392906665802 train acc 0.9838093891402715\n",
            "epoch 42 batch id 231 loss 0.03435014188289642 train acc 0.9841044372294372\n",
            "epoch 42 batch id 241 loss 0.18527182936668396 train acc 0.9841156639004149\n",
            "epoch 42 batch id 251 loss 0.106510691344738 train acc 0.9842504980079682\n",
            "epoch 42 batch id 261 loss 0.04742740839719772 train acc 0.9841355363984674\n",
            "epoch 42 batch id 271 loss 0.0001462772343074903 train acc 0.9844903136531366\n",
            "epoch 42 batch id 281 loss 0.01992780528962612 train acc 0.9845418149466192\n",
            "epoch 42 batch id 291 loss 0.05332070589065552 train acc 0.9845897766323024\n",
            "epoch 42 batch id 301 loss 0.09535485506057739 train acc 0.984375\n",
            "epoch 42 batch id 311 loss 0.022549718618392944 train acc 0.9844754823151125\n",
            "epoch 42 batch id 321 loss 0.24516995251178741 train acc 0.9842289719626168\n",
            "epoch 42 batch id 331 loss 0.001808222383260727 train acc 0.9842333836858006\n",
            "epoch 42 batch id 341 loss 0.14002026617527008 train acc 0.9842375366568915\n",
            "epoch 42 batch id 351 loss 0.005449230317026377 train acc 0.9843304843304843\n",
            "epoch 42 batch id 361 loss 0.03773457929491997 train acc 0.9841153047091413\n",
            "epoch 42 batch id 371 loss 0.029060784727334976 train acc 0.9842907681940701\n",
            "epoch 42 batch id 381 loss 0.05873533710837364 train acc 0.9843339895013123\n",
            "epoch 42 batch id 391 loss 0.11636175960302353 train acc 0.9843350383631714\n",
            "epoch 42 batch id 401 loss 0.0002032936317846179 train acc 0.9842970698254364\n",
            "epoch 42 batch id 411 loss 0.0005713978316634893 train acc 0.9843369829683698\n",
            "epoch 42 batch id 421 loss 0.03033829666674137 train acc 0.9842636579572447\n",
            "epoch 42 batch id 431 loss 0.07297869026660919 train acc 0.9841574825986079\n",
            "epoch 42 batch id 441 loss 0.100679412484169 train acc 0.9840561224489796\n",
            "epoch 42 batch id 451 loss 0.08127181977033615 train acc 0.9840285476718403\n",
            "epoch 42 batch id 461 loss 0.019909460097551346 train acc 0.9838665943600867\n",
            "epoch 42 batch id 471 loss 0.02908550202846527 train acc 0.983943736730361\n",
            "epoch 42 batch id 481 loss 0.13046857714653015 train acc 0.9840501559251559\n",
            "epoch 42 batch id 491 loss 0.002043018816038966 train acc 0.984024949083503\n",
            "epoch 42 batch id 501 loss 0.22251106798648834 train acc 0.984000748502994\n",
            "epoch 42 batch id 511 loss 0.06520120799541473 train acc 0.9838246086105675\n",
            "epoch 42 batch id 521 loss 0.12541718780994415 train acc 0.9837452015355086\n",
            "epoch 42 batch id 531 loss 0.08849286288022995 train acc 0.983492231638418\n",
            "epoch 42 batch id 541 loss 0.015321777202188969 train acc 0.9832486136783734\n",
            "epoch 42 batch id 551 loss 0.029415758326649666 train acc 0.9831839836660617\n",
            "epoch 42 batch id 561 loss 0.03233995661139488 train acc 0.9830381016042781\n",
            "epoch 42 batch id 571 loss 0.01999700628221035 train acc 0.9831983362521891\n",
            "epoch 42 batch id 581 loss 0.032261546701192856 train acc 0.9831648020654045\n",
            "epoch 42 batch id 591 loss 0.022957947105169296 train acc 0.9832910321489001\n",
            "epoch 42 batch id 601 loss 0.01451745443046093 train acc 0.9832830698835274\n",
            "epoch 42 batch id 611 loss 0.04764788597822189 train acc 0.9832753682487725\n",
            "epoch 42 batch id 621 loss 0.1388716995716095 train acc 0.9832427536231884\n",
            "epoch 42 batch id 631 loss 0.06112071871757507 train acc 0.9832111727416799\n",
            "epoch 42 batch id 641 loss 0.020627213642001152 train acc 0.983229329173167\n",
            "epoch 42 batch id 651 loss 0.019118964672088623 train acc 0.9831749231950845\n",
            "epoch 42 batch id 661 loss 0.000233958286116831 train acc 0.9831930786686838\n",
            "epoch 42 batch id 671 loss 0.007708216086030006 train acc 0.9831874068554396\n",
            "epoch 42 batch id 681 loss 0.0794794037938118 train acc 0.9831819016152716\n",
            "epoch 42 batch id 691 loss 0.027021348476409912 train acc 0.9831313314037626\n",
            "epoch 42 batch id 701 loss 0.03432666137814522 train acc 0.9830376248216833\n",
            "epoch 42 batch id 711 loss 0.07644205540418625 train acc 0.983012482419128\n",
            "epoch 42 batch id 721 loss 0.04892144352197647 train acc 0.982879680998613\n",
            "epoch 42 batch id 731 loss 0.030029362067580223 train acc 0.9828146374829001\n",
            "epoch 42 batch id 741 loss 0.03149094805121422 train acc 0.982814608636977\n",
            "epoch 42 batch id 751 loss 0.01425047405064106 train acc 0.9827729693741678\n",
            "epoch 42 batch id 761 loss 0.004663780797272921 train acc 0.9827734888304862\n",
            "epoch 42 batch id 771 loss 0.03587088733911514 train acc 0.9827334630350194\n",
            "epoch 42 batch id 781 loss 0.020657096058130264 train acc 0.9827744878361075\n",
            "epoch 42 batch id 791 loss 0.013791272416710854 train acc 0.9828144753476612\n",
            "epoch 42 batch id 801 loss 0.08334466814994812 train acc 0.9827559300873908\n",
            "epoch 42 batch id 811 loss 0.05745337903499603 train acc 0.9827758939580764\n",
            "epoch 42 batch id 821 loss 0.011333183385431767 train acc 0.9828714981729598\n",
            "epoch 42 batch id 831 loss 0.0968826487660408 train acc 0.9828895908543923\n",
            "epoch 42 batch id 841 loss 0.010082860477268696 train acc 0.982888674197384\n",
            "epoch 42 batch id 851 loss 0.04832203686237335 train acc 0.982832696827262\n",
            "epoch 42 batch id 861 loss 0.03366892784833908 train acc 0.9827780197444832\n",
            "epoch 42 batch id 871 loss 0.007856376469135284 train acc 0.9827963547646383\n",
            "epoch 42 batch id 881 loss 0.02615291252732277 train acc 0.9827788024971623\n",
            "epoch 42 batch id 891 loss 0.04896159470081329 train acc 0.9827967171717171\n",
            "epoch 42 batch id 901 loss 0.08399634063243866 train acc 0.982727524972253\n",
            "epoch 42 batch id 911 loss 0.007840503007173538 train acc 0.9828142151481888\n",
            "epoch 42 batch id 921 loss 0.00045822144602425396 train acc 0.9827972312703583\n",
            "epoch 42 batch id 931 loss 0.0205887109041214 train acc 0.9828141783029001\n",
            "epoch 42 batch id 941 loss 0.012825094163417816 train acc 0.9829303931987248\n",
            "epoch 42 batch id 951 loss 0.0337921679019928 train acc 0.9828798633017876\n",
            "epoch 42 batch id 961 loss 0.023285431787371635 train acc 0.9829604578563996\n",
            "epoch 42 batch id 971 loss 0.03044523485004902 train acc 0.9829589340885685\n",
            "epoch 42 batch id 981 loss 0.01195751503109932 train acc 0.983053007135576\n",
            "epoch 42 batch id 991 loss 0.059907857328653336 train acc 0.9831451816347124\n",
            "epoch 42 batch id 1001 loss 0.012131604366004467 train acc 0.9832355144855145\n",
            "epoch 42 batch id 1011 loss 0.0001684246672084555 train acc 0.9832158753709199\n",
            "epoch 42 batch id 1021 loss 0.03388432040810585 train acc 0.9832272282076395\n",
            "epoch 42 batch id 1031 loss 0.003805638523772359 train acc 0.9831928952473327\n",
            "epoch 42 batch id 1041 loss 0.02583605982363224 train acc 0.983219260326609\n",
            "epoch 42 batch id 1051 loss 0.018706344068050385 train acc 0.9831559229305423\n",
            "epoch 42 batch id 1061 loss 0.016250506043434143 train acc 0.983123232799246\n",
            "epoch 42 batch id 1071 loss 0.00010253683285554871 train acc 0.9831786881419234\n",
            "epoch 42 batch id 1081 loss 0.03459200635552406 train acc 0.9831030296022202\n",
            "epoch 42 batch id 1091 loss 0.006447072606533766 train acc 0.9831433318056828\n",
            "epoch 42 batch id 1101 loss 0.045563336461782455 train acc 0.9831261353315168\n",
            "epoch 42 batch id 1111 loss 0.07081557810306549 train acc 0.9830389288928892\n",
            "epoch 42 batch id 1121 loss 0.021641260012984276 train acc 0.9831066012488849\n",
            "epoch 42 batch id 1131 loss 0.04871464893221855 train acc 0.9830211096374889\n",
            "epoch 42 batch id 1141 loss 0.04445979744195938 train acc 0.9830329754601227\n",
            "epoch 42 batch id 1151 loss 0.027046913281083107 train acc 0.982868158123371\n",
            "epoch 42 batch id 1161 loss 0.023508621379733086 train acc 0.9828273040482343\n",
            "epoch 42 batch id 1171 loss 0.01219211146235466 train acc 0.9828004910333049\n",
            "epoch 42 batch id 1181 loss 0.0016608894802629948 train acc 0.982840283657917\n",
            "epoch 42 batch id 1191 loss 0.04569273069500923 train acc 0.982813811922754\n",
            "epoch 42 batch id 1201 loss 0.01565452106297016 train acc 0.9828788509575354\n",
            "epoch 42 batch id 1211 loss 0.0003737949882633984 train acc 0.982891205615194\n",
            "epoch 42 batch id 1221 loss 0.08808138221502304 train acc 0.9828777641277642\n",
            "epoch 42 batch id 1231 loss 0.08462829142808914 train acc 0.9828010763606824\n",
            "epoch 42 batch id 1241 loss 0.15724727511405945 train acc 0.982788577759871\n",
            "epoch 42 batch id 1251 loss 0.07262106984853745 train acc 0.982826239008793\n",
            "epoch 42 batch id 1261 loss 0.02013542130589485 train acc 0.9828633029341792\n",
            "epoch 42 batch id 1271 loss 0.02427203767001629 train acc 0.9828260228166797\n",
            "epoch 42 batch id 1281 loss 0.00020274911366868764 train acc 0.9828625097580016\n",
            "epoch 42 batch id 1291 loss 0.13546603918075562 train acc 0.9828863284275755\n",
            "epoch 42 batch id 1301 loss 0.0006998007302172482 train acc 0.9828497309761721\n",
            "epoch 42 batch id 1311 loss 0.027270818129181862 train acc 0.982837528604119\n",
            "epoch 42 batch id 1321 loss 0.020201344043016434 train acc 0.9827900264950795\n",
            "epoch 42 batch id 1331 loss 0.012621376663446426 train acc 0.9828019346356123\n",
            "epoch 42 batch id 1341 loss 0.13220755755901337 train acc 0.9828020134228188\n",
            "epoch 42 batch id 1351 loss 0.06303665786981583 train acc 0.9828020910436713\n",
            "epoch 42 batch id 1361 loss 0.007039369083940983 train acc 0.9827447648787656\n",
            "epoch 42 batch id 1371 loss 0.006865434814244509 train acc 0.9827680525164114\n",
            "epoch 42 batch id 1381 loss 0.08755967020988464 train acc 0.9827683743664012\n",
            "epoch 42 batch id 1391 loss 0.01633503846824169 train acc 0.9827237598849748\n",
            "epoch 42 batch id 1401 loss 0.010781199671328068 train acc 0.9827243932905068\n",
            "epoch 42 batch id 1411 loss 0.04967961832880974 train acc 0.9827250177179305\n",
            "epoch 42 batch id 1421 loss 0.00788476038724184 train acc 0.9827476249120338\n",
            "epoch 42 batch id 1431 loss 0.006934562232345343 train acc 0.9827371593291404\n",
            "epoch 42 batch id 1441 loss 0.023009510710835457 train acc 0.9827159958362248\n",
            "epoch 42 batch id 1451 loss 0.010671542957425117 train acc 0.9827705031013094\n",
            "epoch 42 batch id 1461 loss 0.008570500649511814 train acc 0.9828135694729637\n",
            "epoch 42 batch id 1471 loss 0.019819539040327072 train acc 0.9828666723317471\n",
            "epoch 42 batch id 1481 loss 0.07829699665307999 train acc 0.9828557562457799\n",
            "epoch 42 batch id 1491 loss 0.033965855836868286 train acc 0.9828869047619048\n",
            "epoch 42 batch id 1501 loss 0.0715908482670784 train acc 0.9829072285143238\n",
            "epoch 42 batch id 1511 loss 0.024806804955005646 train acc 0.9829272832561218\n",
            "epoch 42 batch id 1521 loss 0.001145290327258408 train acc 0.9829881656804734\n",
            "epoch 42 batch id 1531 loss 0.021130535751581192 train acc 0.9829870182887002\n",
            "epoch 42 batch id 1541 loss 0.009891130961477757 train acc 0.9829757462686567\n",
            "epoch 42 batch id 1551 loss 0.04873266443610191 train acc 0.9829545454545454\n",
            "epoch 42 batch id 1561 loss 0.03161786496639252 train acc 0.9829436258808456\n",
            "epoch 42 batch id 1571 loss 0.011666073463857174 train acc 0.9829626830044558\n",
            "epoch 42 batch id 1581 loss 0.056879572570323944 train acc 0.9829419671094244\n",
            "epoch 42 batch id 1591 loss 0.12025308609008789 train acc 0.982941153362665\n",
            "epoch 42 batch id 1601 loss 0.01165516022592783 train acc 0.9830086664584634\n",
            "epoch 42 batch id 1611 loss 0.056098103523254395 train acc 0.9830074487895717\n",
            "epoch 42 batch id 1621 loss 0.048968058079481125 train acc 0.9829966070326959\n",
            "epoch 42 batch id 1631 loss 0.0007328970823436975 train acc 0.9830146382587369\n",
            "epoch 42 batch id 1641 loss 0.07791970670223236 train acc 0.9829848415600244\n",
            "epoch 42 batch id 1651 loss 0.03972243517637253 train acc 0.9829270139309509\n",
            "epoch 42 batch id 1661 loss 0.04817279800772667 train acc 0.9829545454545454\n",
            "epoch 42 batch id 1671 loss 0.016990462318062782 train acc 0.9828882405745063\n",
            "epoch 42 batch id 1681 loss 0.03193742781877518 train acc 0.9828413146936348\n",
            "epoch 42 batch id 1691 loss 0.030755745247006416 train acc 0.9828503843879361\n",
            "epoch 42 batch id 1701 loss 0.02982749044895172 train acc 0.9828317901234568\n",
            "epoch 42 batch id 1711 loss 0.0026115512009710073 train acc 0.9828408094681473\n",
            "epoch 42 batch id 1721 loss 0.01067088358104229 train acc 0.982804328878559\n",
            "epoch 42 batch id 1731 loss 0.0001364347554044798 train acc 0.982822429231658\n",
            "epoch 42 batch id 1741 loss 0.012460910715162754 train acc 0.9828223721998851\n",
            "epoch 42 batch id 1751 loss 0.03500237688422203 train acc 0.982795545402627\n",
            "epoch 42 batch id 1761 loss 0.015855133533477783 train acc 0.9828045144804088\n",
            "epoch 42 batch id 1771 loss 0.0003881698939949274 train acc 0.982813382269904\n",
            "epoch 42 batch id 1781 loss 0.004797494970262051 train acc 0.9828484699606962\n",
            "epoch 42 batch id 1791 loss 0.0419316403567791 train acc 0.9828482691233947\n",
            "epoch 42 batch id 1801 loss 0.03866637498140335 train acc 0.9828827734591893\n",
            "epoch 42 batch id 1811 loss 0.03743601217865944 train acc 0.982899641082275\n",
            "epoch 42 batch id 1821 loss 0.09451211243867874 train acc 0.9828476798462383\n",
            "epoch 42 batch id 1831 loss 0.05551394447684288 train acc 0.9828133533588204\n",
            "epoch 42 batch id 1841 loss 0.009315544739365578 train acc 0.9827624253123303\n",
            "epoch 42 batch id 1851 loss 0.007555479183793068 train acc 0.9828133441383036\n",
            "epoch 42 batch id 1861 loss 0.018735839053988457 train acc 0.9828049435787212\n",
            "epoch 42 batch id 1871 loss 0.10782482475042343 train acc 0.9828634420096205\n",
            "epoch 42 batch id 1881 loss 0.02093101292848587 train acc 0.982888091440723\n",
            "epoch 42 batch id 1891 loss 0.0955306813120842 train acc 0.98290421734532\n",
            "epoch 42 batch id 1901 loss 0.08492347598075867 train acc 0.9829201735928459\n",
            "epoch 42 batch id 1911 loss 0.019540222361683846 train acc 0.9828787284144427\n",
            "epoch 42 batch id 1921 loss 0.002720463089644909 train acc 0.9828295809474232\n",
            "epoch 42 batch id 1931 loss 0.013845691457390785 train acc 0.9828294924909373\n",
            "epoch 42 batch id 1941 loss 0.09116248041391373 train acc 0.9828294049459042\n",
            "epoch 42 batch id 1951 loss 0.03765377029776573 train acc 0.9828453357252691\n",
            "epoch 42 batch id 1961 loss 0.03613154590129852 train acc 0.9828531361550229\n",
            "epoch 42 batch id 1971 loss 0.01012472528964281 train acc 0.9828370750887874\n",
            "epoch 42 batch id 1981 loss 0.04606439918279648 train acc 0.9828211761736497\n",
            "epoch 42 batch id 1991 loss 0.13088488578796387 train acc 0.9827975891511803\n",
            "epoch 42 batch id 2001 loss 0.02005036734044552 train acc 0.9828132808595702\n",
            "epoch 42 batch id 2011 loss 0.07895065099000931 train acc 0.9828288165091994\n",
            "epoch 42 batch id 2021 loss 0.0773790255188942 train acc 0.9828441984166254\n",
            "epoch 42 batch id 2031 loss 0.0005694303545169532 train acc 0.9828363490891187\n",
            "epoch 42 batch id 2041 loss 0.008417879231274128 train acc 0.9828209211170995\n",
            "epoch 42 batch id 2051 loss 0.31750723719596863 train acc 0.9827827888834715\n",
            "epoch 42 batch id 2061 loss 0.03504296392202377 train acc 0.9828284206695779\n",
            "epoch 42 batch id 2071 loss 0.06870283931493759 train acc 0.9828358884596813\n",
            "epoch 42 batch id 2081 loss 0.05180828273296356 train acc 0.9828132508409418\n",
            "epoch 42 batch id 2091 loss 0.04778260365128517 train acc 0.9828207197513151\n",
            "epoch 42 batch id 2101 loss 0.003897889517247677 train acc 0.9828429914326511\n",
            "epoch 42 batch id 2111 loss 0.006943110376596451 train acc 0.9828650521080057\n",
            "epoch 42 batch id 2121 loss 0.016736147925257683 train acc 0.9828869047619048\n",
            "epoch 42 batch id 2131 loss 0.05839250609278679 train acc 0.9829158845612389\n",
            "epoch 42 batch id 2141 loss 0.03774756193161011 train acc 0.9829081036898646\n",
            "epoch 42 batch id 2151 loss 0.01342327706515789 train acc 0.9829221873547187\n",
            "epoch 42 batch id 2161 loss 0.08114533126354218 train acc 0.9828855275335493\n",
            "epoch 42 batch id 2171 loss 0.002864413196220994 train acc 0.9828851911561493\n",
            "epoch 42 batch id 2181 loss 0.07317676395177841 train acc 0.9828705295735901\n",
            "epoch 42 batch id 2191 loss 0.026797335594892502 train acc 0.9828845276129621\n",
            "epoch 42 batch id 2201 loss 0.008098707534372807 train acc 0.9828771013175829\n",
            "epoch 42 batch id 2211 loss 0.00012122699263272807 train acc 0.9828909430122117\n",
            "epoch 42 batch id 2221 loss 0.014285466633737087 train acc 0.9828694844664565\n",
            "epoch 42 batch id 2231 loss 0.027028629556298256 train acc 0.9828552218735993\n",
            "epoch 42 batch id 2241 loss 0.09360728412866592 train acc 0.9828480589022758\n",
            "epoch 42 batch id 2251 loss 0.026515614241361618 train acc 0.9828409595735229\n",
            "epoch 42 batch id 2261 loss 0.0005420624511316419 train acc 0.9828408337019018\n",
            "epoch 42 batch id 2271 loss 0.0030585129279643297 train acc 0.9828475891677675\n",
            "epoch 42 batch id 2281 loss 0.03619373217225075 train acc 0.9828679855326611\n",
            "epoch 42 batch id 2291 loss 0.014942583627998829 train acc 0.9828677433435181\n",
            "epoch 42 batch id 2301 loss 0.02199561707675457 train acc 0.9828675032594524\n",
            "epoch 42 batch id 2311 loss 0.047779060900211334 train acc 0.9828943098225876\n",
            "epoch 42 batch id 2321 loss 0.179070383310318 train acc 0.9828670292977165\n",
            "epoch 42 batch id 2331 loss 0.0778283104300499 train acc 0.9827997640497641\n",
            "epoch 42 batch id 2341 loss 0.00022196532518137246 train acc 0.9828198419478855\n",
            "epoch 42 batch id 2351 loss 0.03745241463184357 train acc 0.9827998723947257\n",
            "epoch 42 batch id 2361 loss 0.07443173229694366 train acc 0.9828065438373571\n",
            "epoch 42 batch id 2371 loss 0.0028711764607578516 train acc 0.9828395191902151\n",
            "epoch 42 batch id 2381 loss 0.1471434086561203 train acc 0.9828590928181437\n",
            "epoch 42 batch id 2391 loss 0.014556823298335075 train acc 0.9828654328732748\n",
            "epoch 42 batch id 2401 loss 0.041209518909454346 train acc 0.9828456892961266\n",
            "epoch 42 batch id 2411 loss 0.1279429793357849 train acc 0.9828196287847366\n",
            "epoch 42 batch id 2421 loss 0.0018003301229327917 train acc 0.982832507228418\n",
            "epoch 42 batch id 2431 loss 0.055368147790431976 train acc 0.982851707116413\n",
            "epoch 42 batch id 2441 loss 0.024639403447508812 train acc 0.9828259422367882\n",
            "epoch 42 batch id 2451 loss 0.00021205426310189068 train acc 0.9828131374949001\n",
            "epoch 42 batch id 2461 loss 0.07008810341358185 train acc 0.9828004368143032\n",
            "epoch 42 batch id 2471 loss 0.08628800511360168 train acc 0.9827815155807366\n",
            "epoch 42 batch id 2481 loss 0.10110308974981308 train acc 0.9827879383313181\n",
            "epoch 42 batch id 2491 loss 0.1563085913658142 train acc 0.9827566740264954\n",
            "epoch 42 batch id 2501 loss 0.0007428217213600874 train acc 0.9827568972411036\n",
            "epoch 42 batch id 2511 loss 0.0160520002245903 train acc 0.9827695639187575\n",
            "epoch 42 batch id 2521 loss 0.0205426886677742 train acc 0.9827387445458151\n",
            "epoch 42 batch id 2531 loss 0.01346284057945013 train acc 0.9827328625049387\n",
            "epoch 42 batch id 2541 loss 0.01433756947517395 train acc 0.9827270267611177\n",
            "epoch 42 batch id 2551 loss 0.023418432101607323 train acc 0.9827518620148962\n",
            "epoch 42 batch id 2561 loss 0.0149535508826375 train acc 0.9827704021866458\n",
            "epoch 42 batch id 2571 loss 0.06337946653366089 train acc 0.9827766433294438\n",
            "epoch 42 batch id 2581 loss 0.045043542981147766 train acc 0.9827646745447501\n",
            "epoch 42 batch id 2591 loss 0.14605359733104706 train acc 0.9827648591277499\n",
            "epoch 42 batch id 2601 loss 0.06190396472811699 train acc 0.9827650422914264\n",
            "epoch 42 batch id 2611 loss 0.004498758353292942 train acc 0.9827891612409039\n",
            "epoch 42 batch id 2621 loss 0.04849782586097717 train acc 0.9827892502861503\n",
            "epoch 42 batch id 2631 loss 0.16542160511016846 train acc 0.9827299505891296\n",
            "epoch 42 batch id 2641 loss 0.027649404481053352 train acc 0.9827302631578947\n",
            "epoch 42 batch id 2651 loss 0.0001633601204957813 train acc 0.9827305733685402\n",
            "epoch 42 batch id 2661 loss 0.0048043858259916306 train acc 0.9827191375422774\n",
            "epoch 42 batch id 2671 loss 0.004804691299796104 train acc 0.9827136372145264\n",
            "epoch 42 batch id 2681 loss 0.024096636101603508 train acc 0.982708177918687\n",
            "epoch 42 batch id 2691 loss 0.013255096971988678 train acc 0.9827143719806763\n",
            "epoch 42 batch id 2701 loss 0.03710094466805458 train acc 0.982708950388745\n",
            "epoch 42 batch id 2711 loss 0.004171166103333235 train acc 0.9826747510143858\n",
            "epoch 42 batch id 2721 loss 0.13750553131103516 train acc 0.9826408030135979\n",
            "epoch 42 batch id 2731 loss 0.0007963001262396574 train acc 0.9826471530574881\n",
            "epoch 42 batch id 2741 loss 0.047711946070194244 train acc 0.9826477562933236\n",
            "epoch 42 batch id 2751 loss 0.13612805306911469 train acc 0.9826369956379498\n",
            "epoch 42 batch id 2761 loss 0.04838221147656441 train acc 0.9826432904744657\n",
            "epoch 42 batch id 2771 loss 0.08159603923559189 train acc 0.9826326236015879\n",
            "epoch 42 batch id 2781 loss 0.025496812537312508 train acc 0.9826613628191299\n",
            "epoch 42 batch id 2791 loss 0.0228111669421196 train acc 0.9826786993908994\n",
            "epoch 42 batch id 2801 loss 0.04216811805963516 train acc 0.9826903338093538\n",
            "epoch 42 batch id 2811 loss 0.021287301555275917 train acc 0.982713002490217\n",
            "epoch 42 batch id 2821 loss 0.01496400311589241 train acc 0.9827244328252392\n",
            "epoch 42 batch id 2831 loss 0.009123013354837894 train acc 0.9827357824090427\n",
            "epoch 42 batch id 2841 loss 0.06851615756750107 train acc 0.9827305526223161\n",
            "epoch 42 batch id 2851 loss 0.0020812416914850473 train acc 0.9827527621887057\n",
            "epoch 42 batch id 2861 loss 0.0620671771466732 train acc 0.982736586857742\n",
            "epoch 42 batch id 2871 loss 0.12424607574939728 train acc 0.9827259665621735\n",
            "epoch 42 batch id 2881 loss 0.12209504842758179 train acc 0.982715419993058\n",
            "epoch 42 batch id 2891 loss 0.0033129039220511913 train acc 0.9827265652023521\n",
            "epoch 42 batch id 2901 loss 0.03236641362309456 train acc 0.982743019648397\n",
            "epoch 42 batch id 2911 loss 0.027652155607938766 train acc 0.9827486259017519\n",
            "epoch 42 batch id 2921 loss 0.030855441465973854 train acc 0.9827060510099281\n",
            "epoch 42 batch id 2931 loss 0.02177411876618862 train acc 0.9826744285226885\n",
            "epoch 42 batch id 2941 loss 0.0015080842422321439 train acc 0.9826536467188032\n",
            "epoch 42 batch id 2951 loss 0.05852958559989929 train acc 0.9826488902067095\n",
            "epoch 42 batch id 2961 loss 0.019355105236172676 train acc 0.9826705504896994\n",
            "epoch 42 batch id 2971 loss 0.01183002907782793 train acc 0.9826342140693369\n",
            "epoch 42 batch id 2981 loss 0.016754914075136185 train acc 0.982634812143576\n",
            "epoch 42 batch id 2991 loss 0.013419940136373043 train acc 0.982635406218656\n",
            "epoch 42 batch id 3001 loss 0.003282948164269328 train acc 0.982656822725758\n",
            "epoch 42 batch id 3011 loss 0.003301353193819523 train acc 0.9826625290601129\n",
            "epoch 42 batch id 3021 loss 0.03021777607500553 train acc 0.9826475091029461\n",
            "epoch 42 batch id 3031 loss 0.19439482688903809 train acc 0.9826325882547015\n",
            "epoch 42 batch id 3041 loss 0.07279632985591888 train acc 0.9825817987504111\n",
            "epoch 42 batch id 3051 loss 0.005524662788957357 train acc 0.9825927974434612\n",
            "epoch 42 batch id 3061 loss 0.0904383659362793 train acc 0.9825884106501144\n",
            "epoch 42 batch id 3071 loss 0.0002503043506294489 train acc 0.982594228264409\n",
            "epoch 42 batch id 3081 loss 0.015143058262765408 train acc 0.9826101509250244\n",
            "epoch 42 batch id 3091 loss 0.03427590802311897 train acc 0.982620915561307\n",
            "epoch 42 batch id 3101 loss 0.06314364820718765 train acc 0.9826013785875524\n",
            "epoch 42 batch id 3111 loss 0.03930070251226425 train acc 0.9826020572163292\n",
            "epoch 42 batch id 3121 loss 0.03858048841357231 train acc 0.9826177507209228\n",
            "epoch 42 batch id 3131 loss 0.003934833221137524 train acc 0.9826283535611625\n",
            "epoch 42 batch id 3141 loss 0.06352700293064117 train acc 0.9826239652976759\n",
            "epoch 42 batch id 3151 loss 0.0010880670743063092 train acc 0.9826295223738496\n",
            "epoch 42 batch id 3161 loss 0.07530785351991653 train acc 0.9826251581777918\n",
            "epoch 42 batch id 3171 loss 0.01648608408868313 train acc 0.9826257489750867\n",
            "epoch 42 batch id 3181 loss 0.03440741449594498 train acc 0.9825919522162841\n",
            "epoch 42 batch id 3191 loss 0.01947895437479019 train acc 0.9826171262926983\n",
            "epoch 42 batch id 3201 loss 0.09674927592277527 train acc 0.9826128553577007\n",
            "epoch 42 batch id 3211 loss 0.1437244564294815 train acc 0.9825794145126129\n",
            "epoch 42 batch id 3221 loss 0.09918931126594543 train acc 0.9825558832660665\n",
            "epoch 42 batch id 3231 loss 0.027288369834423065 train acc 0.982537333642835\n",
            "epoch 42 batch id 3241 loss 0.015322171151638031 train acc 0.982538182659673\n",
            "epoch 42 batch id 3251 loss 0.060881055891513824 train acc 0.9825294140264534\n",
            "epoch 42 batch id 3261 loss 0.07760065793991089 train acc 0.9825063247470102\n",
            "epoch 42 batch id 3271 loss 0.01804349385201931 train acc 0.982497707123204\n",
            "epoch 42 batch id 3281 loss 0.0017778094625100493 train acc 0.9825034288326729\n",
            "epoch 42 batch id 3291 loss 0.009822683408856392 train acc 0.9825091157702825\n",
            "epoch 42 batch id 3301 loss 0.026432812213897705 train acc 0.9825147682520449\n",
            "epoch 42 batch id 3311 loss 0.09825851768255234 train acc 0.9825109483539716\n",
            "epoch 42 batch id 3321 loss 0.018339011818170547 train acc 0.9825212661848841\n",
            "epoch 42 batch id 3331 loss 0.1359197199344635 train acc 0.9824799234464124\n",
            "epoch 42 batch id 3341 loss 0.039413824677467346 train acc 0.9824949491170308\n",
            "epoch 42 batch id 3351 loss 0.029093477874994278 train acc 0.9824632572366457\n",
            "epoch 42 batch id 3361 loss 0.014448813162744045 train acc 0.9824828919964297\n",
            "epoch 42 batch id 3371 loss 0.03271312639117241 train acc 0.98248850489469\n",
            "epoch 42 batch id 3381 loss 0.00021414741058833897 train acc 0.9824987060041408\n",
            "epoch 42 batch id 3391 loss 0.06032326817512512 train acc 0.9824996313771749\n",
            "epoch 42 batch id 3401 loss 0.07492919266223907 train acc 0.9824913628344605\n",
            "epoch 42 batch id 3411 loss 0.03232156112790108 train acc 0.9825106273819995\n",
            "epoch 42 batch id 3421 loss 0.026799378916621208 train acc 0.9825206445483776\n",
            "epoch 42 batch id 3431 loss 0.1118721291422844 train acc 0.9825214951909065\n",
            "epoch 42 batch id 3441 loss 0.03377344459295273 train acc 0.9825314225515839\n",
            "epoch 42 batch id 3451 loss 0.014815467409789562 train acc 0.9825367647058824\n",
            "epoch 42 batch id 3461 loss 0.006299867760390043 train acc 0.9825330468072812\n",
            "epoch 42 batch id 3471 loss 0.03168967738747597 train acc 0.9825248487467588\n",
            "epoch 42 batch id 3481 loss 0.07942972332239151 train acc 0.982516697787992\n",
            "epoch 42 batch id 3491 loss 0.0005392931634560227 train acc 0.9825085935262102\n",
            "epoch 42 batch id 3501 loss 0.13450229167938232 train acc 0.9825139245929735\n",
            "epoch 42 batch id 3511 loss 0.04996514692902565 train acc 0.9824969737966391\n",
            "epoch 42 batch id 3521 loss 0.06180431321263313 train acc 0.9825067452428288\n",
            "epoch 42 batch id 3531 loss 0.08920523524284363 train acc 0.982512036250354\n",
            "epoch 42 batch id 3541 loss 0.04169628396630287 train acc 0.9825128847783112\n",
            "epoch 42 batch id 3551 loss 0.025497958064079285 train acc 0.9824873275133765\n",
            "epoch 42 batch id 3561 loss 0.009513542987406254 train acc 0.9825101797247964\n",
            "epoch 42 batch id 3571 loss 0.038726385682821274 train acc 0.9825241528983478\n",
            "epoch 42 batch id 3581 loss 0.034215398132801056 train acc 0.9825293214185982\n",
            "epoch 42 batch id 3591 loss 0.028333565220236778 train acc 0.982547514619883\n",
            "epoch 42 batch id 3601 loss 0.022436531260609627 train acc 0.9825222160510969\n",
            "epoch 42 batch id 3611 loss 0.007339006755501032 train acc 0.9825403281639435\n",
            "epoch 42 batch id 3621 loss 0.04375044256448746 train acc 0.982528134493234\n",
            "epoch 42 batch id 3631 loss 0.010905013419687748 train acc 0.9825160079867805\n",
            "epoch 42 batch id 3641 loss 0.0141264284029603 train acc 0.9825468621257896\n",
            "epoch 42 batch id 3651 loss 0.019413983449339867 train acc 0.9825304711038072\n",
            "epoch 42 batch id 3661 loss 0.052069902420043945 train acc 0.9825013657470636\n",
            "epoch 42 batch id 3671 loss 0.060104046016931534 train acc 0.9825022132933805\n",
            "epoch 42 batch id 3681 loss 0.002615159610286355 train acc 0.9825285248573757\n",
            "epoch 42 batch id 3691 loss 0.002436070702970028 train acc 0.9825377607694392\n",
            "epoch 42 batch id 3701 loss 0.05246448144316673 train acc 0.9825131721156444\n",
            "epoch 42 batch id 3711 loss 0.013209662400186062 train acc 0.9825223996227432\n",
            "epoch 42 batch id 3721 loss 0.06755165755748749 train acc 0.9825063826928245\n",
            "epoch 42 batch id 3731 loss 0.039115749299526215 train acc 0.9825113910479765\n",
            "epoch 42 batch id 3741 loss 0.04654374718666077 train acc 0.9824954891740176\n",
            "epoch 42 batch id 3751 loss 0.007645789068192244 train acc 0.9825129965342575\n",
            "epoch 42 batch id 3761 loss 0.24204911291599274 train acc 0.9824805570327041\n",
            "epoch 42 batch id 3771 loss 0.026669276878237724 train acc 0.9824772938212676\n",
            "epoch 42 batch id 3781 loss 0.11758962273597717 train acc 0.98245751785242\n",
            "epoch 42 batch id 3791 loss 0.1067051813006401 train acc 0.9824831838565022\n",
            "epoch 42 batch id 3801 loss 0.04967939853668213 train acc 0.9824922717705867\n",
            "epoch 42 batch id 3811 loss 0.0484728068113327 train acc 0.982493112044083\n",
            "epoch 42 batch id 3821 loss 0.12879519164562225 train acc 0.9824980371630463\n",
            "epoch 42 batch id 3831 loss 0.0002954447118099779 train acc 0.9825110937092143\n",
            "epoch 42 batch id 3841 loss 0.007878546603024006 train acc 0.9825078104660244\n",
            "epoch 42 batch id 3851 loss 0.00024702734663151205 train acc 0.982508601661906\n",
            "epoch 42 batch id 3861 loss 0.015933455899357796 train acc 0.9824972481222481\n",
            "epoch 42 batch id 3871 loss 0.07208624482154846 train acc 0.9825142082149315\n",
            "epoch 42 batch id 3881 loss 0.03622417896986008 train acc 0.9824988727132182\n",
            "epoch 42 batch id 3891 loss 0.013228352181613445 train acc 0.9824996787458237\n",
            "epoch 42 batch id 3901 loss 0.01043802872300148 train acc 0.982488464496283\n",
            "epoch 42 batch id 3911 loss 0.02943859063088894 train acc 0.9824813027358732\n",
            "epoch 42 batch id 3921 loss 0.011947451159358025 train acc 0.9824861323641928\n",
            "epoch 42 batch id 3931 loss 0.04692435264587402 train acc 0.9824710633426609\n",
            "epoch 42 batch id 3941 loss 0.001633831299841404 train acc 0.9824838239025628\n",
            "epoch 42 batch id 3951 loss 0.01850838214159012 train acc 0.9824727916983043\n",
            "epoch 42 batch id 3961 loss 0.0001636435918044299 train acc 0.982493372885635\n",
            "epoch 42 batch id 3971 loss 0.055079299956560135 train acc 0.9824823721984387\n",
            "epoch 42 batch id 3981 loss 0.001489670597948134 train acc 0.9824910512434062\n",
            "epoch 42 batch id 3991 loss 0.12291061133146286 train acc 0.9824957717364069\n",
            "epoch 42 batch id 4001 loss 0.07750453799962997 train acc 0.9824926580854786\n",
            "epoch 42 batch id 4011 loss 0.010317642241716385 train acc 0.9824661867364747\n",
            "epoch 42 batch id 4021 loss 0.021770693361759186 train acc 0.9824787055458841\n",
            "epoch 42 batch id 4031 loss 0.0005250143585726619 train acc 0.982506667080129\n",
            "epoch 42 batch id 4041 loss 0.040818165987730026 train acc 0.9824842242019303\n",
            "epoch 42 batch id 4051 loss 0.022399185225367546 train acc 0.9824927487040237\n",
            "epoch 42 batch id 4061 loss 0.015126683749258518 train acc 0.9825166215217926\n",
            "epoch 42 batch id 4071 loss 0.21090321242809296 train acc 0.9825135101940555\n",
            "epoch 42 batch id 4081 loss 0.015604997985064983 train acc 0.9825180715510904\n",
            "epoch 42 batch id 4091 loss 0.06786905229091644 train acc 0.9825187912490834\n",
            "epoch 42 batch id 4101 loss 0.0434914231300354 train acc 0.9825118873445501\n",
            "epoch 42 batch id 4111 loss 0.05231534689664841 train acc 0.9825050170274873\n",
            "epoch 42 batch id 4121 loss 0.012010418809950352 train acc 0.9824981800533851\n",
            "epoch 42 batch id 4131 loss 0.07149355113506317 train acc 0.9824875938029533\n",
            "epoch 42 batch id 4141 loss 0.0002268051030114293 train acc 0.9824883784110119\n",
            "epoch 42 batch id 4151 loss 0.044968314468860626 train acc 0.9824929233919537\n",
            "epoch 42 batch id 4161 loss 0.024026526138186455 train acc 0.982463650564768\n",
            "epoch 42 batch id 4171 loss 0.10875238478183746 train acc 0.9824607408295373\n",
            "epoch 42 batch id 4181 loss 0.009731248952448368 train acc 0.9824615821573787\n",
            "epoch 42 batch id 4191 loss 0.2094474583864212 train acc 0.9824475065616798\n",
            "epoch 42 batch id 4201 loss 0.020523276180028915 train acc 0.9824632527969531\n",
            "epoch 42 batch id 4211 loss 0.06334234774112701 train acc 0.9824715032058894\n",
            "epoch 42 batch id 4221 loss 0.09574151039123535 train acc 0.982461205875385\n",
            "epoch 42 batch id 4231 loss 0.028559545055031776 train acc 0.982458343181281\n",
            "epoch 42 batch id 4241 loss 0.09085444360971451 train acc 0.9824518097146899\n",
            "epoch 42 batch id 4251 loss 0.1252598613500595 train acc 0.9824600094095507\n",
            "epoch 42 batch id 4261 loss 0.03615702688694 train acc 0.9824718375968082\n",
            "epoch 42 batch id 4271 loss 0.008132442831993103 train acc 0.9824836103956919\n",
            "epoch 42 batch id 4281 loss 0.10683687031269073 train acc 0.9824916783461808\n",
            "epoch 42 batch id 4291 loss 0.011304542422294617 train acc 0.982470577953857\n",
            "epoch 42 batch id 4301 loss 0.03770003467798233 train acc 0.9824532085561497\n",
            "epoch 42 batch id 4311 loss 0.04539204016327858 train acc 0.9824649153328694\n",
            "epoch 42 batch id 4321 loss 0.02506832592189312 train acc 0.9824548715575099\n",
            "epoch 42 batch id 4331 loss 0.06269034743309021 train acc 0.9824593050103902\n",
            "epoch 42 batch id 4341 loss 0.035935964435338974 train acc 0.9824565192351993\n",
            "epoch 42 batch id 4351 loss 0.0552341528236866 train acc 0.9824393817513215\n",
            "epoch 42 batch id 4361 loss 0.01174152735620737 train acc 0.9824581518000458\n",
            "epoch 42 batch id 4371 loss 0.037905171513557434 train acc 0.9824446636925189\n",
            "epoch 42 batch id 4381 loss 0.1722332090139389 train acc 0.9824241040858251\n",
            "epoch 42 batch id 4391 loss 0.0002183037286158651 train acc 0.9824321054429515\n",
            "epoch 42 batch id 4401 loss 0.007913835346698761 train acc 0.9824400704385367\n",
            "epoch 42 batch id 4411 loss 0.01996658742427826 train acc 0.9824338301972342\n",
            "epoch 42 batch id 4421 loss 0.04951927438378334 train acc 0.982434686722461\n",
            "epoch 42 batch id 4431 loss 0.01873668096959591 train acc 0.9824355393816294\n",
            "epoch 42 batch id 4441 loss 0.011562516912817955 train acc 0.9824399065525783\n",
            "epoch 42 batch id 4451 loss 0.22703461349010468 train acc 0.9824372332060212\n",
            "epoch 42 batch id 4461 loss 0.00045714230509474874 train acc 0.9824450795785699\n",
            "epoch 42 batch id 4471 loss 0.1093573123216629 train acc 0.9824214381570119\n",
            "epoch 42 batch id 4481 loss 0.00758480466902256 train acc 0.9824223108681098\n",
            "epoch 42 batch id 4491 loss 0.08761916309595108 train acc 0.9824127421509686\n",
            "epoch 42 batch id 4501 loss 0.012246619910001755 train acc 0.9824066874027994\n",
            "epoch 42 batch id 4511 loss 0.07082981616258621 train acc 0.9823971957437375\n",
            "epoch 42 batch id 4521 loss 0.04628843441605568 train acc 0.9823946582614466\n",
            "epoch 42 batch id 4531 loss 0.00023782617063261569 train acc 0.9824024773780622\n",
            "epoch 42 batch id 4541 loss 0.0645504742860794 train acc 0.9824171438009249\n",
            "epoch 42 batch id 4551 loss 0.06295140832662582 train acc 0.9824283124588002\n",
            "epoch 42 batch id 4561 loss 0.08642296493053436 train acc 0.9824017485200613\n",
            "epoch 42 batch id 4571 loss 0.03352251276373863 train acc 0.9824060654123824\n",
            "epoch 42 batch id 4581 loss 0.029436012730002403 train acc 0.9824035418030997\n",
            "epoch 42 batch id 4591 loss 0.03317395970225334 train acc 0.9824180461773034\n",
            "epoch 42 batch id 4601 loss 0.016935579478740692 train acc 0.9824121114975005\n",
            "epoch 42 batch id 4611 loss 0.035866595804691315 train acc 0.9824333116460637\n",
            "epoch 42 batch id 4621 loss 0.05905771628022194 train acc 0.9824341322224627\n",
            "epoch 42 batch id 4631 loss 0.09876285493373871 train acc 0.9824349492550205\n",
            "epoch 42 batch id 4641 loss 0.05017818138003349 train acc 0.9824525964231846\n",
            "epoch 42 batch id 4651 loss 0.13496124744415283 train acc 0.982460089228123\n",
            "epoch 42 batch id 4661 loss 0.03847817704081535 train acc 0.98245414074233\n",
            "epoch 42 batch id 4671 loss 0.007153662852942944 train acc 0.9824615981588525\n",
            "epoch 42 batch id 4681 loss 0.17451784014701843 train acc 0.982462347788934\n",
            "epoch 42 batch id 4691 loss 0.010477245785295963 train acc 0.9824597633766787\n",
            "epoch 42 batch id 4701 loss 0.0348343625664711 train acc 0.9824704850031908\n",
            "epoch 42 batch id 4711 loss 0.03355300426483154 train acc 0.9824844778178731\n",
            "epoch 42 batch id 4721 loss 0.005362114403396845 train acc 0.9824686242321542\n",
            "epoch 42 batch id 4731 loss 0.06294693797826767 train acc 0.9824726537729866\n",
            "epoch 42 batch id 4741 loss 0.1652367115020752 train acc 0.9824700748787176\n",
            "epoch 42 batch id 4751 loss 0.1862660050392151 train acc 0.9824707956219744\n",
            "epoch 42 batch id 4761 loss 0.03332826867699623 train acc 0.9824813589582021\n",
            "epoch 42 batch id 4771 loss 0.06564824283123016 train acc 0.9824755030391952\n",
            "epoch 42 batch id 4781 loss 0.12736789882183075 train acc 0.9824598671825978\n",
            "epoch 42 batch id 4791 loss 0.1252291351556778 train acc 0.9824345126278439\n",
            "epoch 42 batch id 4801 loss 0.08009273558855057 train acc 0.9824222818162883\n",
            "epoch 42 batch id 4811 loss 0.0029191861394792795 train acc 0.9824328362086884\n",
            "epoch 42 batch id 4821 loss 0.05856805294752121 train acc 0.9824433468160133\n",
            "epoch 42 batch id 4831 loss 0.02231988124549389 train acc 0.9824570482301801\n",
            "epoch 42 batch id 4841 loss 0.004341284278780222 train acc 0.9824577824829581\n",
            "epoch 42 batch id 4851 loss 0.11659854650497437 train acc 0.9824585137085137\n",
            "epoch 42 batch id 4861 loss 0.0495423786342144 train acc 0.9824463844887883\n",
            "epoch 42 batch id 4871 loss 0.010423456318676472 train acc 0.9824535516321083\n",
            "epoch 42 batch id 4881 loss 0.030158281326293945 train acc 0.9824574882196271\n",
            "epoch 42 batch id 4891 loss 0.020107999444007874 train acc 0.9824518247802085\n",
            "epoch 42 batch id 4901 loss 0.0004213735228404403 train acc 0.9824461844521526\n",
            "epoch 42 batch id 4911 loss 0.20718301832675934 train acc 0.9824437487273467\n",
            "epoch 42 batch id 4921 loss 0.30550283193588257 train acc 0.9824413229018493\n",
            "epoch 42 batch id 4931 loss 0.05561757832765579 train acc 0.9824198945447171\n",
            "epoch 42 batch id 4941 loss 0.09413965791463852 train acc 0.9823985529245092\n",
            "epoch 42 batch id 4951 loss 0.10675308853387833 train acc 0.9823899212280347\n",
            "epoch 42 train acc 0.9823671071212426\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "809ea17484244e75bbf4d0b980061f14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 42 loss 0.5582692623138428 test acc 0.8515017870234605\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11b411007a31408ba324ef05db2c7446",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 43 batch id 1 loss 0.043395742774009705 train acc 0.96875\n",
            "epoch 43 batch id 11 loss 0.02264236845076084 train acc 0.9829545454545454\n",
            "epoch 43 batch id 21 loss 0.003175917314365506 train acc 0.9873511904761905\n",
            "epoch 43 batch id 31 loss 0.08120772987604141 train acc 0.9863911290322581\n",
            "epoch 43 batch id 41 loss 0.02836868166923523 train acc 0.9874237804878049\n",
            "epoch 43 batch id 51 loss 0.010109338909387589 train acc 0.9868259803921569\n",
            "epoch 43 batch id 61 loss 0.11070794612169266 train acc 0.983094262295082\n",
            "epoch 43 batch id 71 loss 0.0871705710887909 train acc 0.9837147887323944\n",
            "epoch 43 batch id 81 loss 0.0020907295402139425 train acc 0.9832175925925926\n",
            "epoch 43 batch id 91 loss 0.06828628480434418 train acc 0.9831730769230769\n",
            "epoch 43 batch id 101 loss 0.06606235355138779 train acc 0.9826732673267327\n",
            "epoch 43 batch id 111 loss 0.00014392094453796744 train acc 0.9818412162162162\n",
            "epoch 43 batch id 121 loss 0.09271051734685898 train acc 0.9819214876033058\n",
            "epoch 43 batch id 131 loss 0.02570134960114956 train acc 0.9821087786259542\n",
            "epoch 43 batch id 141 loss 0.00029854162130504847 train acc 0.9822695035460993\n",
            "epoch 43 batch id 151 loss 0.0632801353931427 train acc 0.9824089403973509\n",
            "epoch 43 batch id 161 loss 0.006973564624786377 train acc 0.983016304347826\n",
            "epoch 43 batch id 171 loss 0.1106635332107544 train acc 0.9827302631578947\n",
            "epoch 43 batch id 181 loss 0.02662072889506817 train acc 0.9830801104972375\n",
            "epoch 43 batch id 191 loss 0.04866321012377739 train acc 0.9828206806282722\n",
            "epoch 43 batch id 201 loss 0.05552030727267265 train acc 0.9829757462686567\n",
            "epoch 43 batch id 211 loss 0.010350197553634644 train acc 0.9828939573459715\n",
            "epoch 43 batch id 221 loss 0.03601425886154175 train acc 0.9826781674208145\n",
            "epoch 43 batch id 231 loss 0.028013549745082855 train acc 0.9829545454545454\n",
            "epoch 43 batch id 241 loss 0.06035319343209267 train acc 0.9830134854771784\n",
            "epoch 43 batch id 251 loss 0.07757584750652313 train acc 0.9831922310756972\n",
            "epoch 43 batch id 261 loss 0.05209748446941376 train acc 0.9831776819923371\n",
            "epoch 43 batch id 271 loss 0.001009404775686562 train acc 0.9833371771217713\n",
            "epoch 43 batch id 281 loss 0.002293841214850545 train acc 0.9834297153024911\n",
            "epoch 43 batch id 291 loss 0.07151325047016144 train acc 0.9837306701030928\n",
            "epoch 43 batch id 301 loss 0.11166362464427948 train acc 0.9834925249169435\n",
            "epoch 43 batch id 311 loss 0.016286566853523254 train acc 0.9836716237942122\n",
            "epoch 43 batch id 321 loss 0.14415784180164337 train acc 0.9836448598130841\n",
            "epoch 43 batch id 331 loss 0.001145665650255978 train acc 0.9834780966767371\n",
            "epoch 43 batch id 341 loss 0.14402355253696442 train acc 0.9834585777126099\n",
            "epoch 43 batch id 351 loss 0.012252704240381718 train acc 0.9836182336182336\n",
            "epoch 43 batch id 361 loss 0.03318410739302635 train acc 0.9834660664819944\n",
            "epoch 43 batch id 371 loss 0.1696763038635254 train acc 0.9835326819407008\n",
            "epoch 43 batch id 381 loss 0.015774790197610855 train acc 0.9836778215223098\n",
            "epoch 43 batch id 391 loss 0.06911305338144302 train acc 0.983815537084399\n",
            "epoch 43 batch id 401 loss 0.00017141572607215494 train acc 0.9838684538653366\n",
            "epoch 43 batch id 411 loss 0.05147003382444382 train acc 0.9838047445255474\n",
            "epoch 43 batch id 421 loss 0.010484638623893261 train acc 0.9837811757719715\n",
            "epoch 43 batch id 431 loss 0.23798272013664246 train acc 0.9835049303944315\n",
            "epoch 43 batch id 441 loss 0.10185104608535767 train acc 0.9833829365079365\n",
            "epoch 43 batch id 451 loss 0.02306932769715786 train acc 0.9834395787139689\n",
            "epoch 43 batch id 461 loss 0.003486617701128125 train acc 0.9831887201735358\n",
            "epoch 43 batch id 471 loss 0.04994542524218559 train acc 0.9834129511677282\n",
            "epoch 43 batch id 481 loss 0.07284550368785858 train acc 0.983497920997921\n",
            "epoch 43 batch id 491 loss 0.005695273168385029 train acc 0.983420315682281\n",
            "epoch 43 batch id 501 loss 0.18942788243293762 train acc 0.983439371257485\n",
            "epoch 43 batch id 511 loss 0.07419535517692566 train acc 0.9833659491193738\n",
            "epoch 43 batch id 521 loss 0.3079449534416199 train acc 0.9833553262955854\n",
            "epoch 43 batch id 531 loss 0.023123379796743393 train acc 0.983168549905838\n",
            "epoch 43 batch id 541 loss 0.010110961273312569 train acc 0.9830464417744916\n",
            "epoch 43 batch id 551 loss 0.032048072665929794 train acc 0.9830421960072595\n",
            "epoch 43 batch id 561 loss 0.07224836200475693 train acc 0.9828152852049911\n",
            "epoch 43 batch id 571 loss 0.029044032096862793 train acc 0.9829794220665499\n",
            "epoch 43 batch id 581 loss 0.03131738305091858 train acc 0.9830572289156626\n",
            "epoch 43 batch id 591 loss 0.026182567700743675 train acc 0.983105964467005\n",
            "epoch 43 batch id 601 loss 0.037437692284584045 train acc 0.9830230865224625\n",
            "epoch 43 batch id 611 loss 0.08505812287330627 train acc 0.9829684942716858\n",
            "epoch 43 batch id 621 loss 0.2511977553367615 train acc 0.9829659822866345\n",
            "epoch 43 batch id 631 loss 0.04255610704421997 train acc 0.9829883122028527\n",
            "epoch 43 batch id 641 loss 0.026084695011377335 train acc 0.983034321372855\n",
            "epoch 43 batch id 651 loss 0.011405106633901596 train acc 0.9830069124423964\n",
            "epoch 43 batch id 661 loss 0.00013682157441508025 train acc 0.9830512481089259\n",
            "epoch 43 batch id 671 loss 0.036998309195041656 train acc 0.9830244038748137\n",
            "epoch 43 batch id 681 loss 0.06624814867973328 train acc 0.9830212922173275\n",
            "epoch 43 batch id 691 loss 0.026657208800315857 train acc 0.9829278219971056\n",
            "epoch 43 batch id 701 loss 0.05507584288716316 train acc 0.9828147289586305\n",
            "epoch 43 batch id 711 loss 0.09697422385215759 train acc 0.9827707454289732\n",
            "epoch 43 batch id 721 loss 0.03221219405531883 train acc 0.9827279819694869\n",
            "epoch 43 batch id 731 loss 0.01104616466909647 train acc 0.982750512995896\n",
            "epoch 43 batch id 741 loss 0.008381610736250877 train acc 0.9827935222672065\n",
            "epoch 43 batch id 751 loss 0.13998182117938995 train acc 0.9827521637816246\n",
            "epoch 43 batch id 761 loss 0.001964560942724347 train acc 0.9827940210249672\n",
            "epoch 43 batch id 771 loss 0.06908872723579407 train acc 0.9827131971465629\n",
            "epoch 43 batch id 781 loss 0.019974006339907646 train acc 0.9827144686299616\n",
            "epoch 43 batch id 791 loss 0.013658230192959309 train acc 0.9827749683944375\n",
            "epoch 43 batch id 801 loss 0.004795856308192015 train acc 0.9827364232209738\n",
            "epoch 43 batch id 811 loss 0.0473199263215065 train acc 0.9827180949445129\n",
            "epoch 43 batch id 821 loss 0.010914651677012444 train acc 0.9827763398294762\n",
            "epoch 43 batch id 831 loss 0.19402235746383667 train acc 0.9827579723225031\n",
            "epoch 43 batch id 841 loss 0.016505232080817223 train acc 0.9827214625445898\n",
            "epoch 43 batch id 851 loss 0.11979750543832779 train acc 0.9826307285546416\n",
            "epoch 43 batch id 861 loss 0.04117085784673691 train acc 0.9826328397212544\n",
            "epoch 43 batch id 871 loss 0.013999728485941887 train acc 0.9826349024110218\n",
            "epoch 43 batch id 881 loss 0.017845453694462776 train acc 0.9826546538024972\n",
            "epoch 43 batch id 891 loss 0.0002448699960950762 train acc 0.9827090347923682\n",
            "epoch 43 batch id 901 loss 0.04612842947244644 train acc 0.9827101831298557\n",
            "epoch 43 batch id 911 loss 0.010022754780948162 train acc 0.9827799121844127\n",
            "epoch 43 batch id 921 loss 0.00013799798034597188 train acc 0.9826784744842563\n",
            "epoch 43 batch id 931 loss 0.00038750964449718595 train acc 0.9827134801288937\n",
            "epoch 43 batch id 941 loss 0.07870680093765259 train acc 0.9827477417640808\n",
            "epoch 43 batch id 951 loss 0.03726806864142418 train acc 0.9826662723449001\n",
            "epoch 43 batch id 961 loss 0.01077953353524208 train acc 0.9827165712799167\n",
            "epoch 43 batch id 971 loss 0.030795127153396606 train acc 0.9826692842430484\n",
            "epoch 43 batch id 981 loss 0.01342692505568266 train acc 0.9826707441386341\n",
            "epoch 43 batch id 991 loss 0.07829689979553223 train acc 0.9827037083753785\n",
            "epoch 43 batch id 1001 loss 0.10583032667636871 train acc 0.9827984515484516\n",
            "epoch 43 batch id 1011 loss 0.013377822004258633 train acc 0.9827676805143423\n",
            "epoch 43 batch id 1021 loss 0.03375500813126564 train acc 0.9828140303623898\n",
            "epoch 43 batch id 1031 loss 0.01630437932908535 train acc 0.9828897914645974\n",
            "epoch 43 batch id 1041 loss 0.01518876850605011 train acc 0.9829490874159462\n",
            "epoch 43 batch id 1051 loss 0.013148215599358082 train acc 0.982843720266413\n",
            "epoch 43 batch id 1061 loss 0.033263713121414185 train acc 0.9827550659754948\n",
            "epoch 43 batch id 1071 loss 0.0009274751646444201 train acc 0.9827701914098973\n",
            "epoch 43 batch id 1081 loss 0.10094379633665085 train acc 0.9826694033302498\n",
            "epoch 43 batch id 1091 loss 0.14217150211334229 train acc 0.9826563932172319\n",
            "epoch 43 batch id 1101 loss 0.06672296673059464 train acc 0.9827003860127157\n",
            "epoch 43 batch id 1111 loss 0.0673128068447113 train acc 0.9826310756075608\n",
            "epoch 43 batch id 1121 loss 0.007859472185373306 train acc 0.9827163247100803\n",
            "epoch 43 batch id 1131 loss 0.03931046277284622 train acc 0.9826757294429708\n",
            "epoch 43 batch id 1141 loss 0.0615970753133297 train acc 0.9826769281332165\n",
            "epoch 43 batch id 1151 loss 0.029367180541157722 train acc 0.9826102302345786\n",
            "epoch 43 batch id 1161 loss 0.275667279958725 train acc 0.982665805340224\n",
            "epoch 43 batch id 1171 loss 0.009477991610765457 train acc 0.9825869982920581\n",
            "epoch 43 batch id 1181 loss 0.002761864336207509 train acc 0.9825889077053345\n",
            "epoch 43 batch id 1191 loss 0.013431154191493988 train acc 0.982590785054576\n",
            "epoch 43 batch id 1201 loss 0.04505285993218422 train acc 0.9826446711074105\n",
            "epoch 43 batch id 1211 loss 0.00012557800801005214 train acc 0.9826718620974402\n",
            "epoch 43 batch id 1221 loss 0.19159696996212006 train acc 0.9826090294840295\n",
            "epoch 43 batch id 1231 loss 0.06840992718935013 train acc 0.9825472177091795\n",
            "epoch 43 batch id 1241 loss 0.008714854717254639 train acc 0.9825619460112812\n",
            "epoch 43 batch id 1251 loss 0.08959892392158508 train acc 0.9825389688249401\n",
            "epoch 43 batch id 1261 loss 0.019809681922197342 train acc 0.9825907018239493\n",
            "epoch 43 batch id 1271 loss 0.0265615563839674 train acc 0.9825924468922108\n",
            "epoch 43 batch id 1281 loss 0.0007745718467049301 train acc 0.9826185597189696\n",
            "epoch 43 batch id 1291 loss 0.05506519973278046 train acc 0.9826321649883811\n",
            "epoch 43 batch id 1301 loss 0.0027249460108578205 train acc 0.9825975211375865\n",
            "epoch 43 batch id 1311 loss 0.025055894628167152 train acc 0.9825991609458429\n",
            "epoch 43 batch id 1321 loss 0.014877351932227612 train acc 0.9825534632853898\n",
            "epoch 43 batch id 1331 loss 0.05001691356301308 train acc 0.9824967129977461\n",
            "epoch 43 batch id 1341 loss 0.09369339048862457 train acc 0.9825107196122297\n",
            "epoch 43 batch id 1351 loss 0.001338292728178203 train acc 0.9825476498889711\n",
            "epoch 43 batch id 1361 loss 0.001370284822769463 train acc 0.9825381153563556\n",
            "epoch 43 batch id 1371 loss 0.008404696360230446 train acc 0.9825629102844639\n",
            "epoch 43 batch id 1381 loss 0.123582623898983 train acc 0.982564717595945\n",
            "epoch 43 batch id 1391 loss 0.005704774986952543 train acc 0.9825552659956865\n",
            "epoch 43 batch id 1401 loss 0.0073760440573096275 train acc 0.9825794075660242\n",
            "epoch 43 batch id 1411 loss 0.016527084633708 train acc 0.9825921332388377\n",
            "epoch 43 batch id 1421 loss 0.007622360717505217 train acc 0.9826266713581985\n",
            "epoch 43 batch id 1431 loss 0.0055799176916480064 train acc 0.9826279699510831\n",
            "epoch 43 batch id 1441 loss 0.018999669700860977 train acc 0.9826075641915336\n",
            "epoch 43 batch id 1451 loss 0.0350751169025898 train acc 0.9826520503101309\n",
            "epoch 43 batch id 1461 loss 0.018638193607330322 train acc 0.9826852327173169\n",
            "epoch 43 batch id 1471 loss 0.12828218936920166 train acc 0.9827073419442556\n",
            "epoch 43 batch id 1481 loss 0.0780925378203392 train acc 0.9827080519918974\n",
            "epoch 43 batch id 1491 loss 0.046443913131952286 train acc 0.9827401911468813\n",
            "epoch 43 batch id 1501 loss 0.06244398280978203 train acc 0.9827406728847435\n",
            "epoch 43 batch id 1511 loss 0.027129271999001503 train acc 0.9827204665784249\n",
            "epoch 43 batch id 1521 loss 0.000162006908794865 train acc 0.9827518902038133\n",
            "epoch 43 batch id 1531 loss 0.014380500651896 train acc 0.9827624918354017\n",
            "epoch 43 batch id 1541 loss 0.017144201323390007 train acc 0.9827729558728099\n",
            "epoch 43 batch id 1551 loss 0.0380169078707695 train acc 0.9827329142488717\n",
            "epoch 43 batch id 1561 loss 0.014819062314927578 train acc 0.9827334240871236\n",
            "epoch 43 batch id 1571 loss 0.011071890592575073 train acc 0.9827438733290897\n",
            "epoch 43 batch id 1581 loss 0.03387213498353958 train acc 0.9827344244149273\n",
            "epoch 43 batch id 1591 loss 0.05597955361008644 train acc 0.9827545568824638\n",
            "epoch 43 batch id 1601 loss 0.0431126169860363 train acc 0.9828134759525297\n",
            "epoch 43 batch id 1611 loss 0.0536528117954731 train acc 0.9827746741154563\n",
            "epoch 43 batch id 1621 loss 0.0441129095852375 train acc 0.9827749074645281\n",
            "epoch 43 batch id 1631 loss 0.0003359986876603216 train acc 0.9828134580012262\n",
            "epoch 43 batch id 1641 loss 0.07580279558897018 train acc 0.9828229737964655\n",
            "epoch 43 batch id 1651 loss 0.04566957801580429 train acc 0.9827566626287099\n",
            "epoch 43 batch id 1661 loss 0.028678370639681816 train acc 0.982794626730885\n",
            "epoch 43 batch id 1671 loss 0.17142222821712494 train acc 0.9827199281867145\n",
            "epoch 43 batch id 1681 loss 0.0775943323969841 train acc 0.9827204788816181\n",
            "epoch 43 batch id 1691 loss 0.0339258536696434 train acc 0.9827302631578947\n",
            "epoch 43 batch id 1701 loss 0.021193787455558777 train acc 0.9827215608465608\n",
            "epoch 43 batch id 1711 loss 0.0039997464045882225 train acc 0.9827220923436587\n",
            "epoch 43 batch id 1721 loss 0.013183454982936382 train acc 0.9827498547356188\n",
            "epoch 43 batch id 1731 loss 0.00011351703142281622 train acc 0.9827411900635471\n",
            "epoch 43 batch id 1741 loss 0.004625470377504826 train acc 0.9827415996553704\n",
            "epoch 43 batch id 1751 loss 0.050838589668273926 train acc 0.9827152341519132\n",
            "epoch 43 batch id 1761 loss 0.018397510051727295 train acc 0.9827601504826803\n",
            "epoch 43 batch id 1771 loss 0.0004510505241341889 train acc 0.9828045595708639\n",
            "epoch 43 batch id 1781 loss 0.0005727463867515326 train acc 0.9828484699606962\n",
            "epoch 43 batch id 1791 loss 0.04632577672600746 train acc 0.9828657174762703\n",
            "epoch 43 batch id 1801 loss 0.023454809561371803 train acc 0.9828654219877846\n",
            "epoch 43 batch id 1811 loss 0.035888805985450745 train acc 0.9828823854224186\n",
            "epoch 43 batch id 1821 loss 0.10513515770435333 train acc 0.9828476798462383\n",
            "epoch 43 batch id 1831 loss 0.05239119008183479 train acc 0.982796286182414\n",
            "epoch 43 batch id 1841 loss 0.0007588837761431932 train acc 0.9827793997827268\n",
            "epoch 43 batch id 1851 loss 0.10317470878362656 train acc 0.9828133441383036\n",
            "epoch 43 batch id 1861 loss 0.019969873130321503 train acc 0.982796547555078\n",
            "epoch 43 batch id 1871 loss 0.06130629777908325 train acc 0.9828634420096205\n",
            "epoch 43 batch id 1881 loss 0.015182181261479855 train acc 0.9828382509303561\n",
            "epoch 43 batch id 1891 loss 0.027784433215856552 train acc 0.9828711660497091\n",
            "epoch 43 batch id 1901 loss 0.08436252176761627 train acc 0.9828379800105208\n",
            "epoch 43 batch id 1911 loss 0.021853918209671974 train acc 0.982796964939822\n",
            "epoch 43 batch id 1921 loss 0.0020973612554371357 train acc 0.982756376887038\n",
            "epoch 43 batch id 1931 loss 0.014212124980986118 train acc 0.9827243008803729\n",
            "epoch 43 batch id 1941 loss 0.010378699749708176 train acc 0.982716705306543\n",
            "epoch 43 batch id 1951 loss 0.03242048621177673 train acc 0.9827171963095849\n",
            "epoch 43 batch id 1961 loss 0.04662880674004555 train acc 0.98276548954615\n",
            "epoch 43 batch id 1971 loss 0.008857627399265766 train acc 0.9827895104008117\n",
            "epoch 43 batch id 1981 loss 0.05602164566516876 train acc 0.9827265270065624\n",
            "epoch 43 batch id 1991 loss 0.09172163158655167 train acc 0.9826877197388247\n",
            "epoch 43 batch id 2001 loss 0.012818587012588978 train acc 0.982696151924038\n",
            "epoch 43 batch id 2011 loss 0.10753007978200912 train acc 0.9826967304823471\n",
            "epoch 43 batch id 2021 loss 0.19346825778484344 train acc 0.9826895719940624\n",
            "epoch 43 batch id 2031 loss 0.00018270961300004274 train acc 0.9826824839980305\n",
            "epoch 43 batch id 2041 loss 0.028681427240371704 train acc 0.9826678098971092\n",
            "epoch 43 batch id 2051 loss 0.17374785244464874 train acc 0.9826532788883472\n",
            "epoch 43 batch id 2061 loss 0.025523316115140915 train acc 0.9826767952450267\n",
            "epoch 43 batch id 2071 loss 0.07009928673505783 train acc 0.9826774505070014\n",
            "epoch 43 batch id 2081 loss 0.034815713763237 train acc 0.9826780994714079\n",
            "epoch 43 batch id 2091 loss 0.07180207222700119 train acc 0.9826264347202296\n",
            "epoch 43 batch id 2101 loss 0.0018485535401850939 train acc 0.9826793788672061\n",
            "epoch 43 batch id 2111 loss 0.006828261073678732 train acc 0.9826948128848887\n",
            "epoch 43 batch id 2121 loss 0.038705095648765564 train acc 0.9826880009429514\n",
            "epoch 43 batch id 2131 loss 0.00041856171446852386 train acc 0.9827032496480526\n",
            "epoch 43 batch id 2141 loss 0.0932585746049881 train acc 0.9827110579168613\n",
            "epoch 43 batch id 2151 loss 0.007178264670073986 train acc 0.9827405857740585\n",
            "epoch 43 batch id 2161 loss 0.21081872284412384 train acc 0.9827047663118926\n",
            "epoch 43 batch id 2171 loss 0.0010544087272137403 train acc 0.9826980654076463\n",
            "epoch 43 batch id 2181 loss 0.0930778980255127 train acc 0.9826770976616231\n",
            "epoch 43 batch id 2191 loss 0.01773444563150406 train acc 0.9826991099954359\n",
            "epoch 43 batch id 2201 loss 0.06000690907239914 train acc 0.9827138232621536\n",
            "epoch 43 batch id 2211 loss 0.00011878187069669366 train acc 0.9827213364993216\n",
            "epoch 43 batch id 2221 loss 0.01689789816737175 train acc 0.9826654660063034\n",
            "epoch 43 batch id 2231 loss 0.009033595211803913 train acc 0.9826801322277006\n",
            "epoch 43 batch id 2241 loss 0.047285355627536774 train acc 0.9826876952253458\n",
            "epoch 43 batch id 2251 loss 0.0001285071630263701 train acc 0.982681308307419\n",
            "epoch 43 batch id 2261 loss 0.0006421247380785644 train acc 0.9826957098628926\n",
            "epoch 43 batch id 2271 loss 0.001586149213835597 train acc 0.9827375055041832\n",
            "epoch 43 batch id 2281 loss 0.029913516715168953 train acc 0.9827446843489698\n",
            "epoch 43 batch id 2291 loss 0.15393665432929993 train acc 0.9827586206896551\n",
            "epoch 43 batch id 2301 loss 0.1465262621641159 train acc 0.9827724358974359\n",
            "epoch 43 batch id 2311 loss 0.040950801223516464 train acc 0.9827928926871484\n",
            "epoch 43 batch id 2321 loss 0.10360851138830185 train acc 0.9827593171046962\n",
            "epoch 43 batch id 2331 loss 0.05693875998258591 train acc 0.9827528421278421\n",
            "epoch 43 batch id 2341 loss 0.0035109741147607565 train acc 0.9827597714651858\n",
            "epoch 43 batch id 2351 loss 0.027629651129245758 train acc 0.9827267652062952\n",
            "epoch 43 batch id 2361 loss 0.11418796330690384 train acc 0.9827403642524354\n",
            "epoch 43 batch id 2371 loss 0.00044645529123954475 train acc 0.9827802087726698\n",
            "epoch 43 batch id 2381 loss 0.0002478459500707686 train acc 0.98280003149937\n",
            "epoch 43 batch id 2391 loss 0.015092591755092144 train acc 0.9828066185696361\n",
            "epoch 43 batch id 2401 loss 0.03842613473534584 train acc 0.9827871199500208\n",
            "epoch 43 batch id 2411 loss 0.055187296122312546 train acc 0.9827677830775612\n",
            "epoch 43 batch id 2421 loss 0.08269728720188141 train acc 0.9828066914498141\n",
            "epoch 43 batch id 2431 loss 0.05837763473391533 train acc 0.9828324249280131\n",
            "epoch 43 batch id 2441 loss 0.004260498099029064 train acc 0.9828259422367882\n",
            "epoch 43 batch id 2451 loss 0.0001471893338020891 train acc 0.9828195124439004\n",
            "epoch 43 batch id 2461 loss 0.03158113732933998 train acc 0.9828067858594067\n",
            "epoch 43 batch id 2471 loss 0.08793023973703384 train acc 0.9828004856333469\n",
            "epoch 43 batch id 2481 loss 0.014658181928098202 train acc 0.9828320233776703\n",
            "epoch 43 batch id 2491 loss 0.07774081826210022 train acc 0.9828444901645925\n",
            "epoch 43 batch id 2501 loss 0.002956896089017391 train acc 0.9828693522590963\n",
            "epoch 43 batch id 2511 loss 0.01960570178925991 train acc 0.9828815710872163\n",
            "epoch 43 batch id 2521 loss 0.1439400613307953 train acc 0.9828503074176914\n",
            "epoch 43 batch id 2531 loss 0.02043992280960083 train acc 0.9828748518372185\n",
            "epoch 43 batch id 2541 loss 0.028118062764406204 train acc 0.9828746064541519\n",
            "epoch 43 batch id 2551 loss 0.03348845988512039 train acc 0.9828927381419051\n",
            "epoch 43 batch id 2561 loss 0.01804906316101551 train acc 0.9828985259664194\n",
            "epoch 43 batch id 2571 loss 0.05075027793645859 train acc 0.9828799591598599\n",
            "epoch 43 batch id 2581 loss 0.04112938046455383 train acc 0.9828554823711739\n",
            "epoch 43 batch id 2591 loss 0.02588541805744171 train acc 0.98286737746044\n",
            "epoch 43 batch id 2601 loss 0.035731684416532516 train acc 0.9828851883890811\n",
            "epoch 43 batch id 2611 loss 0.0003501643077470362 train acc 0.9829088471849866\n",
            "epoch 43 batch id 2621 loss 0.04850366339087486 train acc 0.9828905951926745\n",
            "epoch 43 batch id 2631 loss 0.08144260197877884 train acc 0.9828724819460282\n",
            "epoch 43 batch id 2641 loss 0.036286789923906326 train acc 0.9828663385081409\n",
            "epoch 43 batch id 2651 loss 0.00016381122986786067 train acc 0.9828838174273858\n",
            "epoch 43 batch id 2661 loss 0.00129727425519377 train acc 0.9828835494175122\n",
            "epoch 43 batch id 2671 loss 0.015603027306497097 train acc 0.9828774335454886\n",
            "epoch 43 batch id 2681 loss 0.028870021924376488 train acc 0.982853879149571\n",
            "epoch 43 batch id 2691 loss 0.04734216257929802 train acc 0.9828304998141955\n",
            "epoch 43 batch id 2701 loss 0.03720640763640404 train acc 0.9828072935949649\n",
            "epoch 43 batch id 2711 loss 0.0007113220635801554 train acc 0.9827842585761711\n",
            "epoch 43 batch id 2721 loss 0.06720519065856934 train acc 0.9827671352443954\n",
            "epoch 43 batch id 2731 loss 0.0003989687829744071 train acc 0.982767301354815\n",
            "epoch 43 batch id 2741 loss 0.08741262555122375 train acc 0.9827731667274717\n",
            "epoch 43 batch id 2751 loss 0.10631651431322098 train acc 0.9827392311886587\n",
            "epoch 43 batch id 2761 loss 0.04933139309287071 train acc 0.9827338373777617\n",
            "epoch 43 batch id 2771 loss 0.0015643449733033776 train acc 0.9827397600144352\n",
            "epoch 43 batch id 2781 loss 0.05248839035630226 train acc 0.9827737324703344\n",
            "epoch 43 batch id 2791 loss 0.015910733491182327 train acc 0.9827794697241132\n",
            "epoch 43 batch id 2801 loss 0.14629776775836945 train acc 0.9827684309175294\n",
            "epoch 43 batch id 2811 loss 0.05012620612978935 train acc 0.9827852632515119\n",
            "epoch 43 batch id 2821 loss 0.05397675931453705 train acc 0.9828019762495569\n",
            "epoch 43 batch id 2831 loss 0.018894361332058907 train acc 0.9827964941716708\n",
            "epoch 43 batch id 2841 loss 0.07348756492137909 train acc 0.9827910506863781\n",
            "epoch 43 batch id 2851 loss 0.00037946540396660566 train acc 0.9827966064538758\n",
            "epoch 43 batch id 2861 loss 0.05748731270432472 train acc 0.9827802778748689\n",
            "epoch 43 batch id 2871 loss 0.09216185659170151 train acc 0.9827912748171369\n",
            "epoch 43 batch id 2881 loss 0.049313824623823166 train acc 0.9827913484901076\n",
            "epoch 43 batch id 2891 loss 0.03215116262435913 train acc 0.9828076357661709\n",
            "epoch 43 batch id 2901 loss 0.03240818902850151 train acc 0.9827968803860737\n",
            "epoch 43 batch id 2911 loss 0.02152562141418457 train acc 0.9827861989007214\n",
            "epoch 43 batch id 2921 loss 0.0005571680958382785 train acc 0.9827755905511811\n",
            "epoch 43 batch id 2931 loss 0.0022250828333199024 train acc 0.9827543926987377\n",
            "epoch 43 batch id 2941 loss 0.00021082241437397897 train acc 0.9827599030941857\n",
            "epoch 43 batch id 2951 loss 0.03179214894771576 train acc 0.9828077346662148\n",
            "epoch 43 batch id 2961 loss 0.02537173219025135 train acc 0.9828183046268153\n",
            "epoch 43 batch id 2971 loss 0.008219819515943527 train acc 0.9827972484012117\n",
            "epoch 43 batch id 2981 loss 0.008847653865814209 train acc 0.9828025410935928\n",
            "epoch 43 batch id 2991 loss 0.018920747563242912 train acc 0.9828077983951855\n",
            "epoch 43 batch id 3001 loss 0.10118388384580612 train acc 0.9828234338553815\n",
            "epoch 43 batch id 3011 loss 0.024882687255740166 train acc 0.9828233975423447\n",
            "epoch 43 batch id 3021 loss 0.029043998569250107 train acc 0.9827975008275406\n",
            "epoch 43 batch id 3031 loss 0.15453565120697021 train acc 0.9827769300560871\n",
            "epoch 43 batch id 3041 loss 0.08291027694940567 train acc 0.9827564945741533\n",
            "epoch 43 batch id 3051 loss 0.0003469662042334676 train acc 0.982746435594887\n",
            "epoch 43 batch id 3061 loss 0.08087746798992157 train acc 0.9827313377981052\n",
            "epoch 43 batch id 3071 loss 0.004926916677504778 train acc 0.9827265141647672\n",
            "epoch 43 batch id 3081 loss 0.05573089420795441 train acc 0.9827267932489452\n",
            "epoch 43 batch id 3091 loss 0.014260444790124893 train acc 0.9827422355224846\n",
            "epoch 43 batch id 3101 loss 0.11748097091913223 train acc 0.9827172686230248\n",
            "epoch 43 batch id 3111 loss 0.03641236573457718 train acc 0.9827276197364192\n",
            "epoch 43 batch id 3121 loss 0.0290482547134161 train acc 0.9827429109259853\n",
            "epoch 43 batch id 3131 loss 0.004762477241456509 train acc 0.9827581044394762\n",
            "epoch 43 batch id 3141 loss 0.06185813620686531 train acc 0.9827682266794014\n",
            "epoch 43 batch id 3151 loss 0.0005480567924678326 train acc 0.9827782846715328\n",
            "epoch 43 batch id 3161 loss 0.06439892202615738 train acc 0.9827685068016451\n",
            "epoch 43 batch id 3171 loss 0.012541492469608784 train acc 0.9827735730053611\n",
            "epoch 43 batch id 3181 loss 0.03649498522281647 train acc 0.9827687834014461\n",
            "epoch 43 batch id 3191 loss 0.01772857829928398 train acc 0.9827689204011282\n",
            "epoch 43 batch id 3201 loss 0.07858065515756607 train acc 0.9827739378319276\n",
            "epoch 43 batch id 3211 loss 0.10636384785175323 train acc 0.9827205309872314\n",
            "epoch 43 batch id 3221 loss 0.07288340479135513 train acc 0.982667455759081\n",
            "epoch 43 batch id 3231 loss 0.08463478088378906 train acc 0.9826243809965954\n",
            "epoch 43 batch id 3241 loss 0.012026707641780376 train acc 0.9826346035174329\n",
            "epoch 43 batch id 3251 loss 0.03898032754659653 train acc 0.9826159258689634\n",
            "epoch 43 batch id 3261 loss 0.1123928502202034 train acc 0.9825973627721558\n",
            "epoch 43 batch id 3271 loss 0.0661085993051529 train acc 0.9825693595230817\n",
            "epoch 43 batch id 3281 loss 0.0004626732552424073 train acc 0.982574862846693\n",
            "epoch 43 batch id 3291 loss 0.009678998962044716 train acc 0.9825755849285931\n",
            "epoch 43 batch id 3301 loss 0.013592109084129333 train acc 0.982576302635565\n",
            "epoch 43 batch id 3311 loss 0.016600389033555984 train acc 0.9825770160072486\n",
            "epoch 43 batch id 3321 loss 0.010103125125169754 train acc 0.9826012496236074\n",
            "epoch 43 batch id 3331 loss 0.06174391135573387 train acc 0.9825737391173822\n",
            "epoch 43 batch id 3341 loss 0.06568773090839386 train acc 0.9825978374738102\n",
            "epoch 43 batch id 3351 loss 0.015996117144823074 train acc 0.9825751641301104\n",
            "epoch 43 batch id 3361 loss 0.017782723531126976 train acc 0.9825805191907171\n",
            "epoch 43 batch id 3371 loss 0.10318222641944885 train acc 0.9825812073568674\n",
            "epoch 43 batch id 3381 loss 0.0012797060189768672 train acc 0.9826003771073647\n",
            "epoch 43 batch id 3391 loss 0.06183519586920738 train acc 0.9826056104393984\n",
            "epoch 43 batch id 3401 loss 0.14575804769992828 train acc 0.9826062187591885\n",
            "epoch 43 batch id 3411 loss 0.0851021260023117 train acc 0.9826159850483729\n",
            "epoch 43 batch id 3421 loss 0.020666703581809998 train acc 0.9826256942414499\n",
            "epoch 43 batch id 3431 loss 0.0968574583530426 train acc 0.9826262387059166\n",
            "epoch 43 batch id 3441 loss 0.0010284795425832272 train acc 0.9826358616681198\n",
            "epoch 43 batch id 3451 loss 0.006492916494607925 train acc 0.9826137351492321\n",
            "epoch 43 batch id 3461 loss 0.004194753710180521 train acc 0.982618824039295\n",
            "epoch 43 batch id 3471 loss 0.030409885570406914 train acc 0.9826193820224719\n",
            "epoch 43 batch id 3481 loss 0.0750608742237091 train acc 0.9826064708417122\n",
            "epoch 43 batch id 3491 loss 0.003636198118329048 train acc 0.9826160126038385\n",
            "epoch 43 batch id 3501 loss 0.0165661983191967 train acc 0.9826210368466153\n",
            "epoch 43 batch id 3511 loss 0.03717871382832527 train acc 0.9825904300769012\n",
            "epoch 43 batch id 3521 loss 0.06070910766720772 train acc 0.9826176867367226\n",
            "epoch 43 batch id 3531 loss 0.01748497225344181 train acc 0.9826315137354857\n",
            "epoch 43 batch id 3541 loss 0.06367515027523041 train acc 0.9826276122564247\n",
            "epoch 43 batch id 3551 loss 0.015225645154714584 train acc 0.9826017319065052\n",
            "epoch 43 batch id 3561 loss 0.01059732399880886 train acc 0.9826198750351025\n",
            "epoch 43 batch id 3571 loss 0.0017792102880775928 train acc 0.982642292075049\n",
            "epoch 43 batch id 3581 loss 0.034978438168764114 train acc 0.9826514939960905\n",
            "epoch 43 batch id 3591 loss 0.039374254643917084 train acc 0.9826606446672236\n",
            "epoch 43 batch id 3601 loss 0.026110928505659103 train acc 0.9826480491530131\n",
            "epoch 43 batch id 3611 loss 0.004538802430033684 train acc 0.9826441775131542\n",
            "epoch 43 batch id 3621 loss 0.07656202465295792 train acc 0.9826273819386909\n",
            "epoch 43 batch id 3631 loss 0.00829536747187376 train acc 0.9826192853208483\n",
            "epoch 43 batch id 3641 loss 0.009524543769657612 train acc 0.9826498558088437\n",
            "epoch 43 batch id 3651 loss 0.025537295266985893 train acc 0.982633182689674\n",
            "epoch 43 batch id 3661 loss 0.0027647879905998707 train acc 0.9826251365747064\n",
            "epoch 43 batch id 3671 loss 0.06086549907922745 train acc 0.9826384159629529\n",
            "epoch 43 batch id 3681 loss 0.005805436056107283 train acc 0.9826643575115458\n",
            "epoch 43 batch id 3691 loss 0.0027243834920227528 train acc 0.9826774586832837\n",
            "epoch 43 batch id 3701 loss 0.03418748825788498 train acc 0.9826524925695758\n",
            "epoch 43 batch id 3711 loss 0.0996895432472229 train acc 0.9826655551064403\n",
            "epoch 43 batch id 3721 loss 0.06687505543231964 train acc 0.9826449543133566\n",
            "epoch 43 batch id 3731 loss 0.020230840891599655 train acc 0.9826537791476816\n",
            "epoch 43 batch id 3741 loss 0.036748625338077545 train acc 0.9826500267308207\n",
            "epoch 43 batch id 3751 loss 0.002262983238324523 train acc 0.982650459877366\n",
            "epoch 43 batch id 3761 loss 0.0750574916601181 train acc 0.982621809359213\n",
            "epoch 43 batch id 3771 loss 0.013937640003859997 train acc 0.9826223150357996\n",
            "epoch 43 batch id 3781 loss 0.08556664735078812 train acc 0.9826228180375562\n",
            "epoch 43 batch id 3791 loss 0.00011638964497251436 train acc 0.9826439264046426\n",
            "epoch 43 batch id 3801 loss 0.0635099783539772 train acc 0.9826361483820047\n",
            "epoch 43 batch id 3811 loss 0.010713095776736736 train acc 0.9826489110469693\n",
            "epoch 43 batch id 3821 loss 0.01930410787463188 train acc 0.9826575176655326\n",
            "epoch 43 batch id 3831 loss 0.03746771439909935 train acc 0.9826538436439571\n",
            "epoch 43 batch id 3841 loss 0.00022984518727753311 train acc 0.9826420528508201\n",
            "epoch 43 batch id 3851 loss 0.0001306701306020841 train acc 0.9826546676188003\n",
            "epoch 43 batch id 3861 loss 0.024958765134215355 train acc 0.9826550764050764\n",
            "epoch 43 batch id 3871 loss 0.1161055862903595 train acc 0.9826474102299148\n",
            "epoch 43 batch id 3881 loss 0.1238943487405777 train acc 0.9826277054882763\n",
            "epoch 43 batch id 3891 loss 0.010597357526421547 train acc 0.9826281804163454\n",
            "epoch 43 batch id 3901 loss 0.007437011692672968 train acc 0.9826206421430402\n",
            "epoch 43 batch id 3911 loss 0.03162756189703941 train acc 0.9826291229864484\n",
            "epoch 43 batch id 3921 loss 0.019734274595975876 train acc 0.9826295906656465\n",
            "epoch 43 batch id 3931 loss 0.126880943775177 train acc 0.9825942826252861\n",
            "epoch 43 batch id 3941 loss 0.0018519258592277765 train acc 0.9826186247145394\n",
            "epoch 43 batch id 3951 loss 0.014221694320440292 train acc 0.9825993419387496\n",
            "epoch 43 batch id 3961 loss 0.0025936004240065813 train acc 0.9826117142135824\n",
            "epoch 43 batch id 3971 loss 0.06306653469800949 train acc 0.9826161546210023\n",
            "epoch 43 batch id 3981 loss 0.010480662807822227 train acc 0.9826166478271791\n",
            "epoch 43 batch id 3991 loss 0.1435432881116867 train acc 0.9826132235028815\n",
            "epoch 43 batch id 4001 loss 0.14675405621528625 train acc 0.9825863846538365\n",
            "epoch 43 batch id 4011 loss 0.008793276734650135 train acc 0.9825596796310148\n",
            "epoch 43 batch id 4021 loss 0.045109815895557404 train acc 0.9825525366824173\n",
            "epoch 43 batch id 4031 loss 0.0011594024254009128 train acc 0.9825880674770529\n",
            "epoch 43 batch id 4041 loss 0.028731711208820343 train acc 0.9825808896312794\n",
            "epoch 43 batch id 4051 loss 0.025843150913715363 train acc 0.9825776042952358\n",
            "epoch 43 batch id 4061 loss 0.013545634225010872 train acc 0.9825897254370844\n",
            "epoch 43 batch id 4071 loss 0.05379455164074898 train acc 0.9826017870302137\n",
            "epoch 43 batch id 4081 loss 0.0265488363802433 train acc 0.9826137895123744\n",
            "epoch 43 batch id 4091 loss 0.03452533483505249 train acc 0.9826180945978978\n",
            "epoch 43 batch id 4101 loss 0.02656622976064682 train acc 0.9826109485491343\n",
            "epoch 43 batch id 4111 loss 0.016936734318733215 train acc 0.9825848333738749\n",
            "epoch 43 batch id 4121 loss 0.009782147593796253 train acc 0.9825853858286824\n",
            "epoch 43 batch id 4131 loss 0.07216128706932068 train acc 0.9825859356088115\n",
            "epoch 43 batch id 4141 loss 0.00023502456315327436 train acc 0.9825940292199952\n",
            "epoch 43 batch id 4151 loss 0.040339019149541855 train acc 0.9826058479884365\n",
            "epoch 43 batch id 4161 loss 0.038774412125349045 train acc 0.9825800588800769\n",
            "epoch 43 batch id 4171 loss 0.057301115244627 train acc 0.9825843622632462\n",
            "epoch 43 batch id 4181 loss 0.006780932191759348 train acc 0.9825849079167663\n",
            "epoch 43 batch id 4191 loss 0.059317197650671005 train acc 0.9825705380577427\n",
            "epoch 43 batch id 4201 loss 0.021346058696508408 train acc 0.9825822720780767\n",
            "epoch 43 batch id 4211 loss 0.14272460341453552 train acc 0.9825753977677512\n",
            "epoch 43 batch id 4221 loss 0.0796506479382515 train acc 0.9825537491115849\n",
            "epoch 43 batch id 4231 loss 0.07251185923814774 train acc 0.9825322027889388\n",
            "epoch 43 batch id 4241 loss 0.0395338349044323 train acc 0.9825439165291205\n",
            "epoch 43 batch id 4251 loss 0.08615396171808243 train acc 0.9825518995530463\n",
            "epoch 43 batch id 4261 loss 0.05246537923812866 train acc 0.9825671790659469\n",
            "epoch 43 batch id 4271 loss 0.0029339261818677187 train acc 0.9825787286349801\n",
            "epoch 43 batch id 4281 loss 0.10429737716913223 train acc 0.982586574398505\n",
            "epoch 43 batch id 4291 loss 0.015187734737992287 train acc 0.9825798182241902\n",
            "epoch 43 batch id 4301 loss 0.0334605909883976 train acc 0.9825694605905604\n",
            "epoch 43 batch id 4311 loss 0.03583722189068794 train acc 0.9825663999072141\n",
            "epoch 43 batch id 4321 loss 0.024170583114027977 train acc 0.982556121268225\n",
            "epoch 43 batch id 4331 loss 0.1314295381307602 train acc 0.9825567132302009\n",
            "epoch 43 batch id 4341 loss 0.13435117900371552 train acc 0.9825609018659295\n",
            "epoch 43 batch id 4351 loss 0.03089257888495922 train acc 0.9825363422201793\n",
            "epoch 43 batch id 4361 loss 0.009770665317773819 train acc 0.9825513070396698\n",
            "epoch 43 batch id 4371 loss 0.06009543314576149 train acc 0.9825590539922214\n",
            "epoch 43 batch id 4381 loss 0.12094493210315704 train acc 0.9825560659666743\n",
            "epoch 43 batch id 4391 loss 0.0008365352405235171 train acc 0.9825352994762013\n",
            "epoch 43 batch id 4401 loss 0.04161496087908745 train acc 0.9825323790047716\n",
            "epoch 43 batch id 4411 loss 0.03062627650797367 train acc 0.9825188449331217\n",
            "epoch 43 batch id 4421 loss 0.03980303928256035 train acc 0.9825371805021489\n",
            "epoch 43 batch id 4431 loss 0.014750467613339424 train acc 0.9825448544346649\n",
            "epoch 43 batch id 4441 loss 0.1270914524793625 train acc 0.9825419387525333\n",
            "epoch 43 batch id 4451 loss 0.010059542022645473 train acc 0.9825425466187374\n",
            "epoch 43 batch id 4461 loss 0.00014611968072131276 train acc 0.9825501569154897\n",
            "epoch 43 batch id 4471 loss 0.06336303055286407 train acc 0.982529775218072\n",
            "epoch 43 batch id 4481 loss 0.017601680010557175 train acc 0.9825234322695827\n",
            "epoch 43 batch id 4491 loss 0.03902144357562065 train acc 0.9825205967490537\n",
            "epoch 43 batch id 4501 loss 0.013465577736496925 train acc 0.9825004165740946\n",
            "epoch 43 batch id 4511 loss 0.06805191934108734 train acc 0.9825011084016848\n",
            "epoch 43 batch id 4521 loss 0.09841284900903702 train acc 0.9824914288874143\n",
            "epoch 43 batch id 4531 loss 0.0004572005709633231 train acc 0.9824990344294857\n",
            "epoch 43 batch id 4541 loss 0.0676642656326294 train acc 0.9825031656022902\n",
            "epoch 43 batch id 4551 loss 0.18572556972503662 train acc 0.9824832454405625\n",
            "epoch 43 batch id 4561 loss 0.05542213097214699 train acc 0.9824771157640868\n",
            "epoch 43 batch id 4571 loss 0.026344066485762596 train acc 0.9824778494858893\n",
            "epoch 43 batch id 4581 loss 0.028294580057263374 train acc 0.9824649366950448\n",
            "epoch 43 batch id 4591 loss 0.07747860997915268 train acc 0.9824690971465911\n",
            "epoch 43 batch id 4601 loss 0.0934559553861618 train acc 0.9824426755053249\n",
            "epoch 43 batch id 4611 loss 0.035271305590867996 train acc 0.982457032097159\n",
            "epoch 43 batch id 4621 loss 0.14783397316932678 train acc 0.9824578013417009\n",
            "epoch 43 batch id 4631 loss 0.11286444216966629 train acc 0.9824720632692723\n",
            "epoch 43 batch id 4641 loss 0.16952593624591827 train acc 0.9824828970049558\n",
            "epoch 43 batch id 4651 loss 0.13218745589256287 train acc 0.9824869651687809\n",
            "epoch 43 batch id 4661 loss 0.026608550921082497 train acc 0.9825044250160909\n",
            "epoch 43 batch id 4671 loss 0.008020000532269478 train acc 0.9825151198886748\n",
            "epoch 43 batch id 4681 loss 0.16110387444496155 train acc 0.9825257690664388\n",
            "epoch 43 batch id 4691 loss 0.0033369571901857853 train acc 0.9825330419953102\n",
            "epoch 43 batch id 4701 loss 0.09857489168643951 train acc 0.9825369602212295\n",
            "epoch 43 batch id 4711 loss 0.05677039548754692 train acc 0.9825342284016132\n",
            "epoch 43 batch id 4721 loss 0.004205659497529268 train acc 0.9825248887947469\n",
            "epoch 43 batch id 4731 loss 0.10923466831445694 train acc 0.9825221940393152\n",
            "epoch 43 batch id 4741 loss 0.13771992921829224 train acc 0.9825228063699641\n",
            "epoch 43 batch id 4751 loss 0.16137519478797913 train acc 0.9825299936855398\n",
            "epoch 43 batch id 4761 loss 0.08296909928321838 train acc 0.9825305870615417\n",
            "epoch 43 batch id 4771 loss 0.07523927837610245 train acc 0.9825311779501152\n",
            "epoch 43 batch id 4781 loss 0.165674090385437 train acc 0.9824990849194729\n",
            "epoch 43 batch id 4791 loss 0.16553136706352234 train acc 0.9824834324775621\n",
            "epoch 43 batch id 4801 loss 0.073420949280262 train acc 0.9824776088314935\n",
            "epoch 43 batch id 4811 loss 0.026196058839559555 train acc 0.9824783049262108\n",
            "epoch 43 batch id 4821 loss 0.05806202068924904 train acc 0.982488721219664\n",
            "epoch 43 batch id 4831 loss 0.03403686732053757 train acc 0.9825023287104119\n",
            "epoch 43 batch id 4841 loss 0.0012474246323108673 train acc 0.9824900588721338\n",
            "epoch 43 batch id 4851 loss 0.026690466329455376 train acc 0.9824907235621522\n",
            "epoch 43 batch id 4861 loss 0.16515491902828217 train acc 0.9824817424398272\n",
            "epoch 43 batch id 4871 loss 0.0008272078703157604 train acc 0.9824792137138164\n",
            "epoch 43 batch id 4881 loss 0.031091958284378052 train acc 0.9824830977258758\n",
            "epoch 43 batch id 4891 loss 0.020090024918317795 train acc 0.9824933551420977\n",
            "epoch 43 batch id 4901 loss 0.0010732995579019189 train acc 0.9824876300754948\n",
            "epoch 43 batch id 4911 loss 0.11477279663085938 train acc 0.982491473223376\n",
            "epoch 43 batch id 4921 loss 0.14243386685848236 train acc 0.9824794249136355\n",
            "epoch 43 batch id 4931 loss 0.048449043184518814 train acc 0.9824484131007909\n",
            "epoch 43 batch id 4941 loss 0.04797163978219032 train acc 0.9824238514470754\n",
            "epoch 43 batch id 4951 loss 0.08559714257717133 train acc 0.9824277923651787\n",
            "epoch 43 train acc 0.9824269971757111\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb030857ca2401ba94f51051b1fabae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 43 loss 0.5352237224578857 test acc 0.8517160007331379\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "473a6c8775cf43eca91a0d2ce7ba8963",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 44 batch id 1 loss 0.0618046373128891 train acc 0.96875\n",
            "epoch 44 batch id 11 loss 0.01707920990884304 train acc 0.9815340909090909\n",
            "epoch 44 batch id 21 loss 0.032341886311769485 train acc 0.9858630952380952\n",
            "epoch 44 batch id 31 loss 0.10078783333301544 train acc 0.9838709677419355\n",
            "epoch 44 batch id 41 loss 0.08443940430879593 train acc 0.984375\n",
            "epoch 44 batch id 51 loss 0.015555452555418015 train acc 0.9837622549019608\n",
            "epoch 44 batch id 61 loss 0.1616542637348175 train acc 0.9800204918032787\n",
            "epoch 44 batch id 71 loss 0.07438096404075623 train acc 0.9810739436619719\n",
            "epoch 44 batch id 81 loss 0.006249902304261923 train acc 0.9814814814814815\n",
            "epoch 44 batch id 91 loss 0.12056539207696915 train acc 0.9814560439560439\n",
            "epoch 44 batch id 101 loss 0.03255312517285347 train acc 0.9809715346534653\n",
            "epoch 44 batch id 111 loss 0.0032421189825981855 train acc 0.9812781531531531\n",
            "epoch 44 batch id 121 loss 0.020410075783729553 train acc 0.9823088842975206\n",
            "epoch 44 batch id 131 loss 0.022493626922369003 train acc 0.982824427480916\n",
            "epoch 44 batch id 141 loss 0.004796300083398819 train acc 0.9831560283687943\n",
            "epoch 44 batch id 151 loss 0.05174199864268303 train acc 0.9833402317880795\n",
            "epoch 44 batch id 161 loss 0.002414928749203682 train acc 0.9835986024844721\n",
            "epoch 44 batch id 171 loss 0.030069468542933464 train acc 0.9834612573099415\n",
            "epoch 44 batch id 181 loss 0.02938704565167427 train acc 0.9836843922651933\n",
            "epoch 44 batch id 191 loss 0.09569407254457474 train acc 0.9836387434554974\n",
            "epoch 44 batch id 201 loss 0.10272714495658875 train acc 0.9836753731343284\n",
            "epoch 44 batch id 211 loss 0.0028366870246827602 train acc 0.9837825829383886\n",
            "epoch 44 batch id 221 loss 0.05764082074165344 train acc 0.9834558823529411\n",
            "epoch 44 batch id 231 loss 0.029700495302677155 train acc 0.9837662337662337\n",
            "epoch 44 batch id 241 loss 0.07985923439264297 train acc 0.9837266597510373\n",
            "epoch 44 batch id 251 loss 0.10146164894104004 train acc 0.9836279880478087\n",
            "epoch 44 batch id 261 loss 0.05556128919124603 train acc 0.9834770114942529\n",
            "epoch 44 batch id 271 loss 0.0054147071205079556 train acc 0.983510147601476\n",
            "epoch 44 batch id 281 loss 0.07078465819358826 train acc 0.9837633451957295\n",
            "epoch 44 batch id 291 loss 0.06672966480255127 train acc 0.9838917525773195\n",
            "epoch 44 batch id 301 loss 0.13284620642662048 train acc 0.9838039867109635\n",
            "epoch 44 batch id 311 loss 0.018525373190641403 train acc 0.9840735530546624\n",
            "epoch 44 batch id 321 loss 0.1222284808754921 train acc 0.9840342679127726\n",
            "epoch 44 batch id 331 loss 0.0671486034989357 train acc 0.9841389728096677\n",
            "epoch 44 batch id 341 loss 0.14326506853103638 train acc 0.9841458944281525\n",
            "epoch 44 batch id 351 loss 0.014302652329206467 train acc 0.9842859686609686\n",
            "epoch 44 batch id 361 loss 0.028959909453988075 train acc 0.9842884349030471\n",
            "epoch 44 batch id 371 loss 0.1545889526605606 train acc 0.984417115902965\n",
            "epoch 44 batch id 381 loss 0.01825789362192154 train acc 0.984498031496063\n",
            "epoch 44 batch id 391 loss 0.08491417020559311 train acc 0.9846147698209718\n",
            "epoch 44 batch id 401 loss 0.00012065864575561136 train acc 0.9846087905236908\n",
            "epoch 44 batch id 411 loss 0.0002135227114195004 train acc 0.9847551703163017\n",
            "epoch 44 batch id 421 loss 0.010328632779419422 train acc 0.9848574821852731\n",
            "epoch 44 batch id 431 loss 0.06041097268462181 train acc 0.9847012761020881\n",
            "epoch 44 batch id 441 loss 0.08323154598474503 train acc 0.9844458616780045\n",
            "epoch 44 batch id 451 loss 0.022610928863286972 train acc 0.9846175166297118\n",
            "epoch 44 batch id 461 loss 0.004495672415941954 train acc 0.9844088937093276\n",
            "epoch 44 batch id 471 loss 0.12495163828134537 train acc 0.9844413481953291\n",
            "epoch 44 batch id 481 loss 0.07396601140499115 train acc 0.9844074844074844\n",
            "epoch 44 batch id 491 loss 0.04433107003569603 train acc 0.9843431771894093\n",
            "epoch 44 batch id 501 loss 0.1815231442451477 train acc 0.984437375249501\n",
            "epoch 44 batch id 511 loss 0.14332474768161774 train acc 0.9844361545988258\n",
            "epoch 44 batch id 521 loss 0.09518679976463318 train acc 0.9844049904030711\n",
            "epoch 44 batch id 531 loss 0.019769474864006042 train acc 0.984257297551789\n",
            "epoch 44 batch id 541 loss 0.011498468928039074 train acc 0.9841150646950092\n",
            "epoch 44 batch id 551 loss 0.11825714260339737 train acc 0.9841197822141561\n",
            "epoch 44 batch id 561 loss 0.1301734894514084 train acc 0.9839850713012478\n",
            "epoch 44 batch id 571 loss 0.02366659790277481 train acc 0.984101357267951\n",
            "epoch 44 batch id 581 loss 0.28202182054519653 train acc 0.9840253872633391\n",
            "epoch 44 batch id 591 loss 0.0753260999917984 train acc 0.9840048646362098\n",
            "epoch 44 batch id 601 loss 0.027077415958046913 train acc 0.9839850249584027\n",
            "epoch 44 batch id 611 loss 0.072948157787323 train acc 0.9840169803600655\n",
            "epoch 44 batch id 621 loss 0.2792583703994751 train acc 0.983972423510467\n",
            "epoch 44 batch id 631 loss 0.09101027995347977 train acc 0.9838549920760697\n",
            "epoch 44 batch id 641 loss 0.05988585576415062 train acc 0.983765600624025\n",
            "epoch 44 batch id 651 loss 0.10191068053245544 train acc 0.9837029569892473\n",
            "epoch 44 batch id 661 loss 0.0017705679638311267 train acc 0.9837367624810892\n",
            "epoch 44 batch id 671 loss 0.06871077418327332 train acc 0.9837462742175856\n",
            "epoch 44 batch id 681 loss 0.06394375115633011 train acc 0.9837325624082232\n",
            "epoch 44 batch id 691 loss 0.025509342551231384 train acc 0.9837644717800289\n",
            "epoch 44 batch id 701 loss 0.049891747534275055 train acc 0.9837508915834522\n",
            "epoch 44 batch id 711 loss 0.1175866350531578 train acc 0.9836717651195499\n",
            "epoch 44 batch id 721 loss 0.05094335600733757 train acc 0.9836381761442441\n",
            "epoch 44 batch id 731 loss 0.03917156532406807 train acc 0.9836055061559508\n",
            "epoch 44 batch id 741 loss 0.0011647846549749374 train acc 0.9836791497975709\n",
            "epoch 44 batch id 751 loss 0.015861213207244873 train acc 0.9834595539280959\n",
            "epoch 44 batch id 761 loss 0.008763080462813377 train acc 0.9834510512483574\n",
            "epoch 44 batch id 771 loss 0.026769952848553658 train acc 0.9834022373540856\n",
            "epoch 44 batch id 781 loss 0.019880929961800575 train acc 0.9834747119078106\n",
            "epoch 44 batch id 791 loss 0.019846269860863686 train acc 0.9835058470290771\n",
            "epoch 44 batch id 801 loss 0.0035251600202172995 train acc 0.983497191011236\n",
            "epoch 44 batch id 811 loss 0.11354511976242065 train acc 0.9834694821208385\n",
            "epoch 44 batch id 821 loss 0.012830223888158798 train acc 0.983518574908648\n",
            "epoch 44 batch id 831 loss 0.0864320918917656 train acc 0.983453670276775\n",
            "epoch 44 batch id 841 loss 0.0017461758106946945 train acc 0.9834832045184304\n",
            "epoch 44 batch id 851 loss 0.04946344345808029 train acc 0.9834018801410106\n",
            "epoch 44 batch id 861 loss 0.0338946096599102 train acc 0.9833405923344948\n",
            "epoch 44 batch id 871 loss 0.01668202318251133 train acc 0.9833165901262916\n",
            "epoch 44 batch id 881 loss 0.01780492626130581 train acc 0.9833463393870602\n",
            "epoch 44 batch id 891 loss 0.00010101045336341485 train acc 0.9833403479236813\n",
            "epoch 44 batch id 901 loss 0.17740733921527863 train acc 0.9832998057713651\n",
            "epoch 44 batch id 911 loss 0.010783976875245571 train acc 0.9833116081229418\n",
            "epoch 44 batch id 921 loss 0.0005127518670633435 train acc 0.9832722584147665\n",
            "epoch 44 batch id 931 loss 0.001589816645719111 train acc 0.9833344522019334\n",
            "epoch 44 batch id 941 loss 0.011250600218772888 train acc 0.9834451381509033\n",
            "epoch 44 batch id 951 loss 0.037054598331451416 train acc 0.983339905362776\n",
            "epoch 44 batch id 961 loss 0.012768668122589588 train acc 0.9833506763787722\n",
            "epoch 44 batch id 971 loss 0.029396219179034233 train acc 0.9833451338825953\n",
            "epoch 44 batch id 981 loss 0.1623295694589615 train acc 0.9833874872579002\n",
            "epoch 44 batch id 991 loss 0.07607007771730423 train acc 0.9834447527749748\n",
            "epoch 44 batch id 1001 loss 0.043127384036779404 train acc 0.9835008741258742\n",
            "epoch 44 batch id 1011 loss 0.0002905770670622587 train acc 0.9835095202769535\n",
            "epoch 44 batch id 1021 loss 0.057324204593896866 train acc 0.9835333006856023\n",
            "epoch 44 batch id 1031 loss 0.005473291967064142 train acc 0.9836020853540253\n",
            "epoch 44 batch id 1041 loss 0.04611203074455261 train acc 0.9836395292987512\n",
            "epoch 44 batch id 1051 loss 0.019415907561779022 train acc 0.9835870599429115\n",
            "epoch 44 batch id 1061 loss 0.01465106476098299 train acc 0.9835503063147973\n",
            "epoch 44 batch id 1071 loss 0.08885158598423004 train acc 0.9835725957049486\n",
            "epoch 44 batch id 1081 loss 0.04253455996513367 train acc 0.9834643848288621\n",
            "epoch 44 batch id 1091 loss 0.349209725856781 train acc 0.9834440879926672\n",
            "epoch 44 batch id 1101 loss 0.05530056357383728 train acc 0.9834667347865577\n",
            "epoch 44 batch id 1111 loss 0.060643792152404785 train acc 0.9834186543654365\n",
            "epoch 44 batch id 1121 loss 0.008038858883082867 train acc 0.9834968777876896\n",
            "epoch 44 batch id 1131 loss 0.04818722978234291 train acc 0.9834631962864722\n",
            "epoch 44 batch id 1141 loss 0.02832074649631977 train acc 0.9834985758106923\n",
            "epoch 44 batch id 1151 loss 0.013657204806804657 train acc 0.983411164205039\n",
            "epoch 44 batch id 1161 loss 0.060981523245573044 train acc 0.9834194659776055\n",
            "epoch 44 batch id 1171 loss 0.00996999442577362 train acc 0.9833475661827498\n",
            "epoch 44 batch id 1181 loss 0.10296588391065598 train acc 0.9833827265029635\n",
            "epoch 44 batch id 1191 loss 0.01076168566942215 train acc 0.9834041771620488\n",
            "epoch 44 batch id 1201 loss 0.13608916103839874 train acc 0.9834382805995004\n",
            "epoch 44 batch id 1211 loss 0.000167210673680529 train acc 0.9834073080099092\n",
            "epoch 44 batch id 1221 loss 0.0748283863067627 train acc 0.9833896396396397\n",
            "epoch 44 batch id 1231 loss 0.0857025757431984 train acc 0.9833087936636881\n",
            "epoch 44 batch id 1241 loss 0.00810461025685072 train acc 0.9832796132151491\n",
            "epoch 44 batch id 1251 loss 0.1311560422182083 train acc 0.9832633892885692\n",
            "epoch 44 batch id 1261 loss 0.020127829164266586 train acc 0.9832845955590801\n",
            "epoch 44 batch id 1271 loss 0.08274158090353012 train acc 0.9832685877261998\n",
            "epoch 44 batch id 1281 loss 0.00033764587715268135 train acc 0.9832894223263076\n",
            "epoch 44 batch id 1291 loss 0.06896238774061203 train acc 0.9832615220759101\n",
            "epoch 44 batch id 1301 loss 0.013248603790998459 train acc 0.9832220407378939\n",
            "epoch 44 batch id 1311 loss 0.051391687244176865 train acc 0.9832189168573608\n",
            "epoch 44 batch id 1321 loss 0.015693211928009987 train acc 0.9832394965934897\n",
            "epoch 44 batch id 1331 loss 0.011499183252453804 train acc 0.9831658527422991\n",
            "epoch 44 batch id 1341 loss 0.11689862608909607 train acc 0.9831282624906786\n",
            "epoch 44 batch id 1351 loss 0.0027699172496795654 train acc 0.9831374907475944\n",
            "epoch 44 batch id 1361 loss 0.0016682390123605728 train acc 0.9831121418074945\n",
            "epoch 44 batch id 1371 loss 0.005839461926370859 train acc 0.9831213530269876\n",
            "epoch 44 batch id 1381 loss 0.11479974538087845 train acc 0.9830964880521361\n",
            "epoch 44 batch id 1391 loss 0.008043991401791573 train acc 0.9830382818116463\n",
            "epoch 44 batch id 1401 loss 0.007221996784210205 train acc 0.9830366702355461\n",
            "epoch 44 batch id 1411 loss 0.01708291843533516 train acc 0.983101523742027\n",
            "epoch 44 batch id 1421 loss 0.007905441336333752 train acc 0.9831324771287826\n",
            "epoch 44 batch id 1431 loss 0.022613298147916794 train acc 0.9831084032145353\n",
            "epoch 44 batch id 1441 loss 0.020101435482501984 train acc 0.9831171929215823\n",
            "epoch 44 batch id 1451 loss 0.004605745431035757 train acc 0.98315816678153\n",
            "epoch 44 batch id 1461 loss 0.007530171889811754 train acc 0.9831558008213552\n",
            "epoch 44 batch id 1471 loss 0.07834984362125397 train acc 0.9832171991842285\n",
            "epoch 44 batch id 1481 loss 0.05811115726828575 train acc 0.9832039162727887\n",
            "epoch 44 batch id 1491 loss 0.05414123460650444 train acc 0.9832327297116029\n",
            "epoch 44 batch id 1501 loss 0.08536238223314285 train acc 0.9832195203197868\n",
            "epoch 44 batch id 1511 loss 0.008400177583098412 train acc 0.9832375082726671\n",
            "epoch 44 batch id 1521 loss 0.000997411087155342 train acc 0.9832655325443787\n",
            "epoch 44 batch id 1531 loss 0.016813896596431732 train acc 0.9833033964728936\n",
            "epoch 44 batch id 1541 loss 0.005781545303761959 train acc 0.9833306294613887\n",
            "epoch 44 batch id 1551 loss 0.04744898900389671 train acc 0.9833272888459059\n",
            "epoch 44 batch id 1561 loss 0.09184549748897552 train acc 0.9833139814221653\n",
            "epoch 44 batch id 1571 loss 0.013080895878374577 train acc 0.9833207352005092\n",
            "epoch 44 batch id 1581 loss 0.023705679923295975 train acc 0.983287871600253\n",
            "epoch 44 batch id 1591 loss 0.05582369118928909 train acc 0.9833045254556882\n",
            "epoch 44 batch id 1601 loss 0.012869284488260746 train acc 0.9833795284197376\n",
            "epoch 44 batch id 1611 loss 0.08308054506778717 train acc 0.9833275139664804\n",
            "epoch 44 batch id 1621 loss 0.0278510469943285 train acc 0.9833243368291178\n",
            "epoch 44 batch id 1631 loss 0.0008096900419332087 train acc 0.983340358675659\n",
            "epoch 44 batch id 1641 loss 0.09682708233594894 train acc 0.983327620353443\n",
            "epoch 44 batch id 1651 loss 0.0793645828962326 train acc 0.9832487886129618\n",
            "epoch 44 batch id 1661 loss 0.06476018577814102 train acc 0.983283789885611\n",
            "epoch 44 batch id 1671 loss 0.025436386466026306 train acc 0.983243566726511\n",
            "epoch 44 batch id 1681 loss 0.00491449935361743 train acc 0.9832131171921475\n",
            "epoch 44 batch id 1691 loss 0.016957120969891548 train acc 0.9832477084565346\n",
            "epoch 44 batch id 1701 loss 0.020733360201120377 train acc 0.9832635214579659\n",
            "epoch 44 batch id 1711 loss 0.014439670369029045 train acc 0.9832700175336061\n",
            "epoch 44 batch id 1721 loss 0.05050269886851311 train acc 0.9832492010459035\n",
            "epoch 44 batch id 1731 loss 0.00011770201672334224 train acc 0.9832466782206817\n",
            "epoch 44 batch id 1741 loss 0.004342221654951572 train acc 0.9832621338311315\n",
            "epoch 44 batch id 1751 loss 0.04671332612633705 train acc 0.9832595659623072\n",
            "epoch 44 batch id 1761 loss 0.014612758532166481 train acc 0.9833013912549687\n",
            "epoch 44 batch id 1771 loss 0.014063691720366478 train acc 0.9833250988142292\n",
            "epoch 44 batch id 1781 loss 0.00015176409215200692 train acc 0.9833748596294217\n",
            "epoch 44 batch id 1791 loss 0.08939577639102936 train acc 0.9833891680625348\n",
            "epoch 44 batch id 1801 loss 0.014605199918150902 train acc 0.983411993337035\n",
            "epoch 44 batch id 1811 loss 0.034583885222673416 train acc 0.9834345665378245\n",
            "epoch 44 batch id 1821 loss 0.2127998173236847 train acc 0.9834054091158704\n",
            "epoch 44 batch id 1831 loss 0.014978386461734772 train acc 0.9833680365920262\n",
            "epoch 44 batch id 1841 loss 0.00012983562191948295 train acc 0.9833565317762086\n",
            "epoch 44 batch id 1851 loss 0.005996497347950935 train acc 0.9834211237169098\n",
            "epoch 44 batch id 1861 loss 0.011527281254529953 train acc 0.9834094572810317\n",
            "epoch 44 batch id 1871 loss 0.11723412573337555 train acc 0.9834480224478889\n",
            "epoch 44 batch id 1881 loss 0.013733230531215668 train acc 0.9834363370547581\n",
            "epoch 44 batch id 1891 loss 0.03148321434855461 train acc 0.9834660893707033\n",
            "epoch 44 batch id 1901 loss 0.1039317175745964 train acc 0.9834626512361915\n",
            "epoch 44 batch id 1911 loss 0.02292170189321041 train acc 0.9833856619570905\n",
            "epoch 44 batch id 1921 loss 0.11513301730155945 train acc 0.9833582769390942\n",
            "epoch 44 batch id 1931 loss 0.014519951306283474 train acc 0.9833716338684619\n",
            "epoch 44 batch id 1941 loss 0.009687023237347603 train acc 0.9833768031942298\n",
            "epoch 44 batch id 1951 loss 0.0393611341714859 train acc 0.9833578933880062\n",
            "epoch 44 batch id 1961 loss 0.03854254633188248 train acc 0.9834108873023968\n",
            "epoch 44 batch id 1971 loss 0.012684113346040249 train acc 0.983423706240487\n",
            "epoch 44 batch id 1981 loss 0.04715530946850777 train acc 0.9833890711761737\n",
            "epoch 44 batch id 1991 loss 0.09329713135957718 train acc 0.9833704796584631\n",
            "epoch 44 batch id 2001 loss 0.001259076059795916 train acc 0.9833911169415293\n",
            "epoch 44 batch id 2011 loss 0.30564063787460327 train acc 0.9833571606166086\n",
            "epoch 44 batch id 2021 loss 0.0805547684431076 train acc 0.983323540326571\n",
            "epoch 44 batch id 2031 loss 0.012479376047849655 train acc 0.9832902511078286\n",
            "epoch 44 batch id 2041 loss 0.0041702124290168285 train acc 0.9833032214600685\n",
            "epoch 44 batch id 2051 loss 0.2791507840156555 train acc 0.9832703559239395\n",
            "epoch 44 batch id 2061 loss 0.0006094368873164058 train acc 0.9833060407569141\n",
            "epoch 44 batch id 2071 loss 0.10049346089363098 train acc 0.9832961129888943\n",
            "epoch 44 batch id 2081 loss 0.15479832887649536 train acc 0.9832637554060548\n",
            "epoch 44 batch id 2091 loss 0.03087158501148224 train acc 0.98325412482066\n",
            "epoch 44 batch id 2101 loss 0.0050492133013904095 train acc 0.9832817705854355\n",
            "epoch 44 batch id 2111 loss 0.015505263581871986 train acc 0.9833091544291804\n",
            "epoch 44 batch id 2121 loss 0.012042044661939144 train acc 0.9833289132484677\n",
            "epoch 44 batch id 2131 loss 0.08329250663518906 train acc 0.9833778155795401\n",
            "epoch 44 batch id 2141 loss 0.04061121866106987 train acc 0.983404367118169\n",
            "epoch 44 batch id 2151 loss 0.05132634937763214 train acc 0.983408879590888\n",
            "epoch 44 batch id 2161 loss 0.0515214204788208 train acc 0.9834061198519204\n",
            "epoch 44 batch id 2171 loss 0.006158516742289066 train acc 0.9833817941040995\n",
            "epoch 44 batch id 2181 loss 0.10589498281478882 train acc 0.9833863480055021\n",
            "epoch 44 batch id 2191 loss 0.05535605177283287 train acc 0.9833908603377454\n",
            "epoch 44 batch id 2201 loss 0.007313363254070282 train acc 0.9834024307133121\n",
            "epoch 44 batch id 2211 loss 0.0013288564514368773 train acc 0.983406829488919\n",
            "epoch 44 batch id 2221 loss 0.02088705077767372 train acc 0.983404153534444\n",
            "epoch 44 batch id 2231 loss 0.013141656294465065 train acc 0.9833944979829673\n",
            "epoch 44 batch id 2241 loss 0.027763061225414276 train acc 0.9833849286033021\n",
            "epoch 44 batch id 2251 loss 0.00022700043336953968 train acc 0.9833615615282096\n",
            "epoch 44 batch id 2261 loss 0.00012448355846572667 train acc 0.9833867757629368\n",
            "epoch 44 batch id 2271 loss 0.00953622255474329 train acc 0.9834324086305593\n",
            "epoch 44 batch id 2281 loss 0.048399366438388824 train acc 0.9834365409907935\n",
            "epoch 44 batch id 2291 loss 0.022526048123836517 train acc 0.9834338171104321\n",
            "epoch 44 batch id 2301 loss 0.02082451991736889 train acc 0.9834379074315515\n",
            "epoch 44 batch id 2311 loss 0.03811532258987427 train acc 0.983448723496322\n",
            "epoch 44 batch id 2321 loss 0.07767251133918762 train acc 0.9834392503231366\n",
            "epoch 44 batch id 2331 loss 0.061402253806591034 train acc 0.9834231552981553\n",
            "epoch 44 batch id 2341 loss 0.02726942114531994 train acc 0.9834272212729602\n",
            "epoch 44 batch id 2351 loss 0.017673706635832787 train acc 0.9834113143343258\n",
            "epoch 44 batch id 2361 loss 0.09971489757299423 train acc 0.983448485811097\n",
            "epoch 44 batch id 2371 loss 0.0006943123880773783 train acc 0.983472163644032\n",
            "epoch 44 batch id 2381 loss 0.008618504740297794 train acc 0.9834890802183957\n",
            "epoch 44 batch id 2391 loss 0.016350872814655304 train acc 0.9834731806775407\n",
            "epoch 44 batch id 2401 loss 0.026249494403600693 train acc 0.9834639212827988\n",
            "epoch 44 batch id 2411 loss 0.024678001180291176 train acc 0.9834417772708419\n",
            "epoch 44 batch id 2421 loss 0.004587741103023291 train acc 0.983484355638166\n",
            "epoch 44 batch id 2431 loss 0.05988555774092674 train acc 0.9834944467297408\n",
            "epoch 44 batch id 2441 loss 0.00016236054943874478 train acc 0.983491653011061\n",
            "epoch 44 batch id 2451 loss 0.0011645843042060733 train acc 0.9834697572419421\n",
            "epoch 44 batch id 2461 loss 0.02586730197072029 train acc 0.9834797846403901\n",
            "epoch 44 batch id 2471 loss 0.08103574812412262 train acc 0.9834517907729664\n",
            "epoch 44 batch id 2481 loss 0.08845901489257812 train acc 0.9834807033454253\n",
            "epoch 44 batch id 2491 loss 0.07689081132411957 train acc 0.9834717482938579\n",
            "epoch 44 batch id 2501 loss 0.003565765917301178 train acc 0.983469112355058\n",
            "epoch 44 batch id 2511 loss 0.014326339587569237 train acc 0.9834664974113899\n",
            "epoch 44 batch id 2521 loss 0.0799565315246582 train acc 0.9834143197143991\n",
            "epoch 44 batch id 2531 loss 0.01822943426668644 train acc 0.9834242888186487\n",
            "epoch 44 batch id 2541 loss 0.02806670218706131 train acc 0.9834157319952774\n",
            "epoch 44 batch id 2551 loss 0.028844816610217094 train acc 0.9834256174049393\n",
            "epoch 44 batch id 2561 loss 0.02095635049045086 train acc 0.9834110210855135\n",
            "epoch 44 batch id 2571 loss 0.06792780756950378 train acc 0.9833965383119408\n",
            "epoch 44 batch id 2581 loss 0.04392146319150925 train acc 0.9833700600542425\n",
            "epoch 44 batch id 2591 loss 0.013560189865529537 train acc 0.9833739386337321\n",
            "epoch 44 batch id 2601 loss 0.02932615764439106 train acc 0.9833898019992311\n",
            "epoch 44 batch id 2611 loss 0.00012115453137084842 train acc 0.983411528150134\n",
            "epoch 44 batch id 2621 loss 0.0590289942920208 train acc 0.9833913582602061\n",
            "epoch 44 batch id 2631 loss 0.08263854682445526 train acc 0.9833594640820981\n",
            "epoch 44 batch id 2641 loss 0.005217080935835838 train acc 0.9833514767133662\n",
            "epoch 44 batch id 2651 loss 0.00020201006554998457 train acc 0.983361231610713\n",
            "epoch 44 batch id 2661 loss 0.002541894558817148 train acc 0.983365041337843\n",
            "epoch 44 batch id 2671 loss 0.0020060227252542973 train acc 0.9833688225383751\n",
            "epoch 44 batch id 2681 loss 0.08687953650951385 train acc 0.9832968108914584\n",
            "epoch 44 batch id 2691 loss 0.04440871998667717 train acc 0.9832775919732442\n",
            "epoch 44 batch id 2701 loss 0.0645589530467987 train acc 0.9832643002591632\n",
            "epoch 44 batch id 2711 loss 0.07176358997821808 train acc 0.9832107617115455\n",
            "epoch 44 batch id 2721 loss 0.09722664952278137 train acc 0.9831805861815509\n",
            "epoch 44 batch id 2731 loss 0.0031642902176827192 train acc 0.9831906810692054\n",
            "epoch 44 batch id 2741 loss 0.034267716109752655 train acc 0.9832007022984313\n",
            "epoch 44 batch id 2751 loss 0.13678410649299622 train acc 0.9831879316612141\n",
            "epoch 44 batch id 2761 loss 0.013632319867610931 train acc 0.9832092086200652\n",
            "epoch 44 batch id 2771 loss 0.008051657117903233 train acc 0.9832190544929629\n",
            "epoch 44 batch id 2781 loss 0.11303340643644333 train acc 0.9832288295577131\n",
            "epoch 44 batch id 2791 loss 0.017254628241062164 train acc 0.9832217395198853\n",
            "epoch 44 batch id 2801 loss 0.12631604075431824 train acc 0.9831923866476259\n",
            "epoch 44 batch id 2811 loss 0.04949271306395531 train acc 0.9832355033795802\n",
            "epoch 44 batch id 2821 loss 0.04759541526436806 train acc 0.98326169797944\n",
            "epoch 44 batch id 2831 loss 0.014390191063284874 train acc 0.9832545920169551\n",
            "epoch 44 batch id 2841 loss 0.06645257025957108 train acc 0.9832310366068285\n",
            "epoch 44 batch id 2851 loss 0.02716686762869358 train acc 0.9832240880392845\n",
            "epoch 44 batch id 2861 loss 0.1250496506690979 train acc 0.983206265291856\n",
            "epoch 44 batch id 2871 loss 0.09520362317562103 train acc 0.9832103361198189\n",
            "epoch 44 batch id 2881 loss 0.044142723083496094 train acc 0.9832089552238806\n",
            "epoch 44 batch id 2891 loss 0.06903508305549622 train acc 0.9832292026980284\n",
            "epoch 44 batch id 2901 loss 0.008392557501792908 train acc 0.9832439245087901\n",
            "epoch 44 batch id 2911 loss 0.0243777297437191 train acc 0.9832263397457919\n",
            "epoch 44 batch id 2921 loss 0.0008240179158747196 train acc 0.9832035261896611\n",
            "epoch 44 batch id 2931 loss 0.03921718895435333 train acc 0.983175537359263\n",
            "epoch 44 batch id 2941 loss 0.0007366652716882527 train acc 0.9831583645018701\n",
            "epoch 44 batch id 2951 loss 0.08200762420892715 train acc 0.9831889613690274\n",
            "epoch 44 batch id 2961 loss 0.07364768534898758 train acc 0.9831824130361364\n",
            "epoch 44 batch id 2971 loss 0.015895389020442963 train acc 0.9831548720969371\n",
            "epoch 44 batch id 2981 loss 0.003866911865770817 train acc 0.9831379989936263\n",
            "epoch 44 batch id 2991 loss 0.011871423572301865 train acc 0.9831682547642929\n",
            "epoch 44 batch id 3001 loss 0.0005011683679185808 train acc 0.9831983088970343\n",
            "epoch 44 batch id 3011 loss 0.00398083683103323 train acc 0.9832022168714712\n",
            "epoch 44 batch id 3021 loss 0.028883177787065506 train acc 0.9831957547169812\n",
            "epoch 44 batch id 3031 loss 0.13204799592494965 train acc 0.9831687149455626\n",
            "epoch 44 batch id 3041 loss 0.08842886984348297 train acc 0.9831315767839527\n",
            "epoch 44 batch id 3051 loss 0.006881992798298597 train acc 0.9831407735168797\n",
            "epoch 44 batch id 3061 loss 0.09321145713329315 train acc 0.9831294919960797\n",
            "epoch 44 batch id 3071 loss 0.0001876160386018455 train acc 0.9831488114620645\n",
            "epoch 44 batch id 3081 loss 0.017579130828380585 train acc 0.9831578627069133\n",
            "epoch 44 batch id 3091 loss 0.01654767245054245 train acc 0.9831769653833711\n",
            "epoch 44 batch id 3101 loss 0.05928676947951317 train acc 0.9831657126733312\n",
            "epoch 44 batch id 3111 loss 0.07085119187831879 train acc 0.9831344423015108\n",
            "epoch 44 batch id 3121 loss 0.032639194279909134 train acc 0.9831384171739826\n",
            "epoch 44 batch id 3131 loss 0.0029794916044920683 train acc 0.9831573379112105\n",
            "epoch 44 batch id 3141 loss 0.0793234333395958 train acc 0.9831512655205349\n",
            "epoch 44 batch id 3151 loss 0.11780467629432678 train acc 0.9831353141859727\n",
            "epoch 44 batch id 3161 loss 0.07293599098920822 train acc 0.9831244068332806\n",
            "epoch 44 batch id 3171 loss 0.019235901534557343 train acc 0.9831382056133712\n",
            "epoch 44 batch id 3181 loss 0.07960178703069687 train acc 0.9831322697265011\n",
            "epoch 44 batch id 3191 loss 0.016303028911352158 train acc 0.9831410607959887\n",
            "epoch 44 batch id 3201 loss 0.07516977190971375 train acc 0.9831400343642611\n",
            "epoch 44 batch id 3211 loss 0.1296432912349701 train acc 0.9831146838990968\n",
            "epoch 44 batch id 3221 loss 0.0550403818488121 train acc 0.9830603849736107\n",
            "epoch 44 batch id 3231 loss 0.02821996808052063 train acc 0.9830402739090065\n",
            "epoch 44 batch id 3241 loss 0.0936741754412651 train acc 0.9830636763344647\n",
            "epoch 44 batch id 3251 loss 0.05806475505232811 train acc 0.9830484850815134\n",
            "epoch 44 batch id 3261 loss 0.17367002367973328 train acc 0.9830046381478075\n",
            "epoch 44 batch id 3271 loss 0.08926190435886383 train acc 0.9829753897890553\n",
            "epoch 44 batch id 3281 loss 0.026144782081246376 train acc 0.9829796555928071\n",
            "epoch 44 batch id 3291 loss 0.008483488112688065 train acc 0.9829791476754786\n",
            "epoch 44 batch id 3301 loss 0.014241761527955532 train acc 0.9829881096637383\n",
            "epoch 44 batch id 3311 loss 0.008125456050038338 train acc 0.982987579281184\n",
            "epoch 44 batch id 3321 loss 0.012631557881832123 train acc 0.9829964619090635\n",
            "epoch 44 batch id 3331 loss 0.017462441697716713 train acc 0.9829677649354548\n",
            "epoch 44 batch id 3341 loss 0.017566366121172905 train acc 0.982981330439988\n",
            "epoch 44 batch id 3351 loss 0.013725681230425835 train acc 0.9829761638316921\n",
            "epoch 44 batch id 3361 loss 0.02739119343459606 train acc 0.9829896236239215\n",
            "epoch 44 batch id 3371 loss 0.08249926567077637 train acc 0.9829937333135568\n",
            "epoch 44 batch id 3381 loss 0.0003564144135452807 train acc 0.9830116829340432\n",
            "epoch 44 batch id 3391 loss 0.042684104293584824 train acc 0.9830203111176644\n",
            "epoch 44 batch id 3401 loss 0.19515539705753326 train acc 0.9830151058512202\n",
            "epoch 44 batch id 3411 loss 0.022263815626502037 train acc 0.9830282541776605\n",
            "epoch 44 batch id 3421 loss 0.01524234376847744 train acc 0.9830458930137387\n",
            "epoch 44 batch id 3431 loss 0.09059441834688187 train acc 0.9830588749635675\n",
            "epoch 44 batch id 3441 loss 0.014873107895255089 train acc 0.9830536181342633\n",
            "epoch 44 batch id 3451 loss 0.12283257395029068 train acc 0.9830348087510866\n",
            "epoch 44 batch id 3461 loss 0.006174315698444843 train acc 0.9830341664258885\n",
            "epoch 44 batch id 3471 loss 0.031028153374791145 train acc 0.9830515341400173\n",
            "epoch 44 batch id 3481 loss 0.07486674189567566 train acc 0.9830418701522551\n",
            "epoch 44 batch id 3491 loss 0.032046638429164886 train acc 0.98304121311945\n",
            "epoch 44 batch id 3501 loss 0.037572890520095825 train acc 0.9830450228506141\n",
            "epoch 44 batch id 3511 loss 0.0457429476082325 train acc 0.9830354599829109\n",
            "epoch 44 batch id 3521 loss 0.058845359832048416 train acc 0.9830570150525418\n",
            "epoch 44 batch id 3531 loss 0.08856201171875 train acc 0.9830695978476353\n",
            "epoch 44 batch id 3541 loss 0.05100390687584877 train acc 0.9830512214063823\n",
            "epoch 44 batch id 3551 loss 0.019420791417360306 train acc 0.9830109476203887\n",
            "epoch 44 batch id 3561 loss 0.011108393780887127 train acc 0.9830191659646167\n",
            "epoch 44 batch id 3571 loss 0.010999483056366444 train acc 0.9830360893307197\n",
            "epoch 44 batch id 3581 loss 0.04421079531311989 train acc 0.9830572814856186\n",
            "epoch 44 batch id 3591 loss 0.07445768266916275 train acc 0.9830609509885826\n",
            "epoch 44 batch id 3601 loss 0.04657824710011482 train acc 0.9830602610386003\n",
            "epoch 44 batch id 3611 loss 0.055011484771966934 train acc 0.9830595749099972\n",
            "epoch 44 batch id 3621 loss 0.04128836840391159 train acc 0.9830330019331677\n",
            "epoch 44 batch id 3631 loss 0.007017732132226229 train acc 0.9830108785458551\n",
            "epoch 44 batch id 3641 loss 0.018660549074411392 train acc 0.9830403735237572\n",
            "epoch 44 batch id 3651 loss 0.023270566016435623 train acc 0.9830183511366749\n",
            "epoch 44 batch id 3661 loss 0.02288912795484066 train acc 0.983017788855504\n",
            "epoch 44 batch id 3671 loss 0.19423268735408783 train acc 0.9830172296377009\n",
            "epoch 44 batch id 3681 loss 0.0005444292328320444 train acc 0.9830294077696278\n",
            "epoch 44 batch id 3691 loss 0.001175137353129685 train acc 0.9830288201029531\n",
            "epoch 44 batch id 3701 loss 0.08365405350923538 train acc 0.9829986827884356\n",
            "epoch 44 batch id 3711 loss 0.021359892562031746 train acc 0.9830023915386689\n",
            "epoch 44 batch id 3721 loss 0.1370859146118164 train acc 0.9830102794947595\n",
            "epoch 44 batch id 3731 loss 0.04304865002632141 train acc 0.9830223130528009\n",
            "epoch 44 batch id 3741 loss 0.09549514949321747 train acc 0.9830175755145683\n",
            "epoch 44 batch id 3751 loss 0.016851307824254036 train acc 0.9830253599040256\n",
            "epoch 44 batch id 3761 loss 0.1372520625591278 train acc 0.9829790946556767\n",
            "epoch 44 batch id 3771 loss 0.017278211191296577 train acc 0.9829537920975868\n",
            "epoch 44 batch id 3781 loss 0.08962906152009964 train acc 0.982936888389315\n",
            "epoch 44 batch id 3791 loss 0.00010396169818704948 train acc 0.9829612898971247\n",
            "epoch 44 batch id 3801 loss 0.12684506177902222 train acc 0.982944455406472\n",
            "epoch 44 batch id 3811 loss 0.024568092077970505 train acc 0.9829523091052217\n",
            "epoch 44 batch id 3821 loss 0.025927143171429634 train acc 0.9829642109395447\n",
            "epoch 44 batch id 3831 loss 0.00019123022502753884 train acc 0.9829597363612633\n",
            "epoch 44 batch id 3841 loss 0.0003061604220420122 train acc 0.9829430812288467\n",
            "epoch 44 batch id 3851 loss 0.0009851529030129313 train acc 0.9829427421448974\n",
            "epoch 44 batch id 3861 loss 0.022313646972179413 train acc 0.9829383579383579\n",
            "epoch 44 batch id 3871 loss 0.0712851956486702 train acc 0.9829501420821494\n",
            "epoch 44 batch id 3881 loss 0.042255956679582596 train acc 0.9829578394743623\n",
            "epoch 44 batch id 3891 loss 0.012371333315968513 train acc 0.9829614816242611\n",
            "epoch 44 batch id 3901 loss 0.013812476769089699 train acc 0.9829651051012561\n",
            "epoch 44 batch id 3911 loss 0.029587702825665474 train acc 0.9829447391971363\n",
            "epoch 44 batch id 3921 loss 0.014279390685260296 train acc 0.9829563567967355\n",
            "epoch 44 batch id 3931 loss 0.16408298909664154 train acc 0.9829440663953193\n",
            "epoch 44 batch id 3941 loss 0.07429774105548859 train acc 0.9829714856635372\n",
            "epoch 44 batch id 3951 loss 0.03579436242580414 train acc 0.9829513097949886\n",
            "epoch 44 batch id 3961 loss 0.001888891914859414 train acc 0.9829627934864933\n",
            "epoch 44 batch id 3971 loss 0.04836747422814369 train acc 0.9829584802316796\n",
            "epoch 44 batch id 3981 loss 0.002802459057420492 train acc 0.9829541886460689\n",
            "epoch 44 batch id 3991 loss 0.13271459937095642 train acc 0.9829499185667753\n",
            "epoch 44 batch id 4001 loss 0.0756562352180481 train acc 0.982953480379905\n",
            "epoch 44 batch id 4011 loss 0.10350421071052551 train acc 0.9829258601346298\n",
            "epoch 44 batch id 4021 loss 0.01891082152724266 train acc 0.9829294640636658\n",
            "epoch 44 batch id 4031 loss 0.01036476343870163 train acc 0.9829524311585215\n",
            "epoch 44 batch id 4041 loss 0.09858288615942001 train acc 0.9829134187082406\n",
            "epoch 44 batch id 4051 loss 0.02035459317266941 train acc 0.9829247408047396\n",
            "epoch 44 batch id 4061 loss 0.016464758664369583 train acc 0.9829360071410983\n",
            "epoch 44 batch id 4071 loss 0.019304456189274788 train acc 0.9829433800049128\n",
            "epoch 44 batch id 4081 loss 0.011616179719567299 train acc 0.9829660316098995\n",
            "epoch 44 batch id 4091 loss 0.04274279996752739 train acc 0.9829427401613298\n",
            "epoch 44 batch id 4101 loss 0.16503366827964783 train acc 0.982950042672519\n",
            "epoch 44 batch id 4111 loss 0.013652794994413853 train acc 0.9829383057650207\n",
            "epoch 44 batch id 4121 loss 0.04702186584472656 train acc 0.9829304173744237\n",
            "epoch 44 batch id 4131 loss 0.06805310398340225 train acc 0.9829225671750181\n",
            "epoch 44 batch id 4141 loss 0.0013907295651733875 train acc 0.982937394349191\n",
            "epoch 44 batch id 4151 loss 0.06888539344072342 train acc 0.9829559142375331\n",
            "epoch 44 batch id 4161 loss 0.025891166180372238 train acc 0.9829330389329488\n",
            "epoch 44 batch id 4171 loss 0.17598240077495575 train acc 0.9829290038360106\n",
            "epoch 44 batch id 4181 loss 0.0069065578281879425 train acc 0.9829287251853623\n",
            "epoch 44 batch id 4191 loss 0.09988105297088623 train acc 0.9829209914101646\n",
            "epoch 44 batch id 4201 loss 0.10101228207349777 train acc 0.9829356105689122\n",
            "epoch 44 batch id 4211 loss 0.017451640218496323 train acc 0.9829390287342674\n",
            "epoch 44 batch id 4221 loss 0.0922853946685791 train acc 0.9829350272447287\n",
            "epoch 44 batch id 4231 loss 0.05801217257976532 train acc 0.9829236587095249\n",
            "epoch 44 batch id 4241 loss 0.1427089422941208 train acc 0.9829270808771516\n",
            "epoch 44 batch id 4251 loss 0.10727342218160629 train acc 0.9829304869442484\n",
            "epoch 44 batch id 4261 loss 0.004889030009508133 train acc 0.982937544003755\n",
            "epoch 44 batch id 4271 loss 0.0034644000697880983 train acc 0.9829372512292204\n",
            "epoch 44 batch id 4281 loss 0.15794456005096436 train acc 0.982944259518804\n",
            "epoch 44 batch id 4291 loss 0.01107774581760168 train acc 0.9829293870892566\n",
            "epoch 44 batch id 4301 loss 0.032823242247104645 train acc 0.9829109509416415\n",
            "epoch 44 batch id 4311 loss 0.032734401524066925 train acc 0.9829215959174206\n",
            "epoch 44 batch id 4321 loss 0.023994021117687225 train acc 0.9829177273779218\n",
            "epoch 44 batch id 4331 loss 0.12025819718837738 train acc 0.9829030535673055\n",
            "epoch 44 batch id 4341 loss 0.17444072663784027 train acc 0.9828956461644782\n",
            "epoch 44 batch id 4351 loss 0.03530649095773697 train acc 0.9828882728108481\n",
            "epoch 44 batch id 4361 loss 0.12264782935380936 train acc 0.982884516166017\n",
            "epoch 44 batch id 4371 loss 0.07342642545700073 train acc 0.9828986501944635\n",
            "epoch 44 batch id 4381 loss 0.13551437854766846 train acc 0.9828948870120977\n",
            "epoch 44 batch id 4391 loss 0.016839291900396347 train acc 0.9829053746299249\n",
            "epoch 44 batch id 4401 loss 0.09710688143968582 train acc 0.9828909622812997\n",
            "epoch 44 batch id 4411 loss 0.0002532158687245101 train acc 0.9828907844026298\n",
            "epoch 44 batch id 4421 loss 0.09348076581954956 train acc 0.9828941415969238\n",
            "epoch 44 batch id 4431 loss 0.12833671271800995 train acc 0.9828833784698714\n",
            "epoch 44 batch id 4441 loss 0.01245154719799757 train acc 0.9828832188696239\n",
            "epoch 44 batch id 4451 loss 0.0003297596122138202 train acc 0.9829041226690631\n",
            "epoch 44 batch id 4461 loss 0.004085235297679901 train acc 0.9829179275947098\n",
            "epoch 44 batch id 4471 loss 0.05975494906306267 train acc 0.9829211865354507\n",
            "epoch 44 batch id 4481 loss 0.02200520969927311 train acc 0.9829000223164472\n",
            "epoch 44 batch id 4491 loss 0.08355386555194855 train acc 0.9828754731685594\n",
            "epoch 44 batch id 4501 loss 0.011819923296570778 train acc 0.9828683903576982\n",
            "epoch 44 batch id 4511 loss 0.09314602613449097 train acc 0.9828544114387053\n",
            "epoch 44 batch id 4521 loss 0.07323507964611053 train acc 0.9828543187347932\n",
            "epoch 44 batch id 4531 loss 0.0001471484574722126 train acc 0.9828542264400795\n",
            "epoch 44 batch id 4541 loss 0.05970060080289841 train acc 0.9828644571680246\n",
            "epoch 44 batch id 4551 loss 0.07145363837480545 train acc 0.9828643430015381\n",
            "epoch 44 batch id 4561 loss 0.05524790659546852 train acc 0.982853951984214\n",
            "epoch 44 batch id 4571 loss 0.026882756501436234 train acc 0.9828504430102822\n",
            "epoch 44 batch id 4581 loss 0.023346491158008575 train acc 0.9828571818380266\n",
            "epoch 44 batch id 4591 loss 0.05662032216787338 train acc 0.9828434709213679\n",
            "epoch 44 batch id 4601 loss 0.04749920219182968 train acc 0.9828162356009563\n",
            "epoch 44 batch id 4611 loss 0.03269311413168907 train acc 0.9828331706788115\n",
            "epoch 44 batch id 4621 loss 0.01447524968534708 train acc 0.9828297446440164\n",
            "epoch 44 batch id 4631 loss 0.07088042795658112 train acc 0.9828364554091988\n",
            "epoch 44 batch id 4641 loss 0.07727299630641937 train acc 0.9828566041801335\n",
            "epoch 44 batch id 4651 loss 0.11412239819765091 train acc 0.9828598688454095\n",
            "epoch 44 batch id 4661 loss 0.046440329402685165 train acc 0.9828698240720876\n",
            "epoch 44 batch id 4671 loss 0.04084198921918869 train acc 0.982886426889317\n",
            "epoch 44 batch id 4681 loss 0.13610169291496277 train acc 0.9828996208075198\n",
            "epoch 44 batch id 4691 loss 0.006516650319099426 train acc 0.9828861117032616\n",
            "epoch 44 batch id 4701 loss 0.016011245548725128 train acc 0.9828959263986385\n",
            "epoch 44 batch id 4711 loss 0.05056338384747505 train acc 0.9828957493101252\n",
            "epoch 44 batch id 4721 loss 0.002121717669069767 train acc 0.982882334251218\n",
            "epoch 44 batch id 4731 loss 0.017961274832487106 train acc 0.9828887920101459\n",
            "epoch 44 batch id 4741 loss 0.08663064986467361 train acc 0.9828787439358785\n",
            "epoch 44 batch id 4751 loss 0.139705091714859 train acc 0.9828851820669333\n",
            "epoch 44 batch id 4761 loss 0.08525015413761139 train acc 0.9828784656584751\n",
            "epoch 44 batch id 4771 loss 0.07032714039087296 train acc 0.9828881523789562\n",
            "epoch 44 batch id 4781 loss 0.11238595843315125 train acc 0.9828781897092659\n",
            "epoch 44 batch id 4791 loss 0.2323758453130722 train acc 0.9828650073053642\n",
            "epoch 44 batch id 4801 loss 0.08221589773893356 train acc 0.9828583888773172\n",
            "epoch 44 batch id 4811 loss 0.050748441368341446 train acc 0.9828615412596133\n",
            "epoch 44 batch id 4821 loss 0.05993613600730896 train acc 0.9828517164488695\n",
            "epoch 44 batch id 4831 loss 0.021783579140901566 train acc 0.9828678068722831\n",
            "epoch 44 batch id 4841 loss 0.0034874742850661278 train acc 0.9828676926254906\n",
            "epoch 44 batch id 4851 loss 0.05715963989496231 train acc 0.982880462791177\n",
            "epoch 44 batch id 4861 loss 0.000255297141848132 train acc 0.9828803229788109\n",
            "epoch 44 batch id 4871 loss 0.04077012464404106 train acc 0.982886599260932\n",
            "epoch 44 batch id 4881 loss 0.02676568739116192 train acc 0.982915258143823\n",
            "epoch 44 batch id 4891 loss 0.01688934676349163 train acc 0.9829214373338786\n",
            "epoch 44 batch id 4901 loss 0.044730253517627716 train acc 0.9829116506835339\n",
            "epoch 44 batch id 4911 loss 0.05275602638721466 train acc 0.9829178120545714\n",
            "epoch 44 batch id 4921 loss 0.22306157648563385 train acc 0.9829271235521235\n",
            "epoch 44 batch id 4931 loss 0.05986752361059189 train acc 0.9829237223686879\n",
            "epoch 44 batch id 4941 loss 0.04552251473069191 train acc 0.9829045233758349\n",
            "epoch 44 batch id 4951 loss 0.1583167165517807 train acc 0.9828980256513835\n",
            "epoch 44 train acc 0.9828966612870688\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcfd670fcfa44cfa8aa22ae1e7810cb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 44 loss 0.5121139883995056 test acc 0.8526106579912024\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fce280defd64609b6cafed8735e95c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 45 batch id 1 loss 0.07196531444787979 train acc 0.96875\n",
            "epoch 45 batch id 11 loss 0.016257675364613533 train acc 0.9829545454545454\n",
            "epoch 45 batch id 21 loss 0.028188563883304596 train acc 0.9821428571428571\n",
            "epoch 45 batch id 31 loss 0.0586230494081974 train acc 0.9828629032258065\n",
            "epoch 45 batch id 41 loss 0.01138958428055048 train acc 0.984375\n",
            "epoch 45 batch id 51 loss 0.00816301442682743 train acc 0.9837622549019608\n",
            "epoch 45 batch id 61 loss 0.03468752279877663 train acc 0.9813012295081968\n",
            "epoch 45 batch id 71 loss 0.07452116161584854 train acc 0.981294014084507\n",
            "epoch 45 batch id 81 loss 0.04266274347901344 train acc 0.9820601851851852\n",
            "epoch 45 batch id 91 loss 0.07188816368579865 train acc 0.9819711538461539\n",
            "epoch 45 batch id 101 loss 0.05828342214226723 train acc 0.9811262376237624\n",
            "epoch 45 batch id 111 loss 0.00012887145567219704 train acc 0.9811373873873874\n",
            "epoch 45 batch id 121 loss 0.030729370191693306 train acc 0.9819214876033058\n",
            "epoch 45 batch id 131 loss 0.031108323484659195 train acc 0.9822280534351145\n",
            "epoch 45 batch id 141 loss 0.09647863358259201 train acc 0.9820478723404256\n",
            "epoch 45 batch id 151 loss 0.05039243400096893 train acc 0.9826158940397351\n",
            "epoch 45 batch id 161 loss 0.00794430822134018 train acc 0.9829192546583851\n",
            "epoch 45 batch id 171 loss 0.009361940436065197 train acc 0.9826388888888888\n",
            "epoch 45 batch id 181 loss 0.019447220489382744 train acc 0.9829074585635359\n",
            "epoch 45 batch id 191 loss 0.049692340195178986 train acc 0.9825752617801047\n",
            "epoch 45 batch id 201 loss 0.13950130343437195 train acc 0.9825870646766169\n",
            "epoch 45 batch id 211 loss 0.14248813688755035 train acc 0.9828939573459715\n",
            "epoch 45 batch id 221 loss 0.12360774725675583 train acc 0.9827488687782805\n",
            "epoch 45 batch id 231 loss 0.02202560380101204 train acc 0.9830221861471862\n",
            "epoch 45 batch id 241 loss 0.10200823098421097 train acc 0.9828838174273858\n",
            "epoch 45 batch id 251 loss 0.0910186916589737 train acc 0.9830677290836654\n",
            "epoch 45 batch id 261 loss 0.1364467293024063 train acc 0.983117816091954\n",
            "epoch 45 batch id 271 loss 0.0007900625350885093 train acc 0.9833371771217713\n",
            "epoch 45 batch id 281 loss 0.005674190819263458 train acc 0.9836521352313167\n",
            "epoch 45 batch id 291 loss 0.06310835480690002 train acc 0.9837843642611683\n",
            "epoch 45 batch id 301 loss 0.09882882982492447 train acc 0.9836482558139535\n",
            "epoch 45 batch id 311 loss 0.016755299642682076 train acc 0.983822347266881\n",
            "epoch 45 batch id 321 loss 0.1776428520679474 train acc 0.9838395638629284\n",
            "epoch 45 batch id 331 loss 0.0024578757584095 train acc 0.9839501510574018\n",
            "epoch 45 batch id 341 loss 0.16085851192474365 train acc 0.9839626099706745\n",
            "epoch 45 batch id 351 loss 0.010191202163696289 train acc 0.984107905982906\n",
            "epoch 45 batch id 361 loss 0.009826522320508957 train acc 0.9841153047091413\n",
            "epoch 45 batch id 371 loss 0.015514394268393517 train acc 0.984375\n",
            "epoch 45 batch id 381 loss 0.016733378171920776 train acc 0.9844570209973753\n",
            "epoch 45 batch id 391 loss 0.044342201203107834 train acc 0.9846147698209718\n",
            "epoch 45 batch id 401 loss 0.0002874622878152877 train acc 0.9846477556109726\n",
            "epoch 45 batch id 411 loss 0.0003559262549970299 train acc 0.9847551703163017\n",
            "epoch 45 batch id 421 loss 0.05352358892560005 train acc 0.9847461401425178\n",
            "epoch 45 batch id 431 loss 0.06297137588262558 train acc 0.9846650232018561\n",
            "epoch 45 batch id 441 loss 0.16154518723487854 train acc 0.9844812925170068\n",
            "epoch 45 batch id 451 loss 0.022961298003792763 train acc 0.9845482261640798\n",
            "epoch 45 batch id 461 loss 0.0032628660555928946 train acc 0.9844088937093276\n",
            "epoch 45 batch id 471 loss 0.011895079165697098 train acc 0.9846072186836518\n",
            "epoch 45 batch id 481 loss 0.06773996353149414 train acc 0.9846348752598753\n",
            "epoch 45 batch id 491 loss 0.0015900526195764542 train acc 0.9845341140529531\n",
            "epoch 45 batch id 501 loss 0.21049943566322327 train acc 0.9845933133732535\n",
            "epoch 45 batch id 511 loss 0.08068754523992538 train acc 0.9844361545988258\n",
            "epoch 45 batch id 521 loss 0.09128343313932419 train acc 0.984434980806142\n",
            "epoch 45 batch id 531 loss 0.03531443327665329 train acc 0.9842867231638418\n",
            "epoch 45 batch id 541 loss 0.012359067797660828 train acc 0.9841439463955638\n",
            "epoch 45 batch id 551 loss 0.03069840744137764 train acc 0.9841481397459165\n",
            "epoch 45 batch id 561 loss 0.036541976034641266 train acc 0.9840129233511586\n",
            "epoch 45 batch id 571 loss 0.025608550757169724 train acc 0.984101357267951\n",
            "epoch 45 batch id 581 loss 0.030564917251467705 train acc 0.9841060671256454\n",
            "epoch 45 batch id 591 loss 0.04127434268593788 train acc 0.984084179357022\n",
            "epoch 45 batch id 601 loss 0.016475660726428032 train acc 0.9839850249584027\n",
            "epoch 45 batch id 611 loss 0.0812043696641922 train acc 0.9840169803600655\n",
            "epoch 45 batch id 621 loss 0.18677562475204468 train acc 0.9840479066022544\n",
            "epoch 45 batch id 631 loss 0.0654270350933075 train acc 0.9838797543581617\n",
            "epoch 45 batch id 641 loss 0.011513994075357914 train acc 0.983936232449298\n",
            "epoch 45 batch id 651 loss 0.036674316972494125 train acc 0.9837989631336406\n",
            "epoch 45 batch id 661 loss 0.0006812228821218014 train acc 0.9838549546142209\n",
            "epoch 45 batch id 671 loss 0.012607424519956112 train acc 0.9838859910581222\n",
            "epoch 45 batch id 681 loss 0.06641025096178055 train acc 0.9838013950073421\n",
            "epoch 45 batch id 691 loss 0.021737171337008476 train acc 0.9837870839363242\n",
            "epoch 45 batch id 701 loss 0.06702432781457901 train acc 0.9837731811697575\n",
            "epoch 45 batch id 711 loss 0.10015829652547836 train acc 0.9837816455696202\n",
            "epoch 45 batch id 721 loss 0.046767622232437134 train acc 0.98374653259362\n",
            "epoch 45 batch id 731 loss 0.011100596748292446 train acc 0.9837551299589603\n",
            "epoch 45 batch id 741 loss 0.010791239328682423 train acc 0.983721322537112\n",
            "epoch 45 batch id 751 loss 0.011975198052823544 train acc 0.9837300266311585\n",
            "epoch 45 batch id 761 loss 0.0008431110181845725 train acc 0.9837590341655716\n",
            "epoch 45 batch id 771 loss 0.048780616372823715 train acc 0.9837062256809338\n",
            "epoch 45 batch id 781 loss 0.021573957055807114 train acc 0.9837147887323944\n",
            "epoch 45 batch id 791 loss 0.01637076586484909 train acc 0.9838021491782554\n",
            "epoch 45 batch id 801 loss 0.001094152918085456 train acc 0.9838093008739076\n",
            "epoch 45 batch id 811 loss 0.10645454376935959 train acc 0.983797009864365\n",
            "epoch 45 batch id 821 loss 0.0072571104392409325 train acc 0.9838801766138855\n",
            "epoch 45 batch id 831 loss 0.2375258356332779 train acc 0.9838485258724429\n",
            "epoch 45 batch id 841 loss 0.0019498476758599281 train acc 0.9837990487514863\n",
            "epoch 45 batch id 851 loss 0.04346075281500816 train acc 0.9836956521739131\n",
            "epoch 45 batch id 861 loss 0.060616377741098404 train acc 0.9836309523809523\n",
            "epoch 45 batch id 871 loss 0.009508379735052586 train acc 0.9836574339839265\n",
            "epoch 45 batch id 881 loss 0.017607999965548515 train acc 0.9835946367763905\n",
            "epoch 45 batch id 891 loss 0.0004569451557472348 train acc 0.9836560044893379\n",
            "epoch 45 batch id 901 loss 0.027351634576916695 train acc 0.9836293007769146\n",
            "epoch 45 batch id 911 loss 0.08398723602294922 train acc 0.9836374862788145\n",
            "epoch 45 batch id 921 loss 0.0022540732752531767 train acc 0.9835437024972855\n",
            "epoch 45 batch id 931 loss 0.001092343358322978 train acc 0.9835694146079484\n",
            "epoch 45 batch id 941 loss 0.01127453614026308 train acc 0.9836609989373007\n",
            "epoch 45 batch id 951 loss 0.04366831108927727 train acc 0.9836192166140905\n",
            "epoch 45 batch id 961 loss 0.07684695720672607 train acc 0.9836108220603538\n",
            "epoch 45 batch id 971 loss 0.019878380000591278 train acc 0.983586508753862\n",
            "epoch 45 batch id 981 loss 0.010521254502236843 train acc 0.9836582568807339\n",
            "epoch 45 batch id 991 loss 0.07479985058307648 train acc 0.9836654894046418\n",
            "epoch 45 batch id 1001 loss 0.1079086884856224 train acc 0.983735014985015\n",
            "epoch 45 batch id 1011 loss 0.005217994097620249 train acc 0.9837258902077152\n",
            "epoch 45 batch id 1021 loss 0.029865650460124016 train acc 0.9837781586679726\n",
            "epoch 45 batch id 1031 loss 0.013569223694503307 train acc 0.9838597235693501\n",
            "epoch 45 batch id 1041 loss 0.014647381380200386 train acc 0.983924711815562\n",
            "epoch 45 batch id 1051 loss 0.08507127314805984 train acc 0.9837951950523312\n",
            "epoch 45 batch id 1061 loss 0.0723336786031723 train acc 0.9836975730442978\n",
            "epoch 45 batch id 1071 loss 0.00011733993596863002 train acc 0.9837330765639589\n",
            "epoch 45 batch id 1081 loss 0.12562060356140137 train acc 0.9836378353376504\n",
            "epoch 45 batch id 1091 loss 0.2850649356842041 train acc 0.9835873052245646\n",
            "epoch 45 batch id 1101 loss 0.054050929844379425 train acc 0.983608651226158\n",
            "epoch 45 batch id 1111 loss 0.05559104308485985 train acc 0.9835874212421242\n",
            "epoch 45 batch id 1121 loss 0.0011785775423049927 train acc 0.983664139161463\n",
            "epoch 45 batch id 1131 loss 0.09541005641222 train acc 0.9836427939876216\n",
            "epoch 45 batch id 1141 loss 0.07033980637788773 train acc 0.9836902936021035\n",
            "epoch 45 batch id 1151 loss 0.03535721078515053 train acc 0.9836283666377064\n",
            "epoch 45 batch id 1161 loss 0.0354708768427372 train acc 0.9836482558139535\n",
            "epoch 45 batch id 1171 loss 0.00912504456937313 train acc 0.9835877455166524\n",
            "epoch 45 batch id 1181 loss 0.0010272656800225377 train acc 0.9836341024555462\n",
            "epoch 45 batch id 1191 loss 0.020062576979398727 train acc 0.9836665617128464\n",
            "epoch 45 batch id 1201 loss 0.021058106794953346 train acc 0.9837505203996669\n",
            "epoch 45 batch id 1211 loss 0.0003788140311371535 train acc 0.9837556771263418\n",
            "epoch 45 batch id 1221 loss 0.09131170064210892 train acc 0.9837863431613432\n",
            "epoch 45 batch id 1231 loss 0.06048645079135895 train acc 0.9837403533712429\n",
            "epoch 45 batch id 1241 loss 0.01160486415028572 train acc 0.9837454673650282\n",
            "epoch 45 batch id 1251 loss 0.09434125572443008 train acc 0.9837629896083133\n",
            "epoch 45 batch id 1261 loss 0.02519005350768566 train acc 0.9837306701030928\n",
            "epoch 45 batch id 1271 loss 0.027203310281038284 train acc 0.9837480330448466\n",
            "epoch 45 batch id 1281 loss 0.0014893641928210855 train acc 0.9837041373926619\n",
            "epoch 45 batch id 1291 loss 0.06972748786211014 train acc 0.983721436870643\n",
            "epoch 45 batch id 1301 loss 0.15609602630138397 train acc 0.9836423904688701\n",
            "epoch 45 batch id 1311 loss 0.01883940026164055 train acc 0.983636060259344\n",
            "epoch 45 batch id 1321 loss 0.011874346993863583 train acc 0.9835943414080243\n",
            "epoch 45 batch id 1331 loss 0.025934699922800064 train acc 0.9834945529676935\n",
            "epoch 45 batch id 1341 loss 0.08758046478033066 train acc 0.9835127703206562\n",
            "epoch 45 batch id 1351 loss 0.0006541190668940544 train acc 0.9835422834937083\n",
            "epoch 45 batch id 1361 loss 0.004910909570753574 train acc 0.983502479794269\n",
            "epoch 45 batch id 1371 loss 0.005598477553576231 train acc 0.9835430342815463\n",
            "epoch 45 batch id 1381 loss 0.01635083369910717 train acc 0.9835716871832005\n",
            "epoch 45 batch id 1391 loss 0.008941425010561943 train acc 0.9835325305535586\n",
            "epoch 45 batch id 1401 loss 0.01033615879714489 train acc 0.9835385438972163\n",
            "epoch 45 batch id 1411 loss 0.018359122797846794 train acc 0.983588766832034\n",
            "epoch 45 batch id 1421 loss 0.013577189296483994 train acc 0.9836052955665024\n",
            "epoch 45 batch id 1431 loss 0.12334335595369339 train acc 0.9835451607267645\n",
            "epoch 45 batch id 1441 loss 0.018700113520026207 train acc 0.9834967036780013\n",
            "epoch 45 batch id 1451 loss 0.009167338721454144 train acc 0.9835458304617505\n",
            "epoch 45 batch id 1461 loss 0.023133322596549988 train acc 0.9835408110882957\n",
            "epoch 45 batch id 1471 loss 0.032137662172317505 train acc 0.9835783480625425\n",
            "epoch 45 batch id 1481 loss 0.08930498361587524 train acc 0.9835520762997975\n",
            "epoch 45 batch id 1491 loss 0.01933087594807148 train acc 0.9835890342052314\n",
            "epoch 45 batch id 1501 loss 0.06446651369333267 train acc 0.9835942704863424\n",
            "epoch 45 batch id 1511 loss 0.028038322925567627 train acc 0.9836201191264063\n",
            "epoch 45 batch id 1521 loss 0.004549961071461439 train acc 0.9836456278763971\n",
            "epoch 45 batch id 1531 loss 0.020451055839657784 train acc 0.9836605976485957\n",
            "epoch 45 batch id 1541 loss 0.010777830146253109 train acc 0.9836753731343284\n",
            "epoch 45 batch id 1551 loss 0.03451285883784294 train acc 0.9837000322372663\n",
            "epoch 45 batch id 1561 loss 0.04683223366737366 train acc 0.9836843369634849\n",
            "epoch 45 batch id 1571 loss 0.01033815462142229 train acc 0.9836787873965627\n",
            "epoch 45 batch id 1581 loss 0.031074391677975655 train acc 0.9836337760910816\n",
            "epoch 45 batch id 1591 loss 0.13323189318180084 train acc 0.9836384349465744\n",
            "epoch 45 batch id 1601 loss 0.10770237445831299 train acc 0.9837015927545284\n",
            "epoch 45 batch id 1611 loss 0.06555867195129395 train acc 0.983657278088144\n",
            "epoch 45 batch id 1621 loss 0.03630780428647995 train acc 0.9836617057371992\n",
            "epoch 45 batch id 1631 loss 0.00026241899468004704 train acc 0.9836756591048437\n",
            "epoch 45 batch id 1641 loss 0.05867789685726166 train acc 0.9837084856794638\n",
            "epoch 45 batch id 1651 loss 0.035038694739341736 train acc 0.9836841308298001\n",
            "epoch 45 batch id 1661 loss 0.04083665832877159 train acc 0.9837165111378687\n",
            "epoch 45 batch id 1671 loss 0.03896331042051315 train acc 0.9836923997606224\n",
            "epoch 45 batch id 1681 loss 0.11498380452394485 train acc 0.9836592801903629\n",
            "epoch 45 batch id 1691 loss 0.13056643307209015 train acc 0.9836635127143702\n",
            "epoch 45 batch id 1701 loss 0.039164505898952484 train acc 0.9836676954732511\n",
            "epoch 45 batch id 1711 loss 0.013215742073953152 train acc 0.9836900935125658\n",
            "epoch 45 batch id 1721 loss 0.006262658629566431 train acc 0.9836759151656014\n",
            "epoch 45 batch id 1731 loss 8.894924394553527e-05 train acc 0.9836889803581744\n",
            "epoch 45 batch id 1741 loss 0.023408906534314156 train acc 0.9836929207352096\n",
            "epoch 45 batch id 1751 loss 0.03556293994188309 train acc 0.9836968161050829\n",
            "epoch 45 batch id 1761 loss 0.01914176344871521 train acc 0.9837095400340715\n",
            "epoch 45 batch id 1771 loss 0.00017941620899364352 train acc 0.9837221202710333\n",
            "epoch 45 batch id 1781 loss 0.04322401061654091 train acc 0.9837608787198203\n",
            "epoch 45 batch id 1791 loss 0.15149176120758057 train acc 0.9837468592964824\n",
            "epoch 45 batch id 1801 loss 0.02596953883767128 train acc 0.9837676985008329\n",
            "epoch 45 batch id 1811 loss 0.030781442299485207 train acc 0.9837883075648812\n",
            "epoch 45 batch id 1821 loss 0.09504511952400208 train acc 0.9837657880285557\n",
            "epoch 45 batch id 1831 loss 0.017615022137761116 train acc 0.9837435144729656\n",
            "epoch 45 batch id 1841 loss 0.00018344570707995445 train acc 0.9837129956545356\n",
            "epoch 45 batch id 1851 loss 0.018585555255413055 train acc 0.9837503376553215\n",
            "epoch 45 batch id 1861 loss 0.011011944152414799 train acc 0.9837033181085438\n",
            "epoch 45 batch id 1871 loss 0.1726701855659485 train acc 0.9837236103687867\n",
            "epoch 45 batch id 1881 loss 0.019626285880804062 train acc 0.9837021531100478\n",
            "epoch 45 batch id 1891 loss 0.02332509681582451 train acc 0.9837304997355897\n",
            "epoch 45 batch id 1901 loss 0.09638218581676483 train acc 0.9837338900578643\n",
            "epoch 45 batch id 1911 loss 0.029468266293406487 train acc 0.9836800104657247\n",
            "epoch 45 batch id 1921 loss 0.0003973538405261934 train acc 0.9836266918271733\n",
            "epoch 45 batch id 1931 loss 0.017045654356479645 train acc 0.9836305670636976\n",
            "epoch 45 batch id 1941 loss 0.007812895812094212 train acc 0.9836505023183926\n",
            "epoch 45 batch id 1951 loss 0.03903292492032051 train acc 0.9836381983598155\n",
            "epoch 45 batch id 1961 loss 0.03800096735358238 train acc 0.9836817950025497\n",
            "epoch 45 batch id 1971 loss 0.04590475931763649 train acc 0.9836694571283613\n",
            "epoch 45 batch id 1981 loss 0.03571309149265289 train acc 0.9836493563856638\n",
            "epoch 45 batch id 1991 loss 0.2382676601409912 train acc 0.9835902184831743\n",
            "epoch 45 batch id 2001 loss 0.06869577616453171 train acc 0.9836019490254873\n",
            "epoch 45 batch id 2011 loss 0.07436347007751465 train acc 0.983598023371457\n",
            "epoch 45 batch id 2021 loss 0.09957817196846008 train acc 0.9835786739238\n",
            "epoch 45 batch id 2031 loss 0.00031039403984323144 train acc 0.9835902880354506\n",
            "epoch 45 batch id 2041 loss 0.008472510613501072 train acc 0.9835711660950515\n",
            "epoch 45 batch id 2051 loss 0.2909100353717804 train acc 0.9835446123842029\n",
            "epoch 45 batch id 2061 loss 0.1297413408756256 train acc 0.9835562227074236\n",
            "epoch 45 batch id 2071 loss 0.07876382023096085 train acc 0.9835526315789473\n",
            "epoch 45 batch id 2081 loss 0.04099404811859131 train acc 0.9835490749639596\n",
            "epoch 45 batch id 2091 loss 0.04959723725914955 train acc 0.9835380798660928\n",
            "epoch 45 batch id 2101 loss 0.007449397351592779 train acc 0.9835718110423608\n",
            "epoch 45 batch id 2111 loss 0.016306227073073387 train acc 0.9835904192325912\n",
            "epoch 45 batch id 2121 loss 0.012047321535646915 train acc 0.9836014851485149\n",
            "epoch 45 batch id 2131 loss 0.0011511322809383273 train acc 0.9836271116846551\n",
            "epoch 45 batch id 2141 loss 0.06005223095417023 train acc 0.9836160088743577\n",
            "epoch 45 batch id 2151 loss 0.010595028288662434 train acc 0.9836485936773594\n",
            "epoch 45 batch id 2161 loss 0.25575724244117737 train acc 0.9836230333179083\n",
            "epoch 45 batch id 2171 loss 0.00861404649913311 train acc 0.9836192998618148\n",
            "epoch 45 batch id 2181 loss 0.08118758350610733 train acc 0.9836156006419073\n",
            "epoch 45 batch id 2191 loss 0.014407319948077202 train acc 0.9836190666362392\n",
            "epoch 45 batch id 2201 loss 0.01219081413000822 train acc 0.9836083030440709\n",
            "epoch 45 batch id 2211 loss 0.07163679599761963 train acc 0.9835835029398462\n",
            "epoch 45 batch id 2221 loss 0.020193085074424744 train acc 0.9835589261593877\n",
            "epoch 45 batch id 2231 loss 0.007883761078119278 train acc 0.9835835948005379\n",
            "epoch 45 batch id 2241 loss 0.031868599355220795 train acc 0.9835731816153503\n",
            "epoch 45 batch id 2251 loss 0.0002867311704903841 train acc 0.9835698023100844\n",
            "epoch 45 batch id 2261 loss 0.007057638838887215 train acc 0.9835664528969482\n",
            "epoch 45 batch id 2271 loss 0.027865534648299217 train acc 0.9835768934390137\n",
            "epoch 45 batch id 2281 loss 0.02480081096291542 train acc 0.9835803923717668\n",
            "epoch 45 batch id 2291 loss 0.02839416265487671 train acc 0.9835838607594937\n",
            "epoch 45 batch id 2301 loss 0.019190657883882523 train acc 0.9836144611038679\n",
            "epoch 45 batch id 2311 loss 0.04801785573363304 train acc 0.9836312743401125\n",
            "epoch 45 batch id 2321 loss 0.0721331387758255 train acc 0.9836344786729858\n",
            "epoch 45 batch id 2331 loss 0.0548219159245491 train acc 0.9836041398541399\n",
            "epoch 45 batch id 2341 loss 0.063018299639225 train acc 0.9836074327210593\n",
            "epoch 45 batch id 2351 loss 0.047863565385341644 train acc 0.9835641748192259\n",
            "epoch 45 batch id 2361 loss 0.08243253827095032 train acc 0.9835808449809402\n",
            "epoch 45 batch id 2371 loss 0.038907941430807114 train acc 0.9836105546183045\n",
            "epoch 45 batch id 2381 loss 0.016517721116542816 train acc 0.983640014699706\n",
            "epoch 45 batch id 2391 loss 0.048078835010528564 train acc 0.9836104140526977\n",
            "epoch 45 batch id 2401 loss 0.022832456976175308 train acc 0.9836201062057476\n",
            "epoch 45 batch id 2411 loss 0.027056317776441574 train acc 0.9836102758191622\n",
            "epoch 45 batch id 2421 loss 0.05200957506895065 train acc 0.9836457042544403\n",
            "epoch 45 batch id 2431 loss 0.08013565838336945 train acc 0.983635849444673\n",
            "epoch 45 batch id 2441 loss 0.00027768706786446273 train acc 0.983651679639492\n",
            "epoch 45 batch id 2451 loss 0.0005593668902292848 train acc 0.9836418808649531\n",
            "epoch 45 batch id 2461 loss 0.028028076514601707 train acc 0.9836575579032913\n",
            "epoch 45 batch id 2471 loss 0.08826515823602676 train acc 0.9836351679481992\n",
            "epoch 45 batch id 2481 loss 0.06249161437153816 train acc 0.9836507456670698\n",
            "epoch 45 batch id 2491 loss 0.08449622243642807 train acc 0.9836536531513448\n",
            "epoch 45 batch id 2501 loss 0.002088357461616397 train acc 0.9836627848860455\n",
            "epoch 45 batch id 2511 loss 0.058461420238018036 train acc 0.983640730784548\n",
            "epoch 45 batch id 2521 loss 0.023429229855537415 train acc 0.9836002578341928\n",
            "epoch 45 batch id 2531 loss 0.022558851167559624 train acc 0.9835847984986171\n",
            "epoch 45 batch id 2541 loss 0.037851087749004364 train acc 0.9835633116883117\n",
            "epoch 45 batch id 2551 loss 0.04884044826030731 train acc 0.983548118384947\n",
            "epoch 45 batch id 2561 loss 0.014095236547291279 train acc 0.9835208414681765\n",
            "epoch 45 batch id 2571 loss 0.09965421259403229 train acc 0.9834937767405679\n",
            "epoch 45 batch id 2581 loss 0.04018069803714752 train acc 0.9834608678806664\n",
            "epoch 45 batch id 2591 loss 0.016008149832487106 train acc 0.983470426476264\n",
            "epoch 45 batch id 2601 loss 0.025196291506290436 train acc 0.9834919261822376\n",
            "epoch 45 batch id 2611 loss 0.0057889134623110294 train acc 0.9835132612026044\n",
            "epoch 45 batch id 2621 loss 0.052178602665662766 train acc 0.9835105875619993\n",
            "epoch 45 batch id 2631 loss 0.05907383933663368 train acc 0.9834901178259217\n",
            "epoch 45 batch id 2641 loss 0.02605990320444107 train acc 0.9834993847027641\n",
            "epoch 45 batch id 2651 loss 0.0018843651050701737 train acc 0.9835144756695586\n",
            "epoch 45 batch id 2661 loss 0.010512889362871647 train acc 0.9835177095077039\n",
            "epoch 45 batch id 2671 loss 0.009066303260624409 train acc 0.9835150692624485\n",
            "epoch 45 batch id 2681 loss 0.02504643425345421 train acc 0.9834891365162253\n",
            "epoch 45 batch id 2691 loss 0.016999755054712296 train acc 0.9834808156819026\n",
            "epoch 45 batch id 2701 loss 0.037751853466033936 train acc 0.9834841262495372\n",
            "epoch 45 batch id 2711 loss 0.11334371566772461 train acc 0.9834470675027666\n",
            "epoch 45 batch id 2721 loss 0.01110758539289236 train acc 0.9834275082690187\n",
            "epoch 45 batch id 2731 loss 0.0006889994256198406 train acc 0.9834023709264006\n",
            "epoch 45 batch id 2741 loss 0.08528926968574524 train acc 0.9834002188982124\n",
            "epoch 45 batch id 2751 loss 0.11262819916009903 train acc 0.9833867230098147\n",
            "epoch 45 batch id 2761 loss 0.018849732354283333 train acc 0.983395961608113\n",
            "epoch 45 batch id 2771 loss 0.000584112131036818 train acc 0.9833994947672321\n",
            "epoch 45 batch id 2781 loss 0.0279622171074152 train acc 0.9834198579647608\n",
            "epoch 45 batch id 2791 loss 0.014706666581332684 train acc 0.9834400752418488\n",
            "epoch 45 batch id 2801 loss 0.09711340814828873 train acc 0.9834378347018922\n",
            "epoch 45 batch id 2811 loss 0.03733206167817116 train acc 0.9834634027036642\n",
            "epoch 45 batch id 2821 loss 0.04766369238495827 train acc 0.9834887894363701\n",
            "epoch 45 batch id 2831 loss 0.012884879484772682 train acc 0.9834919198163193\n",
            "epoch 45 batch id 2841 loss 0.046356335282325745 train acc 0.9834840285110876\n",
            "epoch 45 batch id 2851 loss 0.00028363827732391655 train acc 0.9834981146965976\n",
            "epoch 45 batch id 2861 loss 0.06329043209552765 train acc 0.9835011796574624\n",
            "epoch 45 batch id 2871 loss 0.07812460511922836 train acc 0.9834933385579937\n",
            "epoch 45 batch id 2881 loss 0.03844095394015312 train acc 0.9834747049635543\n",
            "epoch 45 batch id 2891 loss 0.0006047582137398422 train acc 0.9834832237979938\n",
            "epoch 45 batch id 2901 loss 0.017207210883498192 train acc 0.9834916839021027\n",
            "epoch 45 batch id 2911 loss 0.05536018684506416 train acc 0.9834732480247338\n",
            "epoch 45 batch id 2921 loss 0.0005171452648937702 train acc 0.983438890790825\n",
            "epoch 45 batch id 2931 loss 0.030558262020349503 train acc 0.9834100989423405\n",
            "epoch 45 batch id 2941 loss 0.0007083069067448378 train acc 0.9833868157089425\n",
            "epoch 45 batch id 2951 loss 0.026494497433304787 train acc 0.9834219332429684\n",
            "epoch 45 batch id 2961 loss 0.14500421285629272 train acc 0.9834198750422155\n",
            "epoch 45 batch id 2971 loss 0.02059631049633026 train acc 0.9834125715247392\n",
            "epoch 45 batch id 2981 loss 0.008685331791639328 train acc 0.9834053170077155\n",
            "epoch 45 batch id 2991 loss 0.012759259901940823 train acc 0.9834137830157138\n",
            "epoch 45 batch id 3001 loss 0.00253268308006227 train acc 0.9834326057980673\n",
            "epoch 45 batch id 3011 loss 0.025225332006812096 train acc 0.9834461142477582\n",
            "epoch 45 batch id 3021 loss 0.03176049143075943 train acc 0.9834388447533929\n",
            "epoch 45 batch id 3031 loss 0.1784430593252182 train acc 0.9834316232266579\n",
            "epoch 45 batch id 3041 loss 0.08863289654254913 train acc 0.9834038967444919\n",
            "epoch 45 batch id 3051 loss 0.0013715928653255105 train acc 0.9834224434611603\n",
            "epoch 45 batch id 3061 loss 0.09790832549333572 train acc 0.9834102417510617\n",
            "epoch 45 batch id 3071 loss 0.002769686048850417 train acc 0.983433734939759\n",
            "epoch 45 batch id 3081 loss 0.009857866913080215 train acc 0.9834520042194093\n",
            "epoch 45 batch id 3091 loss 0.01573582924902439 train acc 0.9834752102879327\n",
            "epoch 45 batch id 3101 loss 0.06418322026729584 train acc 0.9834478797162206\n",
            "epoch 45 batch id 3111 loss 0.05432356148958206 train acc 0.9834558823529411\n",
            "epoch 45 batch id 3121 loss 0.02717224322259426 train acc 0.9834738465235502\n",
            "epoch 45 batch id 3131 loss 0.004204567521810532 train acc 0.9834817151069946\n",
            "epoch 45 batch id 3141 loss 0.06575796008110046 train acc 0.9834596864056033\n",
            "epoch 45 batch id 3151 loss 0.024927260354161263 train acc 0.9834477150111076\n",
            "epoch 45 batch id 3161 loss 0.01215213630348444 train acc 0.9834358193609617\n",
            "epoch 45 batch id 3171 loss 0.014185843989253044 train acc 0.9834239987385682\n",
            "epoch 45 batch id 3181 loss 0.031185666099190712 train acc 0.9834171644137064\n",
            "epoch 45 batch id 3191 loss 0.01593220978975296 train acc 0.9834446490128487\n",
            "epoch 45 batch id 3201 loss 0.08174390345811844 train acc 0.9834329115901281\n",
            "epoch 45 batch id 3211 loss 0.10034918040037155 train acc 0.9834017829336655\n",
            "epoch 45 batch id 3221 loss 0.07001254707574844 train acc 0.9833514436510401\n",
            "epoch 45 batch id 3231 loss 0.050544463098049164 train acc 0.9833352677189725\n",
            "epoch 45 batch id 3241 loss 0.014331409707665443 train acc 0.9833288336933045\n",
            "epoch 45 batch id 3251 loss 0.03734077513217926 train acc 0.983317633035989\n",
            "epoch 45 batch id 3261 loss 0.10069650411605835 train acc 0.9832825436982521\n",
            "epoch 45 batch id 3271 loss 0.05302325263619423 train acc 0.9832572225619077\n",
            "epoch 45 batch id 3281 loss 0.008018756285309792 train acc 0.9832749161840902\n",
            "epoch 45 batch id 3291 loss 0.013736768625676632 train acc 0.9832592676997873\n",
            "epoch 45 batch id 3301 loss 0.03792648762464523 train acc 0.9832768479248712\n",
            "epoch 45 batch id 3311 loss 0.047147199511528015 train acc 0.9832754454847478\n",
            "epoch 45 batch id 3321 loss 0.01689218543469906 train acc 0.9832693465823548\n",
            "epoch 45 batch id 3331 loss 0.018161609768867493 train acc 0.9832492119483639\n",
            "epoch 45 batch id 3341 loss 0.020798806101083755 train acc 0.9832572583058964\n",
            "epoch 45 batch id 3351 loss 0.016210993751883507 train acc 0.9832232915547597\n",
            "epoch 45 batch id 3361 loss 0.02819485031068325 train acc 0.9832406649806605\n",
            "epoch 45 batch id 3371 loss 0.0711790919303894 train acc 0.983234759715218\n",
            "epoch 45 batch id 3381 loss 0.00013329611101653427 train acc 0.9832473750369713\n",
            "epoch 45 batch id 3391 loss 0.03217492997646332 train acc 0.98326452373931\n",
            "epoch 45 batch id 3401 loss 0.06654602289199829 train acc 0.983272383122611\n",
            "epoch 45 batch id 3411 loss 0.021837793290615082 train acc 0.983275615655233\n",
            "epoch 45 batch id 3421 loss 0.012361214496195316 train acc 0.9832925314235603\n",
            "epoch 45 batch id 3431 loss 0.16739973425865173 train acc 0.983304794520548\n",
            "epoch 45 batch id 3441 loss 0.0022332407534122467 train acc 0.9833079046788724\n",
            "epoch 45 batch id 3451 loss 0.010202893987298012 train acc 0.9833155244856563\n",
            "epoch 45 batch id 3461 loss 0.0032094207126647234 train acc 0.9833185856688819\n",
            "epoch 45 batch id 3471 loss 0.04337069392204285 train acc 0.9833126260443676\n",
            "epoch 45 batch id 3481 loss 0.13429898023605347 train acc 0.9832977233553577\n",
            "epoch 45 batch id 3491 loss 0.0007731831283308566 train acc 0.9833008092237181\n",
            "epoch 45 batch id 3501 loss 0.016952449455857277 train acc 0.9833217295058555\n",
            "epoch 45 batch id 3511 loss 0.10697398334741592 train acc 0.983315828823697\n",
            "epoch 45 batch id 3521 loss 0.07842253148555756 train acc 0.9833188369781312\n",
            "epoch 45 batch id 3531 loss 0.08561165630817413 train acc 0.9833306782781082\n",
            "epoch 45 batch id 3541 loss 0.04875362291932106 train acc 0.9833159771251059\n",
            "epoch 45 batch id 3551 loss 0.022226734086871147 train acc 0.9832837580963109\n",
            "epoch 45 batch id 3561 loss 0.006967853754758835 train acc 0.9832999859590003\n",
            "epoch 45 batch id 3571 loss 0.005027095787227154 train acc 0.9833204984598152\n",
            "epoch 45 batch id 3581 loss 0.04509151354432106 train acc 0.9833234431722983\n",
            "epoch 45 batch id 3591 loss 0.06786857545375824 train acc 0.9833176691729323\n",
            "epoch 45 batch id 3601 loss 0.03089558519423008 train acc 0.9833249444598723\n",
            "epoch 45 batch id 3611 loss 0.0015798956155776978 train acc 0.9833451606203267\n",
            "epoch 45 batch id 3621 loss 0.09290345013141632 train acc 0.9833350593758631\n",
            "epoch 45 batch id 3631 loss 0.024013565853238106 train acc 0.9833121041035527\n",
            "epoch 45 batch id 3641 loss 0.02633342519402504 train acc 0.9833321889590771\n",
            "epoch 45 batch id 3651 loss 0.02724817395210266 train acc 0.9833222062448644\n",
            "epoch 45 batch id 3661 loss 0.002728801453486085 train acc 0.9833250819448238\n",
            "epoch 45 batch id 3671 loss 0.05123761296272278 train acc 0.983336454644511\n",
            "epoch 45 batch id 3681 loss 0.029765015468001366 train acc 0.983368989405053\n",
            "epoch 45 batch id 3691 loss 0.004079224541783333 train acc 0.9833759482525061\n",
            "epoch 45 batch id 3701 loss 0.03534238040447235 train acc 0.9833490948392326\n",
            "epoch 45 batch id 3711 loss 0.015183266252279282 train acc 0.9833602802479117\n",
            "epoch 45 batch id 3721 loss 0.08428620547056198 train acc 0.9833462106960494\n",
            "epoch 45 batch id 3731 loss 0.043825820088386536 train acc 0.9833447802197802\n",
            "epoch 45 batch id 3741 loss 0.037655651569366455 train acc 0.9833391807003475\n",
            "epoch 45 batch id 3751 loss 0.00020433073223102838 train acc 0.9833627699280192\n",
            "epoch 45 batch id 3761 loss 0.13928338885307312 train acc 0.983332225471949\n",
            "epoch 45 batch id 3771 loss 0.04994186758995056 train acc 0.9833225603288253\n",
            "epoch 45 batch id 3781 loss 0.0947607234120369 train acc 0.9833212113197567\n",
            "epoch 45 batch id 3791 loss 0.0024200964253395796 train acc 0.983340477446584\n",
            "epoch 45 batch id 3801 loss 0.04016751050949097 train acc 0.9833267561168114\n",
            "epoch 45 batch id 3811 loss 0.010062150657176971 train acc 0.9833541065337181\n",
            "epoch 45 batch id 3821 loss 0.0854085311293602 train acc 0.98335677833028\n",
            "epoch 45 batch id 3831 loss 0.000547729548998177 train acc 0.9833675933176717\n",
            "epoch 45 batch id 3841 loss 0.0003154492878820747 train acc 0.9833539442853424\n",
            "epoch 45 batch id 3851 loss 0.0005591305089183152 train acc 0.9833647104648143\n",
            "epoch 45 batch id 3861 loss 0.08049479871988297 train acc 0.9833511396011396\n",
            "epoch 45 batch id 3871 loss 0.16986407339572906 train acc 0.9833457117024025\n",
            "epoch 45 batch id 3881 loss 0.03595605120062828 train acc 0.9833523898479773\n",
            "epoch 45 batch id 3891 loss 0.008467629551887512 train acc 0.9833550179902338\n",
            "epoch 45 batch id 3901 loss 0.007122778799384832 train acc 0.9833496218918226\n",
            "epoch 45 batch id 3911 loss 0.03877841681241989 train acc 0.983332267962158\n",
            "epoch 45 batch id 3921 loss 0.04162413254380226 train acc 0.9833309423616424\n",
            "epoch 45 batch id 3931 loss 0.032821174710989 train acc 0.983305774612058\n",
            "epoch 45 batch id 3941 loss 0.004145309329032898 train acc 0.9833243466125349\n",
            "epoch 45 batch id 3951 loss 0.014761395752429962 train acc 0.9833032776512275\n",
            "epoch 45 batch id 3961 loss 0.0012085015187039971 train acc 0.9833020386266095\n",
            "epoch 45 batch id 3971 loss 0.032282039523124695 train acc 0.9832968710652229\n",
            "epoch 45 batch id 3981 loss 0.0020099494140595198 train acc 0.9832956543582014\n",
            "epoch 45 batch id 3991 loss 0.12614016234874725 train acc 0.9832983588073164\n",
            "epoch 45 batch id 4001 loss 0.08383800834417343 train acc 0.9832815233691578\n",
            "epoch 45 batch id 4011 loss 0.008991108275949955 train acc 0.9832491897282474\n",
            "epoch 45 batch id 4021 loss 0.01761208288371563 train acc 0.9832519895548371\n",
            "epoch 45 batch id 4031 loss 0.004572554957121611 train acc 0.9832741565368395\n",
            "epoch 45 batch id 4041 loss 0.03319457173347473 train acc 0.9832652808710716\n",
            "epoch 45 batch id 4051 loss 0.03956189379096031 train acc 0.9832564490249321\n",
            "epoch 45 batch id 4061 loss 0.015774402767419815 train acc 0.9832745936961339\n",
            "epoch 45 batch id 4071 loss 0.05604415014386177 train acc 0.9832888111029231\n",
            "epoch 45 batch id 4081 loss 0.01532858144491911 train acc 0.9832914726782651\n",
            "epoch 45 batch id 4091 loss 0.032721735537052155 train acc 0.9832979406013199\n",
            "epoch 45 batch id 4101 loss 0.09545252472162247 train acc 0.9832891367959035\n",
            "epoch 45 batch id 4111 loss 0.014212939888238907 train acc 0.9832727742641693\n",
            "epoch 45 batch id 4121 loss 0.03679298236966133 train acc 0.9832792404756128\n",
            "epoch 45 batch id 4131 loss 0.06927009671926498 train acc 0.9832856753812637\n",
            "epoch 45 batch id 4141 loss 0.0002549427736084908 train acc 0.9832845327215648\n",
            "epoch 45 batch id 4151 loss 0.07992225885391235 train acc 0.9833022163334136\n",
            "epoch 45 batch id 4161 loss 0.03108809143304825 train acc 0.9832935291997116\n",
            "epoch 45 batch id 4171 loss 0.06369930505752563 train acc 0.9832923759290338\n",
            "epoch 45 batch id 4181 loss 0.009103067219257355 train acc 0.9832949653193016\n",
            "epoch 45 batch id 4191 loss 0.12405192852020264 train acc 0.9832863576712002\n",
            "epoch 45 batch id 4201 loss 0.02553621307015419 train acc 0.9832926684122828\n",
            "epoch 45 batch id 4211 loss 0.13968569040298462 train acc 0.9832878176205176\n",
            "epoch 45 batch id 4221 loss 0.14679591357707977 train acc 0.9832718846244966\n",
            "epoch 45 batch id 4231 loss 0.03801095858216286 train acc 0.9832560269439848\n",
            "epoch 45 batch id 4241 loss 0.08029407262802124 train acc 0.9832770867719878\n",
            "epoch 45 batch id 4251 loss 0.09488935768604279 train acc 0.9832833450952717\n",
            "epoch 45 batch id 4261 loss 0.0011693518608808517 train acc 0.9833042419619807\n",
            "epoch 45 batch id 4271 loss 0.0021686512045562267 train acc 0.9833067490049169\n",
            "epoch 45 batch id 4281 loss 0.11637097597122192 train acc 0.9833092443354357\n",
            "epoch 45 batch id 4291 loss 0.013873168267309666 train acc 0.9833008040083897\n",
            "epoch 45 batch id 4301 loss 0.03837372735142708 train acc 0.9832887700534759\n",
            "epoch 45 batch id 4311 loss 0.03516319766640663 train acc 0.9833021630712132\n",
            "epoch 45 batch id 4321 loss 0.030975863337516785 train acc 0.9832901816709095\n",
            "epoch 45 batch id 4331 loss 0.05290500819683075 train acc 0.9832818633110136\n",
            "epoch 45 batch id 4341 loss 0.04287933185696602 train acc 0.9832843814789219\n",
            "epoch 45 batch id 4351 loss 0.02821139059960842 train acc 0.9832725235578028\n",
            "epoch 45 batch id 4361 loss 0.015719782561063766 train acc 0.9832786344875029\n",
            "epoch 45 batch id 4371 loss 0.04038315266370773 train acc 0.9832882921528254\n",
            "epoch 45 batch id 4381 loss 0.11414158344268799 train acc 0.9832693734307236\n",
            "epoch 45 batch id 4391 loss 0.0008037036750465631 train acc 0.9832718913687087\n",
            "epoch 45 batch id 4401 loss 0.09277376532554626 train acc 0.9832672972051807\n",
            "epoch 45 batch id 4411 loss 0.0005669842939823866 train acc 0.9832591815914759\n",
            "epoch 45 batch id 4421 loss 0.04330660030245781 train acc 0.9832652397647591\n",
            "epoch 45 batch id 4431 loss 0.013988766819238663 train acc 0.9832501128413451\n",
            "epoch 45 batch id 4441 loss 0.011594327166676521 train acc 0.9832632008556631\n",
            "epoch 45 batch id 4451 loss 0.006360091734677553 train acc 0.9832656987193888\n",
            "epoch 45 batch id 4461 loss 0.04020601883530617 train acc 0.9832611802286483\n",
            "epoch 45 batch id 4471 loss 0.048343855887651443 train acc 0.983263671438157\n",
            "epoch 45 batch id 4481 loss 0.008501281030476093 train acc 0.9832522037491631\n",
            "epoch 45 batch id 4491 loss 0.03388533741235733 train acc 0.9832512246715653\n",
            "epoch 45 batch id 4501 loss 0.011096563190221786 train acc 0.9832363641413019\n",
            "epoch 45 batch id 4511 loss 0.06136731803417206 train acc 0.9832215694967856\n",
            "epoch 45 batch id 4521 loss 0.0364108607172966 train acc 0.9832241207697412\n",
            "epoch 45 batch id 4531 loss 0.0046419114805758 train acc 0.98321976384904\n",
            "epoch 45 batch id 4541 loss 0.06388286501169205 train acc 0.9832154261175953\n",
            "epoch 45 batch id 4551 loss 0.04393165186047554 train acc 0.9832179740716326\n",
            "epoch 45 batch id 4561 loss 0.09183819591999054 train acc 0.9831965303661477\n",
            "epoch 45 batch id 4571 loss 0.035909056663513184 train acc 0.9831751804856705\n",
            "epoch 45 batch id 4581 loss 0.02741112932562828 train acc 0.9831743887797424\n",
            "epoch 45 batch id 4591 loss 0.033373329788446426 train acc 0.9831736005227619\n",
            "epoch 45 batch id 4601 loss 0.01170365046709776 train acc 0.9831660236905021\n",
            "epoch 45 batch id 4611 loss 0.03414074704051018 train acc 0.9831788115376274\n",
            "epoch 45 batch id 4621 loss 0.01531448494642973 train acc 0.9831847814325904\n",
            "epoch 45 batch id 4631 loss 0.06048065051436424 train acc 0.9831806035413517\n",
            "epoch 45 batch id 4641 loss 0.10084052383899689 train acc 0.9831831771170007\n",
            "epoch 45 batch id 4651 loss 0.10784181952476501 train acc 0.9831890991184692\n",
            "epoch 45 batch id 4661 loss 0.027878284454345703 train acc 0.9831949957090753\n",
            "epoch 45 batch id 4671 loss 0.04390082508325577 train acc 0.983204212160137\n",
            "epoch 45 batch id 4681 loss 0.22374318540096283 train acc 0.983206713309122\n",
            "epoch 45 batch id 4691 loss 0.009488494135439396 train acc 0.9832092037945\n",
            "epoch 45 batch id 4701 loss 0.03037681058049202 train acc 0.9832116836843224\n",
            "epoch 45 batch id 4711 loss 0.04357581213116646 train acc 0.9832141530460624\n",
            "epoch 45 batch id 4721 loss 0.0006881102453917265 train acc 0.983213302266469\n",
            "epoch 45 batch id 4731 loss 0.020577380433678627 train acc 0.9832190604523356\n",
            "epoch 45 batch id 4741 loss 0.07440558820962906 train acc 0.9832182029107783\n",
            "epoch 45 batch id 4751 loss 0.1998542845249176 train acc 0.9832206377604715\n",
            "epoch 45 batch id 4761 loss 0.03851683437824249 train acc 0.9832329080025205\n",
            "epoch 45 batch id 4771 loss 0.15445730090141296 train acc 0.9832320268287571\n",
            "epoch 45 batch id 4781 loss 0.07355387508869171 train acc 0.9832213449069233\n",
            "epoch 45 batch id 4791 loss 0.13808760046958923 train acc 0.9832009236067627\n",
            "epoch 45 batch id 4801 loss 0.08318788558244705 train acc 0.9832001145594668\n",
            "epoch 45 batch id 4811 loss 0.07847639918327332 train acc 0.983202556641031\n",
            "epoch 45 batch id 4821 loss 0.11792083084583282 train acc 0.9832017475627464\n",
            "epoch 45 batch id 4831 loss 0.02271617390215397 train acc 0.9832171134340716\n",
            "epoch 45 batch id 4841 loss 0.009264436550438404 train acc 0.9832098223507539\n",
            "epoch 45 batch id 4851 loss 0.06917863339185715 train acc 0.9832122242836528\n",
            "epoch 45 batch id 4861 loss 0.10943794995546341 train acc 0.9831985445381609\n",
            "epoch 45 batch id 4871 loss 0.11799433827400208 train acc 0.9831849209607884\n",
            "epoch 45 batch id 4881 loss 0.1662261188030243 train acc 0.9831905603359967\n",
            "epoch 45 batch id 4891 loss 0.019964110106229782 train acc 0.9831961766509916\n",
            "epoch 45 batch id 4901 loss 0.10296710580587387 train acc 0.9831826412976944\n",
            "epoch 45 batch id 4911 loss 0.27038517594337463 train acc 0.9831787059661984\n",
            "epoch 45 batch id 4921 loss 0.11213777959346771 train acc 0.9831652611257874\n",
            "epoch 45 batch id 4931 loss 0.05250642076134682 train acc 0.9831360271750152\n",
            "epoch 45 batch id 4941 loss 0.07052608579397202 train acc 0.9831037492410444\n",
            "epoch 45 batch id 4951 loss 0.08028950542211533 train acc 0.9831094728337709\n",
            "epoch 45 train acc 0.9831078525317732\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3648bb92109c4d07b09631bca727fa09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 45 loss 0.570527970790863 test acc 0.8525476539589444\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c740342bb5cd4362b32303056998aad7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 46 batch id 1 loss 0.035027407109737396 train acc 0.984375\n",
            "epoch 46 batch id 11 loss 0.017901409417390823 train acc 0.9801136363636364\n",
            "epoch 46 batch id 21 loss 0.05534704029560089 train acc 0.9813988095238095\n",
            "epoch 46 batch id 31 loss 0.060697998851537704 train acc 0.9818548387096774\n",
            "epoch 46 batch id 41 loss 0.011926853097975254 train acc 0.9836128048780488\n",
            "epoch 46 batch id 51 loss 0.009344642981886864 train acc 0.9819240196078431\n",
            "epoch 46 batch id 61 loss 0.03577009215950966 train acc 0.9802766393442623\n",
            "epoch 46 batch id 71 loss 0.0711752325296402 train acc 0.9810739436619719\n",
            "epoch 46 batch id 81 loss 0.0030026352033019066 train acc 0.9816743827160493\n",
            "epoch 46 batch id 91 loss 0.13472850620746613 train acc 0.9817994505494505\n",
            "epoch 46 batch id 101 loss 0.04958820715546608 train acc 0.9808168316831684\n",
            "epoch 46 batch id 111 loss 0.0004513493331614882 train acc 0.981418918918919\n",
            "epoch 46 batch id 121 loss 0.12776333093643188 train acc 0.9820506198347108\n",
            "epoch 46 batch id 131 loss 0.022121980786323547 train acc 0.9824666030534351\n",
            "epoch 46 batch id 141 loss 0.007418977562338114 train acc 0.9830452127659575\n",
            "epoch 46 batch id 151 loss 0.12572050094604492 train acc 0.9830298013245033\n",
            "epoch 46 batch id 161 loss 0.0004515655746217817 train acc 0.9833074534161491\n",
            "epoch 46 batch id 171 loss 0.01131441630423069 train acc 0.9835526315789473\n",
            "epoch 46 batch id 181 loss 0.019148703664541245 train acc 0.9839433701657458\n",
            "epoch 46 batch id 191 loss 0.04950406402349472 train acc 0.9835569371727748\n",
            "epoch 46 batch id 201 loss 0.015123600140213966 train acc 0.9838308457711443\n",
            "epoch 46 batch id 211 loss 0.00013252749340608716 train acc 0.9841528436018957\n",
            "epoch 46 batch id 221 loss 0.07014493644237518 train acc 0.9841628959276018\n",
            "epoch 46 batch id 231 loss 0.032028332352638245 train acc 0.9842397186147186\n",
            "epoch 46 batch id 241 loss 0.06066400185227394 train acc 0.9840508298755186\n",
            "epoch 46 batch id 251 loss 0.08229084312915802 train acc 0.9842504980079682\n",
            "epoch 46 batch id 261 loss 0.06704546511173248 train acc 0.9842552681992337\n",
            "epoch 46 batch id 271 loss 0.0007004911894910038 train acc 0.9844903136531366\n",
            "epoch 46 batch id 281 loss 0.004902111366391182 train acc 0.9845974199288257\n",
            "epoch 46 batch id 291 loss 0.06901714205741882 train acc 0.9848045532646048\n",
            "epoch 46 batch id 301 loss 0.09504897892475128 train acc 0.9845307308970099\n",
            "epoch 46 batch id 311 loss 0.01814357377588749 train acc 0.9847769292604501\n",
            "epoch 46 batch id 321 loss 0.08815795183181763 train acc 0.9848617601246106\n",
            "epoch 46 batch id 331 loss 0.092422254383564 train acc 0.9848470543806647\n",
            "epoch 46 batch id 341 loss 0.1386096328496933 train acc 0.984741568914956\n",
            "epoch 46 batch id 351 loss 0.0564909353852272 train acc 0.9848646723646723\n",
            "epoch 46 batch id 361 loss 0.14479960501194 train acc 0.9848943905817175\n",
            "epoch 46 batch id 371 loss 0.012106053531169891 train acc 0.9850909703504043\n",
            "epoch 46 batch id 381 loss 0.01540571078658104 train acc 0.9851952099737533\n",
            "epoch 46 batch id 391 loss 0.049880582839250565 train acc 0.9851742327365729\n",
            "epoch 46 batch id 401 loss 0.00016085579409264028 train acc 0.9851543017456359\n",
            "epoch 46 batch id 411 loss 0.00022470329713542014 train acc 0.9852113746958637\n",
            "epoch 46 batch id 421 loss 0.01345154084265232 train acc 0.9853770783847982\n",
            "epoch 46 batch id 431 loss 0.05664971470832825 train acc 0.9853175754060325\n",
            "epoch 46 batch id 441 loss 0.11677587777376175 train acc 0.9853316326530612\n",
            "epoch 46 batch id 451 loss 0.024209951981902122 train acc 0.985414356984479\n",
            "epoch 46 batch id 461 loss 0.008869205601513386 train acc 0.9852223427331888\n",
            "epoch 46 batch id 471 loss 0.007551923859864473 train acc 0.9853702229299363\n",
            "epoch 46 batch id 481 loss 0.05396139249205589 train acc 0.9853495322245323\n",
            "epoch 46 batch id 491 loss 0.002087742555886507 train acc 0.9852978615071283\n",
            "epoch 46 batch id 501 loss 0.13124550879001617 train acc 0.985310628742515\n",
            "epoch 46 batch id 511 loss 0.08956283330917358 train acc 0.985078277886497\n",
            "epoch 46 batch id 521 loss 0.09520504623651505 train acc 0.9849748080614203\n",
            "epoch 46 batch id 531 loss 0.032842714339494705 train acc 0.9848458097928436\n",
            "epoch 46 batch id 541 loss 0.010654794983565807 train acc 0.9846926987060998\n",
            "epoch 46 batch id 551 loss 0.03696466237306595 train acc 0.9847436479128857\n",
            "epoch 46 batch id 561 loss 0.007360226474702358 train acc 0.984597816399287\n",
            "epoch 46 batch id 571 loss 0.033955104649066925 train acc 0.9846760070052539\n",
            "epoch 46 batch id 581 loss 0.1429825723171234 train acc 0.9845363597246127\n",
            "epoch 46 batch id 591 loss 0.024772409349679947 train acc 0.9845600676818951\n",
            "epoch 46 batch id 601 loss 0.028629818931221962 train acc 0.9845829866888519\n",
            "epoch 46 batch id 611 loss 0.1045738011598587 train acc 0.9844005728314239\n",
            "epoch 46 batch id 621 loss 0.19095784425735474 train acc 0.9843498389694042\n",
            "epoch 46 batch id 631 loss 0.1726563274860382 train acc 0.9842759508716323\n",
            "epoch 46 batch id 641 loss 0.014406799338757992 train acc 0.9842774960998439\n",
            "epoch 46 batch id 651 loss 0.12285589426755905 train acc 0.9841589861751152\n",
            "epoch 46 batch id 661 loss 0.0006833425723016262 train acc 0.9841858925869894\n",
            "epoch 46 batch id 671 loss 0.020511066541075706 train acc 0.9841887108792846\n",
            "epoch 46 batch id 681 loss 0.07165184617042542 train acc 0.9841914464023495\n",
            "epoch 46 batch id 691 loss 0.14044363796710968 train acc 0.9841488784370478\n",
            "epoch 46 batch id 701 loss 0.04195556789636612 train acc 0.9840629457917262\n",
            "epoch 46 batch id 711 loss 0.08260004967451096 train acc 0.9841552390998594\n",
            "epoch 46 batch id 721 loss 0.037348099052906036 train acc 0.9840716019417476\n",
            "epoch 46 batch id 731 loss 0.009695177897810936 train acc 0.9840971272229823\n",
            "epoch 46 batch id 741 loss 0.013048773631453514 train acc 0.9841008771929824\n",
            "epoch 46 batch id 751 loss 0.03886070102453232 train acc 0.9840004993342211\n",
            "epoch 46 batch id 761 loss 0.001127393334172666 train acc 0.9839232917214192\n",
            "epoch 46 batch id 771 loss 0.04855938255786896 train acc 0.9838886186770428\n",
            "epoch 46 batch id 781 loss 0.043869759887456894 train acc 0.9839348591549296\n",
            "epoch 46 batch id 791 loss 0.01697062887251377 train acc 0.9839996839443742\n",
            "epoch 46 batch id 801 loss 0.0014171189395710826 train acc 0.9839458489388264\n",
            "epoch 46 batch id 811 loss 0.0700075626373291 train acc 0.9838933415536375\n",
            "epoch 46 batch id 821 loss 0.012998208403587341 train acc 0.9839182399512789\n",
            "epoch 46 batch id 831 loss 0.10515157133340836 train acc 0.9839049338146811\n",
            "epoch 46 batch id 841 loss 0.001255896408110857 train acc 0.9838547859690844\n",
            "epoch 46 batch id 851 loss 0.03523419797420502 train acc 0.9837507344300822\n",
            "epoch 46 batch id 861 loss 0.026335999369621277 train acc 0.9837942799070848\n",
            "epoch 46 batch id 871 loss 0.006449547596275806 train acc 0.9837471297359357\n",
            "epoch 46 batch id 881 loss 0.018066057935357094 train acc 0.9837187854710556\n",
            "epoch 46 batch id 891 loss 0.0001640815899008885 train acc 0.9837436868686869\n",
            "epoch 46 batch id 901 loss 0.11476369202136993 train acc 0.9837160099889012\n",
            "epoch 46 batch id 911 loss 0.010435882955789566 train acc 0.9837232436882547\n",
            "epoch 46 batch id 921 loss 0.0005824834806844592 train acc 0.9836285287730727\n",
            "epoch 46 batch id 931 loss 0.003203747095540166 train acc 0.9836365467239527\n",
            "epoch 46 batch id 941 loss 0.02393200807273388 train acc 0.9837108129649309\n",
            "epoch 46 batch id 951 loss 0.045081160962581635 train acc 0.9836685068349106\n",
            "epoch 46 batch id 961 loss 0.01081199198961258 train acc 0.9837083766909469\n",
            "epoch 46 batch id 971 loss 0.010680465959012508 train acc 0.9836991503604532\n",
            "epoch 46 batch id 981 loss 0.013833913020789623 train acc 0.983769750254842\n",
            "epoch 46 batch id 991 loss 0.078030064702034 train acc 0.9838389253279516\n",
            "epoch 46 batch id 1001 loss 0.05775589868426323 train acc 0.9838754995004995\n",
            "epoch 46 batch id 1011 loss 0.000482198636746034 train acc 0.9838804401582592\n",
            "epoch 46 batch id 1021 loss 0.038961805403232574 train acc 0.983931194906954\n",
            "epoch 46 batch id 1031 loss 0.009767383337020874 train acc 0.9839809650824443\n",
            "epoch 46 batch id 1041 loss 0.014376814477145672 train acc 0.9840297790585975\n",
            "epoch 46 batch id 1051 loss 0.019774483516812325 train acc 0.9840033301617507\n",
            "epoch 46 batch id 1061 loss 0.014758560806512833 train acc 0.9839184731385485\n",
            "epoch 46 batch id 1071 loss 0.00032691314117982984 train acc 0.9839519140989729\n",
            "epoch 46 batch id 1081 loss 0.10605300962924957 train acc 0.9838401942645698\n",
            "epoch 46 batch id 1091 loss 0.07810968905687332 train acc 0.9838164527956004\n",
            "epoch 46 batch id 1101 loss 0.06734022498130798 train acc 0.9838073342415985\n",
            "epoch 46 batch id 1111 loss 0.058756958693265915 train acc 0.9837280603060305\n",
            "epoch 46 batch id 1121 loss 0.0016342344461008906 train acc 0.9838314005352364\n",
            "epoch 46 batch id 1131 loss 0.15155749022960663 train acc 0.983822391688771\n",
            "epoch 46 batch id 1141 loss 0.03724464774131775 train acc 0.9838409290096407\n",
            "epoch 46 batch id 1151 loss 0.02498999610543251 train acc 0.9837641181581234\n",
            "epoch 46 batch id 1161 loss 0.09335552155971527 train acc 0.9837290051679587\n",
            "epoch 46 batch id 1171 loss 0.010954294353723526 train acc 0.9836544619982921\n",
            "epoch 46 batch id 1181 loss 0.0005214741104282439 train acc 0.9836870237087214\n",
            "epoch 46 batch id 1191 loss 0.012242240831255913 train acc 0.983705919395466\n",
            "epoch 46 batch id 1201 loss 0.01710309274494648 train acc 0.9837765403830142\n",
            "epoch 46 batch id 1211 loss 0.00024774271878413856 train acc 0.9837685796862098\n",
            "epoch 46 batch id 1221 loss 0.07246977090835571 train acc 0.9837351556101556\n",
            "epoch 46 batch id 1231 loss 0.0733543112874031 train acc 0.983664195775792\n",
            "epoch 46 batch id 1241 loss 0.008419683203101158 train acc 0.9836825141015311\n",
            "epoch 46 batch id 1251 loss 0.10748439282178879 train acc 0.9836755595523581\n",
            "epoch 46 batch id 1261 loss 0.020115982741117477 train acc 0.983705888183981\n",
            "epoch 46 batch id 1271 loss 0.020573608577251434 train acc 0.9836619787568843\n",
            "epoch 46 batch id 1281 loss 0.0006028140778653324 train acc 0.983643149882904\n",
            "epoch 46 batch id 1291 loss 0.06377822905778885 train acc 0.9836730247869868\n",
            "epoch 46 batch id 1301 loss 0.060525987297296524 train acc 0.9836784204458109\n",
            "epoch 46 batch id 1311 loss 0.054171279072761536 train acc 0.9836479786422578\n",
            "epoch 46 batch id 1321 loss 0.014625267125666142 train acc 0.9836061695685087\n",
            "epoch 46 batch id 1331 loss 0.015466759912669659 train acc 0.983564988730278\n",
            "epoch 46 batch id 1341 loss 0.10140959918498993 train acc 0.983571029082774\n",
            "epoch 46 batch id 1351 loss 0.0007971206796355546 train acc 0.9835885455218357\n",
            "epoch 46 batch id 1361 loss 0.006491147913038731 train acc 0.9835369213813373\n",
            "epoch 46 batch id 1371 loss 0.13212329149246216 train acc 0.9835430342815463\n",
            "epoch 46 batch id 1381 loss 0.04464512690901756 train acc 0.9835377443881246\n",
            "epoch 46 batch id 1391 loss 0.12895599007606506 train acc 0.9834875988497483\n",
            "epoch 46 batch id 1401 loss 0.011949415318667889 train acc 0.9835385438972163\n",
            "epoch 46 batch id 1411 loss 0.019113745540380478 train acc 0.983599840538625\n",
            "epoch 46 batch id 1421 loss 0.011801035143435001 train acc 0.9836162913441239\n",
            "epoch 46 batch id 1431 loss 0.00563022680580616 train acc 0.9836325122292103\n",
            "epoch 46 batch id 1441 loss 0.022298259660601616 train acc 0.983648507980569\n",
            "epoch 46 batch id 1451 loss 0.013598841615021229 train acc 0.983664283252929\n",
            "epoch 46 batch id 1461 loss 0.01341935433447361 train acc 0.9836798425735798\n",
            "epoch 46 batch id 1471 loss 0.02410184219479561 train acc 0.983737678450034\n",
            "epoch 46 batch id 1481 loss 0.06486450135707855 train acc 0.9837314314652262\n",
            "epoch 46 batch id 1491 loss 0.05553539842367172 train acc 0.9837252682763246\n",
            "epoch 46 batch id 1501 loss 0.06054067611694336 train acc 0.9837608261159227\n",
            "epoch 46 batch id 1511 loss 0.015027963556349277 train acc 0.9837752316346791\n",
            "epoch 46 batch id 1521 loss 0.0022810576483607292 train acc 0.9837894477317555\n",
            "epoch 46 batch id 1531 loss 0.041401077061891556 train acc 0.9837830666231221\n",
            "epoch 46 batch id 1541 loss 0.009042511694133282 train acc 0.9838071868916288\n",
            "epoch 46 batch id 1551 loss 0.0375707782804966 train acc 0.9838209219858156\n",
            "epoch 46 batch id 1561 loss 0.1123998761177063 train acc 0.9838044522741832\n",
            "epoch 46 batch id 1571 loss 0.016237743198871613 train acc 0.9838180299172502\n",
            "epoch 46 batch id 1581 loss 0.030559416860342026 train acc 0.9837919038583175\n",
            "epoch 46 batch id 1591 loss 0.07387274503707886 train acc 0.9838053896920176\n",
            "epoch 46 batch id 1601 loss 0.05520663782954216 train acc 0.9838479856339788\n",
            "epoch 46 batch id 1611 loss 0.04848986119031906 train acc 0.9838027622594662\n",
            "epoch 46 batch id 1621 loss 0.04340623691678047 train acc 0.9837677359654534\n",
            "epoch 46 batch id 1631 loss 0.00010763188765849918 train acc 0.9837810392397303\n",
            "epoch 46 batch id 1641 loss 0.07860206067562103 train acc 0.9837656154783668\n",
            "epoch 46 batch id 1651 loss 0.05462553724646568 train acc 0.983712522713507\n",
            "epoch 46 batch id 1661 loss 0.018792368471622467 train acc 0.9837353251053582\n",
            "epoch 46 batch id 1671 loss 0.02218891680240631 train acc 0.9837204518252544\n",
            "epoch 46 batch id 1681 loss 0.012423882260918617 train acc 0.9836778703152885\n",
            "epoch 46 batch id 1691 loss 0.017484106123447418 train acc 0.9837004730928445\n",
            "epoch 46 batch id 1701 loss 0.030040904879570007 train acc 0.983695252792475\n",
            "epoch 46 batch id 1711 loss 0.0067059872671961784 train acc 0.9837540181180596\n",
            "epoch 46 batch id 1721 loss 0.007327092811465263 train acc 0.9837485473561882\n",
            "epoch 46 batch id 1731 loss 8.310877456096932e-05 train acc 0.9837431398035817\n",
            "epoch 46 batch id 1741 loss 0.005276011303067207 train acc 0.9837377943710511\n",
            "epoch 46 batch id 1751 loss 0.04134293273091316 train acc 0.983732509994289\n",
            "epoch 46 batch id 1761 loss 0.017903653904795647 train acc 0.9837539040318001\n",
            "epoch 46 batch id 1771 loss 0.008612227626144886 train acc 0.9837574110671937\n",
            "epoch 46 batch id 1781 loss 0.0001463272055843845 train acc 0.983795971364402\n",
            "epoch 46 batch id 1791 loss 0.08316706866025925 train acc 0.9837817560022334\n",
            "epoch 46 batch id 1801 loss 0.011474204249680042 train acc 0.9838024014436424\n",
            "epoch 46 batch id 1811 loss 0.026373926550149918 train acc 0.9838141910546659\n",
            "epoch 46 batch id 1821 loss 0.07678599655628204 train acc 0.9837829489291599\n",
            "epoch 46 batch id 1831 loss 0.020627781748771667 train acc 0.983769115237575\n",
            "epoch 46 batch id 1841 loss 0.010435293428599834 train acc 0.9837384573601303\n",
            "epoch 46 batch id 1851 loss 0.0030555182602256536 train acc 0.9837418962722853\n",
            "epoch 46 batch id 1861 loss 0.015054982155561447 train acc 0.9837452982267598\n",
            "epoch 46 batch id 1871 loss 0.06730492413043976 train acc 0.9837737172634955\n",
            "epoch 46 batch id 1881 loss 0.0179333183914423 train acc 0.9837686071238703\n",
            "epoch 46 batch id 1891 loss 0.027335627004504204 train acc 0.9837800766790058\n",
            "epoch 46 batch id 1901 loss 0.0849795788526535 train acc 0.9837832062072593\n",
            "epoch 46 batch id 1911 loss 0.026821499690413475 train acc 0.983712715855573\n",
            "epoch 46 batch id 1921 loss 0.014542418532073498 train acc 0.9836754945340969\n",
            "epoch 46 batch id 1931 loss 0.014593541622161865 train acc 0.9836791170378042\n",
            "epoch 46 batch id 1941 loss 0.0047519030049443245 train acc 0.9836746522411128\n",
            "epoch 46 batch id 1951 loss 0.037493638694286346 train acc 0.983654215786776\n",
            "epoch 46 batch id 1961 loss 0.03823889046907425 train acc 0.9836897628760837\n",
            "epoch 46 batch id 1971 loss 0.00898612942546606 train acc 0.9837170218163369\n",
            "epoch 46 batch id 1981 loss 0.07096417993307114 train acc 0.9836887935386168\n",
            "epoch 46 batch id 1991 loss 0.11633598804473877 train acc 0.9836216097438473\n",
            "epoch 46 batch id 2001 loss 0.19669947028160095 train acc 0.9836331834082959\n",
            "epoch 46 batch id 2011 loss 0.20579317212104797 train acc 0.9836213326703133\n",
            "epoch 46 batch id 2021 loss 0.05677938833832741 train acc 0.9836095992083127\n",
            "epoch 46 batch id 2031 loss 0.007422040682286024 train acc 0.9836210610536681\n",
            "epoch 46 batch id 2041 loss 0.0047706374898552895 train acc 0.9836170994610485\n",
            "epoch 46 batch id 2051 loss 0.27925992012023926 train acc 0.9836055582642613\n",
            "epoch 46 batch id 2061 loss 0.08397635817527771 train acc 0.9836017103347889\n",
            "epoch 46 batch id 2071 loss 0.0852845162153244 train acc 0.9836054442298406\n",
            "epoch 46 batch id 2081 loss 0.07276897132396698 train acc 0.9836091422393081\n",
            "epoch 46 batch id 2091 loss 0.037448715418577194 train acc 0.9836128048780488\n",
            "epoch 46 batch id 2101 loss 0.0038734732661396265 train acc 0.9836610542598763\n",
            "epoch 46 batch id 2111 loss 0.10311315953731537 train acc 0.9836644362861203\n",
            "epoch 46 batch id 2121 loss 0.010087071917951107 train acc 0.9836972536539368\n",
            "epoch 46 batch id 2131 loss 0.1347988247871399 train acc 0.9837370952604411\n",
            "epoch 46 batch id 2141 loss 0.13020755350589752 train acc 0.983710882765063\n",
            "epoch 46 batch id 2151 loss 0.03537891060113907 train acc 0.9837357624360762\n",
            "epoch 46 batch id 2161 loss 0.200311079621315 train acc 0.9837387204997686\n",
            "epoch 46 batch id 2171 loss 0.0015335583593696356 train acc 0.9837560456011055\n",
            "epoch 46 batch id 2181 loss 0.10398773849010468 train acc 0.983708734525447\n",
            "epoch 46 batch id 2191 loss 0.02138814888894558 train acc 0.9837260383386581\n",
            "epoch 46 batch id 2201 loss 0.010918881744146347 train acc 0.9837360858700591\n",
            "epoch 46 batch id 2211 loss 0.0003799812402576208 train acc 0.9837460425146992\n",
            "epoch 46 batch id 2221 loss 0.023274505510926247 train acc 0.9837488743809095\n",
            "epoch 46 batch id 2231 loss 0.027214903384447098 train acc 0.9837516808606006\n",
            "epoch 46 batch id 2241 loss 0.046645086258649826 train acc 0.9837614346273985\n",
            "epoch 46 batch id 2251 loss 0.005764513276517391 train acc 0.9837502776543758\n",
            "epoch 46 batch id 2261 loss 0.00012088233779650182 train acc 0.983766862007961\n",
            "epoch 46 batch id 2271 loss 0.0006071194657124579 train acc 0.9837970607661823\n",
            "epoch 46 batch id 2281 loss 0.07669315487146378 train acc 0.9837858943445857\n",
            "epoch 46 batch id 2291 loss 0.06443493068218231 train acc 0.9837884657354867\n",
            "epoch 46 batch id 2301 loss 0.01687433570623398 train acc 0.983784224250326\n",
            "epoch 46 batch id 2311 loss 0.03460242971777916 train acc 0.9838070640415405\n",
            "epoch 46 batch id 2321 loss 0.10569160431623459 train acc 0.9837960469625161\n",
            "epoch 46 batch id 2331 loss 0.09615213423967361 train acc 0.983765015015015\n",
            "epoch 46 batch id 2341 loss 0.0004074489406775683 train acc 0.983774295173003\n",
            "epoch 46 batch id 2351 loss 0.015050429850816727 train acc 0.9837303275202042\n",
            "epoch 46 batch id 2361 loss 0.06392503529787064 train acc 0.9837264400677679\n",
            "epoch 46 batch id 2371 loss 0.003685097675770521 train acc 0.9837555356389709\n",
            "epoch 46 batch id 2381 loss 0.000315345183480531 train acc 0.9837581373372533\n",
            "epoch 46 batch id 2391 loss 0.01702539063990116 train acc 0.9837476474278545\n",
            "epoch 46 batch id 2401 loss 0.02444804646074772 train acc 0.9837242294877134\n",
            "epoch 46 batch id 2411 loss 0.02196975238621235 train acc 0.98372044794691\n",
            "epoch 46 batch id 2421 loss 0.009778834879398346 train acc 0.9837425134242048\n",
            "epoch 46 batch id 2431 loss 0.04266134276986122 train acc 0.983751542575072\n",
            "epoch 46 batch id 2441 loss 0.00240013818256557 train acc 0.9837540966816878\n",
            "epoch 46 batch id 2451 loss 0.0004158032825216651 train acc 0.9837247552019583\n",
            "epoch 46 batch id 2461 loss 0.08607219904661179 train acc 0.9837337464445347\n",
            "epoch 46 batch id 2471 loss 0.084092877805233 train acc 0.9836984014569\n",
            "epoch 46 batch id 2481 loss 0.19957704842090607 train acc 0.9837326178960096\n",
            "epoch 46 batch id 2491 loss 0.07799236476421356 train acc 0.9837101063829787\n",
            "epoch 46 batch id 2501 loss 0.07834897935390472 train acc 0.9837127648940424\n",
            "epoch 46 batch id 2511 loss 0.018342629075050354 train acc 0.9837029569892473\n",
            "epoch 46 batch id 2521 loss 0.013754621148109436 train acc 0.9836436433954779\n",
            "epoch 46 batch id 2531 loss 0.01292052399367094 train acc 0.9836588798893718\n",
            "epoch 46 batch id 2541 loss 0.028744468465447426 train acc 0.9836493998425817\n",
            "epoch 46 batch id 2551 loss 0.01757154054939747 train acc 0.9836644943159545\n",
            "epoch 46 batch id 2561 loss 0.013719236478209496 train acc 0.9836794709098009\n",
            "epoch 46 batch id 2571 loss 0.1075940877199173 train acc 0.9836578665888759\n",
            "epoch 46 batch id 2581 loss 0.05528084933757782 train acc 0.9836364296784192\n",
            "epoch 46 batch id 2591 loss 0.025478074327111244 train acc 0.9836272192203782\n",
            "epoch 46 batch id 2601 loss 0.07487882673740387 train acc 0.983642108804306\n",
            "epoch 46 batch id 2611 loss 0.0006768275634385645 train acc 0.9836628686327078\n",
            "epoch 46 batch id 2621 loss 0.053665827959775925 train acc 0.9836178939336131\n",
            "epoch 46 batch id 2631 loss 0.09759849309921265 train acc 0.9835970163435956\n",
            "epoch 46 batch id 2641 loss 0.054892901331186295 train acc 0.9835703805376751\n",
            "epoch 46 batch id 2651 loss 0.00021545907657127827 train acc 0.983579309694455\n",
            "epoch 46 batch id 2661 loss 0.0027601991314440966 train acc 0.9835764280345735\n",
            "epoch 46 batch id 2671 loss 0.008805128745734692 train acc 0.983556018345189\n",
            "epoch 46 batch id 2681 loss 0.015812190249562263 train acc 0.9835474170085788\n",
            "epoch 46 batch id 2691 loss 0.05667577311396599 train acc 0.9835388795986622\n",
            "epoch 46 batch id 2701 loss 0.02962157130241394 train acc 0.9835246205109219\n",
            "epoch 46 batch id 2711 loss 0.010698235593736172 train acc 0.9835219937292512\n",
            "epoch 46 batch id 2721 loss 0.01025058887898922 train acc 0.9835021591326718\n",
            "epoch 46 batch id 2731 loss 0.000445408106315881 train acc 0.9834939124862687\n",
            "epoch 46 batch id 2741 loss 0.043902549892663956 train acc 0.983508527909522\n",
            "epoch 46 batch id 2751 loss 0.07268450409173965 train acc 0.9834832788077063\n",
            "epoch 46 batch id 2761 loss 0.012841854244470596 train acc 0.9835148044186889\n",
            "epoch 46 batch id 2771 loss 0.0004384803760331124 train acc 0.9835179086972212\n",
            "epoch 46 batch id 2781 loss 0.12559358775615692 train acc 0.9835322276159655\n",
            "epoch 46 batch id 2791 loss 0.015730807557702065 train acc 0.9835352472232175\n",
            "epoch 46 batch id 2801 loss 0.0870269164443016 train acc 0.9835270885398072\n",
            "epoch 46 batch id 2811 loss 0.027267662808299065 train acc 0.9835467805051583\n",
            "epoch 46 batch id 2821 loss 0.014858697541058064 train acc 0.983544177596597\n",
            "epoch 46 batch id 2831 loss 0.024308739230036736 train acc 0.9835471123277993\n",
            "epoch 46 batch id 2841 loss 0.059417638927698135 train acc 0.9835445265751496\n",
            "epoch 46 batch id 2851 loss 0.004379007034003735 train acc 0.9835474394949141\n",
            "epoch 46 batch id 2861 loss 0.07612435519695282 train acc 0.9835448706745893\n",
            "epoch 46 batch id 2871 loss 0.06895080953836441 train acc 0.9835423197492164\n",
            "epoch 46 batch id 2881 loss 0.05294293910264969 train acc 0.983523516140229\n",
            "epoch 46 batch id 2891 loss 0.0003652338054962456 train acc 0.9835264614320305\n",
            "epoch 46 batch id 2901 loss 0.010659064166247845 train acc 0.9835240003447088\n",
            "epoch 46 batch id 2911 loss 0.021882396191358566 train acc 0.9835376588801099\n",
            "epoch 46 batch id 2921 loss 0.00024495989782735705 train acc 0.983508430332078\n",
            "epoch 46 batch id 2931 loss 0.005402646493166685 train acc 0.9834953940634595\n",
            "epoch 46 batch id 2941 loss 0.0001734478719299659 train acc 0.983493072084325\n",
            "epoch 46 batch id 2951 loss 0.03654012456536293 train acc 0.9835172399186717\n",
            "epoch 46 batch id 2961 loss 0.024747435003519058 train acc 0.9835412445119892\n",
            "epoch 46 batch id 2971 loss 0.039589330554008484 train acc 0.9835335324806462\n",
            "epoch 46 batch id 2981 loss 0.015514744445681572 train acc 0.9835258721905401\n",
            "epoch 46 batch id 2991 loss 0.01720454730093479 train acc 0.983539159144099\n",
            "epoch 46 batch id 3001 loss 0.000530178309418261 train acc 0.9835627707430856\n",
            "epoch 46 batch id 3011 loss 0.008341060020029545 train acc 0.983560278977084\n",
            "epoch 46 batch id 3021 loss 0.03139624372124672 train acc 0.9835422873220788\n",
            "epoch 46 batch id 3031 loss 0.11161311715841293 train acc 0.9835398795776972\n",
            "epoch 46 batch id 3041 loss 0.09163744002580643 train acc 0.9835220733311411\n",
            "epoch 46 batch id 3051 loss 0.002122629899531603 train acc 0.9835453539823009\n",
            "epoch 46 batch id 3061 loss 0.07398386299610138 train acc 0.9835327507350539\n",
            "epoch 46 batch id 3071 loss 0.002477758564054966 train acc 0.9835405812438945\n",
            "epoch 46 batch id 3081 loss 0.025852268561720848 train acc 0.9835432895163908\n",
            "epoch 46 batch id 3091 loss 0.017354000359773636 train acc 0.9835560902620512\n",
            "epoch 46 batch id 3101 loss 0.047754473984241486 train acc 0.9835284988713319\n",
            "epoch 46 batch id 3111 loss 0.061757419258356094 train acc 0.9835261973641916\n",
            "epoch 46 batch id 3121 loss 0.02035754732787609 train acc 0.9835389298301827\n",
            "epoch 46 batch id 3131 loss 0.020193422213196754 train acc 0.9835615618013415\n",
            "epoch 46 batch id 3141 loss 0.07511977106332779 train acc 0.9835641515440943\n",
            "epoch 46 batch id 3151 loss 0.0005851259920746088 train acc 0.9835716835925103\n",
            "epoch 46 batch id 3161 loss 0.07784546911716461 train acc 0.9835643388168301\n",
            "epoch 46 batch id 3171 loss 0.026579298079013824 train acc 0.9835668953011668\n",
            "epoch 46 batch id 3181 loss 0.02158346399664879 train acc 0.983559611757309\n",
            "epoch 46 batch id 3191 loss 0.014309032820165157 train acc 0.9835915465371357\n",
            "epoch 46 batch id 3201 loss 0.07401888072490692 train acc 0.9835793502030615\n",
            "epoch 46 batch id 3211 loss 0.11839626729488373 train acc 0.9835623637496107\n",
            "epoch 46 batch id 3221 loss 0.13631968200206757 train acc 0.9835212278795405\n",
            "epoch 46 batch id 3231 loss 0.10059848427772522 train acc 0.9834996904982978\n",
            "epoch 46 batch id 3241 loss 0.016199635341763496 train acc 0.9834831070657205\n",
            "epoch 46 batch id 3251 loss 0.03115095943212509 train acc 0.9834666256536451\n",
            "epoch 46 batch id 3261 loss 0.10776562988758087 train acc 0.9834214964734744\n",
            "epoch 46 batch id 3271 loss 0.031099535524845123 train acc 0.9834100810149802\n",
            "epoch 46 batch id 3281 loss 0.09870008379220963 train acc 0.9834082596769278\n",
            "epoch 46 batch id 3291 loss 0.010314857587218285 train acc 0.9834017016104527\n",
            "epoch 46 batch id 3301 loss 0.012207760475575924 train acc 0.9834046501060285\n",
            "epoch 46 batch id 3311 loss 0.009017105214297771 train acc 0.9834122999093929\n",
            "epoch 46 batch id 3321 loss 0.011091542430222034 train acc 0.9834387232761217\n",
            "epoch 46 batch id 3331 loss 0.055863283574581146 train acc 0.9834040078054638\n",
            "epoch 46 batch id 3341 loss 0.041306041181087494 train acc 0.9834022373540856\n",
            "epoch 46 batch id 3351 loss 0.014202159829437733 train acc 0.983372500746046\n",
            "epoch 46 batch id 3361 loss 0.016018832102417946 train acc 0.9833940791431122\n",
            "epoch 46 batch id 3371 loss 0.06165899336338043 train acc 0.9834062592702462\n",
            "epoch 46 batch id 3381 loss 0.004938011057674885 train acc 0.9834276101745045\n",
            "epoch 46 batch id 3391 loss 0.04659859463572502 train acc 0.9834488351518726\n",
            "epoch 46 batch id 3401 loss 0.07323827594518661 train acc 0.9834423698912085\n",
            "epoch 46 batch id 3411 loss 0.031184615567326546 train acc 0.9834451040750513\n",
            "epoch 46 batch id 3421 loss 0.027664247900247574 train acc 0.9834615244080678\n",
            "epoch 46 batch id 3431 loss 0.16736993193626404 train acc 0.9834505246283882\n",
            "epoch 46 batch id 3441 loss 0.005337534938007593 train acc 0.9834622929380994\n",
            "epoch 46 batch id 3451 loss 0.012808579951524734 train acc 0.9834649376992176\n",
            "epoch 46 batch id 3461 loss 0.003868017578497529 train acc 0.9834359650390061\n",
            "epoch 46 batch id 3471 loss 0.0507885217666626 train acc 0.9834386704119851\n",
            "epoch 46 batch id 3481 loss 0.06422173231840134 train acc 0.983423405630566\n",
            "epoch 46 batch id 3491 loss 0.03279396519064903 train acc 0.983426131480951\n",
            "epoch 46 batch id 3501 loss 0.01597539149224758 train acc 0.9834377677806341\n",
            "epoch 46 batch id 3511 loss 0.03467772901058197 train acc 0.9834181857020792\n",
            "epoch 46 batch id 3521 loss 0.052680227905511856 train acc 0.9834342161317807\n",
            "epoch 46 batch id 3531 loss 0.011064554564654827 train acc 0.9834545808552818\n",
            "epoch 46 batch id 3541 loss 0.04950380697846413 train acc 0.9834439423891556\n",
            "epoch 46 batch id 3551 loss 0.03784291446208954 train acc 0.9834157631653055\n",
            "epoch 46 batch id 3561 loss 0.013555672019720078 train acc 0.9834140690817186\n",
            "epoch 46 batch id 3571 loss 0.003692098194733262 train acc 0.9834167600112014\n",
            "epoch 46 batch id 3581 loss 0.05189274251461029 train acc 0.9834237992180955\n",
            "epoch 46 batch id 3591 loss 0.10801011323928833 train acc 0.9834177457532721\n",
            "epoch 46 batch id 3601 loss 0.20367154479026794 train acc 0.9834117259094696\n",
            "epoch 46 batch id 3611 loss 0.0015164142241701484 train acc 0.983427374688452\n",
            "epoch 46 batch id 3621 loss 0.04549159109592438 train acc 0.9834170463960232\n",
            "epoch 46 batch id 3631 loss 0.012890727259218693 train acc 0.9833938653263564\n",
            "epoch 46 batch id 3641 loss 0.02229033038020134 train acc 0.9834137256248283\n",
            "epoch 46 batch id 3651 loss 0.034334857016801834 train acc 0.9833864009860313\n",
            "epoch 46 batch id 3661 loss 0.00710594467818737 train acc 0.9833634935809888\n",
            "epoch 46 batch id 3671 loss 0.04973341152071953 train acc 0.98336624897848\n",
            "epoch 46 batch id 3681 loss 0.0006721719400957227 train acc 0.9833987027981527\n",
            "epoch 46 batch id 3691 loss 0.006083813030272722 train acc 0.9834098144134381\n",
            "epoch 46 batch id 3701 loss 0.02913653291761875 train acc 0.9833913131586057\n",
            "epoch 46 batch id 3711 loss 0.021636880934238434 train acc 0.9834023848019402\n",
            "epoch 46 batch id 3721 loss 0.06727663427591324 train acc 0.9834133969363075\n",
            "epoch 46 batch id 3731 loss 0.025345494970679283 train acc 0.9834285379254891\n",
            "epoch 46 batch id 3741 loss 0.08584274351596832 train acc 0.9834185378241111\n",
            "epoch 46 batch id 3751 loss 0.0003597118775360286 train acc 0.9834460810450546\n",
            "epoch 46 batch id 3761 loss 0.23205548524856567 train acc 0.9834153150757777\n",
            "epoch 46 batch id 3771 loss 0.02080143429338932 train acc 0.9834178599840891\n",
            "epoch 46 batch id 3781 loss 0.07644060999155045 train acc 0.9834245239354668\n",
            "epoch 46 batch id 3791 loss 0.00010296174150425941 train acc 0.9834476391453443\n",
            "epoch 46 batch id 3801 loss 0.0527472160756588 train acc 0.9834377466456196\n",
            "epoch 46 batch id 3811 loss 0.05297118052840233 train acc 0.9834566058777224\n",
            "epoch 46 batch id 3821 loss 0.024245526641607285 train acc 0.9834671879089244\n",
            "epoch 46 batch id 3831 loss 0.0024546347558498383 train acc 0.9834654789872096\n",
            "epoch 46 batch id 3841 loss 0.00010561816452536732 train acc 0.9834637789638115\n",
            "epoch 46 batch id 3851 loss 0.0001058029301930219 train acc 0.9834702025447936\n",
            "epoch 46 batch id 3861 loss 0.1312604397535324 train acc 0.9834563584563585\n",
            "epoch 46 batch id 3871 loss 0.08408636599779129 train acc 0.9834587315939034\n",
            "epoch 46 batch id 3881 loss 0.04325282573699951 train acc 0.9834530404534914\n",
            "epoch 46 batch id 3891 loss 0.01082609687000513 train acc 0.9834674569519404\n",
            "epoch 46 batch id 3901 loss 0.011632764711976051 train acc 0.9834217187900538\n",
            "epoch 46 batch id 3911 loss 0.028261631727218628 train acc 0.9834081756583994\n",
            "epoch 46 batch id 3921 loss 0.013669364154338837 train acc 0.9834225962764601\n",
            "epoch 46 batch id 3931 loss 0.04929080978035927 train acc 0.9834170694479776\n",
            "epoch 46 batch id 3941 loss 0.0051717609167099 train acc 0.9834234648566353\n",
            "epoch 46 batch id 3951 loss 0.025400131940841675 train acc 0.9834100544166033\n",
            "epoch 46 batch id 3961 loss 0.00207383930683136 train acc 0.9834124905326938\n",
            "epoch 46 batch id 3971 loss 0.05053732171654701 train acc 0.98338737093931\n",
            "epoch 46 batch id 3981 loss 0.01249243225902319 train acc 0.9833937766892741\n",
            "epoch 46 batch id 3991 loss 0.10142727196216583 train acc 0.9833805750438487\n",
            "epoch 46 batch id 4001 loss 0.06892278790473938 train acc 0.983371344663834\n",
            "epoch 46 batch id 4011 loss 0.013286437839269638 train acc 0.9833387870855148\n",
            "epoch 46 batch id 4021 loss 0.02478720061480999 train acc 0.9833491357871176\n",
            "epoch 46 batch id 4031 loss 0.0005779389175586402 train acc 0.9833671855618953\n",
            "epoch 46 batch id 4041 loss 0.03808267414569855 train acc 0.9833464798317249\n",
            "epoch 46 batch id 4051 loss 0.0312807522714138 train acc 0.9833490187607998\n",
            "epoch 46 batch id 4061 loss 0.01713605970144272 train acc 0.983366935483871\n",
            "epoch 46 batch id 4071 loss 0.055531539022922516 train acc 0.9833809260623926\n",
            "epoch 46 batch id 4081 loss 0.017579231411218643 train acc 0.9834063342318059\n",
            "epoch 46 batch id 4091 loss 0.03405798226594925 train acc 0.9833896052309948\n",
            "epoch 46 batch id 4101 loss 0.040235500782728195 train acc 0.9833920080468178\n",
            "epoch 46 batch id 4111 loss 0.014243636280298233 train acc 0.9833677937241547\n",
            "epoch 46 batch id 4121 loss 0.03547905385494232 train acc 0.9833740293618054\n",
            "epoch 46 batch id 4131 loss 0.07613837718963623 train acc 0.9833651053013798\n",
            "epoch 46 batch id 4141 loss 0.00013836412108503282 train acc 0.9833713173146583\n",
            "epoch 46 batch id 4151 loss 0.09819472581148148 train acc 0.9833887918573838\n",
            "epoch 46 batch id 4161 loss 0.027589598670601845 train acc 0.9833836517664023\n",
            "epoch 46 batch id 4171 loss 0.06290574371814728 train acc 0.9833785363222249\n",
            "epoch 46 batch id 4181 loss 0.011798916384577751 train acc 0.9833809196364506\n",
            "epoch 46 batch id 4191 loss 0.047099627554416656 train acc 0.9833758351228824\n",
            "epoch 46 batch id 4201 loss 0.029649771749973297 train acc 0.9833930909307308\n",
            "epoch 46 batch id 4211 loss 0.10379500687122345 train acc 0.98338058062218\n",
            "epoch 46 batch id 4221 loss 0.09559277445077896 train acc 0.9833681295901445\n",
            "epoch 46 batch id 4231 loss 0.04315754026174545 train acc 0.9833557374143228\n",
            "epoch 46 batch id 4241 loss 0.1590680629014969 train acc 0.9833618250412639\n",
            "epoch 46 batch id 4251 loss 0.16333623230457306 train acc 0.983382586450247\n",
            "epoch 46 batch id 4261 loss 0.0027503701858222485 train acc 0.9834032504107018\n",
            "epoch 46 batch id 4271 loss 0.001807764987461269 train acc 0.9834165008194802\n",
            "epoch 46 batch id 4281 loss 0.03739049285650253 train acc 0.9834187397804252\n",
            "epoch 46 batch id 4291 loss 0.009453832171857357 train acc 0.9834209683057562\n",
            "epoch 46 batch id 4301 loss 0.04843084141612053 train acc 0.9833977563357359\n",
            "epoch 46 batch id 4311 loss 0.04789146035909653 train acc 0.9834072720946416\n",
            "epoch 46 batch id 4321 loss 0.027508167549967766 train acc 0.9833733510761398\n",
            "epoch 46 batch id 4331 loss 0.0609268844127655 train acc 0.9833720561071346\n",
            "epoch 46 batch id 4341 loss 0.04840828478336334 train acc 0.9833635683022345\n",
            "epoch 46 batch id 4351 loss 0.03745437413454056 train acc 0.9833515283842795\n",
            "epoch 46 batch id 4361 loss 0.01731766201555729 train acc 0.9833610410456317\n",
            "epoch 46 batch id 4371 loss 0.03291604667901993 train acc 0.9833705101807366\n",
            "epoch 46 batch id 4381 loss 0.1642189621925354 train acc 0.98336210340105\n",
            "epoch 46 batch id 4391 loss 0.00024349332670681179 train acc 0.9833537349123207\n",
            "epoch 46 batch id 4401 loss 0.08134531229734421 train acc 0.9833347534651216\n",
            "epoch 46 batch id 4411 loss 0.0002595272089820355 train acc 0.9833371117660394\n",
            "epoch 46 batch id 4421 loss 0.03981274366378784 train acc 0.9833288565935309\n",
            "epoch 46 batch id 4431 loss 0.10329323261976242 train acc 0.9833241649740465\n",
            "epoch 46 batch id 4441 loss 0.010409710928797722 train acc 0.9833265311866697\n",
            "epoch 46 batch id 4451 loss 0.02666555717587471 train acc 0.9833288867670187\n",
            "epoch 46 batch id 4461 loss 0.000610751158092171 train acc 0.9833277292086976\n",
            "epoch 46 batch id 4471 loss 0.05063836649060249 train acc 0.9833160925967345\n",
            "epoch 46 batch id 4481 loss 0.018799616023898125 train acc 0.9833079948672171\n",
            "epoch 46 batch id 4491 loss 0.03762415423989296 train acc 0.9832999331997327\n",
            "epoch 46 batch id 4501 loss 0.008492625318467617 train acc 0.9832884359031326\n",
            "epoch 46 batch id 4511 loss 0.09284953773021698 train acc 0.9832769895810242\n",
            "epoch 46 batch id 4521 loss 0.0410580113530159 train acc 0.9832655938951559\n",
            "epoch 46 batch id 4531 loss 0.00014341012865770608 train acc 0.9832749393069963\n",
            "epoch 46 batch id 4541 loss 0.060484450310468674 train acc 0.9832808026866329\n",
            "epoch 46 batch id 4551 loss 0.04754554480314255 train acc 0.9832797736761152\n",
            "epoch 46 batch id 4561 loss 0.06084722280502319 train acc 0.9832650460425345\n",
            "epoch 46 batch id 4571 loss 0.02800118364393711 train acc 0.9832708925836797\n",
            "epoch 46 batch id 4581 loss 0.021808085963129997 train acc 0.9832767135996507\n",
            "epoch 46 batch id 4591 loss 0.04641987383365631 train acc 0.9832825092572425\n",
            "epoch 46 batch id 4601 loss 0.013138055801391602 train acc 0.9832746957183222\n",
            "epoch 46 batch id 4611 loss 0.029551587998867035 train acc 0.9832940251572327\n",
            "epoch 46 batch id 4621 loss 0.0187411829829216 train acc 0.98328622051504\n",
            "epoch 46 batch id 4631 loss 0.09033147990703583 train acc 0.9832784495789246\n",
            "epoch 46 batch id 4641 loss 0.15990278124809265 train acc 0.9832707121310063\n",
            "epoch 46 batch id 4651 loss 0.11091107875108719 train acc 0.9832831649107718\n",
            "epoch 46 batch id 4661 loss 0.03622839227318764 train acc 0.9832922119716799\n",
            "epoch 46 batch id 4671 loss 0.009616708382964134 train acc 0.9833079105116678\n",
            "epoch 46 batch id 4681 loss 0.13122455775737762 train acc 0.9833068521683401\n",
            "epoch 46 batch id 4691 loss 0.004538794979453087 train acc 0.9833024674909401\n",
            "epoch 46 batch id 4701 loss 0.01670144498348236 train acc 0.9833147202722825\n",
            "epoch 46 batch id 4711 loss 0.031149473041296005 train acc 0.9833202876247081\n",
            "epoch 46 batch id 4721 loss 0.0024952490348368883 train acc 0.9833092829908917\n",
            "epoch 46 batch id 4731 loss 0.024235740303993225 train acc 0.9833148383005708\n",
            "epoch 46 batch id 4741 loss 0.08156579732894897 train acc 0.9833170744568657\n",
            "epoch 46 batch id 4751 loss 0.15539313852787018 train acc 0.9833225899810566\n",
            "epoch 46 batch id 4761 loss 0.06158558279275894 train acc 0.9833346460827557\n",
            "epoch 46 batch id 4771 loss 0.07075441628694534 train acc 0.9833401016558373\n",
            "epoch 46 batch id 4781 loss 0.06672036647796631 train acc 0.9833128529596319\n",
            "epoch 46 batch id 4791 loss 0.1497187465429306 train acc 0.9832987633061991\n",
            "epoch 46 batch id 4801 loss 0.07309243083000183 train acc 0.98329124140804\n",
            "epoch 46 batch id 4811 loss 0.010176481679081917 train acc 0.9832999896071503\n",
            "epoch 46 batch id 4821 loss 0.06038147583603859 train acc 0.9833087015142087\n",
            "epoch 46 batch id 4831 loss 0.02688215672969818 train acc 0.9833141430345684\n",
            "epoch 46 batch id 4841 loss 0.000737550959456712 train acc 0.9833098791571989\n",
            "epoch 46 batch id 4851 loss 0.06213381141424179 train acc 0.9833024118738405\n",
            "epoch 46 batch id 4861 loss 0.0003204523818567395 train acc 0.9833078327504629\n",
            "epoch 46 batch id 4871 loss 0.0017389353597536683 train acc 0.9833068158489017\n",
            "epoch 46 batch id 4881 loss 0.03248348459601402 train acc 0.9833090043023971\n",
            "epoch 46 batch id 4891 loss 0.017372243106365204 train acc 0.9832984052341035\n",
            "epoch 46 batch id 4901 loss 0.11341970413923264 train acc 0.983300601917976\n",
            "epoch 46 batch id 4911 loss 0.06294532865285873 train acc 0.9832996080228059\n",
            "epoch 46 batch id 4921 loss 0.17048892378807068 train acc 0.9832922678317415\n",
            "epoch 46 batch id 4931 loss 0.16388805210590363 train acc 0.9832754512269316\n",
            "epoch 46 batch id 4941 loss 0.050296101719141006 train acc 0.9832650273224044\n",
            "epoch 46 batch id 4951 loss 0.0917411595582962 train acc 0.9832546455261564\n",
            "epoch 46 train acc 0.9832528495057494\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df82e0a6d42e49c38f3eb0f725e48056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 46 loss 0.5542480945587158 test acc 0.8525728555718476\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed7a1fb7ae0c4c659df0c42e1ef8d2b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 47 batch id 1 loss 0.03301485627889633 train acc 0.984375\n",
            "epoch 47 batch id 11 loss 0.01456680241972208 train acc 0.9872159090909091\n",
            "epoch 47 batch id 21 loss 0.0024953624233603477 train acc 0.9836309523809523\n",
            "epoch 47 batch id 31 loss 0.09689588844776154 train acc 0.984375\n",
            "epoch 47 batch id 41 loss 0.016851620748639107 train acc 0.9855182926829268\n",
            "epoch 47 batch id 51 loss 0.01170636247843504 train acc 0.9852941176470589\n",
            "epoch 47 batch id 61 loss 0.030525770038366318 train acc 0.9833504098360656\n",
            "epoch 47 batch id 71 loss 0.1393774300813675 train acc 0.983274647887324\n",
            "epoch 47 batch id 81 loss 0.041170962154865265 train acc 0.9834104938271605\n",
            "epoch 47 batch id 91 loss 0.048316776752471924 train acc 0.9838598901098901\n",
            "epoch 47 batch id 101 loss 0.046626560389995575 train acc 0.9832920792079208\n",
            "epoch 47 batch id 111 loss 0.00012867122131865472 train acc 0.9832488738738738\n",
            "epoch 47 batch id 121 loss 0.08832768350839615 train acc 0.9837293388429752\n",
            "epoch 47 batch id 131 loss 0.02498624287545681 train acc 0.9840171755725191\n",
            "epoch 47 batch id 141 loss 0.0003250587033107877 train acc 0.984375\n",
            "epoch 47 batch id 151 loss 0.05966418236494064 train acc 0.984375\n",
            "epoch 47 batch id 161 loss 0.0032114589121192694 train acc 0.984472049689441\n",
            "epoch 47 batch id 171 loss 0.00870706606656313 train acc 0.9847404970760234\n",
            "epoch 47 batch id 181 loss 0.01373193971812725 train acc 0.9850656077348067\n",
            "epoch 47 batch id 191 loss 0.05007025599479675 train acc 0.9849476439790575\n",
            "epoch 47 batch id 201 loss 0.11526886373758316 train acc 0.9849191542288557\n",
            "epoch 47 batch id 211 loss 0.00023510023311246186 train acc 0.9852636255924171\n",
            "epoch 47 batch id 221 loss 0.05624321848154068 train acc 0.9851527149321267\n",
            "epoch 47 batch id 231 loss 0.02686094492673874 train acc 0.9851866883116883\n",
            "epoch 47 batch id 241 loss 0.0226906705647707 train acc 0.985088174273859\n",
            "epoch 47 batch id 251 loss 0.09480278193950653 train acc 0.9850597609561753\n",
            "epoch 47 batch id 261 loss 0.0508580319583416 train acc 0.9849736590038314\n",
            "epoch 47 batch id 271 loss 0.0017446917481720448 train acc 0.985239852398524\n",
            "epoch 47 batch id 281 loss 0.006776025053113699 train acc 0.9854314946619217\n",
            "epoch 47 batch id 291 loss 0.05493485927581787 train acc 0.9856099656357389\n",
            "epoch 47 batch id 301 loss 0.2259882241487503 train acc 0.9853093853820598\n",
            "epoch 47 batch id 311 loss 0.02309565246105194 train acc 0.9856310289389068\n",
            "epoch 47 batch id 321 loss 0.10002242028713226 train acc 0.9854458722741433\n",
            "epoch 47 batch id 331 loss 0.00016248233441729099 train acc 0.9854135196374623\n",
            "epoch 47 batch id 341 loss 0.13436050713062286 train acc 0.9854288856304986\n",
            "epoch 47 batch id 351 loss 0.009535110555589199 train acc 0.9853988603988604\n",
            "epoch 47 batch id 361 loss 0.019696561619639397 train acc 0.9853704986149584\n",
            "epoch 47 batch id 371 loss 0.023185383528470993 train acc 0.9855963611859838\n",
            "epoch 47 batch id 381 loss 0.023246249184012413 train acc 0.9857693569553806\n",
            "epoch 47 batch id 391 loss 0.023960614576935768 train acc 0.9858535805626598\n",
            "epoch 47 batch id 401 loss 0.0005178843275643885 train acc 0.9856218827930174\n",
            "epoch 47 batch id 411 loss 0.012242673896253109 train acc 0.9856675790754258\n",
            "epoch 47 batch id 421 loss 0.008230583742260933 train acc 0.9857853325415677\n",
            "epoch 47 batch id 431 loss 0.07200819253921509 train acc 0.9857526102088167\n",
            "epoch 47 batch id 441 loss 0.11976651102304459 train acc 0.9856150793650794\n",
            "epoch 47 batch id 451 loss 0.01961786486208439 train acc 0.9857261640798226\n",
            "epoch 47 batch id 461 loss 0.006819778122007847 train acc 0.9856629609544468\n",
            "epoch 47 batch id 471 loss 0.010455510579049587 train acc 0.9857019639065817\n",
            "epoch 47 batch id 481 loss 0.06580980122089386 train acc 0.9857068607068608\n",
            "epoch 47 batch id 491 loss 0.0037942412309348583 train acc 0.9856797352342159\n",
            "epoch 47 batch id 501 loss 0.10921595245599747 train acc 0.9857784431137725\n",
            "epoch 47 batch id 511 loss 0.09067387133836746 train acc 0.9856592465753424\n",
            "epoch 47 batch id 521 loss 0.12547101080417633 train acc 0.9856046065259118\n",
            "epoch 47 batch id 531 loss 0.01935415528714657 train acc 0.985463747645951\n",
            "epoch 47 batch id 541 loss 0.019568009302020073 train acc 0.9853280961182994\n",
            "epoch 47 batch id 551 loss 0.07914368808269501 train acc 0.9850839382940109\n",
            "epoch 47 batch id 561 loss 0.02164509892463684 train acc 0.9848763368983957\n",
            "epoch 47 batch id 571 loss 0.06533339619636536 train acc 0.9849770140105079\n",
            "epoch 47 batch id 581 loss 0.09556570649147034 train acc 0.9848859724612736\n",
            "epoch 47 batch id 591 loss 0.02792336978018284 train acc 0.9849302030456852\n",
            "epoch 47 batch id 601 loss 0.04449406638741493 train acc 0.9848949667221298\n",
            "epoch 47 batch id 611 loss 0.12303170561790466 train acc 0.9847585924713584\n",
            "epoch 47 batch id 621 loss 0.12908782064914703 train acc 0.9847524154589372\n",
            "epoch 47 batch id 631 loss 0.03683843836188316 train acc 0.9846969096671949\n",
            "epoch 47 batch id 641 loss 0.00611356133595109 train acc 0.984716263650546\n",
            "epoch 47 batch id 651 loss 0.01598323881626129 train acc 0.9847110215053764\n",
            "epoch 47 batch id 661 loss 0.00015641689242329448 train acc 0.9847768532526475\n",
            "epoch 47 batch id 671 loss 0.015281235799193382 train acc 0.9847475782414307\n",
            "epoch 47 batch id 681 loss 0.05036519467830658 train acc 0.984742107195301\n",
            "epoch 47 batch id 691 loss 0.048642922192811966 train acc 0.9847141823444283\n",
            "epoch 47 batch id 701 loss 0.03290753811597824 train acc 0.9845978958630528\n",
            "epoch 47 batch id 711 loss 0.11447591334581375 train acc 0.9845288326300985\n",
            "epoch 47 batch id 721 loss 0.04431741684675217 train acc 0.9844183425797504\n",
            "epoch 47 batch id 731 loss 0.011173879727721214 train acc 0.9844177496580028\n",
            "epoch 47 batch id 741 loss 0.001793630770407617 train acc 0.9843539136302294\n",
            "epoch 47 batch id 751 loss 0.018809176981449127 train acc 0.9842709720372836\n",
            "epoch 47 batch id 761 loss 0.03809237480163574 train acc 0.9841902102496715\n",
            "epoch 47 batch id 771 loss 0.032446686178445816 train acc 0.984192607003891\n",
            "epoch 47 batch id 781 loss 0.10033529251813889 train acc 0.9842749679897568\n",
            "epoch 47 batch id 791 loss 0.010235614143311977 train acc 0.9843552465233881\n",
            "epoch 47 batch id 801 loss 0.002065804088488221 train acc 0.9843554931335831\n",
            "epoch 47 batch id 811 loss 0.08461513370275497 train acc 0.984259401972873\n",
            "epoch 47 batch id 821 loss 0.013425487093627453 train acc 0.9842608099878197\n",
            "epoch 47 batch id 831 loss 0.0887610912322998 train acc 0.9842621841155235\n",
            "epoch 47 batch id 841 loss 0.0014859241200610995 train acc 0.9842821046373365\n",
            "epoch 47 batch id 851 loss 0.0652552992105484 train acc 0.9842648354876615\n",
            "epoch 47 batch id 861 loss 0.028969887644052505 train acc 0.9842479674796748\n",
            "epoch 47 batch id 871 loss 0.007751808501780033 train acc 0.9842314867967853\n",
            "epoch 47 batch id 881 loss 0.015593142248690128 train acc 0.9841267026106697\n",
            "epoch 47 batch id 891 loss 0.00010744763858383521 train acc 0.9841470258136925\n",
            "epoch 47 batch id 901 loss 0.062136538326740265 train acc 0.9840975305216426\n",
            "epoch 47 batch id 911 loss 0.013608434237539768 train acc 0.9840662733260154\n",
            "epoch 47 batch id 921 loss 0.0003655936452560127 train acc 0.9840356948968513\n",
            "epoch 47 batch id 931 loss 0.0046884119510650635 train acc 0.9840225563909775\n",
            "epoch 47 batch id 941 loss 0.012325736694037914 train acc 0.9840927205100957\n",
            "epoch 47 batch id 951 loss 0.04249048978090286 train acc 0.9839806782334385\n",
            "epoch 47 batch id 961 loss 0.013346872292459011 train acc 0.9839685223725286\n",
            "epoch 47 batch id 971 loss 0.009977736510336399 train acc 0.9839727085478888\n",
            "epoch 47 batch id 981 loss 0.00958666205406189 train acc 0.9840564475025484\n",
            "epoch 47 batch id 991 loss 0.0826895460486412 train acc 0.9841227295660948\n",
            "epoch 47 batch id 1001 loss 0.011903944425284863 train acc 0.984172077922078\n",
            "epoch 47 batch id 1011 loss 0.00036362840910442173 train acc 0.9841277200791295\n",
            "epoch 47 batch id 1021 loss 0.04304765909910202 train acc 0.984145445641528\n",
            "epoch 47 batch id 1031 loss 0.008135740645229816 train acc 0.9841476721629486\n",
            "epoch 47 batch id 1041 loss 0.018027571961283684 train acc 0.9841648655139289\n",
            "epoch 47 batch id 1051 loss 0.01690913736820221 train acc 0.9840925309229306\n",
            "epoch 47 batch id 1061 loss 0.018745187669992447 train acc 0.9840215598491989\n",
            "epoch 47 batch id 1071 loss 0.0002711396082304418 train acc 0.9840540382819795\n",
            "epoch 47 batch id 1081 loss 0.046281009912490845 train acc 0.9839847363552267\n",
            "epoch 47 batch id 1091 loss 0.23565006256103516 train acc 0.9839739917506874\n",
            "epoch 47 batch id 1101 loss 0.037781283259391785 train acc 0.9840202089009991\n",
            "epoch 47 batch id 1111 loss 0.055508632212877274 train acc 0.9840093384338434\n",
            "epoch 47 batch id 1121 loss 0.0037117069587111473 train acc 0.984096231043711\n",
            "epoch 47 batch id 1131 loss 0.04215513914823532 train acc 0.9840572502210433\n",
            "epoch 47 batch id 1141 loss 0.04392215237021446 train acc 0.9841011174408414\n",
            "epoch 47 batch id 1151 loss 0.026805289089679718 train acc 0.9839948957428323\n",
            "epoch 47 batch id 1161 loss 0.06877113878726959 train acc 0.9839577950043066\n",
            "epoch 47 batch id 1171 loss 0.011787105351686478 train acc 0.9839213279248505\n",
            "epoch 47 batch id 1181 loss 0.0002164233010262251 train acc 0.9839119390347163\n",
            "epoch 47 batch id 1191 loss 0.01951451040804386 train acc 0.9839158270361041\n",
            "epoch 47 batch id 1201 loss 0.09020566940307617 train acc 0.983945670274771\n",
            "epoch 47 batch id 1211 loss 0.00029445200925692916 train acc 0.9839363129644921\n",
            "epoch 47 batch id 1221 loss 0.07534664869308472 train acc 0.9839910933660934\n",
            "epoch 47 batch id 1231 loss 0.028751617297530174 train acc 0.9839053614947197\n",
            "epoch 47 batch id 1241 loss 0.0055777630768716335 train acc 0.9838965551974215\n",
            "epoch 47 batch id 1251 loss 0.08777700364589691 train acc 0.9838629096722622\n",
            "epoch 47 batch id 1261 loss 0.02735810913145542 train acc 0.9838421887390959\n",
            "epoch 47 batch id 1271 loss 0.02215254120528698 train acc 0.9838340873328089\n",
            "epoch 47 batch id 1281 loss 0.00013379566371440887 train acc 0.9838749024199844\n",
            "epoch 47 batch id 1291 loss 0.06063717231154442 train acc 0.9838908791634392\n",
            "epoch 47 batch id 1301 loss 0.0003876730042975396 train acc 0.9838705803228286\n",
            "epoch 47 batch id 1311 loss 0.017809778451919556 train acc 0.9838386727688787\n",
            "epoch 47 batch id 1321 loss 0.01651175506412983 train acc 0.9838309046177138\n",
            "epoch 47 batch id 1331 loss 0.012738402001559734 train acc 0.9837880353117956\n",
            "epoch 47 batch id 1341 loss 0.11950815469026566 train acc 0.9837807606263982\n",
            "epoch 47 batch id 1351 loss 0.014183350838720798 train acc 0.9837735936343449\n",
            "epoch 47 batch id 1361 loss 0.004518933594226837 train acc 0.9837320903747244\n",
            "epoch 47 batch id 1371 loss 0.029368702322244644 train acc 0.9837139861415025\n",
            "epoch 47 batch id 1381 loss 0.01810092106461525 train acc 0.9837414011585808\n",
            "epoch 47 batch id 1391 loss 0.009302209131419659 train acc 0.9837234902947519\n",
            "epoch 47 batch id 1401 loss 0.012180318124592304 train acc 0.983705835117773\n",
            "epoch 47 batch id 1411 loss 0.020938420668244362 train acc 0.9837216513111269\n",
            "epoch 47 batch id 1421 loss 0.02965235337615013 train acc 0.9837372448979592\n",
            "epoch 47 batch id 1431 loss 0.021782243624329567 train acc 0.9837417016072676\n",
            "epoch 47 batch id 1441 loss 0.023899821564555168 train acc 0.9836701943095073\n",
            "epoch 47 batch id 1451 loss 0.009844004176557064 train acc 0.9836858201240524\n",
            "epoch 47 batch id 1461 loss 0.008365709334611893 train acc 0.9837012320328542\n",
            "epoch 47 batch id 1471 loss 0.05319763347506523 train acc 0.9836845683208701\n",
            "epoch 47 batch id 1481 loss 0.06650698184967041 train acc 0.9836681296421337\n",
            "epoch 47 batch id 1491 loss 0.04758088290691376 train acc 0.9836938296445339\n",
            "epoch 47 batch id 1501 loss 0.06712445616722107 train acc 0.9836775483011326\n",
            "epoch 47 batch id 1511 loss 0.03334467113018036 train acc 0.9837028457974851\n",
            "epoch 47 batch id 1521 loss 0.0034879627637565136 train acc 0.9837072649572649\n",
            "epoch 47 batch id 1531 loss 0.01721571944653988 train acc 0.9837116263879817\n",
            "epoch 47 batch id 1541 loss 0.013677341863512993 train acc 0.9837057916937054\n",
            "epoch 47 batch id 1551 loss 0.026781130582094193 train acc 0.9837403288201161\n",
            "epoch 47 batch id 1561 loss 0.02097909525036812 train acc 0.9837544042280589\n",
            "epoch 47 batch id 1571 loss 0.12638454139232635 train acc 0.9837285168682368\n",
            "epoch 47 batch id 1581 loss 0.037615858018398285 train acc 0.9837326059456041\n",
            "epoch 47 batch id 1591 loss 0.047088295221328735 train acc 0.9837366436203645\n",
            "epoch 47 batch id 1601 loss 0.11174245923757553 train acc 0.9837894284821986\n",
            "epoch 47 batch id 1611 loss 0.10471887141466141 train acc 0.9837833643699565\n",
            "epoch 47 batch id 1621 loss 0.05130915716290474 train acc 0.9837484577421345\n",
            "epoch 47 batch id 1631 loss 0.0001820938050514087 train acc 0.9837714592274678\n",
            "epoch 47 batch id 1641 loss 0.06016811728477478 train acc 0.9837751371115173\n",
            "epoch 47 batch id 1651 loss 0.06979843229055405 train acc 0.983712522713507\n",
            "epoch 47 batch id 1661 loss 0.17110022902488708 train acc 0.9836976971703792\n",
            "epoch 47 batch id 1671 loss 0.10204928368330002 train acc 0.9836643476959904\n",
            "epoch 47 batch id 1681 loss 0.08272582292556763 train acc 0.9836592801903629\n",
            "epoch 47 batch id 1691 loss 0.13399794697761536 train acc 0.9836727528089888\n",
            "epoch 47 batch id 1701 loss 0.03160608932375908 train acc 0.9836585097001763\n",
            "epoch 47 batch id 1711 loss 0.004967500455677509 train acc 0.9836718293395675\n",
            "epoch 47 batch id 1721 loss 0.026158364489674568 train acc 0.9836486780941314\n",
            "epoch 47 batch id 1731 loss 0.000134691916173324 train acc 0.9836709272097054\n",
            "epoch 47 batch id 1741 loss 0.009239137172698975 train acc 0.9836929207352096\n",
            "epoch 47 batch id 1751 loss 0.04020702466368675 train acc 0.9836700456881782\n",
            "epoch 47 batch id 1761 loss 0.020877039059996605 train acc 0.9837184128336173\n",
            "epoch 47 batch id 1771 loss 0.000390557455830276 train acc 0.9837574110671937\n",
            "epoch 47 batch id 1781 loss 0.0007118467474356294 train acc 0.9837608787198203\n",
            "epoch 47 batch id 1791 loss 0.029760856181383133 train acc 0.9837643076493578\n",
            "epoch 47 batch id 1801 loss 0.12813091278076172 train acc 0.9837676985008329\n",
            "epoch 47 batch id 1811 loss 0.03510347753763199 train acc 0.9837279127553837\n",
            "epoch 47 batch id 1821 loss 0.06881297379732132 train acc 0.9836971444261395\n",
            "epoch 47 batch id 1831 loss 0.010878017172217369 train acc 0.9836923129437466\n",
            "epoch 47 batch id 1841 loss 0.000909488822799176 train acc 0.9836790467137425\n",
            "epoch 47 batch id 1851 loss 0.01082395389676094 train acc 0.9837418962722853\n",
            "epoch 47 batch id 1861 loss 0.014018119312822819 train acc 0.9837369022031166\n",
            "epoch 47 batch id 1871 loss 0.1800633817911148 train acc 0.9837653661143774\n",
            "epoch 47 batch id 1881 loss 0.024024279788136482 train acc 0.9837436868686869\n",
            "epoch 47 batch id 1891 loss 0.018228285014629364 train acc 0.9837800766790058\n",
            "epoch 47 batch id 1901 loss 0.08968929201364517 train acc 0.9837996449237244\n",
            "epoch 47 batch id 1911 loss 0.026608025655150414 train acc 0.9837454212454212\n",
            "epoch 47 batch id 1921 loss 0.12058784067630768 train acc 0.9836754945340969\n",
            "epoch 47 batch id 1931 loss 0.01767888478934765 train acc 0.9836791170378042\n",
            "epoch 47 batch id 1941 loss 0.011315123178064823 train acc 0.9837149021123133\n",
            "epoch 47 batch id 1951 loss 0.03398897126317024 train acc 0.9837102767811379\n",
            "epoch 47 batch id 1961 loss 0.04271858185529709 train acc 0.9837375701172871\n",
            "epoch 47 batch id 1971 loss 0.009847518056631088 train acc 0.9837566590563166\n",
            "epoch 47 batch id 1981 loss 0.04688861966133118 train acc 0.9837282306915699\n",
            "epoch 47 batch id 1991 loss 0.10662268102169037 train acc 0.9837079357106981\n",
            "epoch 47 batch id 2001 loss 0.0033469051122665405 train acc 0.983750312343828\n",
            "epoch 47 batch id 2011 loss 0.04619103670120239 train acc 0.9837844977623074\n",
            "epoch 47 batch id 2021 loss 0.1082398071885109 train acc 0.9837874195942603\n",
            "epoch 47 batch id 2031 loss 0.00508615467697382 train acc 0.9837749261447563\n",
            "epoch 47 batch id 2041 loss 0.0060575613752007484 train acc 0.9837778662420382\n",
            "epoch 47 batch id 2051 loss 0.16516709327697754 train acc 0.9837503047294003\n",
            "epoch 47 batch id 2061 loss 0.03768108785152435 train acc 0.9837684983017952\n",
            "epoch 47 batch id 2071 loss 0.18251077830791473 train acc 0.9837487928536939\n",
            "epoch 47 batch id 2081 loss 0.04450075328350067 train acc 0.9837367851994233\n",
            "epoch 47 batch id 2091 loss 0.023891570046544075 train acc 0.9837547824007652\n",
            "epoch 47 batch id 2101 loss 0.008094375021755695 train acc 0.9837800452165636\n",
            "epoch 47 batch id 2111 loss 0.006103756371885538 train acc 0.9837902652771199\n",
            "epoch 47 batch id 2121 loss 0.00901839043945074 train acc 0.9838298561999057\n",
            "epoch 47 batch id 2131 loss 0.04356006905436516 train acc 0.98387640778977\n",
            "epoch 47 batch id 2141 loss 0.032623663544654846 train acc 0.9838933325548809\n",
            "epoch 47 batch id 2151 loss 0.048756565898656845 train acc 0.9839028358902836\n",
            "epoch 47 batch id 2161 loss 0.23293305933475494 train acc 0.9838833294770939\n",
            "epoch 47 batch id 2171 loss 0.014217768795788288 train acc 0.9838927913403961\n",
            "epoch 47 batch id 2181 loss 0.08803751319646835 train acc 0.983902166437414\n",
            "epoch 47 batch id 2191 loss 0.021702297031879425 train acc 0.9838971930625285\n",
            "epoch 47 batch id 2201 loss 0.009154554456472397 train acc 0.9839064629713766\n",
            "epoch 47 batch id 2211 loss 0.004646334797143936 train acc 0.9839227159656264\n",
            "epoch 47 batch id 2221 loss 0.01783967949450016 train acc 0.9838684714092751\n",
            "epoch 47 batch id 2231 loss 0.0072072227485477924 train acc 0.9838917525773195\n",
            "epoch 47 batch id 2241 loss 0.031448692083358765 train acc 0.9838939089692101\n",
            "epoch 47 batch id 2251 loss 0.0007631608168594539 train acc 0.9838891048422923\n",
            "epoch 47 batch id 2261 loss 0.0001591976615600288 train acc 0.983891253869969\n",
            "epoch 47 batch id 2271 loss 0.0225395318120718 train acc 0.9839140246587407\n",
            "epoch 47 batch id 2281 loss 0.026245595887303352 train acc 0.983909195528277\n",
            "epoch 47 batch id 2291 loss 0.019209934398531914 train acc 0.9839112287210825\n",
            "epoch 47 batch id 2301 loss 0.022227248176932335 train acc 0.9839404063450674\n",
            "epoch 47 batch id 2311 loss 0.03845090791583061 train acc 0.9839422868887927\n",
            "epoch 47 batch id 2321 loss 0.09986945986747742 train acc 0.9839374192158552\n",
            "epoch 47 batch id 2331 loss 0.05824456363916397 train acc 0.9839057807807807\n",
            "epoch 47 batch id 2341 loss 0.005181899294257164 train acc 0.9839211341307134\n",
            "epoch 47 batch id 2351 loss 0.008548478595912457 train acc 0.9838765418970651\n",
            "epoch 47 batch id 2361 loss 0.14115752279758453 train acc 0.9838720351545955\n",
            "epoch 47 batch id 2371 loss 0.0022286607418209314 train acc 0.9839071067060312\n",
            "epoch 47 batch id 2381 loss 0.00010946681868517771 train acc 0.9839287589248215\n",
            "epoch 47 batch id 2391 loss 0.014702955260872841 train acc 0.9839110204935173\n",
            "epoch 47 batch id 2401 loss 0.06837119907140732 train acc 0.983893429820908\n",
            "epoch 47 batch id 2411 loss 0.07201934605836868 train acc 0.9838306200746578\n",
            "epoch 47 batch id 2421 loss 0.004829083103686571 train acc 0.9838586844279223\n",
            "epoch 47 batch id 2431 loss 0.04985624924302101 train acc 0.9838608083093378\n",
            "epoch 47 batch id 2441 loss 0.0013648516032844782 train acc 0.9838629147890209\n",
            "epoch 47 batch id 2451 loss 0.00014469756570179015 train acc 0.9838650040799674\n",
            "epoch 47 batch id 2461 loss 0.01914859563112259 train acc 0.9838607273466071\n",
            "epoch 47 batch id 2471 loss 0.0735752061009407 train acc 0.983831191825172\n",
            "epoch 47 batch id 2481 loss 0.07770857214927673 train acc 0.9838711708988311\n",
            "epoch 47 batch id 2491 loss 0.08836641162633896 train acc 0.9838481031714171\n",
            "epoch 47 batch id 2501 loss 0.008206631056964397 train acc 0.9838314674130347\n",
            "epoch 47 batch id 2511 loss 0.014382122084498405 train acc 0.983821186778176\n",
            "epoch 47 batch id 2521 loss 0.019684303551912308 train acc 0.983792393891313\n",
            "epoch 47 batch id 2531 loss 0.014329681172966957 train acc 0.983794695772422\n",
            "epoch 47 batch id 2541 loss 0.027580635622143745 train acc 0.9837969795356158\n",
            "epoch 47 batch id 2551 loss 0.02858619950711727 train acc 0.9837992453939631\n",
            "epoch 47 batch id 2561 loss 0.02046157233417034 train acc 0.9837648867629832\n",
            "epoch 47 batch id 2571 loss 0.06277519464492798 train acc 0.9837490276157137\n",
            "epoch 47 batch id 2581 loss 0.05272205173969269 train acc 0.9836969682293685\n",
            "epoch 47 batch id 2591 loss 0.07561329752206802 train acc 0.9836995851022771\n",
            "epoch 47 batch id 2601 loss 0.09732846170663834 train acc 0.9837262110726643\n",
            "epoch 47 batch id 2611 loss 0.0006463460740633309 train acc 0.9837466487935657\n",
            "epoch 47 batch id 2621 loss 0.0698120966553688 train acc 0.983743084700496\n",
            "epoch 47 batch id 2631 loss 0.06593465805053711 train acc 0.9837336088939567\n",
            "epoch 47 batch id 2641 loss 0.02751896344125271 train acc 0.9836946232487694\n",
            "epoch 47 batch id 2651 loss 0.00018714689940679818 train acc 0.9836971897397209\n",
            "epoch 47 batch id 2661 loss 0.015413742512464523 train acc 0.9836938650883127\n",
            "epoch 47 batch id 2671 loss 0.0010656599188223481 train acc 0.9836905653313366\n",
            "epoch 47 batch id 2681 loss 0.019320260733366013 train acc 0.9836814621409922\n",
            "epoch 47 batch id 2691 loss 0.010363772511482239 train acc 0.9836608138238573\n",
            "epoch 47 batch id 2701 loss 0.04618434980511665 train acc 0.9836750277674935\n",
            "epoch 47 batch id 2711 loss 0.007017778232693672 train acc 0.9836545555145703\n",
            "epoch 47 batch id 2721 loss 0.010399172082543373 train acc 0.9836399761117236\n",
            "epoch 47 batch id 2731 loss 0.0017892319010570645 train acc 0.983631224826071\n",
            "epoch 47 batch id 2741 loss 0.1550048589706421 train acc 0.9836396388179497\n",
            "epoch 47 batch id 2751 loss 0.06337621808052063 train acc 0.9836082333696837\n",
            "epoch 47 batch id 2761 loss 0.008680270984768867 train acc 0.9836166696848968\n",
            "epoch 47 batch id 2771 loss 0.03344281017780304 train acc 0.9836250451100685\n",
            "epoch 47 batch id 2781 loss 0.03007855825126171 train acc 0.9836558342322905\n",
            "epoch 47 batch id 2791 loss 0.023911749944090843 train acc 0.9836640093156575\n",
            "epoch 47 batch id 2801 loss 0.10334517061710358 train acc 0.9836498125669404\n",
            "epoch 47 batch id 2811 loss 0.06332826614379883 train acc 0.9836523923870508\n",
            "epoch 47 batch id 2821 loss 0.018048129975795746 train acc 0.9836771091811415\n",
            "epoch 47 batch id 2831 loss 0.011785547249019146 train acc 0.9836906128576475\n",
            "epoch 47 batch id 2841 loss 0.068659707903862 train acc 0.983687521999296\n",
            "epoch 47 batch id 2851 loss 0.029463360086083412 train acc 0.9836954138898633\n",
            "epoch 47 batch id 2861 loss 0.057197604328393936 train acc 0.9836868664802516\n",
            "epoch 47 batch id 2871 loss 0.07928464561700821 train acc 0.9836783786137234\n",
            "epoch 47 batch id 2881 loss 0.05514037609100342 train acc 0.9836699496702533\n",
            "epoch 47 batch id 2891 loss 0.0004905297537334263 train acc 0.9836831978554134\n",
            "epoch 47 batch id 2901 loss 0.009814273566007614 train acc 0.9836748104102033\n",
            "epoch 47 batch id 2911 loss 0.01916780136525631 train acc 0.9836664805908623\n",
            "epoch 47 batch id 2921 loss 0.00013665393635164946 train acc 0.983652858610065\n",
            "epoch 47 batch id 2931 loss 0.03675374388694763 train acc 0.9836286676902081\n",
            "epoch 47 batch id 2941 loss 0.013696848414838314 train acc 0.9836258925535533\n",
            "epoch 47 batch id 2951 loss 0.03011586330831051 train acc 0.9836654947475432\n",
            "epoch 47 batch id 2961 loss 0.022724663838744164 train acc 0.9836784447821681\n",
            "epoch 47 batch id 2971 loss 0.004096362739801407 train acc 0.9836597526085493\n",
            "epoch 47 batch id 2981 loss 0.03103230521082878 train acc 0.9836464273733646\n",
            "epoch 47 batch id 2991 loss 0.010548613034188747 train acc 0.9836540872617854\n",
            "epoch 47 batch id 3001 loss 0.0005352976731956005 train acc 0.9836877290903032\n",
            "epoch 47 batch id 3011 loss 0.004515215754508972 train acc 0.9836952009299236\n",
            "epoch 47 batch id 3021 loss 0.03308677673339844 train acc 0.9836715905329361\n",
            "epoch 47 batch id 3031 loss 0.1538616120815277 train acc 0.9836687561860772\n",
            "epoch 47 batch id 3041 loss 0.06777474284172058 train acc 0.9836402499177902\n",
            "epoch 47 batch id 3051 loss 0.0003478380967862904 train acc 0.9836529006882989\n",
            "epoch 47 batch id 3061 loss 0.11002849042415619 train acc 0.9836348415550473\n",
            "epoch 47 batch id 3071 loss 9.782835695659742e-05 train acc 0.9836474275480299\n",
            "epoch 47 batch id 3081 loss 0.08794419467449188 train acc 0.983644717624148\n",
            "epoch 47 batch id 3091 loss 0.01419585756957531 train acc 0.9836622452280815\n",
            "epoch 47 batch id 3101 loss 0.0631672739982605 train acc 0.9836494276039988\n",
            "epoch 47 batch id 3111 loss 0.04229404404759407 train acc 0.9836517598842816\n",
            "epoch 47 batch id 3121 loss 0.016332753002643585 train acc 0.9836640900352451\n",
            "epoch 47 batch id 3131 loss 0.003985769581049681 train acc 0.9836813318428617\n",
            "epoch 47 batch id 3141 loss 0.06728550046682358 train acc 0.9836835402737981\n",
            "epoch 47 batch id 3151 loss 0.004391801077872515 train acc 0.9836956521739131\n",
            "epoch 47 batch id 3161 loss 0.09508593380451202 train acc 0.9836879152167036\n",
            "epoch 47 batch id 3171 loss 0.04612449184060097 train acc 0.983695009460738\n",
            "epoch 47 batch id 3181 loss 0.030478475615382195 train acc 0.9836824111914493\n",
            "epoch 47 batch id 3191 loss 0.0120448749512434 train acc 0.9837139611407082\n",
            "epoch 47 batch id 3201 loss 0.07100237905979156 train acc 0.9836965010934083\n",
            "epoch 47 batch id 3211 loss 0.12179379910230637 train acc 0.9836596854562442\n",
            "epoch 47 batch id 3221 loss 0.07716020196676254 train acc 0.9836230984166408\n",
            "epoch 47 batch id 3231 loss 0.09171485155820847 train acc 0.9836012457443516\n",
            "epoch 47 batch id 3241 loss 0.01516082976013422 train acc 0.9836084541808084\n",
            "epoch 47 batch id 3251 loss 0.025962861254811287 train acc 0.9835867809904645\n",
            "epoch 47 batch id 3261 loss 0.21610645949840546 train acc 0.9835604492486967\n",
            "epoch 47 batch id 3271 loss 0.0105391601100564 train acc 0.9835533858147355\n",
            "epoch 47 batch id 3281 loss 0.0065324935130774975 train acc 0.9835558899725694\n",
            "epoch 47 batch id 3291 loss 0.013078542426228523 train acc 0.9835441355211182\n",
            "epoch 47 batch id 3301 loss 0.16355514526367188 train acc 0.9835561193577703\n",
            "epoch 47 batch id 3311 loss 0.012230215594172478 train acc 0.9835680308064029\n",
            "epoch 47 batch id 3321 loss 0.010584021918475628 train acc 0.9835798705209274\n",
            "epoch 47 batch id 3331 loss 0.1088678389787674 train acc 0.9835400405283699\n",
            "epoch 47 batch id 3341 loss 0.04345007240772247 train acc 0.9835425396587848\n",
            "epoch 47 batch id 3351 loss 0.020308928564190865 train acc 0.9834983960011937\n",
            "epoch 47 batch id 3361 loss 0.017511051148176193 train acc 0.983514950907468\n",
            "epoch 47 batch id 3371 loss 0.08650261908769608 train acc 0.983522137347968\n",
            "epoch 47 batch id 3381 loss 0.00022817552962806076 train acc 0.9835431455190772\n",
            "epoch 47 batch id 3391 loss 0.04975343123078346 train acc 0.9835640297847242\n",
            "epoch 47 batch id 3401 loss 0.08116415143013 train acc 0.9835618200529256\n",
            "epoch 47 batch id 3411 loss 0.027771620079874992 train acc 0.983577946350044\n",
            "epoch 47 batch id 3421 loss 0.018097298219799995 train acc 0.983580276235019\n",
            "epoch 47 batch id 3431 loss 0.22346611320972443 train acc 0.9835689303410085\n",
            "epoch 47 batch id 3441 loss 0.0011530936462804675 train acc 0.9835803545480964\n",
            "epoch 47 batch id 3451 loss 0.011121009476482868 train acc 0.9835736018545349\n",
            "epoch 47 batch id 3461 loss 0.006055738311260939 train acc 0.9835578590002889\n",
            "epoch 47 batch id 3471 loss 0.022380949929356575 train acc 0.9835782195332757\n",
            "epoch 47 batch id 3481 loss 0.10861776769161224 train acc 0.9835670425165183\n",
            "epoch 47 batch id 3491 loss 0.001882215146906674 train acc 0.9835604053279863\n",
            "epoch 47 batch id 3501 loss 0.02106550522148609 train acc 0.983567195087118\n",
            "epoch 47 batch id 3511 loss 0.05219903588294983 train acc 0.983551694673882\n",
            "epoch 47 batch id 3521 loss 0.20414124429225922 train acc 0.9835540329451861\n",
            "epoch 47 batch id 3531 loss 0.03981354832649231 train acc 0.9835607830642877\n",
            "epoch 47 batch id 3541 loss 0.07274337112903595 train acc 0.9835674950578932\n",
            "epoch 47 batch id 3551 loss 0.10788391530513763 train acc 0.9835301675584343\n",
            "epoch 47 batch id 3561 loss 0.009427105076611042 train acc 0.983545703454086\n",
            "epoch 47 batch id 3571 loss 0.05027241259813309 train acc 0.9835480257630915\n",
            "epoch 47 batch id 3581 loss 0.04760249704122543 train acc 0.9835285185702318\n",
            "epoch 47 batch id 3591 loss 0.12399223446846008 train acc 0.9835221734892787\n",
            "epoch 47 batch id 3601 loss 0.03685154393315315 train acc 0.9835158636489864\n",
            "epoch 47 batch id 3611 loss 0.0013876853045076132 train acc 0.9835268969814456\n",
            "epoch 47 batch id 3621 loss 0.2470918893814087 train acc 0.9835033485225076\n",
            "epoch 47 batch id 3631 loss 0.11437726765871048 train acc 0.9834928394381713\n",
            "epoch 47 batch id 3641 loss 0.007982347160577774 train acc 0.9834995536940401\n",
            "epoch 47 batch id 3651 loss 0.027439216151833534 train acc 0.9834805532730758\n",
            "epoch 47 batch id 3661 loss 0.003720204345881939 train acc 0.9834659246107621\n",
            "epoch 47 batch id 3671 loss 0.04655274748802185 train acc 0.9834641446472351\n",
            "epoch 47 batch id 3681 loss 0.0013718947302550077 train acc 0.9834835982070089\n",
            "epoch 47 batch id 3691 loss 0.005894226022064686 train acc 0.9834817800054186\n",
            "epoch 47 batch id 3701 loss 0.04709436744451523 train acc 0.9834715279654147\n",
            "epoch 47 batch id 3711 loss 0.019876480102539062 train acc 0.9834739625437887\n",
            "epoch 47 batch id 3721 loss 0.06955536454916 train acc 0.9834763840365494\n",
            "epoch 47 batch id 3731 loss 0.02576662227511406 train acc 0.9834997319753417\n",
            "epoch 47 batch id 3741 loss 0.06059219688177109 train acc 0.9834978949478749\n",
            "epoch 47 batch id 3751 loss 0.0002125064202118665 train acc 0.9834919021594242\n",
            "epoch 47 batch id 3761 loss 0.06615105271339417 train acc 0.9834776322786493\n",
            "epoch 47 batch id 3771 loss 0.025761423632502556 train acc 0.9834510076902678\n",
            "epoch 47 batch id 3781 loss 0.07767058908939362 train acc 0.9834493189632373\n",
            "epoch 47 batch id 3791 loss 0.0005831955932080746 train acc 0.9834682471643366\n",
            "epoch 47 batch id 3801 loss 0.07419503480195999 train acc 0.9834706327282294\n",
            "epoch 47 batch id 3811 loss 0.010677585378289223 train acc 0.9834771057465233\n",
            "epoch 47 batch id 3821 loss 0.038066938519477844 train acc 0.9834876341271919\n",
            "epoch 47 batch id 3831 loss 0.00039589774678461254 train acc 0.9834899504045941\n",
            "epoch 47 batch id 3841 loss 0.0001247341715497896 train acc 0.9834922546211924\n",
            "epoch 47 batch id 3851 loss 0.00010830226528923959 train acc 0.9835026616463256\n",
            "epoch 47 batch id 3861 loss 0.037164848297834396 train acc 0.983504921004921\n",
            "epoch 47 batch id 3871 loss 0.17826218903064728 train acc 0.9835031322655644\n",
            "epoch 47 batch id 3881 loss 0.16686582565307617 train acc 0.983493300695697\n",
            "epoch 47 batch id 3891 loss 0.010577373206615448 train acc 0.9834995823695708\n",
            "epoch 47 batch id 3901 loss 0.010343662463128567 train acc 0.9835018264547551\n",
            "epoch 47 batch id 3911 loss 0.03009459376335144 train acc 0.9834800882127334\n",
            "epoch 47 batch id 3921 loss 0.045793574303388596 train acc 0.9834823705687324\n",
            "epoch 47 batch id 3931 loss 0.05840429663658142 train acc 0.9834727168659374\n",
            "epoch 47 batch id 3941 loss 0.017643600702285767 train acc 0.9834829358030956\n",
            "epoch 47 batch id 3951 loss 0.012010080739855766 train acc 0.98347728423184\n",
            "epoch 47 batch id 3961 loss 0.0329122357070446 train acc 0.9834637717748044\n",
            "epoch 47 batch id 3971 loss 0.028101643547415733 train acc 0.9834621317048602\n",
            "epoch 47 batch id 3981 loss 0.0004061600484419614 train acc 0.9834644247676463\n",
            "epoch 47 batch id 3991 loss 0.18048612773418427 train acc 0.9834549611626159\n",
            "epoch 47 batch id 4001 loss 0.07682158052921295 train acc 0.9834533554111472\n",
            "epoch 47 batch id 4011 loss 0.010931644588708878 train acc 0.9834283844427824\n",
            "epoch 47 batch id 4021 loss 0.02083682082593441 train acc 0.9834307386222333\n",
            "epoch 47 batch id 4031 loss 0.00041139646782539785 train acc 0.9834563383775738\n",
            "epoch 47 batch id 4041 loss 0.03471491113305092 train acc 0.983443145261074\n",
            "epoch 47 batch id 4051 loss 0.032874301075935364 train acc 0.983430017279684\n",
            "epoch 47 batch id 4061 loss 0.016088765114545822 train acc 0.9834515821226298\n",
            "epoch 47 batch id 4071 loss 0.1610872447490692 train acc 0.9834461741586834\n",
            "epoch 47 batch id 4081 loss 0.014403062872588634 train acc 0.9834522788532223\n",
            "epoch 47 batch id 4091 loss 0.026152845472097397 train acc 0.9834468956245417\n",
            "epoch 47 batch id 4101 loss 0.0013364688493311405 train acc 0.9834529687881005\n",
            "epoch 47 batch id 4111 loss 0.032324060797691345 train acc 0.9834324069569448\n",
            "epoch 47 batch id 4121 loss 0.01435959991067648 train acc 0.9834384858044164\n",
            "epoch 47 batch id 4131 loss 0.09237617999315262 train acc 0.9834294057129025\n",
            "epoch 47 batch id 4141 loss 0.00011827113485196605 train acc 0.9834392356918619\n",
            "epoch 47 batch id 4151 loss 0.05438653752207756 train acc 0.9834414900024091\n",
            "epoch 47 batch id 4161 loss 0.02409450151026249 train acc 0.9834212028358568\n",
            "epoch 47 batch id 4171 loss 0.11834541708230972 train acc 0.9834085051546392\n",
            "epoch 47 batch id 4181 loss 0.010077448561787605 train acc 0.9834070796460177\n",
            "epoch 47 batch id 4191 loss 0.057493340224027634 train acc 0.9833795633500358\n",
            "epoch 47 batch id 4201 loss 0.021234016865491867 train acc 0.9834042489883361\n",
            "epoch 47 batch id 4211 loss 0.0815860852599144 train acc 0.9834028437425789\n",
            "epoch 47 batch id 4221 loss 0.11461590230464935 train acc 0.9833940416962805\n",
            "epoch 47 batch id 4231 loss 0.04868118837475777 train acc 0.983385281257386\n",
            "epoch 47 batch id 4241 loss 0.030832979828119278 train acc 0.9833876149493044\n",
            "epoch 47 batch id 4251 loss 0.10126736015081406 train acc 0.9833862620559868\n",
            "epoch 47 batch id 4261 loss 0.004633606411516666 train acc 0.9834032504107018\n",
            "epoch 47 batch id 4271 loss 0.006906678434461355 train acc 0.9834165008194802\n",
            "epoch 47 batch id 4281 loss 0.18910400569438934 train acc 0.9834077902359262\n",
            "epoch 47 batch id 4291 loss 0.01105558779090643 train acc 0.9833954789093451\n",
            "epoch 47 batch id 4301 loss 0.06039024144411087 train acc 0.9833759590792839\n",
            "epoch 47 batch id 4311 loss 0.027272118255496025 train acc 0.983374652052888\n",
            "epoch 47 batch id 4321 loss 0.043350543826818466 train acc 0.9833625028928489\n",
            "epoch 47 batch id 4331 loss 0.0704241394996643 train acc 0.9833612329716\n",
            "epoch 47 batch id 4341 loss 0.034887753427028656 train acc 0.9833671677032941\n",
            "epoch 47 batch id 4351 loss 0.028784358873963356 train acc 0.9833551195127557\n",
            "epoch 47 batch id 4361 loss 0.027561869472265244 train acc 0.9833502923641366\n",
            "epoch 47 batch id 4371 loss 0.04193859547376633 train acc 0.9833526366964082\n",
            "epoch 47 batch id 4381 loss 0.09131240844726562 train acc 0.9833407041771285\n",
            "epoch 47 batch id 4391 loss 0.0003673752071335912 train acc 0.9833395012525621\n",
            "epoch 47 batch id 4401 loss 0.011177340522408485 train acc 0.9833454044535332\n",
            "epoch 47 batch id 4411 loss 0.0002302076027262956 train acc 0.9833441963273634\n",
            "epoch 47 batch id 4421 loss 0.038530509918928146 train acc 0.9833429936665913\n",
            "epoch 47 batch id 4431 loss 0.013721746392548084 train acc 0.9833312175581133\n",
            "epoch 47 batch id 4441 loss 0.013926236890256405 train acc 0.9833441229452826\n",
            "epoch 47 batch id 4451 loss 0.0008415397605858743 train acc 0.9833534598966525\n",
            "epoch 47 batch id 4461 loss 0.0016422111075371504 train acc 0.9833557498318762\n",
            "epoch 47 batch id 4471 loss 0.09783226251602173 train acc 0.9833440505479758\n",
            "epoch 47 batch id 4481 loss 0.035421013832092285 train acc 0.9833428643160009\n",
            "epoch 47 batch id 4491 loss 0.05906358361244202 train acc 0.9833347250055667\n",
            "epoch 47 batch id 4501 loss 0.00895011704415083 train acc 0.9833266218618085\n",
            "epoch 47 batch id 4511 loss 0.1172899454832077 train acc 0.9833046996231434\n",
            "epoch 47 batch id 4521 loss 0.014158991165459156 train acc 0.9833105231143552\n",
            "epoch 47 batch id 4531 loss 0.00014351960271596909 train acc 0.9833163209004635\n",
            "epoch 47 batch id 4541 loss 0.0628427118062973 train acc 0.9833220931512883\n",
            "epoch 47 batch id 4551 loss 0.0505184605717659 train acc 0.983324406723797\n",
            "epoch 47 batch id 4561 loss 0.08780194073915482 train acc 0.9833061554483666\n",
            "epoch 47 batch id 4571 loss 0.03053552471101284 train acc 0.9832982388973966\n",
            "epoch 47 batch id 4581 loss 0.022032784298062325 train acc 0.9833074110456232\n",
            "epoch 47 batch id 4591 loss 0.061166539788246155 train acc 0.9832927194511\n",
            "epoch 47 batch id 4601 loss 0.014643089845776558 train acc 0.9832780917191914\n",
            "epoch 47 batch id 4611 loss 0.04868600144982338 train acc 0.9832838592496205\n",
            "epoch 47 batch id 4621 loss 0.011150217615067959 train acc 0.9832997457260333\n",
            "epoch 47 batch id 4631 loss 0.12298008054494858 train acc 0.9833088155905851\n",
            "epoch 47 batch id 4641 loss 0.05975506454706192 train acc 0.9833212131006248\n",
            "epoch 47 batch id 4651 loss 0.11804968863725662 train acc 0.9833167598365943\n",
            "epoch 47 batch id 4661 loss 0.030834680423140526 train acc 0.9833223825359365\n",
            "epoch 47 batch id 4671 loss 0.005091341212391853 train acc 0.9833346713765789\n",
            "epoch 47 batch id 4681 loss 0.1722162365913391 train acc 0.9833302179021577\n",
            "epoch 47 batch id 4691 loss 0.008388231508433819 train acc 0.9833291142613515\n",
            "epoch 47 batch id 4701 loss 0.05404972657561302 train acc 0.9833280153158902\n",
            "epoch 47 batch id 4711 loss 0.03905493766069412 train acc 0.9833302377414561\n",
            "epoch 47 batch id 4721 loss 0.00033565069315955043 train acc 0.9833092829908917\n",
            "epoch 47 batch id 4731 loss 0.021869516000151634 train acc 0.983304930247305\n",
            "epoch 47 batch id 4741 loss 0.04350217431783676 train acc 0.9833203701750686\n",
            "epoch 47 batch id 4751 loss 0.18160998821258545 train acc 0.9833258787623658\n",
            "epoch 47 batch id 4761 loss 0.11243914812803268 train acc 0.9833248004620878\n",
            "epoch 47 batch id 4771 loss 0.0635204166173935 train acc 0.9833335516663173\n",
            "epoch 47 batch id 4781 loss 0.06156603991985321 train acc 0.9833324618280694\n",
            "epoch 47 batch id 4791 loss 0.13730370998382568 train acc 0.9832955019828846\n",
            "epoch 47 batch id 4801 loss 0.12317521870136261 train acc 0.9832879868777338\n",
            "epoch 47 batch id 4811 loss 0.0007618816453032196 train acc 0.983296741841613\n",
            "epoch 47 batch id 4821 loss 0.05486413091421127 train acc 0.9832957373988799\n",
            "epoch 47 batch id 4831 loss 0.01618378609418869 train acc 0.9833076743945353\n",
            "epoch 47 batch id 4841 loss 0.013118856586515903 train acc 0.9833066515182813\n",
            "epoch 47 batch id 4851 loss 0.0735056921839714 train acc 0.983312074829932\n",
            "epoch 47 batch id 4861 loss 0.00015990356041584164 train acc 0.9833110471096482\n",
            "epoch 47 batch id 4871 loss 0.015378863550722599 train acc 0.9833132313693287\n",
            "epoch 47 batch id 4881 loss 0.03052724525332451 train acc 0.9833186078672403\n",
            "epoch 47 batch id 4891 loss 0.04430751875042915 train acc 0.9833175730934369\n",
            "epoch 47 batch id 4901 loss 0.0005657498841173947 train acc 0.9833165425423382\n",
            "epoch 47 batch id 4911 loss 0.10424748808145523 train acc 0.9833091529220118\n",
            "epoch 47 batch id 4921 loss 0.14288975298404694 train acc 0.983304968502337\n",
            "epoch 47 batch id 4931 loss 0.04976511001586914 train acc 0.9832849574122896\n",
            "epoch 47 batch id 4941 loss 0.055719129741191864 train acc 0.9832840012143291\n",
            "epoch 47 batch id 4951 loss 0.10313373059034348 train acc 0.9832830488790143\n",
            "epoch 47 train acc 0.9832812184789187\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b6be0bf3f2b423fb905af8a9265a4ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 47 loss 0.535233199596405 test acc 0.8527114644428153\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "408ddf4a3da9444c940d38329b5f2b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 48 batch id 1 loss 0.024446120485663414 train acc 0.984375\n",
            "epoch 48 batch id 11 loss 0.016010567545890808 train acc 0.9872159090909091\n",
            "epoch 48 batch id 21 loss 0.008576007559895515 train acc 0.9880952380952381\n",
            "epoch 48 batch id 31 loss 0.05367700383067131 train acc 0.9868951612903226\n",
            "epoch 48 batch id 41 loss 0.04660508409142494 train acc 0.9874237804878049\n",
            "epoch 48 batch id 51 loss 0.010680255480110645 train acc 0.9865196078431373\n",
            "epoch 48 batch id 61 loss 0.05068766698241234 train acc 0.984375\n",
            "epoch 48 batch id 71 loss 0.08081238716840744 train acc 0.9848151408450704\n",
            "epoch 48 batch id 81 loss 0.11203473061323166 train acc 0.9849537037037037\n",
            "epoch 48 batch id 91 loss 0.053865984082221985 train acc 0.9855769230769231\n",
            "epoch 48 batch id 101 loss 0.030187122523784637 train acc 0.9845297029702971\n",
            "epoch 48 batch id 111 loss 0.0003046135534532368 train acc 0.9840934684684685\n",
            "epoch 48 batch id 121 loss 0.02800184302031994 train acc 0.9847623966942148\n",
            "epoch 48 batch id 131 loss 0.02237294241786003 train acc 0.9848520992366412\n",
            "epoch 48 batch id 141 loss 0.0041964128613471985 train acc 0.985261524822695\n",
            "epoch 48 batch id 151 loss 0.06117865443229675 train acc 0.9852028145695364\n",
            "epoch 48 batch id 161 loss 0.0003768855531234294 train acc 0.98534549689441\n",
            "epoch 48 batch id 171 loss 0.007426446303725243 train acc 0.9851059941520468\n",
            "epoch 48 batch id 181 loss 0.07921513170003891 train acc 0.98541091160221\n",
            "epoch 48 batch id 191 loss 0.11366923898458481 train acc 0.9848658376963351\n",
            "epoch 48 batch id 201 loss 0.04305697977542877 train acc 0.9849191542288557\n",
            "epoch 48 batch id 211 loss 0.1085323691368103 train acc 0.9851155213270142\n",
            "epoch 48 batch id 221 loss 0.07965416461229324 train acc 0.9849406108597285\n",
            "epoch 48 batch id 231 loss 0.027779821306467056 train acc 0.9853219696969697\n",
            "epoch 48 batch id 241 loss 0.10927699506282806 train acc 0.9851530082987552\n",
            "epoch 48 batch id 251 loss 0.07929330319166183 train acc 0.985308764940239\n",
            "epoch 48 batch id 261 loss 0.02936747670173645 train acc 0.9852729885057471\n",
            "epoch 48 batch id 271 loss 0.00301647768355906 train acc 0.9854128228782287\n",
            "epoch 48 batch id 281 loss 0.008727065287530422 train acc 0.9854870996441281\n",
            "epoch 48 batch id 291 loss 0.06259679794311523 train acc 0.985448883161512\n",
            "epoch 48 batch id 301 loss 0.09783785790205002 train acc 0.9852055647840532\n",
            "epoch 48 batch id 311 loss 0.016572657972574234 train acc 0.9852290996784566\n",
            "epoch 48 batch id 321 loss 0.1641291081905365 train acc 0.9851538161993769\n",
            "epoch 48 batch id 331 loss 0.0014167985646054149 train acc 0.9849886706948641\n",
            "epoch 48 batch id 341 loss 0.17785751819610596 train acc 0.9850623167155426\n",
            "epoch 48 batch id 351 loss 0.007927942089736462 train acc 0.9852207977207977\n",
            "epoch 48 batch id 361 loss 0.04809345304965973 train acc 0.9851540858725761\n",
            "epoch 48 batch id 371 loss 0.01638500578701496 train acc 0.985385781671159\n",
            "epoch 48 batch id 381 loss 0.015190393663942814 train acc 0.9854412729658792\n",
            "epoch 48 batch id 391 loss 0.06164869666099548 train acc 0.9854539641943734\n",
            "epoch 48 batch id 401 loss 0.005603224504739046 train acc 0.9853491271820449\n",
            "epoch 48 batch id 411 loss 0.0008018631488084793 train acc 0.9852874087591241\n",
            "epoch 48 batch id 421 loss 0.008753226138651371 train acc 0.9852657363420427\n",
            "epoch 48 batch id 431 loss 0.05400313436985016 train acc 0.9851363109048724\n",
            "epoch 48 batch id 441 loss 0.1073712483048439 train acc 0.985048185941043\n",
            "epoch 48 batch id 451 loss 0.02341746725142002 train acc 0.9851371951219512\n",
            "epoch 48 batch id 461 loss 0.007749132812023163 train acc 0.9848834056399133\n",
            "epoch 48 batch id 471 loss 0.01134487148374319 train acc 0.9850053078556263\n",
            "epoch 48 batch id 481 loss 0.07612831890583038 train acc 0.9850571725571725\n",
            "epoch 48 batch id 491 loss 0.021951919421553612 train acc 0.984979633401222\n",
            "epoch 48 batch id 501 loss 0.1205814853310585 train acc 0.9849675648702595\n",
            "epoch 48 batch id 511 loss 0.062230851501226425 train acc 0.9849253913894325\n",
            "epoch 48 batch id 521 loss 0.12036042660474777 train acc 0.9848848368522073\n",
            "epoch 48 batch id 531 loss 0.022601433098316193 train acc 0.9847281073446328\n",
            "epoch 48 batch id 541 loss 0.023470290005207062 train acc 0.9846060536044362\n",
            "epoch 48 batch id 551 loss 0.02535567805171013 train acc 0.9846018602540835\n",
            "epoch 48 batch id 561 loss 0.11813006550073624 train acc 0.9844585561497327\n",
            "epoch 48 batch id 571 loss 0.03142627701163292 train acc 0.9845665499124343\n",
            "epoch 48 batch id 581 loss 0.06809329241514206 train acc 0.9844018932874354\n",
            "epoch 48 batch id 591 loss 0.020688721910119057 train acc 0.9844014382402707\n",
            "epoch 48 batch id 601 loss 0.013896935619413853 train acc 0.984323003327787\n",
            "epoch 48 batch id 611 loss 0.05368166044354439 train acc 0.9842727086743044\n",
            "epoch 48 batch id 621 loss 0.14703992009162903 train acc 0.9842995169082126\n",
            "epoch 48 batch id 631 loss 0.0367700532078743 train acc 0.9842016640253566\n",
            "epoch 48 batch id 641 loss 0.014056728221476078 train acc 0.984228744149766\n",
            "epoch 48 batch id 651 loss 0.01307236310094595 train acc 0.9842069892473119\n",
            "epoch 48 batch id 661 loss 0.0001110637531382963 train acc 0.984304084720121\n",
            "epoch 48 batch id 671 loss 0.011423042975366116 train acc 0.9841887108792846\n",
            "epoch 48 batch id 681 loss 0.053737346082925797 train acc 0.9841685022026432\n",
            "epoch 48 batch id 691 loss 0.024666793644428253 train acc 0.9841036541244573\n",
            "epoch 48 batch id 701 loss 0.038559358566999435 train acc 0.983973787446505\n",
            "epoch 48 batch id 711 loss 0.12145417928695679 train acc 0.9840014064697609\n",
            "epoch 48 batch id 721 loss 0.04127754271030426 train acc 0.9838982316227461\n",
            "epoch 48 batch id 731 loss 0.01225705910474062 train acc 0.9839475034199726\n",
            "epoch 48 batch id 741 loss 0.0007792039541527629 train acc 0.9839321862348178\n",
            "epoch 48 batch id 751 loss 0.025395885109901428 train acc 0.9838548601864181\n",
            "epoch 48 batch id 761 loss 0.00040771515341475606 train acc 0.9838411629434954\n",
            "epoch 48 batch id 771 loss 0.043915096670389175 train acc 0.9838075551232166\n",
            "epoch 48 batch id 781 loss 0.04042402654886246 train acc 0.983794814340589\n",
            "epoch 48 batch id 791 loss 0.007193364668637514 train acc 0.983861409608091\n",
            "epoch 48 batch id 801 loss 0.02908346615731716 train acc 0.9838288077403246\n",
            "epoch 48 batch id 811 loss 0.11335843801498413 train acc 0.983797009864365\n",
            "epoch 48 batch id 821 loss 0.011118335649371147 train acc 0.9838611449451888\n",
            "epoch 48 batch id 831 loss 0.10616351664066315 train acc 0.9838673285198556\n",
            "epoch 48 batch id 841 loss 0.0015847710892558098 train acc 0.9839291022592153\n",
            "epoch 48 batch id 851 loss 0.04225478693842888 train acc 0.9839159811985899\n",
            "epoch 48 batch id 861 loss 0.03175823390483856 train acc 0.9838850174216028\n",
            "epoch 48 batch id 871 loss 0.008169634267687798 train acc 0.9838906429391504\n",
            "epoch 48 batch id 881 loss 0.015339763835072517 train acc 0.98386066969353\n",
            "epoch 48 batch id 891 loss 0.00011907240696018562 train acc 0.9839015151515151\n",
            "epoch 48 batch id 901 loss 0.039853181689977646 train acc 0.983906770255272\n",
            "epoch 48 batch id 911 loss 0.034653257578611374 train acc 0.9839290614709111\n",
            "epoch 48 batch id 921 loss 0.00016433199925813824 train acc 0.983832111834962\n",
            "epoch 48 batch id 931 loss 0.0008911259938031435 train acc 0.9838379430719656\n",
            "epoch 48 batch id 941 loss 0.05643557757139206 train acc 0.9839100690754516\n",
            "epoch 48 batch id 951 loss 0.050950516015291214 train acc 0.9838163774973712\n",
            "epoch 48 batch id 961 loss 0.026864780113101006 train acc 0.98380593132154\n",
            "epoch 48 batch id 971 loss 0.012319030240178108 train acc 0.9837796086508754\n",
            "epoch 48 batch id 981 loss 0.01141637098044157 train acc 0.9838653160040775\n",
            "epoch 48 batch id 991 loss 0.08429189026355743 train acc 0.983933526740666\n",
            "epoch 48 batch id 1001 loss 0.020960669964551926 train acc 0.9839691558441559\n",
            "epoch 48 batch id 1011 loss 0.00043403723975643516 train acc 0.9839422601384767\n",
            "epoch 48 batch id 1021 loss 0.03717441484332085 train acc 0.9839924094025465\n",
            "epoch 48 batch id 1031 loss 0.010146359913051128 train acc 0.9840264306498545\n",
            "epoch 48 batch id 1041 loss 0.012568945996463299 train acc 0.9840297790585975\n",
            "epoch 48 batch id 1051 loss 0.015763452276587486 train acc 0.9839735965746907\n",
            "epoch 48 batch id 1061 loss 0.014560405164957047 train acc 0.9839184731385485\n",
            "epoch 48 batch id 1071 loss 0.00019445567158982158 train acc 0.9839665032679739\n",
            "epoch 48 batch id 1081 loss 0.08605141937732697 train acc 0.9838980111008325\n",
            "epoch 48 batch id 1091 loss 0.08176012337207794 train acc 0.9839023831347388\n",
            "epoch 48 batch id 1101 loss 0.045407142490148544 train acc 0.9839208673932789\n",
            "epoch 48 batch id 1111 loss 0.1472093015909195 train acc 0.9837983798379838\n",
            "epoch 48 batch id 1121 loss 0.05319506675004959 train acc 0.9838314005352364\n",
            "epoch 48 batch id 1131 loss 0.028288407251238823 train acc 0.9837809460654289\n",
            "epoch 48 batch id 1141 loss 0.04008143022656441 train acc 0.983786152497809\n",
            "epoch 48 batch id 1151 loss 0.0164931770414114 train acc 0.9837098175499566\n",
            "epoch 48 batch id 1161 loss 0.03366991877555847 train acc 0.9837155469422911\n",
            "epoch 48 batch id 1171 loss 0.008013897575438023 train acc 0.9836544619982921\n",
            "epoch 48 batch id 1181 loss 0.00020346631936263293 train acc 0.9836737933954276\n",
            "epoch 48 batch id 1191 loss 0.009006879292428493 train acc 0.9836928001679262\n",
            "epoch 48 batch id 1201 loss 0.047553081065416336 train acc 0.9837375104079933\n",
            "epoch 48 batch id 1211 loss 0.00019011304539162666 train acc 0.9837169694467383\n",
            "epoch 48 batch id 1221 loss 0.0887802392244339 train acc 0.9837351556101556\n",
            "epoch 48 batch id 1231 loss 0.04826216399669647 train acc 0.9836261169780666\n",
            "epoch 48 batch id 1241 loss 0.006973431445658207 train acc 0.9836573327961321\n",
            "epoch 48 batch id 1251 loss 0.08881670981645584 train acc 0.9836505795363709\n",
            "epoch 48 batch id 1261 loss 0.021980421617627144 train acc 0.9836563243457573\n",
            "epoch 48 batch id 1271 loss 0.026296867057681084 train acc 0.9836496852871754\n",
            "epoch 48 batch id 1281 loss 0.00022693182108923793 train acc 0.9836797423887588\n",
            "epoch 48 batch id 1291 loss 0.07113523781299591 train acc 0.9836609217660728\n",
            "epoch 48 batch id 1301 loss 0.0001638600806472823 train acc 0.9836303804765565\n",
            "epoch 48 batch id 1311 loss 0.018775278702378273 train acc 0.9836479786422578\n",
            "epoch 48 batch id 1321 loss 0.014961188659071922 train acc 0.9836298258894777\n",
            "epoch 48 batch id 1331 loss 0.011735319159924984 train acc 0.9836236851990984\n",
            "epoch 48 batch id 1341 loss 0.1372453272342682 train acc 0.9836176360924683\n",
            "epoch 48 batch id 1351 loss 0.0022186017595231533 train acc 0.9836579385640266\n",
            "epoch 48 batch id 1361 loss 0.006153763271868229 train acc 0.9836172850844966\n",
            "epoch 48 batch id 1371 loss 0.006164703983813524 train acc 0.9836683989788475\n",
            "epoch 48 batch id 1381 loss 0.031530171632766724 train acc 0.9836848298334541\n",
            "epoch 48 batch id 1391 loss 0.009949840605258942 train acc 0.9835999281092739\n",
            "epoch 48 batch id 1401 loss 0.009419672191143036 train acc 0.9835720021413277\n",
            "epoch 48 batch id 1411 loss 0.01453309040516615 train acc 0.9836109142452162\n",
            "epoch 48 batch id 1421 loss 0.011512117460370064 train acc 0.9836272871217453\n",
            "epoch 48 batch id 1431 loss 0.0025937010068446398 train acc 0.9835888364779874\n",
            "epoch 48 batch id 1441 loss 0.025623710826039314 train acc 0.9835292331714087\n",
            "epoch 48 batch id 1451 loss 0.01114527229219675 train acc 0.9835565988973122\n",
            "epoch 48 batch id 1461 loss 0.011655539274215698 train acc 0.9835301163586585\n",
            "epoch 48 batch id 1471 loss 0.17205646634101868 train acc 0.9836102141400408\n",
            "epoch 48 batch id 1481 loss 0.06583133339881897 train acc 0.9836259284267387\n",
            "epoch 48 batch id 1491 loss 0.0358644463121891 train acc 0.9836309523809523\n",
            "epoch 48 batch id 1501 loss 0.055019017308950424 train acc 0.9836463191205863\n",
            "epoch 48 batch id 1511 loss 0.012161733582615852 train acc 0.9836925049636003\n",
            "epoch 48 batch id 1521 loss 0.04568639025092125 train acc 0.9837072649572649\n",
            "epoch 48 batch id 1531 loss 0.04262402653694153 train acc 0.9837116263879817\n",
            "epoch 48 batch id 1541 loss 0.021522613242268562 train acc 0.9836956521739131\n",
            "epoch 48 batch id 1551 loss 0.04393850266933441 train acc 0.9837000322372663\n",
            "epoch 48 batch id 1561 loss 0.027256889268755913 train acc 0.9836843369634849\n",
            "epoch 48 batch id 1571 loss 0.005693541374057531 train acc 0.9836787873965627\n",
            "epoch 48 batch id 1581 loss 0.040881309658288956 train acc 0.9836733080328905\n",
            "epoch 48 batch id 1591 loss 0.06437178701162338 train acc 0.9836875392834695\n",
            "epoch 48 batch id 1601 loss 0.008366280235350132 train acc 0.9837503903810119\n",
            "epoch 48 batch id 1611 loss 0.10021962970495224 train acc 0.9837154717566728\n",
            "epoch 48 batch id 1621 loss 0.033218029886484146 train acc 0.9837002621838371\n",
            "epoch 48 batch id 1631 loss 0.004796301946043968 train acc 0.9837139791538934\n",
            "epoch 48 batch id 1641 loss 0.07841223478317261 train acc 0.9837084856794638\n",
            "epoch 48 batch id 1651 loss 0.04696502164006233 train acc 0.983636811023622\n",
            "epoch 48 batch id 1661 loss 0.04984169080853462 train acc 0.9836600692354004\n",
            "epoch 48 batch id 1671 loss 0.022794492542743683 train acc 0.9836456463195691\n",
            "epoch 48 batch id 1681 loss 0.004861786495894194 train acc 0.9836592801903629\n",
            "epoch 48 batch id 1691 loss 0.012524323537945747 train acc 0.9837004730928445\n",
            "epoch 48 batch id 1701 loss 0.025373607873916626 train acc 0.9837136243386243\n",
            "epoch 48 batch id 1711 loss 0.004605592228472233 train acc 0.9837357539450614\n",
            "epoch 48 batch id 1721 loss 0.012564178556203842 train acc 0.9837394683323649\n",
            "epoch 48 batch id 1731 loss 0.00017618655692785978 train acc 0.9837431398035817\n",
            "epoch 48 batch id 1741 loss 0.008762410841882229 train acc 0.9837557438253877\n",
            "epoch 48 batch id 1751 loss 0.03215674310922623 train acc 0.9837146630496859\n",
            "epoch 48 batch id 1761 loss 0.010255249217152596 train acc 0.9837627768313458\n",
            "epoch 48 batch id 1771 loss 0.0003597079194150865 train acc 0.9838103472614342\n",
            "epoch 48 batch id 1781 loss 0.005054056644439697 train acc 0.98385738349242\n",
            "epoch 48 batch id 1791 loss 0.028567947447299957 train acc 0.9838864461194863\n",
            "epoch 48 batch id 1801 loss 0.028330538421869278 train acc 0.9839325374791782\n",
            "epoch 48 batch id 1811 loss 0.04050030931830406 train acc 0.9839090971838763\n",
            "epoch 48 batch id 1821 loss 0.09136280417442322 train acc 0.9838859143327842\n",
            "epoch 48 batch id 1831 loss 0.012890716083347797 train acc 0.9838715182960132\n",
            "epoch 48 batch id 1841 loss 0.004623109474778175 train acc 0.983823329712113\n",
            "epoch 48 batch id 1851 loss 0.005058975424617529 train acc 0.9838769584008644\n",
            "epoch 48 batch id 1861 loss 0.024801475927233696 train acc 0.983837654486835\n",
            "epoch 48 batch id 1871 loss 0.07192762196063995 train acc 0.983882282202031\n",
            "epoch 48 batch id 1881 loss 0.0322858951985836 train acc 0.9838682881446039\n",
            "epoch 48 batch id 1891 loss 0.028591159731149673 train acc 0.9838792305658381\n",
            "epoch 48 batch id 1901 loss 0.08732037991285324 train acc 0.983890057864282\n",
            "epoch 48 batch id 1911 loss 0.021751604974269867 train acc 0.983843537414966\n",
            "epoch 48 batch id 1921 loss 0.11475211381912231 train acc 0.9837893675169183\n",
            "epoch 48 batch id 1931 loss 0.01572995074093342 train acc 0.9837924003107198\n",
            "epoch 48 batch id 1941 loss 0.0035520116798579693 train acc 0.9838034518289541\n",
            "epoch 48 batch id 1951 loss 0.02921326272189617 train acc 0.9838063813429011\n",
            "epoch 48 batch id 1961 loss 0.0377897210419178 train acc 0.9838411524732279\n",
            "epoch 48 batch id 1971 loss 0.010968195274472237 train acc 0.9838517884322678\n",
            "epoch 48 batch id 1981 loss 0.03447796404361725 train acc 0.9838307672892479\n",
            "epoch 48 batch id 1991 loss 0.10680898278951645 train acc 0.9837785660472125\n",
            "epoch 48 batch id 2001 loss 0.00023132425849325955 train acc 0.9837893553223388\n",
            "epoch 48 batch id 2011 loss 0.049959395080804825 train acc 0.9837922675285927\n",
            "epoch 48 batch id 2021 loss 0.06481732428073883 train acc 0.9837874195942603\n",
            "epoch 48 batch id 2031 loss 0.00019094337767455727 train acc 0.9837826193993107\n",
            "epoch 48 batch id 2041 loss 0.018246693536639214 train acc 0.9837702106810388\n",
            "epoch 48 batch id 2051 loss 0.16754189133644104 train acc 0.9837731594344222\n",
            "epoch 48 batch id 2061 loss 0.005225859582424164 train acc 0.9837760795730228\n",
            "epoch 48 batch id 2071 loss 0.19066299498081207 train acc 0.9837563375181072\n",
            "epoch 48 batch id 2081 loss 0.1397225260734558 train acc 0.9837442936088419\n",
            "epoch 48 batch id 2091 loss 0.02987499348819256 train acc 0.983739837398374\n",
            "epoch 48 batch id 2101 loss 0.0036687091924250126 train acc 0.9837874821513565\n",
            "epoch 48 batch id 2111 loss 0.00816288497298956 train acc 0.9837828635717669\n",
            "epoch 48 batch id 2121 loss 0.11765292286872864 train acc 0.983815122583687\n",
            "epoch 48 batch id 2131 loss 0.01094384491443634 train acc 0.9838544110746129\n",
            "epoch 48 batch id 2141 loss 0.047778353095054626 train acc 0.9838422466137319\n",
            "epoch 48 batch id 2151 loss 0.01012401096522808 train acc 0.9838592515109251\n",
            "epoch 48 batch id 2161 loss 0.18704909086227417 train acc 0.9838688685793614\n",
            "epoch 48 batch id 2171 loss 0.0007844695937819779 train acc 0.9838568056195302\n",
            "epoch 48 batch id 2181 loss 0.0979783684015274 train acc 0.9838376891334251\n",
            "epoch 48 batch id 2191 loss 0.02157733589410782 train acc 0.9838258785942492\n",
            "epoch 48 batch id 2201 loss 0.010541464202105999 train acc 0.9838212744207179\n",
            "epoch 48 batch id 2211 loss 0.00021847611060366035 train acc 0.9838167118950701\n",
            "epoch 48 batch id 2221 loss 0.020471591502428055 train acc 0.9838051553354344\n",
            "epoch 48 batch id 2231 loss 0.03803075850009918 train acc 0.9838077095472882\n",
            "epoch 48 batch id 2241 loss 0.04318875074386597 train acc 0.9838102409638554\n",
            "epoch 48 batch id 2251 loss 0.0008703593048267066 train acc 0.9837711017325633\n",
            "epoch 48 batch id 2261 loss 0.0001314909604843706 train acc 0.9837737726669615\n",
            "epoch 48 batch id 2271 loss 0.04492875561118126 train acc 0.9837764200792602\n",
            "epoch 48 batch id 2281 loss 0.02846054919064045 train acc 0.9837927444103464\n",
            "epoch 48 batch id 2291 loss 0.036051660776138306 train acc 0.9837884657354867\n",
            "epoch 48 batch id 2301 loss 0.018784891813993454 train acc 0.9838113863537592\n",
            "epoch 48 batch id 2311 loss 0.04493868723511696 train acc 0.983834108610991\n",
            "epoch 48 batch id 2321 loss 0.0913739800453186 train acc 0.9838095109866437\n",
            "epoch 48 batch id 2331 loss 0.05440496653318405 train acc 0.9837851244101244\n",
            "epoch 48 batch id 2341 loss 0.001635173219256103 train acc 0.9838076676633917\n",
            "epoch 48 batch id 2351 loss 0.014020814560353756 train acc 0.9837901424925564\n",
            "epoch 48 batch id 2361 loss 0.0671524927020073 train acc 0.9837860016941974\n",
            "epoch 48 batch id 2371 loss 0.00019795780826825649 train acc 0.9838148460565163\n",
            "epoch 48 batch id 2381 loss 0.0008806087425909936 train acc 0.9838434481310374\n",
            "epoch 48 batch id 2391 loss 0.016977379098534584 train acc 0.9838456712672522\n",
            "epoch 48 batch id 2401 loss 0.026569567620754242 train acc 0.9838283527696793\n",
            "epoch 48 batch id 2411 loss 0.030048130080103874 train acc 0.9837982165076732\n",
            "epoch 48 batch id 2421 loss 0.02889050543308258 train acc 0.9838070528707146\n",
            "epoch 48 batch id 2431 loss 0.0425456203520298 train acc 0.983822243932538\n",
            "epoch 48 batch id 2441 loss 0.0025565396063029766 train acc 0.9838565137238836\n",
            "epoch 48 batch id 2451 loss 0.006319088861346245 train acc 0.9838458792329662\n",
            "epoch 48 batch id 2461 loss 0.0233735553920269 train acc 0.9838543783015035\n",
            "epoch 48 batch id 2471 loss 0.12608486413955688 train acc 0.983831191825172\n",
            "epoch 48 batch id 2481 loss 0.06929339468479156 train acc 0.9838585751713019\n",
            "epoch 48 batch id 2491 loss 0.08003736287355423 train acc 0.9838543757527097\n",
            "epoch 48 batch id 2501 loss 0.00041544699342921376 train acc 0.9838627049180327\n",
            "epoch 48 batch id 2511 loss 0.009718325920403004 train acc 0.9838522998805257\n",
            "epoch 48 batch id 2521 loss 0.015049890615046024 train acc 0.9838109877032923\n",
            "epoch 48 batch id 2531 loss 0.015019101090729237 train acc 0.983807042670881\n",
            "epoch 48 batch id 2541 loss 0.03137436509132385 train acc 0.983784681227863\n",
            "epoch 48 batch id 2551 loss 0.03351404517889023 train acc 0.9837747451979616\n",
            "epoch 48 batch id 2561 loss 0.025316426530480385 train acc 0.9837648867629832\n",
            "epoch 48 batch id 2571 loss 0.07950357347726822 train acc 0.9837490276157137\n",
            "epoch 48 batch id 2581 loss 0.05413255840539932 train acc 0.983733291359938\n",
            "epoch 48 batch id 2591 loss 0.015236742794513702 train acc 0.9837297375530684\n",
            "epoch 48 batch id 2601 loss 0.1371258795261383 train acc 0.983756247597078\n",
            "epoch 48 batch id 2611 loss 0.0002125088358297944 train acc 0.9837705859823822\n",
            "epoch 48 batch id 2621 loss 0.04559409245848656 train acc 0.9837669305608546\n",
            "epoch 48 batch id 2631 loss 0.0625750720500946 train acc 0.9837276700874192\n",
            "epoch 48 batch id 2641 loss 0.033254530280828476 train acc 0.983724204846649\n",
            "epoch 48 batch id 2651 loss 0.0005347808473743498 train acc 0.9837266597510373\n",
            "epoch 48 batch id 2661 loss 0.009993757121264935 train acc 0.9837232243517474\n",
            "epoch 48 batch id 2671 loss 0.006135579664260149 train acc 0.9837022650692624\n",
            "epoch 48 batch id 2681 loss 0.030635451897978783 train acc 0.9836639779932861\n",
            "epoch 48 batch id 2691 loss 0.02940659038722515 train acc 0.9836666202155333\n",
            "epoch 48 batch id 2701 loss 0.03577754646539688 train acc 0.9836692428730099\n",
            "epoch 48 batch id 2711 loss 0.007877340540289879 train acc 0.9836660826263371\n",
            "epoch 48 batch id 2721 loss 0.010266538709402084 train acc 0.9836514608599779\n",
            "epoch 48 batch id 2731 loss 0.07441474497318268 train acc 0.9836541102160381\n",
            "epoch 48 batch id 2741 loss 0.062040891498327255 train acc 0.9836567402407881\n",
            "epoch 48 batch id 2751 loss 0.031038649380207062 train acc 0.9836309523809523\n",
            "epoch 48 batch id 2761 loss 0.009850900620222092 train acc 0.9836506247736327\n",
            "epoch 48 batch id 2771 loss 0.03735913708806038 train acc 0.9836363226272103\n",
            "epoch 48 batch id 2781 loss 0.02954949252307415 train acc 0.9836558342322905\n",
            "epoch 48 batch id 2791 loss 0.07683642953634262 train acc 0.9836416159082766\n",
            "epoch 48 batch id 2801 loss 0.09132266044616699 train acc 0.9836442342020707\n",
            "epoch 48 batch id 2811 loss 0.047496095299720764 train acc 0.9836635094272501\n",
            "epoch 48 batch id 2821 loss 0.04243522137403488 train acc 0.9836771091811415\n",
            "epoch 48 batch id 2831 loss 0.013264221139252186 train acc 0.9836685358530555\n",
            "epoch 48 batch id 2841 loss 0.07476206123828888 train acc 0.9836435234072509\n",
            "epoch 48 batch id 2851 loss 0.0004789406666532159 train acc 0.9836515696246931\n",
            "epoch 48 batch id 2861 loss 0.06333479285240173 train acc 0.9836486368402656\n",
            "epoch 48 batch id 2871 loss 0.12465358525514603 train acc 0.9836566091954023\n",
            "epoch 48 batch id 2881 loss 0.042841050773859024 train acc 0.9836536792780285\n",
            "epoch 48 batch id 2891 loss 0.0002771299332380295 train acc 0.9836723884469042\n",
            "epoch 48 batch id 2901 loss 0.006764420308172703 train acc 0.9836801964839711\n",
            "epoch 48 batch id 2911 loss 0.020452775061130524 train acc 0.9836772157334249\n",
            "epoch 48 batch id 2921 loss 0.0023321183398365974 train acc 0.983658207805546\n",
            "epoch 48 batch id 2931 loss 0.037782054394483566 train acc 0.9836499914704879\n",
            "epoch 48 batch id 2941 loss 0.000516039552167058 train acc 0.9836418310098606\n",
            "epoch 48 batch id 2951 loss 0.029717646539211273 train acc 0.9836813791934937\n",
            "epoch 48 batch id 2961 loss 0.024918409064412117 train acc 0.9837101063829787\n",
            "epoch 48 batch id 2971 loss 0.016582714393734932 train acc 0.983707085156513\n",
            "epoch 48 batch id 2981 loss 0.011439078487455845 train acc 0.9837040841999329\n",
            "epoch 48 batch id 2991 loss 0.006947071757167578 train acc 0.9837063273152792\n",
            "epoch 48 batch id 3001 loss 0.006179389543831348 train acc 0.9837450016661113\n",
            "epoch 48 batch id 3011 loss 0.0052898055873811245 train acc 0.983757472600465\n",
            "epoch 48 batch id 3021 loss 0.03564709052443504 train acc 0.9837543445878848\n",
            "epoch 48 batch id 3031 loss 0.1861678957939148 train acc 0.9837254618937644\n",
            "epoch 48 batch id 3041 loss 0.07072751224040985 train acc 0.9836967691548832\n",
            "epoch 48 batch id 3051 loss 0.05931146815419197 train acc 0.983709234677155\n",
            "epoch 48 batch id 3061 loss 0.08039140701293945 train acc 0.9836960960470434\n",
            "epoch 48 batch id 3071 loss 0.0002775078755803406 train acc 0.9837186584174537\n",
            "epoch 48 batch id 3081 loss 0.03018638864159584 train acc 0.9837309315157416\n",
            "epoch 48 batch id 3091 loss 0.015198256820440292 train acc 0.9837582901973472\n",
            "epoch 48 batch id 3101 loss 0.06403841078281403 train acc 0.9837502015478877\n",
            "epoch 48 batch id 3111 loss 0.04374018311500549 train acc 0.9837522099003536\n",
            "epoch 48 batch id 3121 loss 0.024561677128076553 train acc 0.9837742310157\n",
            "epoch 48 batch id 3131 loss 0.009500004351139069 train acc 0.9837811402107953\n",
            "epoch 48 batch id 3141 loss 0.17170584201812744 train acc 0.9837681072906718\n",
            "epoch 48 batch id 3151 loss 0.0023787827230989933 train acc 0.9837749920660108\n",
            "epoch 48 batch id 3161 loss 0.051434874534606934 train acc 0.9837620610566277\n",
            "epoch 48 batch id 3171 loss 0.01428557001054287 train acc 0.9837689214758751\n",
            "epoch 48 batch id 3181 loss 0.035203538835048676 train acc 0.9837413549198365\n",
            "epoch 48 batch id 3191 loss 0.018120557069778442 train acc 0.9837580303979944\n",
            "epoch 48 batch id 3201 loss 0.15348303318023682 train acc 0.9837355513901905\n",
            "epoch 48 batch id 3211 loss 0.1225908100605011 train acc 0.9836791497975709\n",
            "epoch 48 batch id 3221 loss 0.06633039563894272 train acc 0.9836230984166408\n",
            "epoch 48 batch id 3231 loss 0.03805052116513252 train acc 0.983615753636645\n",
            "epoch 48 batch id 3241 loss 0.04773209989070892 train acc 0.9836132752236963\n",
            "epoch 48 batch id 3251 loss 0.043040681630373 train acc 0.98359639341741\n",
            "epoch 48 batch id 3261 loss 0.0974116399884224 train acc 0.9835652407237044\n",
            "epoch 48 batch id 3271 loss 0.034709859639406204 train acc 0.9835533858147355\n",
            "epoch 48 batch id 3281 loss 0.005941798444837332 train acc 0.983565414507772\n",
            "epoch 48 batch id 3291 loss 0.009741467423737049 train acc 0.9835726223032513\n",
            "epoch 48 batch id 3301 loss 0.00838503334671259 train acc 0.9835892532565889\n",
            "epoch 48 batch id 3311 loss 0.020404543727636337 train acc 0.9835869072787677\n",
            "epoch 48 batch id 3321 loss 0.007897065952420235 train acc 0.983593985245408\n",
            "epoch 48 batch id 3331 loss 0.0171891488134861 train acc 0.9835494220954668\n",
            "epoch 48 batch id 3341 loss 0.0221567265689373 train acc 0.9835612466327447\n",
            "epoch 48 batch id 3351 loss 0.014626320451498032 train acc 0.9835356982990152\n",
            "epoch 48 batch id 3361 loss 0.04783080145716667 train acc 0.9835474933055638\n",
            "epoch 48 batch id 3371 loss 0.04390691593289375 train acc 0.9835453129635123\n",
            "epoch 48 batch id 3381 loss 9.504785703029484e-05 train acc 0.9835616311742088\n",
            "epoch 48 batch id 3391 loss 0.07657286524772644 train acc 0.9835686375700383\n",
            "epoch 48 batch id 3401 loss 0.0807122215628624 train acc 0.9835847912378712\n",
            "epoch 48 batch id 3411 loss 0.029864097014069557 train acc 0.9836100117267663\n",
            "epoch 48 batch id 3421 loss 0.009509619325399399 train acc 0.9836259500146156\n",
            "epoch 48 batch id 3431 loss 0.1498890221118927 train acc 0.9836144709997086\n",
            "epoch 48 batch id 3441 loss 0.000362437276635319 train acc 0.9836166811973264\n",
            "epoch 48 batch id 3451 loss 0.014341054484248161 train acc 0.9836279339321936\n",
            "epoch 48 batch id 3461 loss 0.011355623602867126 train acc 0.9836346070499855\n",
            "epoch 48 batch id 3471 loss 0.14801256358623505 train acc 0.9836277369634111\n",
            "epoch 48 batch id 3481 loss 0.16162854433059692 train acc 0.9836209063487503\n",
            "epoch 48 batch id 3491 loss 0.007593766786158085 train acc 0.9836185906617015\n",
            "epoch 48 batch id 3501 loss 0.015458967536687851 train acc 0.9836341402456441\n",
            "epoch 48 batch id 3511 loss 0.10758376121520996 train acc 0.9836139988607234\n",
            "epoch 48 batch id 3521 loss 0.09957661479711533 train acc 0.9836205978415223\n",
            "epoch 48 batch id 3531 loss 0.015362592414021492 train acc 0.9836448598130841\n",
            "epoch 48 batch id 3541 loss 0.04196035861968994 train acc 0.9836425091781983\n",
            "epoch 48 batch id 3551 loss 0.027679000049829483 train acc 0.983609370599831\n",
            "epoch 48 batch id 3561 loss 0.011919093318283558 train acc 0.9836071328278574\n",
            "epoch 48 batch id 3571 loss 0.013452533632516861 train acc 0.9836224096891627\n",
            "epoch 48 batch id 3581 loss 0.041419729590415955 train acc 0.9836201480033511\n",
            "epoch 48 batch id 3591 loss 0.05966891348361969 train acc 0.9836222500696185\n",
            "epoch 48 batch id 3601 loss 0.09267769008874893 train acc 0.9836156623160234\n",
            "epoch 48 batch id 3611 loss 0.017175422981381416 train acc 0.9836177651620049\n",
            "epoch 48 batch id 3621 loss 0.05002838373184204 train acc 0.9836112261806131\n",
            "epoch 48 batch id 3631 loss 0.009304807521402836 train acc 0.9836047232167447\n",
            "epoch 48 batch id 3641 loss 0.007533099502325058 train acc 0.9836240043943971\n",
            "epoch 48 batch id 3651 loss 0.05619797110557556 train acc 0.9836046631059984\n",
            "epoch 48 batch id 3661 loss 0.002405791077762842 train acc 0.9835896954384048\n",
            "epoch 48 batch id 3671 loss 0.04018928110599518 train acc 0.9836088599836557\n",
            "epoch 48 batch id 3681 loss 0.0016915916930884123 train acc 0.9836236756316219\n",
            "epoch 48 batch id 3691 loss 0.0027270540595054626 train acc 0.9836341777296126\n",
            "epoch 48 batch id 3701 loss 0.030215313658118248 train acc 0.9836235139151581\n",
            "epoch 48 batch id 3711 loss 0.02094978466629982 train acc 0.9836297493936944\n",
            "epoch 48 batch id 3721 loss 0.07387184351682663 train acc 0.9836275530771298\n",
            "epoch 48 batch id 3731 loss 0.025293773040175438 train acc 0.983633744304476\n",
            "epoch 48 batch id 3741 loss 0.13750404119491577 train acc 0.9836106655974338\n",
            "epoch 48 batch id 3751 loss 0.0011647409992292523 train acc 0.9836168688349773\n",
            "epoch 48 batch id 3761 loss 0.08241425454616547 train acc 0.9835939577240096\n",
            "epoch 48 batch id 3771 loss 0.042360760271549225 train acc 0.983575311588438\n",
            "epoch 48 batch id 3781 loss 0.08129709213972092 train acc 0.9835815591113463\n",
            "epoch 48 batch id 3791 loss 0.0005220006569288671 train acc 0.9835795304668953\n",
            "epoch 48 batch id 3801 loss 0.07239103317260742 train acc 0.9835734017363852\n",
            "epoch 48 batch id 3811 loss 0.0678577646613121 train acc 0.9835755051167673\n",
            "epoch 48 batch id 3821 loss 0.02085127867758274 train acc 0.9835735082439152\n",
            "epoch 48 batch id 3831 loss 0.0025830846279859543 train acc 0.9835756003654398\n",
            "epoch 48 batch id 3841 loss 0.0003386498719919473 train acc 0.9835736136422807\n",
            "epoch 48 batch id 3851 loss 0.001428430201485753 train acc 0.9835675798493898\n",
            "epoch 48 batch id 3861 loss 0.05388234183192253 train acc 0.9835656241906242\n",
            "epoch 48 batch id 3871 loss 0.08031434565782547 train acc 0.9835717514854043\n",
            "epoch 48 batch id 3881 loss 0.04127409681677818 train acc 0.9835738211801082\n",
            "epoch 48 batch id 3891 loss 0.011101300828158855 train acc 0.983575880236443\n",
            "epoch 48 batch id 3901 loss 0.00897226296365261 train acc 0.9835659125865163\n",
            "epoch 48 batch id 3911 loss 0.02976609580218792 train acc 0.9835400153413449\n",
            "epoch 48 batch id 3921 loss 0.01051302719861269 train acc 0.9835381599081867\n",
            "epoch 48 batch id 3931 loss 0.05083785578608513 train acc 0.9835243894683287\n",
            "epoch 48 batch id 3941 loss 0.0009509793017059565 train acc 0.9835503362090839\n",
            "epoch 48 batch id 3951 loss 0.09138647466897964 train acc 0.9835366046570488\n",
            "epoch 48 batch id 3961 loss 0.0011770563432946801 train acc 0.9835505554152991\n",
            "epoch 48 batch id 3971 loss 0.08377357572317123 train acc 0.9835408272475447\n",
            "epoch 48 batch id 3981 loss 0.024213815107941628 train acc 0.9835350728460186\n",
            "epoch 48 batch id 3991 loss 0.18608911335468292 train acc 0.9835332623402656\n",
            "epoch 48 batch id 4001 loss 0.06730538606643677 train acc 0.9835314608847788\n",
            "epoch 48 batch id 4011 loss 0.05155903100967407 train acc 0.9835062951882324\n",
            "epoch 48 batch id 4021 loss 0.01660248637199402 train acc 0.9835123414573489\n",
            "epoch 48 batch id 4031 loss 0.0004397909215185791 train acc 0.9835338625651203\n",
            "epoch 48 batch id 4041 loss 0.03309657424688339 train acc 0.9835243442217273\n",
            "epoch 48 batch id 4051 loss 0.06813841313123703 train acc 0.9835033016539126\n",
            "epoch 48 batch id 4061 loss 0.016140267252922058 train acc 0.9835131433144546\n",
            "epoch 48 batch id 4071 loss 0.11172032356262207 train acc 0.9835152603782854\n",
            "epoch 48 batch id 4081 loss 0.007610511966049671 train acc 0.9835326819407008\n",
            "epoch 48 batch id 4091 loss 0.0294982697814703 train acc 0.9835385602542166\n",
            "epoch 48 batch id 4101 loss 0.0005504166474565864 train acc 0.9835444099000243\n",
            "epoch 48 batch id 4111 loss 0.019403792917728424 train acc 0.9835236256385308\n",
            "epoch 48 batch id 4121 loss 0.00901064369827509 train acc 0.9835256915797137\n",
            "epoch 48 batch id 4131 loss 0.06956112384796143 train acc 0.983531529895909\n",
            "epoch 48 batch id 4141 loss 0.001418379251845181 train acc 0.9835373400144892\n",
            "epoch 48 batch id 4151 loss 0.05157872661948204 train acc 0.9835355938328114\n",
            "epoch 48 batch id 4161 loss 0.021762166172266006 train acc 0.983507570295602\n",
            "epoch 48 batch id 4171 loss 0.11868533492088318 train acc 0.9835059038599856\n",
            "epoch 48 batch id 4181 loss 0.01176717784255743 train acc 0.9835042453958384\n",
            "epoch 48 batch id 4191 loss 0.07294218987226486 train acc 0.9835025948460988\n",
            "epoch 48 batch id 4201 loss 0.020527711138129234 train acc 0.9835307069745298\n",
            "epoch 48 batch id 4211 loss 0.05482294782996178 train acc 0.9835215803847067\n",
            "epoch 48 batch id 4221 loss 0.09452985227108002 train acc 0.9835199004975125\n",
            "epoch 48 batch id 4231 loss 0.10376910865306854 train acc 0.9835108425904041\n",
            "epoch 48 batch id 4241 loss 0.12103579193353653 train acc 0.9835202487620844\n",
            "epoch 48 batch id 4251 loss 0.11466208845376968 train acc 0.98352961067984\n",
            "epoch 48 batch id 4261 loss 0.0010053280275315046 train acc 0.9835462626144098\n",
            "epoch 48 batch id 4271 loss 0.0023053879849612713 train acc 0.983551861390775\n",
            "epoch 48 batch id 4281 loss 0.09290799498558044 train acc 0.9835574340107451\n",
            "epoch 48 batch id 4291 loss 0.010833073407411575 train acc 0.9835265672337451\n",
            "epoch 48 batch id 4301 loss 0.019166547805070877 train acc 0.9835140083701465\n",
            "epoch 48 batch id 4311 loss 0.03708808869123459 train acc 0.9835305033634888\n",
            "epoch 48 batch id 4321 loss 0.027751749381422997 train acc 0.9835179935200186\n",
            "epoch 48 batch id 4331 loss 0.07351630181074142 train acc 0.9835091491572385\n",
            "epoch 48 batch id 4341 loss 0.08713514357805252 train acc 0.9835183425478\n",
            "epoch 48 batch id 4351 loss 0.03512808680534363 train acc 0.9834951735233279\n",
            "epoch 48 batch id 4361 loss 0.027879977598786354 train acc 0.983497191011236\n",
            "epoch 48 batch id 4371 loss 0.046432994306087494 train acc 0.9835063486616334\n",
            "epoch 48 batch id 4381 loss 0.09938599914312363 train acc 0.9835011983565396\n",
            "epoch 48 batch id 4391 loss 9.631102147977799e-05 train acc 0.9834960715099066\n",
            "epoch 48 batch id 4401 loss 0.05839356780052185 train acc 0.9834838673028857\n",
            "epoch 48 batch id 4411 loss 0.008291330188512802 train acc 0.9834681761505327\n",
            "epoch 48 batch id 4421 loss 0.01778058148920536 train acc 0.9834666930558698\n",
            "epoch 48 batch id 4431 loss 0.02640199288725853 train acc 0.983451111487249\n",
            "epoch 48 batch id 4441 loss 0.010788729414343834 train acc 0.9834602285521279\n",
            "epoch 48 batch id 4451 loss 0.059680815786123276 train acc 0.9834587733093687\n",
            "epoch 48 batch id 4461 loss 0.00018164351058658212 train acc 0.9834538220130016\n",
            "epoch 48 batch id 4471 loss 0.06329802423715591 train acc 0.9834488928651308\n",
            "epoch 48 batch id 4481 loss 0.011108177714049816 train acc 0.9834404987725954\n",
            "epoch 48 batch id 4491 loss 0.056310735642910004 train acc 0.9834425796036518\n",
            "epoch 48 batch id 4501 loss 0.009488355368375778 train acc 0.9834342368362586\n",
            "epoch 48 batch id 4511 loss 0.07120928168296814 train acc 0.9834293948126801\n",
            "epoch 48 batch id 4521 loss 0.018113750964403152 train acc 0.9834349424905994\n",
            "epoch 48 batch id 4531 loss 0.00017210394435096532 train acc 0.9834404656808652\n",
            "epoch 48 batch id 4541 loss 0.054967060685157776 train acc 0.9834562871614182\n",
            "epoch 48 batch id 4551 loss 0.0888688713312149 train acc 0.9834548725554824\n",
            "epoch 48 batch id 4561 loss 0.10247455537319183 train acc 0.9834363352335014\n",
            "epoch 48 batch id 4571 loss 0.028869733214378357 train acc 0.9834418070444104\n",
            "epoch 48 batch id 4581 loss 0.026916591450572014 train acc 0.983440433311504\n",
            "epoch 48 batch id 4591 loss 0.050726260989904404 train acc 0.9834492757569157\n",
            "epoch 48 batch id 4601 loss 0.012066710740327835 train acc 0.9834343077591828\n",
            "epoch 48 batch id 4611 loss 0.04989951103925705 train acc 0.9834329592279332\n",
            "epoch 48 batch id 4621 loss 0.00906368438154459 train acc 0.9834383791387146\n",
            "epoch 48 batch id 4631 loss 0.07798047363758087 train acc 0.9834404016411142\n",
            "epoch 48 batch id 4641 loss 0.11191753298044205 train acc 0.9834457821590175\n",
            "epoch 48 batch id 4651 loss 0.12316704541444778 train acc 0.983451139539884\n",
            "epoch 48 batch id 4661 loss 0.08093622326850891 train acc 0.9834564739326325\n",
            "epoch 48 batch id 4671 loss 0.005383373703807592 train acc 0.9834684757011347\n",
            "epoch 48 batch id 4681 loss 0.11702582240104675 train acc 0.9834570604571673\n",
            "epoch 48 batch id 4691 loss 0.013733464293181896 train acc 0.983449024728203\n",
            "epoch 48 batch id 4701 loss 0.05307961627840996 train acc 0.9834543182301638\n",
            "epoch 48 batch id 4711 loss 0.04547281190752983 train acc 0.9834629059647633\n",
            "epoch 48 batch id 4721 loss 0.000988593208603561 train acc 0.9834582185977547\n",
            "epoch 48 batch id 4731 loss 0.046057749539613724 train acc 0.9834535510462904\n",
            "epoch 48 batch id 4741 loss 0.048130739480257034 train acc 0.9834554946213879\n",
            "epoch 48 batch id 4751 loss 0.1547764241695404 train acc 0.9834541412334246\n",
            "epoch 48 batch id 4761 loss 0.06524670869112015 train acc 0.9834462297836589\n",
            "epoch 48 batch id 4771 loss 0.14775577187538147 train acc 0.9834514514776777\n",
            "epoch 48 batch id 4781 loss 0.07339802384376526 train acc 0.9834207017360385\n",
            "epoch 48 batch id 4791 loss 0.11065112054347992 train acc 0.9833933416823211\n",
            "epoch 48 batch id 4801 loss 0.02266312576830387 train acc 0.9833823682566132\n",
            "epoch 48 batch id 4811 loss 0.07391209900379181 train acc 0.983390927042195\n",
            "epoch 48 batch id 4821 loss 0.0532100684940815 train acc 0.9833897272350135\n",
            "epoch 48 batch id 4831 loss 0.020493553951382637 train acc 0.9833982353549989\n",
            "epoch 48 batch id 4841 loss 0.0017449886072427034 train acc 0.9834002530468912\n",
            "epoch 48 batch id 4851 loss 0.0741196945309639 train acc 0.9834054834054834\n",
            "epoch 48 batch id 4861 loss 0.0015511352103203535 train acc 0.9834010491668381\n",
            "epoch 48 batch id 4871 loss 0.024911627173423767 train acc 0.9834062564155204\n",
            "epoch 48 batch id 4881 loss 0.031782880425453186 train acc 0.9834210458922352\n",
            "epoch 48 batch id 4891 loss 0.015132218599319458 train acc 0.983422996319771\n",
            "epoch 48 batch id 4901 loss 0.08886934816837311 train acc 0.9834249387880024\n",
            "epoch 48 batch id 4911 loss 0.20499779284000397 train acc 0.9834077835471391\n",
            "epoch 48 batch id 4921 loss 0.15898926556110382 train acc 0.9833970483641536\n",
            "epoch 48 batch id 4931 loss 0.05181552469730377 train acc 0.9833831879943217\n",
            "epoch 48 batch id 4941 loss 0.19937726855278015 train acc 0.9833693837279903\n",
            "epoch 48 batch id 4951 loss 0.07831820100545883 train acc 0.9833587911533024\n",
            "epoch 48 train acc 0.9833554362976141\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a3400dcee3a4bc38d1e7a440ca59615",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 48 loss 0.5597223043441772 test acc 0.852850073313783\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6d67c269b62498b9a8d7ca7bf0f40bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 49 batch id 1 loss 0.041218049824237823 train acc 0.984375\n",
            "epoch 49 batch id 11 loss 0.018621809780597687 train acc 0.9872159090909091\n",
            "epoch 49 batch id 21 loss 0.005124847404658794 train acc 0.9895833333333334\n",
            "epoch 49 batch id 31 loss 0.10556504875421524 train acc 0.9889112903225806\n",
            "epoch 49 batch id 41 loss 0.2013518363237381 train acc 0.9889481707317073\n",
            "epoch 49 batch id 51 loss 0.05650254711508751 train acc 0.9868259803921569\n",
            "epoch 49 batch id 61 loss 0.04472758620977402 train acc 0.9841188524590164\n",
            "epoch 49 batch id 71 loss 0.08351144939661026 train acc 0.9848151408450704\n",
            "epoch 49 batch id 81 loss 0.006406895816326141 train acc 0.9851466049382716\n",
            "epoch 49 batch id 91 loss 0.06846554577350616 train acc 0.9838598901098901\n",
            "epoch 49 batch id 101 loss 0.04553350806236267 train acc 0.9828279702970297\n",
            "epoch 49 batch id 111 loss 9.354341455036774e-05 train acc 0.9829673423423423\n",
            "epoch 49 batch id 121 loss 0.09862931072711945 train acc 0.9837293388429752\n",
            "epoch 49 batch id 131 loss 0.017992239445447922 train acc 0.9835400763358778\n",
            "epoch 49 batch id 141 loss 0.007209025789052248 train acc 0.9840425531914894\n",
            "epoch 49 batch id 151 loss 0.08612039685249329 train acc 0.984271523178808\n",
            "epoch 49 batch id 161 loss 0.00027242276701144874 train acc 0.984375\n",
            "epoch 49 batch id 171 loss 0.02268512174487114 train acc 0.9845577485380117\n",
            "epoch 49 batch id 181 loss 0.016239192336797714 train acc 0.9847203038674033\n",
            "epoch 49 batch id 191 loss 0.053909529000520706 train acc 0.9845386125654451\n",
            "epoch 49 batch id 201 loss 0.01822328194975853 train acc 0.9846082089552238\n",
            "epoch 49 batch id 211 loss 0.040176376700401306 train acc 0.9849674170616114\n",
            "epoch 49 batch id 221 loss 0.0575176477432251 train acc 0.9846578054298643\n",
            "epoch 49 batch id 231 loss 0.03659547492861748 train acc 0.9849837662337663\n",
            "epoch 49 batch id 241 loss 0.08127963542938232 train acc 0.9851530082987552\n",
            "epoch 49 batch id 251 loss 0.10987551510334015 train acc 0.9852465139442231\n",
            "epoch 49 batch id 261 loss 0.04597962275147438 train acc 0.9853328544061303\n",
            "epoch 49 batch id 271 loss 0.00038860909990035 train acc 0.9855281365313653\n",
            "epoch 49 batch id 281 loss 0.012417955324053764 train acc 0.9858207295373665\n",
            "epoch 49 batch id 291 loss 0.06528282165527344 train acc 0.9858784364261168\n",
            "epoch 49 batch id 301 loss 0.08792804926633835 train acc 0.9856208471760798\n",
            "epoch 49 batch id 311 loss 0.10022453963756561 train acc 0.985681270096463\n",
            "epoch 49 batch id 321 loss 0.1386161893606186 train acc 0.9856405763239875\n",
            "epoch 49 batch id 331 loss 0.04011428356170654 train acc 0.9853663141993958\n",
            "epoch 49 batch id 341 loss 0.1548321396112442 train acc 0.9853372434017595\n",
            "epoch 49 batch id 351 loss 0.006346403155475855 train acc 0.9854878917378918\n",
            "epoch 49 batch id 361 loss 0.028007807210087776 train acc 0.9855436288088643\n",
            "epoch 49 batch id 371 loss 0.020449765026569366 train acc 0.9856805929919138\n",
            "epoch 49 batch id 381 loss 0.01958242803812027 train acc 0.9858513779527559\n",
            "epoch 49 batch id 391 loss 0.10483059287071228 train acc 0.9858535805626598\n",
            "epoch 49 batch id 401 loss 0.0019383544567972422 train acc 0.98589463840399\n",
            "epoch 49 batch id 411 loss 0.0001772449613781646 train acc 0.9859717153284672\n",
            "epoch 49 batch id 421 loss 0.011792302131652832 train acc 0.9861193586698337\n",
            "epoch 49 batch id 431 loss 0.05444745346903801 train acc 0.9861151392111369\n",
            "epoch 49 batch id 441 loss 0.08532267808914185 train acc 0.985969387755102\n",
            "epoch 49 batch id 451 loss 0.02673153579235077 train acc 0.9860726164079823\n",
            "epoch 49 batch id 461 loss 0.003244043793529272 train acc 0.9858324295010846\n",
            "epoch 49 batch id 471 loss 0.01803020015358925 train acc 0.9860005307855626\n",
            "epoch 49 batch id 481 loss 0.14909452199935913 train acc 0.985966735966736\n",
            "epoch 49 batch id 491 loss 0.0029652256052941084 train acc 0.9859024949083504\n",
            "epoch 49 batch id 501 loss 0.17756615579128265 train acc 0.9858408183632734\n",
            "epoch 49 batch id 511 loss 0.08637940883636475 train acc 0.9857509784735812\n",
            "epoch 49 batch id 521 loss 0.09452786296606064 train acc 0.9857245681381958\n",
            "epoch 49 batch id 531 loss 0.09259268641471863 train acc 0.9854931732580038\n",
            "epoch 49 batch id 541 loss 0.005311627872288227 train acc 0.9854147412199631\n",
            "epoch 49 batch id 551 loss 0.040978334844112396 train acc 0.9853675136116152\n",
            "epoch 49 batch id 561 loss 0.07832114398479462 train acc 0.9852105614973262\n",
            "epoch 49 batch id 571 loss 0.03569139912724495 train acc 0.9852780210157618\n",
            "epoch 49 batch id 581 loss 0.05119466036558151 train acc 0.9852624784853701\n",
            "epoch 49 batch id 591 loss 0.02309330739080906 train acc 0.9853267766497462\n",
            "epoch 49 batch id 601 loss 0.008017784915864468 train acc 0.9851549500831946\n",
            "epoch 49 batch id 611 loss 0.06507261842489243 train acc 0.9850398936170213\n",
            "epoch 49 batch id 621 loss 0.13504599034786224 train acc 0.9851298309178744\n",
            "epoch 49 batch id 631 loss 0.04385383427143097 train acc 0.9851178684627575\n",
            "epoch 49 batch id 641 loss 0.05538035184144974 train acc 0.985155031201248\n",
            "epoch 49 batch id 651 loss 0.05268952623009682 train acc 0.9849750384024577\n",
            "epoch 49 batch id 661 loss 0.00019375358533579856 train acc 0.9850132375189108\n",
            "epoch 49 batch id 671 loss 0.00876036286354065 train acc 0.9850037257824144\n",
            "epoch 49 batch id 681 loss 0.04693985357880592 train acc 0.9850174375917768\n",
            "epoch 49 batch id 691 loss 0.026639480143785477 train acc 0.9850081403762663\n",
            "epoch 49 batch id 701 loss 0.03154480829834938 train acc 0.9848653708987162\n",
            "epoch 49 batch id 711 loss 0.10237307846546173 train acc 0.9849024261603375\n",
            "epoch 49 batch id 721 loss 0.04428276792168617 train acc 0.9847650832177531\n",
            "epoch 49 batch id 731 loss 0.01731211692094803 train acc 0.984781121751026\n",
            "epoch 49 batch id 741 loss 0.0012960226740688086 train acc 0.9847334682860999\n",
            "epoch 49 batch id 751 loss 0.014248554594814777 train acc 0.9845414447403462\n",
            "epoch 49 batch id 761 loss 0.0049637057818472385 train acc 0.9844981931668857\n",
            "epoch 49 batch id 771 loss 0.05705290287733078 train acc 0.9843952658884566\n",
            "epoch 49 batch id 781 loss 0.09783864766359329 train acc 0.9843950064020487\n",
            "epoch 49 batch id 791 loss 0.011912891641259193 train acc 0.9844737673830595\n",
            "epoch 49 batch id 801 loss 0.0017830391880124807 train acc 0.984414013732834\n",
            "epoch 49 batch id 811 loss 0.0953911617398262 train acc 0.984413532675709\n",
            "epoch 49 batch id 821 loss 0.008129398338496685 train acc 0.9844891900121803\n",
            "epoch 49 batch id 831 loss 0.1196093037724495 train acc 0.9845066185318893\n",
            "epoch 49 batch id 841 loss 0.001338431378826499 train acc 0.984560790725327\n",
            "epoch 49 batch id 851 loss 0.03347811847925186 train acc 0.9845035252643948\n",
            "epoch 49 batch id 861 loss 0.027687666937708855 train acc 0.9845020325203252\n",
            "epoch 49 batch id 871 loss 0.013653469271957874 train acc 0.9844288174512055\n",
            "epoch 49 batch id 881 loss 0.022490859031677246 train acc 0.9844104710556186\n",
            "epoch 49 batch id 891 loss 0.017558610066771507 train acc 0.984375\n",
            "epoch 49 batch id 901 loss 0.021076440811157227 train acc 0.9843576581576027\n",
            "epoch 49 batch id 911 loss 0.016460059210658073 train acc 0.9843921514818881\n",
            "epoch 49 batch id 921 loss 0.00020995124941691756 train acc 0.9843071389793703\n",
            "epoch 49 batch id 931 loss 0.013369753956794739 train acc 0.9842743018259935\n",
            "epoch 49 batch id 941 loss 0.009116623550653458 train acc 0.9843583953241233\n",
            "epoch 49 batch id 951 loss 0.041685495525598526 train acc 0.984309279705573\n",
            "epoch 49 batch id 961 loss 0.01944374106824398 train acc 0.9843262226847035\n",
            "epoch 49 batch id 971 loss 0.019179007038474083 train acc 0.9843267250257467\n",
            "epoch 49 batch id 981 loss 0.21894043684005737 train acc 0.9843431447502549\n",
            "epoch 49 batch id 991 loss 0.070343516767025 train acc 0.984359233097881\n",
            "epoch 49 batch id 1001 loss 0.013930094428360462 train acc 0.9844374375624375\n",
            "epoch 49 batch id 1011 loss 0.00024288603162858635 train acc 0.9844213649851632\n",
            "epoch 49 batch id 1021 loss 0.07641061395406723 train acc 0.9844056072477962\n",
            "epoch 49 batch id 1031 loss 0.10426453500986099 train acc 0.984435620756547\n",
            "epoch 49 batch id 1041 loss 0.015900302678346634 train acc 0.9844650576368876\n",
            "epoch 49 batch id 1051 loss 0.013414980843663216 train acc 0.9844047335870599\n",
            "epoch 49 batch id 1061 loss 0.015069389715790749 train acc 0.9842866399622997\n",
            "epoch 49 batch id 1071 loss 0.007431549485772848 train acc 0.9843020541549953\n",
            "epoch 49 batch id 1081 loss 0.026704663410782814 train acc 0.9842304579093432\n",
            "epoch 49 batch id 1091 loss 0.07463209331035614 train acc 0.9842174610449129\n",
            "epoch 49 batch id 1101 loss 0.031239286065101624 train acc 0.9842472752043597\n",
            "epoch 49 batch id 1111 loss 0.04952411726117134 train acc 0.9842343609360936\n",
            "epoch 49 batch id 1121 loss 0.003782465821132064 train acc 0.9843053077609277\n",
            "epoch 49 batch id 1131 loss 0.11766427010297775 train acc 0.9842368479221928\n",
            "epoch 49 batch id 1141 loss 0.0348174050450325 train acc 0.9842517528483786\n",
            "epoch 49 batch id 1151 loss 0.061407629400491714 train acc 0.9841170721112077\n",
            "epoch 49 batch id 1161 loss 0.059568554162979126 train acc 0.9841058354866494\n",
            "epoch 49 batch id 1171 loss 0.010534780099987984 train acc 0.9840547608881298\n",
            "epoch 49 batch id 1181 loss 0.0002959718112833798 train acc 0.9841103937341237\n",
            "epoch 49 batch id 1191 loss 0.012215768918395042 train acc 0.9841257346767422\n",
            "epoch 49 batch id 1201 loss 0.01869780942797661 train acc 0.9841278101582015\n",
            "epoch 49 batch id 1211 loss 0.0003134107682853937 train acc 0.9841427539223782\n",
            "epoch 49 batch id 1221 loss 0.08513753116130829 train acc 0.9841190622440622\n",
            "epoch 49 batch id 1231 loss 0.054625846445560455 train acc 0.9840195978878961\n",
            "epoch 49 batch id 1241 loss 0.03326214849948883 train acc 0.9839846897663175\n",
            "epoch 49 batch id 1251 loss 0.08265458047389984 train acc 0.9839878097521982\n",
            "epoch 49 batch id 1261 loss 0.01821024902164936 train acc 0.983978489294211\n",
            "epoch 49 batch id 1271 loss 0.023347459733486176 train acc 0.9840061959087333\n",
            "epoch 49 batch id 1281 loss 0.0003929657978005707 train acc 0.9840212724434035\n",
            "epoch 49 batch id 1291 loss 0.09628859162330627 train acc 0.9839998063516654\n",
            "epoch 49 batch id 1301 loss 0.0011796940816566348 train acc 0.9840267102229054\n",
            "epoch 49 batch id 1311 loss 0.014425115659832954 train acc 0.9840412852784134\n",
            "epoch 49 batch id 1321 loss 0.021779142320156097 train acc 0.98403198334595\n",
            "epoch 49 batch id 1331 loss 0.028374599292874336 train acc 0.9840228211870774\n",
            "epoch 49 batch id 1341 loss 0.17017042636871338 train acc 0.984025447427293\n",
            "epoch 49 batch id 1351 loss 0.00426253629848361 train acc 0.9840742968171725\n",
            "epoch 49 batch id 1361 loss 0.0012773938942700624 train acc 0.9840420646583394\n",
            "epoch 49 batch id 1371 loss 0.00590104702860117 train acc 0.9840330962800875\n",
            "epoch 49 batch id 1381 loss 0.031098948791623116 train acc 0.9840468863142651\n",
            "epoch 49 batch id 1391 loss 0.010170693509280682 train acc 0.9839818475916606\n",
            "epoch 49 batch id 1401 loss 0.09693288803100586 train acc 0.9839511955745895\n",
            "epoch 49 batch id 1411 loss 0.017865769565105438 train acc 0.9839763465627215\n",
            "epoch 49 batch id 1421 loss 0.009467658586800098 train acc 0.9839571604503871\n",
            "epoch 49 batch id 1431 loss 0.09167838841676712 train acc 0.983970999301188\n",
            "epoch 49 batch id 1441 loss 0.022155342623591423 train acc 0.9839304302567662\n",
            "epoch 49 batch id 1451 loss 0.08423390239477158 train acc 0.9839765678842178\n",
            "epoch 49 batch id 1461 loss 0.011885984800755978 train acc 0.9839899897330595\n",
            "epoch 49 batch id 1471 loss 0.1440543234348297 train acc 0.9840350951733514\n",
            "epoch 49 batch id 1481 loss 0.08356388658285141 train acc 0.98403739027684\n",
            "epoch 49 batch id 1491 loss 0.03411704674363136 train acc 0.9840291750503019\n",
            "epoch 49 batch id 1501 loss 0.12124188244342804 train acc 0.984062708194537\n",
            "epoch 49 batch id 1511 loss 0.02947128191590309 train acc 0.9840854566512244\n",
            "epoch 49 batch id 1521 loss 0.010289850644767284 train acc 0.984077087442472\n",
            "epoch 49 batch id 1531 loss 0.0141682680696249 train acc 0.9840892390594382\n",
            "epoch 49 batch id 1541 loss 0.013255656696856022 train acc 0.9840708144062297\n",
            "epoch 49 batch id 1551 loss 0.03718152642250061 train acc 0.9840929239200515\n",
            "epoch 49 batch id 1561 loss 0.026430733501911163 train acc 0.9841047405509289\n",
            "epoch 49 batch id 1571 loss 0.009007764980196953 train acc 0.9840766231699555\n",
            "epoch 49 batch id 1581 loss 0.025664038956165314 train acc 0.9840686274509803\n",
            "epoch 49 batch id 1591 loss 0.07001928240060806 train acc 0.9840803739786298\n",
            "epoch 49 batch id 1601 loss 0.05883046239614487 train acc 0.9841407713928795\n",
            "epoch 49 batch id 1611 loss 0.06365319341421127 train acc 0.9840937306021105\n",
            "epoch 49 batch id 1621 loss 0.042032599449157715 train acc 0.9840858266502159\n",
            "epoch 49 batch id 1631 loss 0.00010866628144867718 train acc 0.9840780196198651\n",
            "epoch 49 batch id 1641 loss 0.08434827625751495 train acc 0.9840512644728824\n",
            "epoch 49 batch id 1651 loss 0.034558895975351334 train acc 0.9839964415505754\n",
            "epoch 49 batch id 1661 loss 0.022491278126835823 train acc 0.9840269416014449\n",
            "epoch 49 batch id 1671 loss 0.08252819627523422 train acc 0.9839822710951526\n",
            "epoch 49 batch id 1681 loss 0.012720420956611633 train acc 0.9839660172516359\n",
            "epoch 49 batch id 1691 loss 0.039451517164707184 train acc 0.9839869160260201\n",
            "epoch 49 batch id 1701 loss 0.044434044510126114 train acc 0.9839891975308642\n",
            "epoch 49 batch id 1711 loss 0.0052063656039536 train acc 0.9840371127995324\n",
            "epoch 49 batch id 1721 loss 0.011671870946884155 train acc 0.9840299970947124\n",
            "epoch 49 batch id 1731 loss 0.004873363301157951 train acc 0.9840590699017908\n",
            "epoch 49 batch id 1741 loss 0.0782693400979042 train acc 0.9840249856404365\n",
            "epoch 49 batch id 1751 loss 0.04003287851810455 train acc 0.9840002141633353\n",
            "epoch 49 batch id 1761 loss 0.01653609238564968 train acc 0.9840378336172629\n",
            "epoch 49 batch id 1771 loss 0.029999202117323875 train acc 0.9840662055335968\n",
            "epoch 49 batch id 1781 loss 0.004999912343919277 train acc 0.9840767125210556\n",
            "epoch 49 batch id 1791 loss 0.07949977368116379 train acc 0.9840434812953657\n",
            "epoch 49 batch id 1801 loss 0.08167315274477005 train acc 0.984062673514714\n",
            "epoch 49 batch id 1811 loss 0.033206384629011154 train acc 0.9840385146327996\n",
            "epoch 49 batch id 1821 loss 0.2481270581483841 train acc 0.9839717188358045\n",
            "epoch 49 batch id 1831 loss 0.045963939279317856 train acc 0.9839483205898416\n",
            "epoch 49 batch id 1841 loss 0.0003345694567542523 train acc 0.9839251765344921\n",
            "epoch 49 batch id 1851 loss 0.022418402135372162 train acc 0.9839698136142626\n",
            "epoch 49 batch id 1861 loss 0.16078370809555054 train acc 0.9839468027941967\n",
            "epoch 49 batch id 1871 loss 0.15430840849876404 train acc 0.9839824959914484\n",
            "epoch 49 batch id 1881 loss 0.021303020417690277 train acc 0.9839347421584264\n",
            "epoch 49 batch id 1891 loss 0.02286895364522934 train acc 0.9839618588048652\n",
            "epoch 49 batch id 1901 loss 0.08881457149982452 train acc 0.9839640320883746\n",
            "epoch 49 batch id 1911 loss 0.028613779693841934 train acc 0.9839253008895866\n",
            "epoch 49 batch id 1921 loss 0.0005930756451562047 train acc 0.9838951067152525\n",
            "epoch 49 batch id 1931 loss 0.0616694875061512 train acc 0.9838814085965821\n",
            "epoch 49 batch id 1941 loss 0.005258064717054367 train acc 0.9839000515198352\n",
            "epoch 49 batch id 1951 loss 0.03703973442316055 train acc 0.9838944771911841\n",
            "epoch 49 batch id 1961 loss 0.04750780388712883 train acc 0.9839208312085671\n",
            "epoch 49 batch id 1971 loss 0.010447069071233273 train acc 0.9839310629122273\n",
            "epoch 49 batch id 1981 loss 0.03933705389499664 train acc 0.9839175290257446\n",
            "epoch 49 batch id 1991 loss 0.10124794393777847 train acc 0.9838884354595681\n",
            "epoch 49 batch id 2001 loss 0.010604329407215118 train acc 0.9839377186406797\n",
            "epoch 49 batch id 2011 loss 0.056215398013591766 train acc 0.9839398930880159\n",
            "epoch 49 batch id 2021 loss 0.040535956621170044 train acc 0.9839033894111826\n",
            "epoch 49 batch id 2031 loss 0.000628533074632287 train acc 0.9839134047267356\n",
            "epoch 49 batch id 2041 loss 0.0002998293493874371 train acc 0.9839156663400294\n",
            "epoch 49 batch id 2051 loss 0.3622840940952301 train acc 0.9838950511945392\n",
            "epoch 49 batch id 2061 loss 0.09722898155450821 train acc 0.9838897986414362\n",
            "epoch 49 batch id 2071 loss 0.09236534684896469 train acc 0.9838619628198938\n",
            "epoch 49 batch id 2081 loss 0.04122110828757286 train acc 0.9838569197501201\n",
            "epoch 49 batch id 2091 loss 0.02798554301261902 train acc 0.9838668699186992\n",
            "epoch 49 batch id 2101 loss 0.001119203050620854 train acc 0.9838915992384579\n",
            "epoch 49 batch id 2111 loss 0.007539310492575169 train acc 0.9839234959734723\n",
            "epoch 49 batch id 2121 loss 0.007260282523930073 train acc 0.9839403583215465\n",
            "epoch 49 batch id 2131 loss 0.04687143862247467 train acc 0.9839790591271703\n",
            "epoch 49 batch id 2141 loss 0.12118130922317505 train acc 0.983966312470808\n",
            "epoch 49 batch id 2151 loss 0.030768031254410744 train acc 0.9839900046490004\n",
            "epoch 49 batch id 2161 loss 0.21535448729991913 train acc 0.9839700948634891\n",
            "epoch 49 batch id 2171 loss 0.0013617882505059242 train acc 0.9839647627821281\n",
            "epoch 49 batch id 2181 loss 0.09589894115924835 train acc 0.98394515130674\n",
            "epoch 49 batch id 2191 loss 0.05586022511124611 train acc 0.983954244637152\n",
            "epoch 49 batch id 2201 loss 0.013108914718031883 train acc 0.9839632553384825\n",
            "epoch 49 batch id 2211 loss 0.00033099172287620604 train acc 0.983965117593849\n",
            "epoch 49 batch id 2221 loss 0.05010390281677246 train acc 0.9839388226024314\n",
            "epoch 49 batch id 2231 loss 0.08470624685287476 train acc 0.9839477812640072\n",
            "epoch 49 batch id 2241 loss 0.031087396666407585 train acc 0.9839636323070058\n",
            "epoch 49 batch id 2251 loss 0.009676501154899597 train acc 0.9839654597956464\n",
            "epoch 49 batch id 2261 loss 0.0004315313999541104 train acc 0.983953449800973\n",
            "epoch 49 batch id 2271 loss 0.013312443159520626 train acc 0.9839759467195068\n",
            "epoch 49 batch id 2281 loss 0.03711514547467232 train acc 0.9839708461201228\n",
            "epoch 49 batch id 2291 loss 0.08225017040967941 train acc 0.9839726102138804\n",
            "epoch 49 batch id 2301 loss 0.02175874076783657 train acc 0.9839811495002173\n",
            "epoch 49 batch id 2311 loss 0.040069498121738434 train acc 0.9840031371700563\n",
            "epoch 49 batch id 2321 loss 0.16518747806549072 train acc 0.9839845433003016\n",
            "epoch 49 batch id 2331 loss 0.15747961401939392 train acc 0.9839661089661089\n",
            "epoch 49 batch id 2341 loss 0.0004331899108365178 train acc 0.9839945536095686\n",
            "epoch 49 batch id 2351 loss 0.019716447219252586 train acc 0.9839762335176521\n",
            "epoch 49 batch id 2361 loss 0.06971763074398041 train acc 0.984011012282931\n",
            "epoch 49 batch id 2371 loss 0.0022282602731138468 train acc 0.9840520877266976\n",
            "epoch 49 batch id 2381 loss 0.0005929506151005626 train acc 0.984060006299874\n",
            "epoch 49 batch id 2391 loss 0.015361454337835312 train acc 0.9840743935591802\n",
            "epoch 49 batch id 2401 loss 0.02169397473335266 train acc 0.9840626301541024\n",
            "epoch 49 batch id 2411 loss 0.023339340463280678 train acc 0.9840509643301535\n",
            "epoch 49 batch id 2421 loss 0.009928647428750992 train acc 0.9840910264353573\n",
            "epoch 49 batch id 2431 loss 0.045428935438394547 train acc 0.9841307589469355\n",
            "epoch 49 batch id 2441 loss 0.00022805941989645362 train acc 0.9841381605899222\n",
            "epoch 49 batch id 2451 loss 0.0002472110209055245 train acc 0.9841072521419829\n",
            "epoch 49 batch id 2461 loss 0.06243623048067093 train acc 0.9841146891507517\n",
            "epoch 49 batch id 2471 loss 0.12980668246746063 train acc 0.9840841258599757\n",
            "epoch 49 batch id 2481 loss 0.05919686332345009 train acc 0.9840915961305925\n",
            "epoch 49 batch id 2491 loss 0.09010764211416245 train acc 0.9840927338418306\n",
            "epoch 49 batch id 2501 loss 0.014872460626065731 train acc 0.9841001099560176\n",
            "epoch 49 batch id 2511 loss 0.021643640473484993 train acc 0.984107427319793\n",
            "epoch 49 batch id 2521 loss 0.02321082539856434 train acc 0.9840774990083301\n",
            "epoch 49 batch id 2531 loss 0.04516013711690903 train acc 0.9840539806400632\n",
            "epoch 49 batch id 2541 loss 0.021222542971372604 train acc 0.984042945690673\n",
            "epoch 49 batch id 2551 loss 0.030799521133303642 train acc 0.9840442473539789\n",
            "epoch 49 batch id 2561 loss 0.046754155308008194 train acc 0.984045538852011\n",
            "epoch 49 batch id 2571 loss 0.0519508421421051 train acc 0.9840650525087514\n",
            "epoch 49 batch id 2581 loss 0.04234076663851738 train acc 0.9840238764044944\n",
            "epoch 49 batch id 2591 loss 0.042595475912094116 train acc 0.9840312620609803\n",
            "epoch 49 batch id 2601 loss 0.1174842119216919 train acc 0.9840686274509803\n",
            "epoch 49 batch id 2611 loss 0.08687059581279755 train acc 0.984069800842589\n",
            "epoch 49 batch id 2621 loss 0.043508268892765045 train acc 0.9840471194200687\n",
            "epoch 49 batch id 2631 loss 0.08059536665678024 train acc 0.9840008551881414\n",
            "epoch 49 batch id 2641 loss 0.00943288579583168 train acc 0.9839963555471413\n",
            "epoch 49 batch id 2651 loss 0.05564747005701065 train acc 0.9839801018483592\n",
            "epoch 49 batch id 2661 loss 0.00039755948819220066 train acc 0.9839698421645998\n",
            "epoch 49 batch id 2671 loss 0.0010630810866132379 train acc 0.9839538094346687\n",
            "epoch 49 batch id 2681 loss 0.09742242097854614 train acc 0.9839087560611712\n",
            "epoch 49 batch id 2691 loss 0.005783676635473967 train acc 0.9838988758825715\n",
            "epoch 49 batch id 2701 loss 0.03334270417690277 train acc 0.9839064235468346\n",
            "epoch 49 batch id 2711 loss 0.02467278763651848 train acc 0.9838966248616746\n",
            "epoch 49 batch id 2721 loss 0.007937059737741947 train acc 0.98386967107681\n",
            "epoch 49 batch id 2731 loss 0.0020042872056365013 train acc 0.9838658000732332\n",
            "epoch 49 batch id 2741 loss 0.2034149318933487 train acc 0.983879058737687\n",
            "epoch 49 batch id 2751 loss 0.06436242163181305 train acc 0.9838638222464559\n",
            "epoch 49 batch id 2761 loss 0.010853170417249203 train acc 0.9838769920318725\n",
            "epoch 49 batch id 2771 loss 0.0037836297415196896 train acc 0.9838844280043305\n",
            "epoch 49 batch id 2781 loss 0.05745561048388481 train acc 0.98388619201726\n",
            "epoch 49 batch id 2791 loss 0.017584286630153656 train acc 0.9838991400931566\n",
            "epoch 49 batch id 2801 loss 0.05269177258014679 train acc 0.9838841038914673\n",
            "epoch 49 batch id 2811 loss 0.10567803680896759 train acc 0.9838969672714336\n",
            "epoch 49 batch id 2821 loss 0.06424178183078766 train acc 0.9839152782701169\n",
            "epoch 49 batch id 2831 loss 0.021735576912760735 train acc 0.9839169021547156\n",
            "epoch 49 batch id 2841 loss 0.05654170736670494 train acc 0.9839185146075325\n",
            "epoch 49 batch id 2851 loss 0.00031461630715057254 train acc 0.9839091546825676\n",
            "epoch 49 batch id 2861 loss 0.05801980569958687 train acc 0.9839107829430269\n",
            "epoch 49 batch id 2871 loss 0.09414781630039215 train acc 0.9839069575060955\n",
            "epoch 49 batch id 2881 loss 0.04180573299527168 train acc 0.9838977351614023\n",
            "epoch 49 batch id 2891 loss 0.00036483933217823505 train acc 0.9839101954341058\n",
            "epoch 49 batch id 2901 loss 0.06021387130022049 train acc 0.983906411582213\n",
            "epoch 49 batch id 2911 loss 0.0209575854241848 train acc 0.9839133888698042\n",
            "epoch 49 batch id 2921 loss 0.07231167703866959 train acc 0.9838989216021911\n",
            "epoch 49 batch id 2931 loss 0.0014272489352151752 train acc 0.9838898839986353\n",
            "epoch 49 batch id 2941 loss 0.015332645736634731 train acc 0.9838702822169331\n",
            "epoch 49 batch id 2951 loss 0.02886415459215641 train acc 0.9839037614368011\n",
            "epoch 49 batch id 2961 loss 0.031909335404634476 train acc 0.9839159067882473\n",
            "epoch 49 batch id 2971 loss 0.01183217391371727 train acc 0.9838858970043757\n",
            "epoch 49 batch id 2981 loss 0.015720568597316742 train acc 0.9838613300905736\n",
            "epoch 49 batch id 2991 loss 0.013347840867936611 train acc 0.9838630474757606\n",
            "epoch 49 batch id 3001 loss 0.0003029122017323971 train acc 0.9838959930023325\n",
            "epoch 49 batch id 3011 loss 0.08106545358896255 train acc 0.9839079624709399\n",
            "epoch 49 batch id 3021 loss 0.03371784836053848 train acc 0.9838784756703078\n",
            "epoch 49 batch id 3031 loss 0.10332980751991272 train acc 0.9838491834378092\n",
            "epoch 49 batch id 3041 loss 0.08076987415552139 train acc 0.9838354981913844\n",
            "epoch 49 batch id 3051 loss 0.0064742835238575935 train acc 0.9838475090134382\n",
            "epoch 49 batch id 3061 loss 0.08281894028186798 train acc 0.9838390231950344\n",
            "epoch 49 batch id 3071 loss 0.009456083178520203 train acc 0.9838458563985673\n",
            "epoch 49 batch id 3081 loss 0.03834809735417366 train acc 0.9838425024342746\n",
            "epoch 49 batch id 3091 loss 0.014106834307312965 train acc 0.9838593901649951\n",
            "epoch 49 batch id 3101 loss 0.06219340115785599 train acc 0.983840898097388\n",
            "epoch 49 batch id 3111 loss 0.055535219609737396 train acc 0.9838375924140148\n",
            "epoch 49 batch id 3121 loss 0.021933099254965782 train acc 0.9838543335469401\n",
            "epoch 49 batch id 3131 loss 0.010187544859945774 train acc 0.9838659773235388\n",
            "epoch 49 batch id 3141 loss 0.06167858839035034 train acc 0.9838725724291627\n",
            "epoch 49 batch id 3151 loss 0.0002831068413797766 train acc 0.9838791256743891\n",
            "epoch 49 batch id 3161 loss 0.053286533802747726 train acc 0.9838856374565011\n",
            "epoch 49 batch id 3171 loss 0.013102174736559391 train acc 0.9838723982970672\n",
            "epoch 49 batch id 3181 loss 0.03581978753209114 train acc 0.9838297705124175\n",
            "epoch 49 batch id 3191 loss 0.013731684535741806 train acc 0.9838559620808524\n",
            "epoch 49 batch id 3201 loss 0.0707005113363266 train acc 0.9838575835676351\n",
            "epoch 49 batch id 3211 loss 0.11353373527526855 train acc 0.983825132357521\n",
            "epoch 49 batch id 3221 loss 0.0914778932929039 train acc 0.9837880316671841\n",
            "epoch 49 batch id 3231 loss 0.03129929304122925 train acc 0.9837656685236769\n",
            "epoch 49 batch id 3241 loss 0.02161330357193947 train acc 0.9837820117247763\n",
            "epoch 49 batch id 3251 loss 0.02822875790297985 train acc 0.9837742233159028\n",
            "epoch 49 batch id 3261 loss 0.10598661750555038 train acc 0.9837425252989881\n",
            "epoch 49 batch id 3271 loss 0.05125836655497551 train acc 0.9837253515744421\n",
            "epoch 49 batch id 3281 loss 0.001393178361468017 train acc 0.983732093873819\n",
            "epoch 49 batch id 3291 loss 0.017427777871489525 train acc 0.9837245518079611\n",
            "epoch 49 batch id 3301 loss 0.10016857832670212 train acc 0.9837217888518631\n",
            "epoch 49 batch id 3311 loss 0.018848981708288193 train acc 0.9837237617034129\n",
            "epoch 49 batch id 3321 loss 0.0704507827758789 train acc 0.983739837398374\n",
            "epoch 49 batch id 3331 loss 0.08359865099191666 train acc 0.9837089087361153\n",
            "epoch 49 batch id 3341 loss 0.027638984844088554 train acc 0.9837109024244238\n",
            "epoch 49 batch id 3351 loss 0.03305920585989952 train acc 0.9836849074903014\n",
            "epoch 49 batch id 3361 loss 0.030993541702628136 train acc 0.9837009074680154\n",
            "epoch 49 batch id 3371 loss 0.08318387717008591 train acc 0.983698272026105\n",
            "epoch 49 batch id 3381 loss 0.0026499966625124216 train acc 0.9837095164152617\n",
            "epoch 49 batch id 3391 loss 0.04034772887825966 train acc 0.9837345178413447\n",
            "epoch 49 batch id 3401 loss 0.07341311126947403 train acc 0.983727212584534\n",
            "epoch 49 batch id 3411 loss 0.054838553071022034 train acc 0.9837336924655526\n",
            "epoch 49 batch id 3421 loss 0.06910530477762222 train acc 0.9837401344636071\n",
            "epoch 49 batch id 3431 loss 0.08152642101049423 train acc 0.9837465389099388\n",
            "epoch 49 batch id 3441 loss 0.011425530537962914 train acc 0.9837438244696309\n",
            "epoch 49 batch id 3451 loss 0.025622384622693062 train acc 0.9837501811069256\n",
            "epoch 49 batch id 3461 loss 0.012192697264254093 train acc 0.9837429572377926\n",
            "epoch 49 batch id 3471 loss 0.023866206407546997 train acc 0.9837402765773552\n",
            "epoch 49 batch id 3481 loss 0.06656676530838013 train acc 0.9837420999712726\n",
            "epoch 49 batch id 3491 loss 0.0005534178344532847 train acc 0.9837394371240332\n",
            "epoch 49 batch id 3501 loss 0.017776234075427055 train acc 0.9837457155098543\n",
            "epoch 49 batch id 3511 loss 0.03560420870780945 train acc 0.983729706636286\n",
            "epoch 49 batch id 3521 loss 0.035734422504901886 train acc 0.9837492899744391\n",
            "epoch 49 batch id 3531 loss 0.009664277546107769 train acc 0.9837599122061739\n",
            "epoch 49 batch id 3541 loss 0.061018481850624084 train acc 0.9837528240609997\n",
            "epoch 49 batch id 3551 loss 0.012471809051930904 train acc 0.9837149746550268\n",
            "epoch 49 batch id 3561 loss 0.009571566246449947 train acc 0.9837168281381634\n",
            "epoch 49 batch id 3571 loss 0.01576080732047558 train acc 0.9837317978157379\n",
            "epoch 49 batch id 3581 loss 0.04631822556257248 train acc 0.9837423205808433\n",
            "epoch 49 batch id 3591 loss 0.05629764497280121 train acc 0.9837484335839599\n",
            "epoch 49 batch id 3601 loss 0.05137280002236366 train acc 0.98372413912802\n",
            "epoch 49 batch id 3611 loss 0.0018244882812723517 train acc 0.98373026862365\n",
            "epoch 49 batch id 3621 loss 0.03925153240561485 train acc 0.9837147887323944\n",
            "epoch 49 batch id 3631 loss 0.019213424995541573 train acc 0.9837123037730653\n",
            "epoch 49 batch id 3641 loss 0.010214319452643394 train acc 0.9837269980774512\n",
            "epoch 49 batch id 3651 loss 0.03335154429078102 train acc 0.9837202136400987\n",
            "epoch 49 batch id 3661 loss 0.004116557538509369 train acc 0.9837134662660475\n",
            "epoch 49 batch id 3671 loss 0.06524153798818588 train acc 0.9837280373195315\n",
            "epoch 49 batch id 3681 loss 0.0007976916385814548 train acc 0.9837552635153491\n",
            "epoch 49 batch id 3691 loss 0.007002784870564938 train acc 0.9837823421836901\n",
            "epoch 49 batch id 3701 loss 0.03909055516123772 train acc 0.983771278032964\n",
            "epoch 49 batch id 3711 loss 0.025266677141189575 train acc 0.9837729048773916\n",
            "epoch 49 batch id 3721 loss 0.05590205267071724 train acc 0.9837661246976619\n",
            "epoch 49 batch id 3731 loss 0.015446550212800503 train acc 0.9837803202894666\n",
            "epoch 49 batch id 3741 loss 0.04219278320670128 train acc 0.9837902632985832\n",
            "epoch 49 batch id 3751 loss 0.0006266120471991599 train acc 0.9838084844041589\n",
            "epoch 49 batch id 3761 loss 0.12494197487831116 train acc 0.9837850638128157\n",
            "epoch 49 batch id 3771 loss 0.017776615917682648 train acc 0.9837659108989658\n",
            "epoch 49 batch id 3781 loss 0.09768103063106537 train acc 0.9837551243057392\n",
            "epoch 49 batch id 3791 loss 0.00011202384484931827 train acc 0.9837732458454234\n",
            "epoch 49 batch id 3801 loss 0.039191629737615585 train acc 0.9837707182320442\n",
            "epoch 49 batch id 3811 loss 0.02140655741095543 train acc 0.9837764038310155\n",
            "epoch 49 batch id 3821 loss 0.025665756314992905 train acc 0.9837861489138969\n",
            "epoch 49 batch id 3831 loss 0.001163106644526124 train acc 0.983783607413208\n",
            "epoch 49 batch id 3841 loss 0.0001714287354843691 train acc 0.9837688752928925\n",
            "epoch 49 batch id 3851 loss 0.0002725099038798362 train acc 0.9837623344585822\n",
            "epoch 49 batch id 3861 loss 0.017589084804058075 train acc 0.9837598743848743\n",
            "epoch 49 batch id 3871 loss 0.07840871810913086 train acc 0.9837695362955309\n",
            "epoch 49 batch id 3881 loss 0.041565850377082825 train acc 0.9837630443184746\n",
            "epoch 49 batch id 3891 loss 0.016945302486419678 train acc 0.9837565857106142\n",
            "epoch 49 batch id 3901 loss 0.008338823914527893 train acc 0.9837381440656242\n",
            "epoch 49 batch id 3911 loss 0.0390799418091774 train acc 0.9837237918690872\n",
            "epoch 49 batch id 3921 loss 0.008973311632871628 train acc 0.9837413925019127\n",
            "epoch 49 batch id 3931 loss 0.03755669295787811 train acc 0.9837271050623251\n",
            "epoch 49 batch id 3941 loss 0.0014932266203686595 train acc 0.9837406432377569\n",
            "epoch 49 batch id 3951 loss 0.0166181530803442 train acc 0.9837185206276892\n",
            "epoch 49 batch id 3961 loss 0.00023989037435967475 train acc 0.9837241226962888\n",
            "epoch 49 batch id 3971 loss 0.04117074981331825 train acc 0.9837178922185847\n",
            "epoch 49 batch id 3981 loss 0.020071109756827354 train acc 0.9837038432554635\n",
            "epoch 49 batch id 3991 loss 0.12466280907392502 train acc 0.9836937797544475\n",
            "epoch 49 batch id 4001 loss 0.07206447422504425 train acc 0.983687671832042\n",
            "epoch 49 batch id 4011 loss 0.00965516921132803 train acc 0.9836738032909499\n",
            "epoch 49 batch id 4021 loss 0.023813428357243538 train acc 0.9836600037304153\n",
            "epoch 49 batch id 4031 loss 0.007985404692590237 train acc 0.9836772823120814\n",
            "epoch 49 batch id 4041 loss 0.07631334662437439 train acc 0.98368287552586\n",
            "epoch 49 batch id 4051 loss 0.06858567893505096 train acc 0.9836768699086645\n",
            "epoch 49 batch id 4061 loss 0.01805197447538376 train acc 0.9836901317409505\n",
            "epoch 49 batch id 4071 loss 0.018323885276913643 train acc 0.9836956521739131\n",
            "epoch 49 batch id 4081 loss 0.013241777196526527 train acc 0.9837164604263661\n",
            "epoch 49 batch id 4091 loss 0.05763740465044975 train acc 0.983710431434857\n",
            "epoch 49 batch id 4101 loss 0.06198076531291008 train acc 0.9837120519385516\n",
            "epoch 49 batch id 4111 loss 0.02832678146660328 train acc 0.9836794575529069\n",
            "epoch 49 batch id 4121 loss 0.01982535980641842 train acc 0.9836773537976219\n",
            "epoch 49 batch id 4131 loss 0.14753879606723785 train acc 0.9836639130961027\n",
            "epoch 49 batch id 4141 loss 0.00018408268806524575 train acc 0.9836694035257184\n",
            "epoch 49 batch id 4151 loss 0.04945620149374008 train acc 0.983682395808239\n",
            "epoch 49 batch id 4161 loss 0.03158419579267502 train acc 0.983676550108147\n",
            "epoch 49 batch id 4171 loss 0.15501834452152252 train acc 0.9836782246463678\n",
            "epoch 49 batch id 4181 loss 0.0169981736689806 train acc 0.9836612054532409\n",
            "epoch 49 batch id 4191 loss 0.06302907317876816 train acc 0.9836405392507754\n",
            "epoch 49 batch id 4201 loss 0.013926810584962368 train acc 0.983668323018329\n",
            "epoch 49 batch id 4211 loss 0.05045764520764351 train acc 0.9836551591071004\n",
            "epoch 49 batch id 4221 loss 0.08935056626796722 train acc 0.9836346541104004\n",
            "epoch 49 batch id 4231 loss 0.11376474797725677 train acc 0.9836216320018908\n",
            "epoch 49 batch id 4241 loss 0.09407490491867065 train acc 0.9836270926668239\n",
            "epoch 49 batch id 4251 loss 0.1300860345363617 train acc 0.983636203246295\n",
            "epoch 49 batch id 4261 loss 0.012025534175336361 train acc 0.9836526050222952\n",
            "epoch 49 batch id 4271 loss 0.07538779079914093 train acc 0.9836616132053383\n",
            "epoch 49 batch id 4281 loss 0.06090319901704788 train acc 0.983670579303901\n",
            "epoch 49 batch id 4291 loss 0.006654310971498489 train acc 0.983657655558145\n",
            "epoch 49 batch id 4301 loss 0.03489582985639572 train acc 0.9836484247849338\n",
            "epoch 49 batch id 4311 loss 0.04084916412830353 train acc 0.9836537346323359\n",
            "epoch 49 batch id 4321 loss 0.059076014906167984 train acc 0.9836409395973155\n",
            "epoch 49 batch id 4331 loss 0.10124867409467697 train acc 0.9836245959362734\n",
            "epoch 49 batch id 4341 loss 0.04281316697597504 train acc 0.9836119269753513\n",
            "epoch 49 batch id 4351 loss 0.046556975692510605 train acc 0.983595725120662\n",
            "epoch 49 batch id 4361 loss 0.0950610563158989 train acc 0.9835939291446916\n",
            "epoch 49 batch id 4371 loss 0.07990789413452148 train acc 0.9835921413864104\n",
            "epoch 49 batch id 4381 loss 0.15076929330825806 train acc 0.9835796621775851\n",
            "epoch 49 batch id 4391 loss 0.00015340829850174487 train acc 0.9835814734684583\n",
            "epoch 49 batch id 4401 loss 0.015096328221261501 train acc 0.9835761758691206\n",
            "epoch 49 batch id 4411 loss 0.000590876501519233 train acc 0.9835779868510541\n",
            "epoch 49 batch id 4421 loss 0.162504181265831 train acc 0.9835833239086179\n",
            "epoch 49 batch id 4431 loss 0.017864633351564407 train acc 0.9835710054163845\n",
            "epoch 49 batch id 4441 loss 0.011049548164010048 train acc 0.9835798525106958\n",
            "epoch 49 batch id 4451 loss 0.0010237726382911205 train acc 0.9835851494046282\n",
            "epoch 49 batch id 4461 loss 0.0003476651618257165 train acc 0.9835834173952028\n",
            "epoch 49 batch id 4471 loss 0.11164900660514832 train acc 0.983557229926191\n",
            "epoch 49 batch id 4481 loss 0.022586174309253693 train acc 0.9835451071189467\n",
            "epoch 49 batch id 4491 loss 0.032202497124671936 train acc 0.9835504342017368\n",
            "epoch 49 batch id 4501 loss 0.010080032981932163 train acc 0.9835418518107087\n",
            "epoch 49 batch id 4511 loss 0.0712294802069664 train acc 0.9835333074706274\n",
            "epoch 49 batch id 4521 loss 0.11398851126432419 train acc 0.9835351692103517\n",
            "epoch 49 batch id 4531 loss 0.006717558950185776 train acc 0.9835404711984109\n",
            "epoch 49 batch id 4541 loss 0.04226255044341087 train acc 0.983556072451002\n",
            "epoch 49 batch id 4551 loss 0.02359672077000141 train acc 0.9835613052076466\n",
            "epoch 49 batch id 4561 loss 0.12171044200658798 train acc 0.9835493860995396\n",
            "epoch 49 batch id 4571 loss 0.031969089061021805 train acc 0.9835546105884927\n",
            "epoch 49 batch id 4581 loss 0.03232460469007492 train acc 0.9835564014407334\n",
            "epoch 49 batch id 4591 loss 0.05218370258808136 train acc 0.9835683946852538\n",
            "epoch 49 batch id 4601 loss 0.012210944667458534 train acc 0.9835667517930885\n",
            "epoch 49 batch id 4611 loss 0.03333055227994919 train acc 0.983582059206246\n",
            "epoch 49 batch id 4621 loss 0.017012527212500572 train acc 0.9835905377623891\n",
            "epoch 49 batch id 4631 loss 0.09461737424135208 train acc 0.983592231699417\n",
            "epoch 49 batch id 4641 loss 0.1262872815132141 train acc 0.9836040185304891\n",
            "epoch 49 batch id 4651 loss 0.12134557962417603 train acc 0.9836157546764137\n",
            "epoch 49 batch id 4661 loss 0.030075887218117714 train acc 0.9836240881785024\n",
            "epoch 49 batch id 4671 loss 0.00767277367413044 train acc 0.9836357311068293\n",
            "epoch 49 batch id 4681 loss 0.19078686833381653 train acc 0.983633972441786\n",
            "epoch 49 batch id 4691 loss 0.002783116651698947 train acc 0.9836322212747814\n",
            "epoch 49 batch id 4701 loss 0.04832698404788971 train acc 0.9836338013188684\n",
            "epoch 49 batch id 4711 loss 0.030548883602023125 train acc 0.9836353746550626\n",
            "epoch 49 batch id 4721 loss 0.0027226496022194624 train acc 0.9836303219656852\n",
            "epoch 49 batch id 4731 loss 0.023790059611201286 train acc 0.9836351986894948\n",
            "epoch 49 batch id 4741 loss 0.0659828782081604 train acc 0.983636759122548\n",
            "epoch 49 batch id 4751 loss 0.1399717777967453 train acc 0.983644890549358\n",
            "epoch 49 batch id 4761 loss 0.04214438050985336 train acc 0.9836529878176854\n",
            "epoch 49 batch id 4771 loss 0.07503369450569153 train acc 0.9836512261580381\n",
            "epoch 49 batch id 4781 loss 0.05959629639983177 train acc 0.9836363992888517\n",
            "epoch 49 batch id 4791 loss 0.11533672362565994 train acc 0.983624895637654\n",
            "epoch 49 batch id 4801 loss 0.09462179988622665 train acc 0.9836166944386586\n",
            "epoch 49 batch id 4811 loss 0.00045238263555802405 train acc 0.9836150228642694\n",
            "epoch 49 batch id 4821 loss 0.06475066393613815 train acc 0.9836101171956025\n",
            "epoch 49 batch id 4831 loss 0.015527568757534027 train acc 0.9836246377561582\n",
            "epoch 49 batch id 4841 loss 0.0011778917396441102 train acc 0.9836261877711217\n",
            "epoch 49 batch id 4851 loss 0.040913213044404984 train acc 0.9836341733663162\n",
            "epoch 49 batch id 4861 loss 0.000552730227354914 train acc 0.9836324830281835\n",
            "epoch 49 batch id 4871 loss 0.00028477568412199616 train acc 0.9836468384315336\n",
            "epoch 49 batch id 4881 loss 0.036231737583875656 train acc 0.9836515314484737\n",
            "epoch 49 batch id 4891 loss 0.024515759199857712 train acc 0.9836466213453282\n",
            "epoch 49 batch id 4901 loss 0.00014421278319787234 train acc 0.9836576719036931\n",
            "epoch 49 batch id 4911 loss 0.11502521485090256 train acc 0.9836527692934229\n",
            "epoch 49 batch id 4921 loss 0.10381008684635162 train acc 0.9836542369437107\n",
            "epoch 49 batch id 4931 loss 0.06764950603246689 train acc 0.983639854998986\n",
            "epoch 49 batch id 4941 loss 0.05026661232113838 train acc 0.9836192066383324\n",
            "epoch 49 batch id 4951 loss 0.1142813116312027 train acc 0.9836207331852151\n",
            "epoch 49 train acc 0.9836184940488198\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c81c36d7273421b954f676d1bb48cf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 49 loss 0.5431835651397705 test acc 0.8529382789589444\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "782f6ec024ce4c0aae4ac2f6d80e4ce4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4957 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 50 batch id 1 loss 0.044347457587718964 train acc 0.984375\n",
            "epoch 50 batch id 11 loss 0.01645469479262829 train acc 0.9857954545454546\n",
            "epoch 50 batch id 21 loss 0.062406543642282486 train acc 0.9858630952380952\n",
            "epoch 50 batch id 31 loss 0.04633522406220436 train acc 0.9858870967741935\n",
            "epoch 50 batch id 41 loss 0.038612086325883865 train acc 0.9870426829268293\n",
            "epoch 50 batch id 51 loss 0.011371780186891556 train acc 0.9868259803921569\n",
            "epoch 50 batch id 61 loss 0.07804035395383835 train acc 0.9841188524590164\n",
            "epoch 50 batch id 71 loss 0.0694441944360733 train acc 0.9848151408450704\n",
            "epoch 50 batch id 81 loss 0.026192741468548775 train acc 0.9839891975308642\n",
            "epoch 50 batch id 91 loss 0.06548288464546204 train acc 0.984375\n",
            "epoch 50 batch id 101 loss 0.03494243696331978 train acc 0.9839108910891089\n",
            "epoch 50 batch id 111 loss 0.0005955218803137541 train acc 0.9836711711711712\n",
            "epoch 50 batch id 121 loss 0.13762372732162476 train acc 0.9837293388429752\n",
            "epoch 50 batch id 131 loss 0.019013183191418648 train acc 0.9840171755725191\n",
            "epoch 50 batch id 141 loss 0.003516064491122961 train acc 0.984375\n",
            "epoch 50 batch id 151 loss 0.053769610822200775 train acc 0.9846854304635762\n",
            "epoch 50 batch id 161 loss 0.06545810401439667 train acc 0.984860248447205\n",
            "epoch 50 batch id 171 loss 0.007965323515236378 train acc 0.9844663742690059\n",
            "epoch 50 batch id 181 loss 0.02235124446451664 train acc 0.9846339779005525\n",
            "epoch 50 batch id 191 loss 0.04600080847740173 train acc 0.9846204188481675\n",
            "epoch 50 batch id 201 loss 0.011199700646102428 train acc 0.9849191542288557\n",
            "epoch 50 batch id 211 loss 0.0001227464817930013 train acc 0.9850414691943128\n",
            "epoch 50 batch id 221 loss 0.046772561967372894 train acc 0.9848699095022625\n",
            "epoch 50 batch id 231 loss 0.027228087186813354 train acc 0.9850514069264069\n",
            "epoch 50 batch id 241 loss 0.12891705334186554 train acc 0.9849585062240664\n",
            "epoch 50 batch id 251 loss 0.09773365408182144 train acc 0.9849975099601593\n",
            "epoch 50 batch id 261 loss 0.037229761481285095 train acc 0.9850933908045977\n",
            "epoch 50 batch id 271 loss 0.00011926292791031301 train acc 0.9851821955719557\n",
            "epoch 50 batch id 281 loss 0.003423858666792512 train acc 0.9854314946619217\n",
            "epoch 50 batch id 291 loss 0.058936454355716705 train acc 0.9855562714776632\n",
            "epoch 50 batch id 301 loss 0.10836630314588547 train acc 0.9853093853820598\n",
            "epoch 50 batch id 311 loss 0.01965123601257801 train acc 0.9852793408360129\n",
            "epoch 50 batch id 321 loss 0.11493033915758133 train acc 0.9852998442367601\n",
            "epoch 50 batch id 331 loss 0.0010103181703016162 train acc 0.9851774924471299\n",
            "epoch 50 batch id 341 loss 0.18207234144210815 train acc 0.985199780058651\n",
            "epoch 50 batch id 351 loss 0.006654425524175167 train acc 0.9853543447293447\n",
            "epoch 50 batch id 361 loss 0.051684413105249405 train acc 0.9853704986149584\n",
            "epoch 50 batch id 371 loss 0.017478158697485924 train acc 0.985427897574124\n",
            "epoch 50 batch id 381 loss 0.01862248219549656 train acc 0.98560531496063\n",
            "epoch 50 batch id 391 loss 0.07267454266548157 train acc 0.985613810741688\n",
            "epoch 50 batch id 401 loss 0.0001292381639359519 train acc 0.9855049875311721\n",
            "epoch 50 batch id 411 loss 0.00012286381388548762 train acc 0.9855915450121655\n",
            "epoch 50 batch id 421 loss 0.01065286435186863 train acc 0.9856368764845606\n",
            "epoch 50 batch id 431 loss 0.06805791705846786 train acc 0.9856075986078886\n",
            "epoch 50 batch id 441 loss 0.1679936945438385 train acc 0.9854024943310657\n",
            "epoch 50 batch id 451 loss 0.015684254467487335 train acc 0.9855529379157428\n",
            "epoch 50 batch id 461 loss 0.022527877241373062 train acc 0.985425704989154\n",
            "epoch 50 batch id 471 loss 0.04194870963692665 train acc 0.9855029193205945\n",
            "epoch 50 batch id 481 loss 0.061066750437021255 train acc 0.9855119542619543\n",
            "epoch 50 batch id 491 loss 0.24119344353675842 train acc 0.9854251527494908\n",
            "epoch 50 batch id 501 loss 0.13238440454006195 train acc 0.9855289421157685\n",
            "epoch 50 batch id 511 loss 0.09791182726621628 train acc 0.9854146281800391\n",
            "epoch 50 batch id 521 loss 0.10589613020420074 train acc 0.9853646833013435\n",
            "epoch 50 batch id 531 loss 0.18251031637191772 train acc 0.9850517890772128\n",
            "epoch 50 batch id 541 loss 0.010626118630170822 train acc 0.984865988909427\n",
            "epoch 50 batch id 551 loss 0.0934697762131691 train acc 0.9848003629764065\n",
            "epoch 50 batch id 561 loss 0.03198600560426712 train acc 0.9846535204991087\n",
            "epoch 50 batch id 571 loss 0.02212354727089405 train acc 0.9848128283712785\n",
            "epoch 50 batch id 581 loss 0.09908363223075867 train acc 0.9847783993115319\n",
            "epoch 50 batch id 591 loss 0.03046875260770321 train acc 0.9848773265651438\n",
            "epoch 50 batch id 601 loss 0.010456905700266361 train acc 0.9848689683860233\n",
            "epoch 50 batch id 611 loss 0.0828491821885109 train acc 0.9847585924713584\n",
            "epoch 50 batch id 621 loss 0.2678207457065582 train acc 0.9846769323671497\n",
            "epoch 50 batch id 631 loss 0.05833764374256134 train acc 0.9845978605388273\n",
            "epoch 50 batch id 641 loss 0.015908963978290558 train acc 0.984545631825273\n",
            "epoch 50 batch id 651 loss 0.014666310511529446 train acc 0.9844470046082949\n",
            "epoch 50 batch id 661 loss 0.008226578123867512 train acc 0.9845404689863843\n",
            "epoch 50 batch id 671 loss 0.028214523568749428 train acc 0.9844681445603577\n",
            "epoch 50 batch id 681 loss 0.04199855029582977 train acc 0.9844438325991189\n",
            "epoch 50 batch id 691 loss 0.06635908782482147 train acc 0.9843976121562952\n",
            "epoch 50 batch id 701 loss 0.02864209935069084 train acc 0.9842635520684736\n",
            "epoch 50 batch id 711 loss 0.1072404757142067 train acc 0.9842431434599156\n",
            "epoch 50 batch id 721 loss 0.05228486657142639 train acc 0.9840932732316228\n",
            "epoch 50 batch id 731 loss 0.009606773965060711 train acc 0.9841398768809849\n",
            "epoch 50 batch id 741 loss 0.0011325730010867119 train acc 0.9841430499325237\n",
            "epoch 50 batch id 751 loss 0.06812534481287003 train acc 0.9840004993342211\n",
            "epoch 50 batch id 761 loss 0.0011696513975039124 train acc 0.983984888304862\n",
            "epoch 50 batch id 771 loss 0.12426289916038513 train acc 0.9839291504539559\n",
            "epoch 50 batch id 781 loss 0.1275317519903183 train acc 0.9839148527528809\n",
            "epoch 50 batch id 791 loss 0.010629124939441681 train acc 0.9839404235145386\n",
            "epoch 50 batch id 801 loss 0.00154917745385319 train acc 0.9839263420724095\n",
            "epoch 50 batch id 811 loss 0.11517957597970963 train acc 0.9839318742293465\n",
            "epoch 50 batch id 821 loss 0.010751377791166306 train acc 0.9840704933008526\n",
            "epoch 50 batch id 831 loss 0.09988900274038315 train acc 0.9840365523465704\n",
            "epoch 50 batch id 841 loss 0.0015240100910887122 train acc 0.9840963139120095\n",
            "epoch 50 batch id 851 loss 0.043579667806625366 train acc 0.9840628672150411\n",
            "epoch 50 batch id 861 loss 0.029420441016554832 train acc 0.9840483449477352\n",
            "epoch 50 batch id 871 loss 0.010517988353967667 train acc 0.9840162169919633\n",
            "epoch 50 batch id 881 loss 0.013189584016799927 train acc 0.9839848183881952\n",
            "epoch 50 batch id 891 loss 0.0012798323296010494 train acc 0.9840242704826038\n",
            "epoch 50 batch id 901 loss 0.05754149332642555 train acc 0.9840455049944506\n",
            "epoch 50 batch id 911 loss 0.011066466569900513 train acc 0.9840834248079035\n",
            "epoch 50 batch id 921 loss 0.002510876627638936 train acc 0.9839678338762216\n",
            "epoch 50 batch id 931 loss 0.003793042618781328 train acc 0.9840057733619764\n",
            "epoch 50 batch id 941 loss 0.08512209355831146 train acc 0.9840595111583422\n",
            "epoch 50 batch id 951 loss 0.05188283324241638 train acc 0.9840299684542587\n",
            "epoch 50 batch id 961 loss 0.011629429645836353 train acc 0.9840660770031218\n",
            "epoch 50 batch id 971 loss 0.01894262805581093 train acc 0.9840692584963955\n",
            "epoch 50 batch id 981 loss 0.018128812313079834 train acc 0.9841201580020388\n",
            "epoch 50 batch id 991 loss 0.07647255808115005 train acc 0.9841857971745711\n",
            "epoch 50 batch id 1001 loss 0.015399164520204067 train acc 0.9842032967032966\n",
            "epoch 50 batch id 1011 loss 0.00013305577158462256 train acc 0.984220450049456\n",
            "epoch 50 batch id 1021 loss 0.07030285894870758 train acc 0.9842525710088149\n",
            "epoch 50 batch id 1031 loss 0.007805630099028349 train acc 0.9842840688651794\n",
            "epoch 50 batch id 1041 loss 0.01450015977025032 train acc 0.9843149615754082\n",
            "epoch 50 batch id 1051 loss 0.02278120443224907 train acc 0.9842709324452902\n",
            "epoch 50 batch id 1061 loss 0.09243254363536835 train acc 0.9841982799245994\n",
            "epoch 50 batch id 1071 loss 0.012449245899915695 train acc 0.9842145191409897\n",
            "epoch 50 batch id 1081 loss 0.0405602790415287 train acc 0.9842015494912119\n",
            "epoch 50 batch id 1091 loss 0.07691820710897446 train acc 0.9842461044912924\n",
            "epoch 50 batch id 1101 loss 0.04113420844078064 train acc 0.9842330835603996\n",
            "epoch 50 batch id 1111 loss 0.07082834094762802 train acc 0.9841499774977498\n",
            "epoch 50 batch id 1121 loss 0.001321087358519435 train acc 0.9842216770740411\n",
            "epoch 50 batch id 1131 loss 0.05099518969655037 train acc 0.984126326259947\n",
            "epoch 50 batch id 1141 loss 0.02751406468451023 train acc 0.9841421998247152\n",
            "epoch 50 batch id 1151 loss 0.015418271534144878 train acc 0.9840899218071243\n",
            "epoch 50 batch id 1161 loss 0.02503405511379242 train acc 0.9841058354866494\n",
            "epoch 50 batch id 1171 loss 0.007153929211199284 train acc 0.9840547608881298\n",
            "epoch 50 batch id 1181 loss 0.005196491722017527 train acc 0.9840707027942421\n",
            "epoch 50 batch id 1191 loss 0.006285986863076687 train acc 0.9841126154492024\n",
            "epoch 50 batch id 1201 loss 0.005566485226154327 train acc 0.9841928601165695\n",
            "epoch 50 batch id 1211 loss 0.0001456641621189192 train acc 0.984168559042114\n",
            "epoch 50 batch id 1221 loss 0.08977750688791275 train acc 0.9841446560196561\n",
            "epoch 50 batch id 1231 loss 0.02421092614531517 train acc 0.9840703696181966\n",
            "epoch 50 batch id 1241 loss 0.0024961247108876705 train acc 0.984085414987913\n",
            "epoch 50 batch id 1251 loss 0.07317960262298584 train acc 0.9841127098321343\n",
            "epoch 50 batch id 1261 loss 0.019587352871894836 train acc 0.9841519627279937\n",
            "epoch 50 batch id 1271 loss 0.022534113377332687 train acc 0.9840799567269867\n",
            "epoch 50 batch id 1281 loss 0.0009096666472032666 train acc 0.9841188524590164\n",
            "epoch 50 batch id 1291 loss 0.08014936745166779 train acc 0.9841329395817195\n",
            "epoch 50 batch id 1301 loss 0.0016432275297120214 train acc 0.9841227901614144\n",
            "epoch 50 batch id 1311 loss 0.017665766179561615 train acc 0.9841247139588101\n",
            "epoch 50 batch id 1321 loss 0.010649596340954304 train acc 0.9841029523088569\n",
            "epoch 50 batch id 1331 loss 0.011750949546694756 train acc 0.9840697783621337\n",
            "epoch 50 batch id 1341 loss 0.08832550793886185 train acc 0.9840720544369873\n",
            "epoch 50 batch id 1351 loss 0.0010794763220474124 train acc 0.984108993338268\n",
            "epoch 50 batch id 1361 loss 0.008464115671813488 train acc 0.9840420646583394\n",
            "epoch 50 batch id 1371 loss 0.006109490059316158 train acc 0.984055889861415\n",
            "epoch 50 batch id 1381 loss 0.07056745886802673 train acc 0.9840242577842143\n",
            "epoch 50 batch id 1391 loss 0.009665394201874733 train acc 0.9839593817397556\n",
            "epoch 50 batch id 1401 loss 0.011068612337112427 train acc 0.9839400428265525\n",
            "epoch 50 batch id 1411 loss 0.01997384987771511 train acc 0.9839874202693125\n",
            "epoch 50 batch id 1421 loss 0.00649164617061615 train acc 0.984012139338494\n",
            "epoch 50 batch id 1431 loss 0.005335148423910141 train acc 0.9840255939902166\n",
            "epoch 50 batch id 1441 loss 0.015486336313188076 train acc 0.9839954892435808\n",
            "epoch 50 batch id 1451 loss 0.011334861628711224 train acc 0.9840196416264645\n",
            "epoch 50 batch id 1461 loss 0.00792696326971054 train acc 0.9839899897330595\n",
            "epoch 50 batch id 1471 loss 0.0227366853505373 train acc 0.984056339225017\n",
            "epoch 50 batch id 1481 loss 0.08689592778682709 train acc 0.9840479405806887\n",
            "epoch 50 batch id 1491 loss 0.09784920513629913 train acc 0.9840291750503019\n",
            "epoch 50 batch id 1501 loss 0.06786473095417023 train acc 0.9840314790139907\n",
            "epoch 50 batch id 1511 loss 0.032540373504161835 train acc 0.9840647749834547\n",
            "epoch 50 batch id 1521 loss 0.003219032660126686 train acc 0.9840873602892833\n",
            "epoch 50 batch id 1531 loss 0.016766434535384178 train acc 0.9840892390594382\n",
            "epoch 50 batch id 1541 loss 0.02453339658677578 train acc 0.9840708144062297\n",
            "epoch 50 batch id 1551 loss 0.017089413478970528 train acc 0.9840828497743391\n",
            "epoch 50 batch id 1561 loss 0.024059254676103592 train acc 0.9840847213324792\n",
            "epoch 50 batch id 1571 loss 0.009003717452287674 train acc 0.9840865690642903\n",
            "epoch 50 batch id 1581 loss 0.03141951933503151 train acc 0.9840883934218849\n",
            "epoch 50 batch id 1591 loss 0.05782942846417427 train acc 0.9841000157133878\n",
            "epoch 50 batch id 1601 loss 0.027305716648697853 train acc 0.9841798094940662\n",
            "epoch 50 batch id 1611 loss 0.05791443586349487 train acc 0.9841228274363749\n",
            "epoch 50 batch id 1621 loss 0.04694304242730141 train acc 0.9840954657618753\n",
            "epoch 50 batch id 1631 loss 0.00021004208247177303 train acc 0.9841259196811772\n",
            "epoch 50 batch id 1641 loss 0.07296484708786011 train acc 0.984117915904936\n",
            "epoch 50 batch id 1651 loss 0.03039361722767353 train acc 0.9840626892792247\n",
            "epoch 50 batch id 1661 loss 0.0770915299654007 train acc 0.9841021974714028\n",
            "epoch 50 batch id 1671 loss 0.027678988873958588 train acc 0.9840664272890485\n",
            "epoch 50 batch id 1681 loss 0.0012738453224301338 train acc 0.984068262938727\n",
            "epoch 50 batch id 1691 loss 0.03025399148464203 train acc 0.9840515966883501\n",
            "epoch 50 batch id 1701 loss 0.04558638855814934 train acc 0.9840443121693122\n",
            "epoch 50 batch id 1711 loss 0.0013377401046454906 train acc 0.9840462448860315\n",
            "epoch 50 batch id 1721 loss 0.012558293528854847 train acc 0.984020918070889\n",
            "epoch 50 batch id 1731 loss 0.0006941221654415131 train acc 0.9840410167533218\n",
            "epoch 50 batch id 1741 loss 0.010499035008251667 train acc 0.9840519098219415\n",
            "epoch 50 batch id 1751 loss 0.032725170254707336 train acc 0.9840269845802398\n",
            "epoch 50 batch id 1761 loss 0.017756031826138496 train acc 0.9840467064168086\n",
            "epoch 50 batch id 1771 loss 0.00039453236968256533 train acc 0.9840573828345568\n",
            "epoch 50 batch id 1781 loss 0.00010138440848095343 train acc 0.9840679393599102\n",
            "epoch 50 batch id 1791 loss 0.04168345406651497 train acc 0.9840609296482412\n",
            "epoch 50 batch id 1801 loss 0.014973635785281658 train acc 0.9840800249861188\n",
            "epoch 50 batch id 1811 loss 0.03793065622448921 train acc 0.9840902816123689\n",
            "epoch 50 batch id 1821 loss 0.07533860951662064 train acc 0.9840317819879187\n",
            "epoch 50 batch id 1831 loss 0.01840006746351719 train acc 0.9839995221190606\n",
            "epoch 50 batch id 1841 loss 0.0001364770287182182 train acc 0.9839760999456817\n",
            "epoch 50 batch id 1851 loss 0.027218131348490715 train acc 0.9839782549972987\n",
            "epoch 50 batch id 1861 loss 0.018355168402194977 train acc 0.9839719908651263\n",
            "epoch 50 batch id 1871 loss 0.09535328298807144 train acc 0.984024251737039\n",
            "epoch 50 batch id 1881 loss 0.017583679407835007 train acc 0.9840095029239766\n",
            "epoch 50 batch id 1891 loss 0.01644374057650566 train acc 0.9840362242199894\n",
            "epoch 50 batch id 1901 loss 0.09678623080253601 train acc 0.9840297869542346\n",
            "epoch 50 batch id 1911 loss 0.022491121664643288 train acc 0.9839988880167452\n",
            "epoch 50 batch id 1921 loss 0.004059823229908943 train acc 0.9839601769911505\n",
            "epoch 50 batch id 1931 loss 0.014225557446479797 train acc 0.9839623252200932\n",
            "epoch 50 batch id 1941 loss 0.007757772691547871 train acc 0.9839805512622359\n",
            "epoch 50 batch id 1951 loss 0.04203604534268379 train acc 0.9839505381855459\n",
            "epoch 50 batch id 1961 loss 0.04577033221721649 train acc 0.9839606705762366\n",
            "epoch 50 batch id 1971 loss 0.006976723670959473 train acc 0.9839627727042111\n",
            "epoch 50 batch id 1981 loss 0.03219921514391899 train acc 0.983949078748107\n",
            "epoch 50 batch id 1991 loss 0.09869372099637985 train acc 0.9839041310899046\n",
            "epoch 50 batch id 2001 loss 0.0016116746701300144 train acc 0.9839142928535732\n",
            "epoch 50 batch id 2011 loss 0.08576958626508713 train acc 0.9839088140228742\n",
            "epoch 50 batch id 2021 loss 0.12954534590244293 train acc 0.9839111207323107\n",
            "epoch 50 batch id 2031 loss 0.00012038540444336832 train acc 0.9838595519448547\n",
            "epoch 50 batch id 2041 loss 0.12150561809539795 train acc 0.9838697329740324\n",
            "epoch 50 batch id 2051 loss 0.2015749216079712 train acc 0.9838417235494881\n",
            "epoch 50 batch id 2061 loss 0.08025608956813812 train acc 0.9838518922852983\n",
            "epoch 50 batch id 2071 loss 0.06675836443901062 train acc 0.9838468734910671\n",
            "epoch 50 batch id 2081 loss 0.036970045417547226 train acc 0.983826886112446\n",
            "epoch 50 batch id 2091 loss 0.018631165847182274 train acc 0.9838369799139168\n",
            "epoch 50 batch id 2101 loss 0.007934752851724625 train acc 0.9838692884340791\n",
            "epoch 50 batch id 2111 loss 0.1760043352842331 train acc 0.9838716840360019\n",
            "epoch 50 batch id 2121 loss 0.007133278530091047 train acc 0.9838814238566714\n",
            "epoch 50 batch id 2131 loss 0.028615066781640053 train acc 0.9839277334584702\n",
            "epoch 50 batch id 2141 loss 0.03506601229310036 train acc 0.9839079285380663\n",
            "epoch 50 batch id 2151 loss 0.05266529694199562 train acc 0.9839318921431892\n",
            "epoch 50 batch id 2161 loss 0.04853128641843796 train acc 0.9838977903748265\n",
            "epoch 50 batch id 2171 loss 0.002463721903041005 train acc 0.9838711999078765\n",
            "epoch 50 batch id 2181 loss 0.08502551168203354 train acc 0.983859181568088\n",
            "epoch 50 batch id 2191 loss 0.023450704291462898 train acc 0.9838757987220448\n",
            "epoch 50 batch id 2201 loss 0.012021037749946117 train acc 0.983885165833712\n",
            "epoch 50 batch id 2211 loss 0.006024444475769997 train acc 0.9838732473993668\n",
            "epoch 50 batch id 2221 loss 0.12148770689964294 train acc 0.9838473660513283\n",
            "epoch 50 batch id 2231 loss 0.010364093817770481 train acc 0.9838287203047961\n",
            "epoch 50 batch id 2241 loss 0.06468639522790909 train acc 0.9838381302989737\n",
            "epoch 50 batch id 2251 loss 0.0033437805250287056 train acc 0.9838335739671257\n",
            "epoch 50 batch id 2261 loss 0.0005751503049395978 train acc 0.9838567005749669\n",
            "epoch 50 batch id 2271 loss 0.0028681715484708548 train acc 0.9838658630559225\n",
            "epoch 50 batch id 2281 loss 0.025226213037967682 train acc 0.9838612450679527\n",
            "epoch 50 batch id 2291 loss 0.01619241014122963 train acc 0.9838498472282846\n",
            "epoch 50 batch id 2301 loss 0.020596042275428772 train acc 0.9838725010864842\n",
            "epoch 50 batch id 2311 loss 0.0419253446161747 train acc 0.9838814366075292\n",
            "epoch 50 batch id 2321 loss 0.0658954381942749 train acc 0.9838700990952176\n",
            "epoch 50 batch id 2331 loss 0.05205908417701721 train acc 0.9838320463320464\n",
            "epoch 50 batch id 2341 loss 0.02660977840423584 train acc 0.9838477146518582\n",
            "epoch 50 batch id 2351 loss 0.039137113839387894 train acc 0.9838499574649086\n",
            "epoch 50 batch id 2361 loss 0.06264352053403854 train acc 0.9838786531130876\n",
            "epoch 50 batch id 2371 loss 0.00094454555073753 train acc 0.9839136967524251\n",
            "epoch 50 batch id 2381 loss 0.00021247129188850522 train acc 0.983955008399832\n",
            "epoch 50 batch id 2391 loss 0.02316473424434662 train acc 0.9839502300292765\n",
            "epoch 50 batch id 2401 loss 0.022868972271680832 train acc 0.9839454914618909\n",
            "epoch 50 batch id 2411 loss 0.03491711616516113 train acc 0.9839278307756117\n",
            "epoch 50 batch id 2421 loss 0.006988023407757282 train acc 0.9839490396530359\n",
            "epoch 50 batch id 2431 loss 0.05020641162991524 train acc 0.983957219251337\n",
            "epoch 50 batch id 2441 loss 0.00014767746324650943 train acc 0.983946128635805\n",
            "epoch 50 batch id 2451 loss 0.00820534024387598 train acc 0.9839478784169726\n",
            "epoch 50 batch id 2461 loss 0.041703782975673676 train acc 0.9839496139780577\n",
            "epoch 50 batch id 2471 loss 0.0714491605758667 train acc 0.9839260420882234\n",
            "epoch 50 batch id 2481 loss 0.050633903592824936 train acc 0.9839530431277711\n",
            "epoch 50 batch id 2491 loss 0.08261634409427643 train acc 0.9839359193095143\n",
            "epoch 50 batch id 2501 loss 0.006371128372848034 train acc 0.9839439224310276\n",
            "epoch 50 batch id 2511 loss 0.028124378994107246 train acc 0.9839518618080446\n",
            "epoch 50 batch id 2521 loss 0.04442078620195389 train acc 0.983928748512495\n",
            "epoch 50 batch id 2531 loss 0.013394470326602459 train acc 0.9839305116554722\n",
            "epoch 50 batch id 2541 loss 0.024197787046432495 train acc 0.9839261117670208\n",
            "epoch 50 batch id 2551 loss 0.031790152192115784 train acc 0.9839339964719718\n",
            "epoch 50 batch id 2561 loss 0.039225880056619644 train acc 0.9839235162046076\n",
            "epoch 50 batch id 2571 loss 0.07939572632312775 train acc 0.9839252722676002\n",
            "epoch 50 batch id 2581 loss 0.05324782431125641 train acc 0.9838967454475009\n",
            "epoch 50 batch id 2591 loss 0.013330004177987576 train acc 0.9839046217676573\n",
            "epoch 50 batch id 2601 loss 0.19325736165046692 train acc 0.9839124375240292\n",
            "epoch 50 batch id 2611 loss 0.0006058791186660528 train acc 0.9839441306013021\n",
            "epoch 50 batch id 2621 loss 0.09237143397331238 train acc 0.9839398130484548\n",
            "epoch 50 batch id 2631 loss 0.10582510381937027 train acc 0.9839236507031547\n",
            "epoch 50 batch id 2641 loss 0.014513800852000713 train acc 0.9839135270730783\n",
            "epoch 50 batch id 2651 loss 0.0003540792968124151 train acc 0.9839152678234628\n",
            "epoch 50 batch id 2661 loss 0.0015041610458865762 train acc 0.9839169954904171\n",
            "epoch 50 batch id 2671 loss 0.0034540065098553896 train acc 0.9839187102208911\n",
            "epoch 50 batch id 2681 loss 0.018818480893969536 train acc 0.9839204121596419\n",
            "epoch 50 batch id 2691 loss 0.0071147289127111435 train acc 0.9839104886659235\n",
            "epoch 50 batch id 2701 loss 0.07799076288938522 train acc 0.9839064235468346\n",
            "epoch 50 batch id 2711 loss 0.00035865959944203496 train acc 0.9838908613057912\n",
            "epoch 50 batch id 2721 loss 0.014156232587993145 train acc 0.9838754134509372\n",
            "epoch 50 batch id 2731 loss 0.0008115561213344336 train acc 0.9838886854632003\n",
            "epoch 50 batch id 2741 loss 0.1041531190276146 train acc 0.9838847592119664\n",
            "epoch 50 batch id 2751 loss 0.24649165570735931 train acc 0.9838467829880043\n",
            "epoch 50 batch id 2761 loss 0.10255908966064453 train acc 0.9838486961245926\n",
            "epoch 50 batch id 2771 loss 0.07962805777788162 train acc 0.9838280404186215\n",
            "epoch 50 batch id 2781 loss 0.027069970965385437 train acc 0.9838412441567781\n",
            "epoch 50 batch id 2791 loss 0.02273334003984928 train acc 0.9838431565747044\n",
            "epoch 50 batch id 2801 loss 0.09398543834686279 train acc 0.9838283202427704\n",
            "epoch 50 batch id 2811 loss 0.0347723513841629 train acc 0.9838358235503379\n",
            "epoch 50 batch id 2821 loss 0.019494013860821724 train acc 0.9838598901098901\n",
            "epoch 50 batch id 2831 loss 0.09270567446947098 train acc 0.9838506711409396\n",
            "epoch 50 batch id 2841 loss 0.051761139184236526 train acc 0.9838470168954594\n",
            "epoch 50 batch id 2851 loss 0.05757099762558937 train acc 0.9838324272185198\n",
            "epoch 50 batch id 2861 loss 0.07373485714197159 train acc 0.9838179395316323\n",
            "epoch 50 batch id 2871 loss 0.12089186906814575 train acc 0.98380355276907\n",
            "epoch 50 batch id 2881 loss 0.05245302990078926 train acc 0.9838001128080528\n",
            "epoch 50 batch id 2891 loss 0.00032836911850608885 train acc 0.9838075060532687\n",
            "epoch 50 batch id 2901 loss 0.010930829681456089 train acc 0.9838202344019303\n",
            "epoch 50 batch id 2911 loss 0.01720746047794819 train acc 0.9838006698728959\n",
            "epoch 50 batch id 2921 loss 0.00018931082740891725 train acc 0.9837651917151661\n",
            "epoch 50 batch id 2931 loss 0.07395949214696884 train acc 0.9837512794268168\n",
            "epoch 50 batch id 2941 loss 0.0017685939092189074 train acc 0.9837480873852431\n",
            "epoch 50 batch id 2951 loss 0.029006125405430794 train acc 0.9837766858691969\n",
            "epoch 50 batch id 2961 loss 0.019032977521419525 train acc 0.9837945373184734\n",
            "epoch 50 batch id 2971 loss 0.010003365576267242 train acc 0.9837754543924605\n",
            "epoch 50 batch id 2981 loss 0.004224628675729036 train acc 0.9837617410265012\n",
            "epoch 50 batch id 2991 loss 0.013292834162712097 train acc 0.983758567368773\n",
            "epoch 50 batch id 3001 loss 0.00045011998736299574 train acc 0.9837814478507164\n",
            "epoch 50 batch id 3011 loss 0.0030616798903793097 train acc 0.9837989870474926\n",
            "epoch 50 batch id 3021 loss 0.03300882503390312 train acc 0.9837853773584906\n",
            "epoch 50 batch id 3031 loss 0.14775662124156952 train acc 0.9837873226657868\n",
            "epoch 50 batch id 3041 loss 0.06878641992807388 train acc 0.9837532883919763\n",
            "epoch 50 batch id 3051 loss 0.037748005241155624 train acc 0.9837706899377253\n",
            "epoch 50 batch id 3061 loss 0.08132997900247574 train acc 0.9837675596210389\n",
            "epoch 50 batch id 3071 loss 0.0001216809032484889 train acc 0.9837797134483881\n",
            "epoch 50 batch id 3081 loss 0.02564348094165325 train acc 0.9837867169750081\n",
            "epoch 50 batch id 3091 loss 0.016870543360710144 train acc 0.9837936751860239\n",
            "epoch 50 batch id 3101 loss 0.06490030139684677 train acc 0.9837905111254434\n",
            "epoch 50 batch id 3111 loss 0.053353264927864075 train acc 0.9837823449051751\n",
            "epoch 50 batch id 3121 loss 0.023017102852463722 train acc 0.9838042694649151\n",
            "epoch 50 batch id 3131 loss 0.0036853139754384756 train acc 0.9838310443947621\n",
            "epoch 50 batch id 3141 loss 0.062303006649017334 train acc 0.9838278016555237\n",
            "epoch 50 batch id 3151 loss 0.03554427623748779 train acc 0.9838146620120597\n",
            "epoch 50 batch id 3161 loss 0.049743156880140305 train acc 0.983811491616577\n",
            "epoch 50 batch id 3171 loss 0.014724955894052982 train acc 0.9837935588142542\n",
            "epoch 50 batch id 3181 loss 0.03564177453517914 train acc 0.9837757387613958\n",
            "epoch 50 batch id 3191 loss 0.02082463540136814 train acc 0.9837874099028517\n",
            "epoch 50 batch id 3201 loss 0.07231201231479645 train acc 0.9837794829740706\n",
            "epoch 50 batch id 3211 loss 0.1663035899400711 train acc 0.9837570071628776\n",
            "epoch 50 batch id 3221 loss 0.08407174050807953 train acc 0.983720117975784\n",
            "epoch 50 batch id 3231 loss 0.015212731435894966 train acc 0.9837028009904054\n",
            "epoch 50 batch id 3241 loss 0.045104287564754486 train acc 0.9837000539956804\n",
            "epoch 50 batch id 3251 loss 0.03980547562241554 train acc 0.9836636804060289\n",
            "epoch 50 batch id 3261 loss 0.0832766517996788 train acc 0.9836275298988041\n",
            "epoch 50 batch id 3271 loss 0.055962368845939636 train acc 0.9836250382146132\n",
            "epoch 50 batch id 3281 loss 0.007541225757449865 train acc 0.9836177994513867\n",
            "epoch 50 batch id 3291 loss 0.016895746812224388 train acc 0.9836201002734731\n",
            "epoch 50 batch id 3301 loss 0.01452921237796545 train acc 0.9836129203271736\n",
            "epoch 50 batch id 3311 loss 0.010361689142882824 train acc 0.983615221987315\n",
            "epoch 50 batch id 3321 loss 0.03913196548819542 train acc 0.9836410343270099\n",
            "epoch 50 batch id 3331 loss 0.021097302436828613 train acc 0.9835916391474032\n",
            "epoch 50 batch id 3341 loss 0.06337378919124603 train acc 0.9836220442981143\n",
            "epoch 50 batch id 3351 loss 0.01292918249964714 train acc 0.9836056401074306\n",
            "epoch 50 batch id 3361 loss 0.04681861028075218 train acc 0.9836218759297828\n",
            "epoch 50 batch id 3371 loss 0.10292598605155945 train acc 0.9836241100563631\n",
            "epoch 50 batch id 3381 loss 0.00030164423515088856 train acc 0.9836448166223011\n",
            "epoch 50 batch id 3391 loss 0.041747622191905975 train acc 0.9836607932763197\n",
            "epoch 50 batch id 3401 loss 0.07023315131664276 train acc 0.9836445163187297\n",
            "epoch 50 batch id 3411 loss 0.025657834485173225 train acc 0.9836420771034887\n",
            "epoch 50 batch id 3421 loss 0.07391978055238724 train acc 0.9836579216603333\n",
            "epoch 50 batch id 3431 loss 0.09374380856752396 train acc 0.9836600116584087\n",
            "epoch 50 batch id 3441 loss 0.06205100193619728 train acc 0.9836575486777099\n",
            "epoch 50 batch id 3451 loss 0.008291970007121563 train acc 0.9836550999710229\n",
            "epoch 50 batch id 3461 loss 0.0017962460406124592 train acc 0.9836571800057787\n",
            "epoch 50 batch id 3471 loss 0.046965599060058594 train acc 0.9836412417170844\n",
            "epoch 50 batch id 3481 loss 0.140618234872818 train acc 0.9836298836541224\n",
            "epoch 50 batch id 3491 loss 0.004570739809423685 train acc 0.9836275422515038\n",
            "epoch 50 batch id 3501 loss 0.01980568654835224 train acc 0.9836475292773493\n",
            "epoch 50 batch id 3511 loss 0.018684078007936478 train acc 0.9836228994588436\n",
            "epoch 50 batch id 3521 loss 0.034050729125738144 train acc 0.9836427861403011\n",
            "epoch 50 batch id 3531 loss 0.015921415761113167 train acc 0.9836581350892099\n",
            "epoch 50 batch id 3541 loss 0.06947016716003418 train acc 0.9836513343688223\n",
            "epoch 50 batch id 3551 loss 0.01743766851723194 train acc 0.9836225711067305\n",
            "epoch 50 batch id 3561 loss 0.010889601893723011 train acc 0.9836334597023308\n",
            "epoch 50 batch id 3571 loss 0.04564964398741722 train acc 0.9836311607392887\n",
            "epoch 50 batch id 3581 loss 0.01972142979502678 train acc 0.9836463278413851\n",
            "epoch 50 batch id 3591 loss 0.06124477460980415 train acc 0.9836483570036202\n",
            "epoch 50 batch id 3601 loss 0.021342379972338676 train acc 0.9836373576784226\n",
            "epoch 50 batch id 3611 loss 0.001079603098332882 train acc 0.983652381611742\n",
            "epoch 50 batch id 3621 loss 0.0636751726269722 train acc 0.9836414319248826\n",
            "epoch 50 batch id 3631 loss 0.013121966272592545 train acc 0.983621936105756\n",
            "epoch 50 batch id 3641 loss 0.009232367388904095 train acc 0.9836454614117001\n",
            "epoch 50 batch id 3651 loss 0.0347469337284565 train acc 0.983626061353054\n",
            "epoch 50 batch id 3661 loss 0.0030875983648002148 train acc 0.983623839114996\n",
            "epoch 50 batch id 3671 loss 0.05761134624481201 train acc 0.9836301416507763\n",
            "epoch 50 batch id 3681 loss 0.003980357199907303 train acc 0.9836576337951644\n",
            "epoch 50 batch id 3691 loss 0.0042318319901824 train acc 0.9836680438905445\n",
            "epoch 50 batch id 3701 loss 0.03021276742219925 train acc 0.9836572885706566\n",
            "epoch 50 batch id 3711 loss 0.01253692526370287 train acc 0.9836508016707087\n",
            "epoch 50 batch id 3721 loss 0.07096286863088608 train acc 0.9836485487772104\n",
            "epoch 50 batch id 3731 loss 0.03250591829419136 train acc 0.9836588716161887\n",
            "epoch 50 batch id 3741 loss 0.04723447188735008 train acc 0.9836566091954023\n",
            "epoch 50 batch id 3751 loss 0.002006572438403964 train acc 0.9836751866169021\n",
            "epoch 50 batch id 3761 loss 0.12621790170669556 train acc 0.9836438114863069\n",
            "epoch 50 batch id 3771 loss 0.024871818721294403 train acc 0.9836333200742509\n",
            "epoch 50 batch id 3781 loss 0.13988004624843597 train acc 0.9836104866437451\n",
            "epoch 50 batch id 3791 loss 0.0008615461993031204 train acc 0.9836331113162754\n",
            "epoch 50 batch id 3801 loss 0.04799959808588028 train acc 0.9836145093396474\n",
            "epoch 50 batch id 3811 loss 0.021902015432715416 train acc 0.9836329047494096\n",
            "epoch 50 batch id 3821 loss 0.018724041059613228 train acc 0.9836307576550641\n",
            "epoch 50 batch id 3831 loss 0.0003082603507209569 train acc 0.9836367789089011\n",
            "epoch 50 batch id 3841 loss 0.0001364345516776666 train acc 0.9836224290549336\n",
            "epoch 50 batch id 3851 loss 0.00013065928942523897 train acc 0.9836324980524539\n",
            "epoch 50 batch id 3861 loss 0.017658833414316177 train acc 0.9836222804972805\n",
            "epoch 50 batch id 3871 loss 0.1004035472869873 train acc 0.9836282614311548\n",
            "epoch 50 batch id 3881 loss 0.09874196350574493 train acc 0.9836140814223139\n",
            "epoch 50 batch id 3891 loss 0.010126740671694279 train acc 0.9836120213312773\n",
            "epoch 50 batch id 3901 loss 0.010607995092868805 train acc 0.9836099718021021\n",
            "epoch 50 batch id 3911 loss 0.034437816590070724 train acc 0.9835799667604194\n",
            "epoch 50 batch id 3921 loss 0.15085375308990479 train acc 0.9835939492476409\n",
            "epoch 50 batch id 3931 loss 0.03597108647227287 train acc 0.9835919613329942\n",
            "epoch 50 batch id 3941 loss 0.0016462822677567601 train acc 0.9836137718853083\n",
            "epoch 50 batch id 3951 loss 0.06274919956922531 train acc 0.9835880156922299\n",
            "epoch 50 batch id 3961 loss 0.06805568188428879 train acc 0.983590002524615\n",
            "epoch 50 batch id 3971 loss 0.03993682190775871 train acc 0.9835919793502896\n",
            "epoch 50 batch id 3981 loss 0.004747667349874973 train acc 0.9835978711379051\n",
            "epoch 50 batch id 3991 loss 0.15107733011245728 train acc 0.9835998183412679\n",
            "epoch 50 batch id 4001 loss 0.07795241475105286 train acc 0.9836017558110473\n",
            "epoch 50 batch id 4011 loss 0.15876714885234833 train acc 0.9835764148591374\n",
            "epoch 50 batch id 4021 loss 0.021922826766967773 train acc 0.9835900584431734\n",
            "epoch 50 batch id 4031 loss 0.001031024381518364 train acc 0.9836152629620442\n",
            "epoch 50 batch id 4041 loss 0.030627762898802757 train acc 0.9836094097995546\n",
            "epoch 50 batch id 4051 loss 0.027537047863006592 train acc 0.9836035855344359\n",
            "epoch 50 batch id 4061 loss 0.01847887597978115 train acc 0.9836054851021916\n",
            "epoch 50 batch id 4071 loss 0.015849992632865906 train acc 0.9836227278309998\n",
            "epoch 50 batch id 4081 loss 0.016822131350636482 train acc 0.9836437147757903\n",
            "epoch 50 batch id 4091 loss 0.03869611769914627 train acc 0.9836493216817404\n",
            "epoch 50 batch id 4101 loss 0.00024500000290572643 train acc 0.9836549012435991\n",
            "epoch 50 batch id 4111 loss 0.018858181312680244 train acc 0.9836338482121139\n",
            "epoch 50 batch id 4121 loss 0.018941713497042656 train acc 0.9836432297985925\n",
            "epoch 50 batch id 4131 loss 0.07014437764883041 train acc 0.9836412188332123\n",
            "epoch 50 batch id 4141 loss 0.000469749269541353 train acc 0.9836505373098285\n",
            "epoch 50 batch id 4151 loss 0.07737812399864197 train acc 0.9836673391953746\n",
            "epoch 50 batch id 4161 loss 0.02709621749818325 train acc 0.9836465092525836\n",
            "epoch 50 batch id 4171 loss 0.11879078298807144 train acc 0.9836520019180053\n",
            "epoch 50 batch id 4181 loss 0.013314181938767433 train acc 0.9836350454436738\n",
            "epoch 50 batch id 4191 loss 0.07377395033836365 train acc 0.9836032569792412\n",
            "epoch 50 batch id 4201 loss 0.02935042232275009 train acc 0.9836199714353725\n",
            "epoch 50 batch id 4211 loss 0.08539924025535583 train acc 0.9835995013061031\n",
            "epoch 50 batch id 4221 loss 0.08065115660429001 train acc 0.9835976368159204\n",
            "epoch 50 batch id 4231 loss 0.0549345500767231 train acc 0.9835994741195935\n",
            "epoch 50 batch id 4241 loss 0.09394779801368713 train acc 0.9836049870313606\n",
            "epoch 50 batch id 4251 loss 0.07861039787530899 train acc 0.9836104740061162\n",
            "epoch 50 batch id 4261 loss 0.002908980008214712 train acc 0.9836306031448017\n",
            "epoch 50 batch id 4271 loss 0.0017963068094104528 train acc 0.9836360044486069\n",
            "epoch 50 batch id 4281 loss 0.02190140075981617 train acc 0.9836450303667368\n",
            "epoch 50 batch id 4291 loss 0.00977652333676815 train acc 0.9836321661617339\n",
            "epoch 50 batch id 4301 loss 0.0452689528465271 train acc 0.9836048302720297\n",
            "epoch 50 batch id 4311 loss 0.029457872733473778 train acc 0.9836138656924147\n",
            "epoch 50 batch id 4321 loss 0.022629333660006523 train acc 0.9836011629252488\n",
            "epoch 50 batch id 4331 loss 0.08613520115613937 train acc 0.9835921265296699\n",
            "epoch 50 batch id 4341 loss 0.04162885248661041 train acc 0.983593929970053\n",
            "epoch 50 batch id 4351 loss 0.02951684221625328 train acc 0.9835741783498047\n",
            "epoch 50 batch id 4361 loss 0.011445967480540276 train acc 0.9835760146755331\n",
            "epoch 50 batch id 4371 loss 0.03119627945125103 train acc 0.983584991992679\n",
            "epoch 50 batch id 4381 loss 0.11854802072048187 train acc 0.9835725291029446\n",
            "epoch 50 batch id 4391 loss 0.000356384611222893 train acc 0.9835601229788203\n",
            "epoch 50 batch id 4401 loss 0.025977294892072678 train acc 0.9835655248807089\n",
            "epoch 50 batch id 4411 loss 0.0025331368669867516 train acc 0.9835673600090682\n",
            "epoch 50 batch id 4421 loss 0.02874135598540306 train acc 0.9835727211038227\n",
            "epoch 50 batch id 4431 loss 0.02691308595240116 train acc 0.9835604265402843\n",
            "epoch 50 batch id 4441 loss 0.01028404850512743 train acc 0.9835728158072506\n",
            "epoch 50 batch id 4451 loss 0.000320677732815966 train acc 0.9835675971691754\n",
            "epoch 50 batch id 4461 loss 0.0003301916876807809 train acc 0.9835729096615109\n",
            "epoch 50 batch id 4471 loss 0.15640367567539215 train acc 0.983557229926191\n",
            "epoch 50 batch id 4481 loss 0.012106504291296005 train acc 0.983548594063825\n",
            "epoch 50 batch id 4491 loss 0.11106981337070465 train acc 0.9835330382988199\n",
            "epoch 50 batch id 4501 loss 0.010247445665299892 train acc 0.9835210231059764\n",
            "epoch 50 batch id 4511 loss 0.07204324007034302 train acc 0.9835159886943028\n",
            "epoch 50 batch id 4521 loss 0.10544592142105103 train acc 0.9835144326476444\n",
            "epoch 50 batch id 4531 loss 0.00013139618386048824 train acc 0.9835163319355551\n",
            "epoch 50 batch id 4541 loss 0.06334912776947021 train acc 0.9835182228584012\n",
            "epoch 50 batch id 4551 loss 0.07353481650352478 train acc 0.9835166721599649\n",
            "epoch 50 batch id 4561 loss 0.06929483264684677 train acc 0.9835082766937076\n",
            "epoch 50 batch id 4571 loss 0.047318026423454285 train acc 0.9835033362502734\n",
            "epoch 50 batch id 4581 loss 0.0236348994076252 train acc 0.9835120606854398\n",
            "epoch 50 batch id 4591 loss 0.05090750753879547 train acc 0.9835071335221085\n",
            "epoch 50 batch id 4601 loss 0.01193386223167181 train acc 0.9835022277765703\n",
            "epoch 50 batch id 4611 loss 0.033597126603126526 train acc 0.9835176751247018\n",
            "epoch 50 batch id 4621 loss 0.017624106258153915 train acc 0.9835229117074227\n",
            "epoch 50 batch id 4631 loss 0.07190676033496857 train acc 0.9835382476786871\n",
            "epoch 50 batch id 4641 loss 0.0731980949640274 train acc 0.9835467840982547\n",
            "epoch 50 batch id 4651 loss 0.09163019806146622 train acc 0.9835452053321866\n",
            "epoch 50 batch id 4661 loss 0.03273581340909004 train acc 0.9835536901952371\n",
            "epoch 50 batch id 4671 loss 0.01196589507162571 train acc 0.9835755191607792\n",
            "epoch 50 batch id 4681 loss 0.22384241223335266 train acc 0.9835672132023072\n",
            "epoch 50 batch id 4691 loss 0.00459512323141098 train acc 0.983542288424643\n",
            "epoch 50 batch id 4701 loss 0.017185019329190254 train acc 0.9835540310572218\n",
            "epoch 50 batch id 4711 loss 0.041122112423181534 train acc 0.9835557737210783\n",
            "epoch 50 batch id 4721 loss 0.00092096789740026 train acc 0.9835475799618725\n",
            "epoch 50 batch id 4731 loss 0.03418320044875145 train acc 0.9835559342633693\n",
            "epoch 50 batch id 4741 loss 0.045661561191082 train acc 0.9835510704492723\n",
            "epoch 50 batch id 4751 loss 0.14553119242191315 train acc 0.9835528046727005\n",
            "epoch 50 batch id 4761 loss 0.051197350025177 train acc 0.9835479678638941\n",
            "epoch 50 batch id 4771 loss 0.05307692289352417 train acc 0.9835497013204779\n",
            "epoch 50 batch id 4781 loss 0.06315234303474426 train acc 0.9835350868019243\n",
            "epoch 50 batch id 4791 loss 0.12561462819576263 train acc 0.9835270559382175\n",
            "epoch 50 batch id 4801 loss 0.06950013339519501 train acc 0.9835190585294731\n",
            "epoch 50 batch id 4811 loss 0.0542018823325634 train acc 0.983527333194762\n",
            "epoch 50 batch id 4821 loss 0.09525268524885178 train acc 0.9835323325036299\n",
            "epoch 50 batch id 4831 loss 0.024292664602398872 train acc 0.9835373111157111\n",
            "epoch 50 batch id 4841 loss 0.013312485069036484 train acc 0.9835358138814294\n",
            "epoch 50 batch id 4851 loss 0.08820000290870667 train acc 0.9835504277468563\n",
            "epoch 50 batch id 4861 loss 0.057857949286699295 train acc 0.9835392666118082\n",
            "epoch 50 batch id 4871 loss 0.0035902594681829214 train acc 0.9835409823444878\n",
            "epoch 50 batch id 4881 loss 0.035595767199993134 train acc 0.9835490934234788\n",
            "epoch 50 batch id 4891 loss 0.016697613522410393 train acc 0.9835507820486608\n",
            "epoch 50 batch id 4901 loss 0.0005323190125636756 train acc 0.983549275658029\n",
            "epoch 50 batch id 4911 loss 0.1413600891828537 train acc 0.983535048869884\n",
            "epoch 50 batch id 4921 loss 0.17051221430301666 train acc 0.9835335805730543\n",
            "epoch 50 batch id 4931 loss 0.0790015161037445 train acc 0.9835131058608801\n",
            "epoch 50 batch id 4941 loss 0.10350946336984634 train acc 0.9834990386561425\n",
            "epoch 50 batch id 4951 loss 0.08347593247890472 train acc 0.9834881842052111\n",
            "epoch 50 train acc 0.9834798012911035\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa1196646ba4c44b604f4c1fcce6301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1240 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 50 loss 0.5398192405700684 test acc 0.8529382789589444\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "best_acc = 0\n",
        "max_patience = 5\n",
        "patience = 0\n",
        "PATH = log_dir\n",
        "writer = SummaryWriter(PATH)\n",
        "\n",
        "# print(\"Starting training loop...\")\n",
        "for e in range(num_epochs):\n",
        "    # print(\"Starting epoch \", e+1)\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    # for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, attention_mask, label) in enumerate(tqdm(train_dataloader)):\n",
        "        # print(\"Starting batch \", batch_id+1)    \n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids, attention_mask)\n",
        "        # print(f'Output: {out.argmax(1)}, Label: {label}')\n",
        "\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            writer.add_scalar('loss/train_loss', loss.data.cpu().numpy(), e*len(train_dataloader)+batch_id+1)\n",
        "            writer.add_scalar('acc/train_acc', train_acc / (batch_id+1), e*len(train_dataloader)+batch_id+1)\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, attention_mask, label) in enumerate(tqdm(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids, attention_mask)\n",
        "        # print(f'Output: {out.argmax(1)}, Label: {label}')\n",
        "        \n",
        "        test_acc += calc_accuracy(out, label)\n",
        "        loss = loss_fn(out, label)\n",
        "    writer.add_scalar('acc/test_acc', test_acc / (batch_id+1), e+1)        \n",
        "    writer.add_scalar('loss/test_loss', loss.data.cpu().numpy(), e+1)        \n",
        "    print(\"epoch {} loss {} test acc {}\".format(e+1, loss.data.cpu().numpy(), test_acc / (batch_id+1)))\n",
        "    if test_acc/(batch_id+1)>best_acc:\n",
        "      best_acc=test_acc/(batch_id+1)\n",
        "      patience=0\n",
        "      torch.save(model, PATH + '{}_{}_model.pt'.format(e+1, test_acc/(batch_id+1)))\n",
        "    else:\n",
        "      patience+=1\n",
        "    if patience>max_patience:\n",
        "      break\n",
        "    test_acc = 0  # Reset test accuracy for the next epoch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the model from PATH?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sU2v6tzs7p72"
      },
      "source": [
        "# Interactive Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup, BertTokenizer, BertModel\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "import io \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def del_bracket(s):\n",
        "  pattern = r'\\([^)]*\\)'  # ()\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\[[^)]*\\]'  # []\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\<[^)]*\\>'  # <>\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  pattern = r'\\{[^)]*\\}'  # {}\n",
        "  s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "  return s\n",
        "\n",
        "def del_special_num(s):\n",
        "  pattern = r'[^a-zA-Z가-힣]'\n",
        "  s = re.sub(pattern=pattern, repl=' ', string=s)\n",
        "\n",
        "  return s\n",
        "\n",
        "def del_unit(s):\n",
        "  units = ['mm', 'cm', 'km', 'ml', 'kg', 'g']\n",
        "  for unit in units:\n",
        "    s = s.lower() # 대문자를 소문자로 변환\n",
        "    s = s.replace(unit, '')\n",
        "  return s\n",
        "\n",
        "def del_whitespace(s):\n",
        "  return \" \".join(s.split())\n",
        "  \n",
        "def del_stopwords(s):\n",
        "  stopwords = open(\"data/stopwords.txt\", 'r', encoding=\"utf-8\").read().split()\n",
        "  #print(stopwords)\n",
        "  s_o=s.split()\n",
        "  s_f=[]\n",
        "  for w in s_o:\n",
        "    if w.strip() not in stopwords:\n",
        "      s_f.append(w.strip())\n",
        "  return \" \".join(s_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "modelname = \"klue/bert-base\"\n",
        "max_length = 64\n",
        "num_classes = 6\n",
        "\n",
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "#BERT 모델 불러오기\n",
        "bertmodel = BertModel.from_pretrained(modelname)\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=num_classes,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        pooler = outputs[1]\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "tensor([[-1.1051, -2.7562,  9.9050, -2.4808, -3.0826,  0.3052]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# assuming you have a function for tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
        "\n",
        "# Define a function to get the valid_length, attention_mask and segment_ids\n",
        "def get_inputs(tokens):\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    valid_length = len(tokens)\n",
        "    segment_ids = [0]*valid_length\n",
        "    attention_mask = [1]*valid_length\n",
        "\n",
        "    # Pad up to max length\n",
        "    if valid_length < max_length:\n",
        "        pad_length = max_length - valid_length\n",
        "        tokens.extend(['[PAD]' for _ in range(pad_length)])\n",
        "        attention_mask.extend([0]*pad_length)\n",
        "        segment_ids.extend([0]*pad_length)\n",
        "\n",
        "    # Convert tokens to IDs\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    return torch.tensor([token_ids], dtype=torch.long), torch.tensor([valid_length], dtype=torch.long), torch.tensor([segment_ids], dtype=torch.long), torch.tensor([attention_mask], dtype=torch.long)\n",
        "\n",
        "# assuming you have a function for pre-processing\n",
        "def preprocess(text):\n",
        "    for t in text:\n",
        "        t=del_bracket(t)\n",
        "        t=del_special_num(t)\n",
        "        t=del_whitespace(t)\n",
        "        t=del_stopwords(t)\n",
        "    return text.lower()\n",
        "\n",
        "# Load text\n",
        "with open('data/test1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Preprocess and tokenize the text\n",
        "tokens = tokenizer.tokenize(preprocess(text))\n",
        "if len(tokens) > max_length-2: # Account for [CLS] and [SEP]\n",
        "    tokens = tokens[:max_length-2]\n",
        "\n",
        "# Get inputs\n",
        "token_ids, valid_length, segment_ids, attention_mask = get_inputs(tokens)\n",
        "\n",
        "# Load the model\n",
        "model_path = 'logs/20230519-modelklue/bert-base-data291355/a4-class6-batch64-lr1e-05-epoch50-maxlen6447_0.9486130537162282_model.pt'  # replace with your actual path\n",
        "model = torch.load(model_path)\n",
        "\n",
        "# Switch to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Determine if a cuda-capable GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Load the model and move to the GPU if available\n",
        "model = torch.load(model_path)\n",
        "model.to(device)\n",
        "\n",
        "# Move all your tensors to the same device as your model\n",
        "token_ids = token_ids.to(device)\n",
        "valid_length = valid_length.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "# Ensure no gradient is calculated\n",
        "with torch.no_grad():\n",
        "    sentiment_vector = model(token_ids, valid_length, segment_ids, attention_mask)\n",
        "\n",
        "print(sentiment_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "당신은 지금 슬픔을 느끼고 있네요.\n",
            "주 감정: 99.99% 슬픔, 부 감정: 0.01% 당황\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Get the output from your model\n",
        "output = sentiment_vector\n",
        "\n",
        "# Apply softmax to convert output to probabilities\n",
        "probabilities = F.softmax(output, dim=-1)\n",
        "\n",
        "emotions = ['기쁨', '불안', '슬픔', '분노', '상처', '당황'] \n",
        "\n",
        "# Get the indices that would sort the probability tensor\n",
        "sorted_indices = torch.argsort(probabilities, dim=-1, descending=True)\n",
        "\n",
        "primary_emotion_idx = sorted_indices[0][0].item()\n",
        "secondary_emotion_idx = sorted_indices[0][1].item()\n",
        "\n",
        "primary_emotion = emotions[primary_emotion_idx]\n",
        "primary_emotion_probability = probabilities[0][primary_emotion_idx].item()\n",
        "\n",
        "secondary_emotion = emotions[secondary_emotion_idx]\n",
        "secondary_emotion_probability = probabilities[0][secondary_emotion_idx].item()\n",
        "\n",
        "print(f\"당신은 지금 {primary_emotion}을 느끼고 있네요.\")\n",
        "print(f\"주 감정: {primary_emotion_probability * 100:.2f}% {primary_emotion}, 부 감정: {secondary_emotion_probability * 100:.2f}% {secondary_emotion}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make music library with lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the music data\n",
        "df_music = pd.read_csv(\"lyrics_indi(1-22000).csv\")\n",
        "\n",
        "def process_lyrics(lyrics, tokenizer, model):\n",
        "    # Preprocess and tokenize the lyrics\n",
        "    tokens = tokenizer.tokenize(preprocess(lyrics))\n",
        "    if len(tokens) > max_length-2: # Account for [CLS] and [SEP]\n",
        "        tokens = tokens[:max_length-2]\n",
        "\n",
        "    # Get inputs\n",
        "    token_ids, valid_length, segment_ids, attention_mask = get_inputs(tokens)\n",
        "\n",
        "    # Move all your tensors to the same device as your model\n",
        "    token_ids = token_ids.to(device)\n",
        "    valid_length = valid_length.to(device)\n",
        "    segment_ids = segment_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # Ensure no gradient is calculated\n",
        "    with torch.no_grad():\n",
        "        sentiment_vector = model(token_ids, valid_length, segment_ids, attention_mask)\n",
        "\n",
        "    return sentiment_vector.detach().cpu().numpy()[0]  # Return as a 1-D numpy array\n",
        "\n",
        "# Process all songs and add a new column for the sentiment vectors\n",
        "df_music['sentiment_vector'] = df_music['lyrics'].apply(lambda x: process_lyrics(x, tokenizer, model))\n",
        "\n",
        "# Save to a new CSV file\n",
        "df_music.to_csv(\"music_library.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to compute cosine similarities and retrieve the top 10 songs\n",
        "def recommend_songs(diary_text):\n",
        "    # Process the diary text\n",
        "    diary_vector = process_text(diary_text, tokenizer, model)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity([diary_vector], df_music['sentiment_vector'].to_list())\n",
        "\n",
        "    # Get the top 10 song indices\n",
        "    top_10_indices = similarities[0].argsort()[-10:][::-1]\n",
        "\n",
        "    # Return the corresponding songs\n",
        "    return df_music.iloc[top_10_indices]\n",
        "\n",
        "# Function to print out the recommended songs\n",
        "def print_recommended_songs(diary_text):\n",
        "    recommended_songs = recommend_songs(diary_text)\n",
        "    for i, song in recommended_songs.iterrows():\n",
        "        similarity = cosine_similarity([process_text(diary_text, tokenizer, model)], [song['sentiment_vector']])[0][0]\n",
        "        print(f\"Title: {song['title']}, Artist: {song['artist']}, Lyrics: {song['lyrics']}, Similarity: {similarity*100:.2f}%\")\n",
        "\n",
        "# Test with a diary text\n",
        "with open('data/test1.txt', 'r', encoding='utf-8') as f:\n",
        "    diary_text = f.read()\n",
        "\n",
        "print_recommended_songs(diary_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3rnTL6EJgqcs",
        "YQs-0d15k6kM",
        "vvyM1NUdf31A",
        "v-zZrUrGe8_l",
        "UgNIl_H7qzDZ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000f4916328040fb8554d90b37165d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73ed57a18a6d448d95e1a48bae73d35b",
              "IPY_MODEL_61052ff7e4444f099292f4f47a358343",
              "IPY_MODEL_5c1143a5cef24d2ab857a8fc50059f01"
            ],
            "layout": "IPY_MODEL_b5427a090db54b09bc6e2072cd17e576"
          }
        },
        "013b1300b7d0453d9a44e5474ef7b599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0363c8b048ec46509cb6416aee1c04fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04bf632fcaf24df8a84d478602219297": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3641c21e45a849159ce773a056f73954",
            "placeholder": "​",
            "style": "IPY_MODEL_8366e96e7e864dac8abdc17f1c8440c1",
            "value": " 144/144 [00:33&lt;00:00,  4.62it/s]"
          }
        },
        "060777b420c541a79d53f68296b169b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06c49b944e4c46fe8b8ac2c302a0d3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06c7cdfeb0ba45ef85ea6b45a8c44720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565961d186404d9a937922a3f403842a",
            "placeholder": "​",
            "style": "IPY_MODEL_f480cc1a318d454191e333c259c637ce",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "0818931c8ac245ae843a05885527b289": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "088d5ca1d8274bc8b0ac7dc5d32bf4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4b16e660014d98beca81dd935f2f7d",
            "placeholder": "​",
            "style": "IPY_MODEL_b20e948a56144e63b563668d0311c016",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "08924d4dba8446d6ac55983d6a59a979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d29f6c76bd406c804de3dd253fcfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79845f08dc9e4ba1921dca78b81921d6",
            "placeholder": "​",
            "style": "IPY_MODEL_b1308fa4a951401d89072e60b61c316f",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "0ad034620c0a48a085a8cffd1f8f5d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7193a948f3c249e2950a20fcc635d86b",
            "placeholder": "​",
            "style": "IPY_MODEL_9ef86f8d144f48148270f3cea7aefab0",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "0c5c64d2e05247a88eef8c050e44ee39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c709a5117ae413c84915bf6de80df9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5e1719cb7b945918d0d40a245464735",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26e7ea617980414e90e62ede2c2b7da2",
            "value": 576
          }
        },
        "0e05e3d70f3b4bfab0f92e8ff08aecb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ea405aae134456b8af80bdbfbd7d995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f6db49db25b4a608ae6f920bd4e11b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e5c18841c9a43638b6c471babb28f1c",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf16ab2065b449c6bbdfde8e18806b62",
            "value": 576
          }
        },
        "10814b075eb5494ba86e5cfa53978bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "115a8cc8a6bd451aa3b0ad4c31fa10d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116ba1d2272f4428a5fbb7e688fcb753": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11737bff28844599822a9a007a4b26a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "139b2ece68a341dfac81de12f6f6f3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe5b648c708340aa9a8e659eda1085cc",
            "placeholder": "​",
            "style": "IPY_MODEL_efc55659dd2446a0a34c32043d84c150",
            "value": "100%"
          }
        },
        "13fde8cee5d44843bd9577f6de9a04b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "145bf764473440c8a3cea3dfd67521b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16ae5377841c49d5aea7933488075eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "178ebf6beaea450ea8b086c276ba3ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f7af4954d54264949563d05727b8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76e534ed8784f54861ad4de857db319",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e139e9c256b4ea6b70cbb1776926fa4",
            "value": 576
          }
        },
        "19fcce698cc44c4596176e6cb014edee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ae33617b216446dac0b69bc6bfeabe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3645c5e3dd7749c3abd15f7a6546dc1a",
            "placeholder": "​",
            "style": "IPY_MODEL_acc626cc4f8346a0819e776961df96e9",
            "value": "100%"
          }
        },
        "1af356108c274cc4b412c2cb54de0a03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5c7a9b73644a61ae0a4d35f9ef3b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640f306faa694e85ad3158761d1d1e98",
            "placeholder": "​",
            "style": "IPY_MODEL_ab97790aca584fb6bcb88cd9143c2567",
            "value": "100%"
          }
        },
        "1cb24bf2ffc944df959d9fde0dcc1a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51a078097f446738f377756597ee609",
            "placeholder": "​",
            "style": "IPY_MODEL_c453ad3af78143a4b9d8900c54ec0d9f",
            "value": " 144/144 [00:33&lt;00:00,  4.64it/s]"
          }
        },
        "1d65266578ff4bcf8b6184b563bb1a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d8c302064b147aaa863ae401d4a0ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8501b75f202642a0aa57db3f7d0396ef",
            "placeholder": "​",
            "style": "IPY_MODEL_466a8ec9ba054731b8cc7c1d462485ff",
            "value": "100%"
          }
        },
        "1f654c9f2c6e431d8f875633bc99c861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e41ea261063947a29ccd7eda19a830a9",
            "placeholder": "​",
            "style": "IPY_MODEL_2d45ac4bb92847059142ceabfc8796ad",
            "value": " 144/144 [00:33&lt;00:00,  4.59it/s]"
          }
        },
        "204ffa7119ac4179be97d4c8cfea73a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2051ec926c534139a12c9dafaa5f1d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20627e86c90f47ceaaae0fd8c82659c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9266c67eb01f4c5aa818511aa70ce9e1",
            "placeholder": "​",
            "style": "IPY_MODEL_23829f30b3064f3d82a199debe5cbc3f",
            "value": "100%"
          }
        },
        "210565db3d154d2da4478a97d27c19b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215620ac6e684ef9a436f1f390331893": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9ba025ca762447f8e1108d8a8ac21ce",
            "placeholder": "​",
            "style": "IPY_MODEL_e1481d88aaec476bae81d700176ff07e",
            "value": "100%"
          }
        },
        "21a5f87ec7774ccb80daeb595a9c1329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd0296a4f45547d6a0df66dcca13a20a",
              "IPY_MODEL_7921db96833a42a09799bf27287d8d45",
              "IPY_MODEL_a1933ab0570a4372bb6e6459b89cb3b2"
            ],
            "layout": "IPY_MODEL_d187ec9b7715432e8bb61def9a97f191"
          }
        },
        "21f3be4e06a54038a671777d39932cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "221e081afee145a7a7b614839ca435e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23829f30b3064f3d82a199debe5cbc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2571eaecd6b74be8b0f40160a23de443": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68402970bb9245f1bc6ac074203782c8",
            "placeholder": "​",
            "style": "IPY_MODEL_c105ca9f1cc0450092ae65a59d3138d6",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "25af9f53ac224586ac42212c3ce69d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "260fbad624e4481f9791898787b06e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26176e057bc947a19964f85c615a10fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26383e1a367846dd935553dd05a22c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a21eebafe44d85b73db64a07a2bbf7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e7ea617980414e90e62ede2c2b7da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "272be9695d0147b595c83cea40c4a0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2751e6becf1e48b492c9015b375d660c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e0009d37454db8b1a615adf92f7238",
            "placeholder": "​",
            "style": "IPY_MODEL_16ae5377841c49d5aea7933488075eb8",
            "value": "100%"
          }
        },
        "27bbe64e53d647f3b873c21292000da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13fde8cee5d44843bd9577f6de9a04b2",
            "placeholder": "​",
            "style": "IPY_MODEL_260fbad624e4481f9791898787b06e1e",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "287b47e2a1cd4c05b00b75fa94788b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2888b901d070495ebc440e421aef9c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bef23c7fb87646f6848929f6c1fecf4c",
            "placeholder": "​",
            "style": "IPY_MODEL_a4d09608760b48deb7934871ee6e11b9",
            "value": "100%"
          }
        },
        "28aac017ffca4c1b92fef83a9827c7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291d5237a3b7419fabea1f7cadf554f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af37427a877432794a60b060b4db831": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b4daa450c6c40dea1dea0073949a217": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_537d419eea514f0e8547f51ff2d289ba",
            "placeholder": "​",
            "style": "IPY_MODEL_a88d7de22bc44d5c9be9e448ae189230",
            "value": "100%"
          }
        },
        "2bc545952c334e86811ab2afb753d68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_435b2800b0e04aa9b7bfd38a409cfb6b",
            "placeholder": "​",
            "style": "IPY_MODEL_8e284d1e4ea74db39f571641d60a46c5",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "2d45ac4bb92847059142ceabfc8796ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e5c18841c9a43638b6c471babb28f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f854ed5b43e43ab883b3e95bb64f79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3120ef27193e4f8ea5795a7e2430a6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31d433deb855466cb715d998392a60dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "322446e27fb94835b0c0a6763f7052e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32debaf7b0844c718d44d376322ca725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6cafc71727e41f6a8e3e6f49969479d",
            "placeholder": "​",
            "style": "IPY_MODEL_b37530d30f7b46dba8d8105decca6d8b",
            "value": " 144/144 [00:33&lt;00:00,  4.61it/s]"
          }
        },
        "333c3c2696ba4686aed6f5337325e5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "344321b6b58d42f4b8b1772f4bd64aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dea7f410ea664215b26150ccff543916",
              "IPY_MODEL_36ae51fcb62841d393f2746328568aee",
              "IPY_MODEL_088d5ca1d8274bc8b0ac7dc5d32bf4e3"
            ],
            "layout": "IPY_MODEL_37768cb504674277aa33155bea00f088"
          }
        },
        "358e3a615e8a43148f2356c9c74ae4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b45ccb7cd94303a159515b413d5d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3626d0bdd8e14904bd70784f6fadde7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bcbf5353ddb41f58b3498e00fe31bd9",
            "placeholder": "​",
            "style": "IPY_MODEL_d45e7cefc7ec4cdaa7661439a3ad88ad",
            "value": "100%"
          }
        },
        "3641c21e45a849159ce773a056f73954": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3645c5e3dd7749c3abd15f7a6546dc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ae51fcb62841d393f2746328568aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d472783fc3b43dab350fa544300db32",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ec9a6e2f5934a16a916ccf914d92b82",
            "value": 576
          }
        },
        "36ff811bfbfa456f9f5dd9e278e49b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3719e0431f5b42e0af7d126e560afd7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37768cb504674277aa33155bea00f088": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a971f7ffc14fb68b26b89160d64f32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b905aaa4ba145429cd5a54775380173": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd67a7d339f4d3d808952704a6de9df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c93720adf5b4ae49c5845659862ec27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec3f6b1ed1e444bb5adeb8b1893d9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ff46994997c48d280730b6df0859635": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "401c1d403c33491c83f22b5eaa85e3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404d5634fb8c4b578d3ddca94f03110c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4050fef1b7b94d8eaa60f210dcd46492": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4080035014494caeb4159afd14728437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4080cc07e9d64f43943cefd02a079fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4180025e6f41468192a6229a66808606": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "419c859c49374c718ae907fd3855f027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42645fc5984743d0b835c24f83bcd008": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b22b471bfe91493082160211f8ddfcb2",
              "IPY_MODEL_c0579052144147a780734481b5560bdd",
              "IPY_MODEL_9fa6e171beea4283a53be22dd3b22cb5"
            ],
            "layout": "IPY_MODEL_64bf220824874e4b847f1b4d44d1e2df"
          }
        },
        "4279391673a84f6693ff0c6335caf26e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43151f3d4d524ced8e0f879d993e827e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "435b2800b0e04aa9b7bfd38a409cfb6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44cb42f2b4504329abcfd6afd8975691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca81033bcf04ecaac09970d2c5b1f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_5fbae9dcc0814cfdb639fa319ece472f",
            "value": " 144/144 [00:33&lt;00:00,  4.60it/s]"
          }
        },
        "45c319c53ff74bd893f348f291ec2313": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ae33617b216446dac0b69bc6bfeabe9",
              "IPY_MODEL_d10460db9e654a03a55366d9bd5650e6",
              "IPY_MODEL_7861853570324c2d9d2ad0695a5dd349"
            ],
            "layout": "IPY_MODEL_8e7c2efa0d9a4125b0a3332da823ed9f"
          }
        },
        "45d140c7af984e8cb533da00f8f76d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "460b35ef15904ed8ba693c9a2f7b6625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e8b231ebf140d5bfaffb139321a680",
            "placeholder": "​",
            "style": "IPY_MODEL_b9b466feadaf4e899e2e965c37ad46b8",
            "value": "100%"
          }
        },
        "4634fc0487b943f9b51caf2f365ed763": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6520da42b254a2bbb7f192726defc78",
            "placeholder": "​",
            "style": "IPY_MODEL_49d5a7383754464cac5b16198bfc0bff",
            "value": "100%"
          }
        },
        "466a8ec9ba054731b8cc7c1d462485ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49a8196824e04bfd9ee0bb70bca8854a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a8fce01db64ae1923e04078dac974b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49d5a7383754464cac5b16198bfc0bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba6ddc2564941f4b357e8067617478b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7775149182ed48129fd95c7612ec6fce",
            "placeholder": "​",
            "style": "IPY_MODEL_ab265d28a61e405caba635138b902cf2",
            "value": "100%"
          }
        },
        "4c1444173e0d41a7bc6f2aa133155114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23fae093596484ebf3a2fc299f79a8b",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_145bf764473440c8a3cea3dfd67521b8",
            "value": 576
          }
        },
        "4cef287096734ab9be76f2f099fddb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ded12fce21f479480314fcbe50304ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05885faad1441448e813b8a6a1502ef",
            "placeholder": "​",
            "style": "IPY_MODEL_64165ae28252483497538155a866b8de",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "4e2fdc91aa9d462caae6ec9d7dec4757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2751e6becf1e48b492c9015b375d660c",
              "IPY_MODEL_5dc5b2dfa64e40e49d4cbf2f1864ff9d",
              "IPY_MODEL_06c7cdfeb0ba45ef85ea6b45a8c44720"
            ],
            "layout": "IPY_MODEL_93977f619a844e6db041b44aeca987b4"
          }
        },
        "50f71735ccfc4d49b68a94ed64a6f0db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520c5562674a432082c71f5c930933c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c14c91d9ada94bbabdcaf42c7fd87b9d",
              "IPY_MODEL_9e5d606be54e431c9fdde3fcf181b7af",
              "IPY_MODEL_09d29f6c76bd406c804de3dd253fcfe1"
            ],
            "layout": "IPY_MODEL_178ebf6beaea450ea8b086c276ba3ea0"
          }
        },
        "52317606e9bc478d9f76f3142955be28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537d419eea514f0e8547f51ff2d289ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541a744abc0545d891e086ebf9e80a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a13476fe43b34ff19c9eb40464f5eeaa",
              "IPY_MODEL_efc8a54b25f04a2c8a4f09ace1cdc886",
              "IPY_MODEL_04bf632fcaf24df8a84d478602219297"
            ],
            "layout": "IPY_MODEL_b5a751a2a2b64c7d9eb5447c84334604"
          }
        },
        "542748c5145648afab8d065ce93f1e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54385da4fe534cc1a107bea3946c0d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54a2659b81a8425b9b83b6c27c1988c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5606bb10da6b4d58bf5f9f62cb996866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b265fdfca745dd95a10e7f58293255",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10814b075eb5494ba86e5cfa53978bc2",
            "value": 144
          }
        },
        "565961d186404d9a937922a3f403842a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "573ab861f8914350b46a6a5e488e2eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f774aa94834e5c97700f48425dc1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58ec95d0eb2f4855bc8701f4e66be41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6578b120201544ee8a66bed3feb0bbe2",
            "placeholder": "​",
            "style": "IPY_MODEL_f299f57658da45d99d5d9fa6995dec8d",
            "value": " 144/144 [00:33&lt;00:00,  4.62it/s]"
          }
        },
        "5983abd90c1547f995c60d718ad3a6df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abdd043cd1a4e93a2ddd4d968ffc263": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acd05b9556243719a4ef6174391aff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1143a5cef24d2ab857a8fc50059f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e44e4fdabffa4307a7459a2921116356",
            "placeholder": "​",
            "style": "IPY_MODEL_1d65266578ff4bcf8b6184b563bb1a7d",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "5c7021ef16a646fb95227c51014bf5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c88e00526db4fa8889f0c25391cb11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc2671081e90437380ef473b5149d1b7",
            "placeholder": "​",
            "style": "IPY_MODEL_43151f3d4d524ced8e0f879d993e827e",
            "value": " 144/144 [00:33&lt;00:00,  4.61it/s]"
          }
        },
        "5d472783fc3b43dab350fa544300db32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9f2e8b26914c03a8c84b0cf0b6b8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc5b2dfa64e40e49d4cbf2f1864ff9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f787c6345cac4988878c348e839d4f32",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87482c46c88441339045f9782ffff869",
            "value": 576
          }
        },
        "5ee2001475ae40afa828cb7c8a7e0fef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fbae9dcc0814cfdb639fa319ece472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6075a088e8ec4a32a9779a35ec7d1fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609a0c954f4e4664a78d41c861908dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c5c7a9b73644a61ae0a4d35f9ef3b5d",
              "IPY_MODEL_d0c63e1c791f4875afa05fbafb7bf6b9",
              "IPY_MODEL_ae95688d91d54d19a1f8d3ebfe3b3468"
            ],
            "layout": "IPY_MODEL_9653cd35b2c242b8a3805fe278f8f4f2"
          }
        },
        "61023530b3db4eb7a5c3d0a1586b5e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d927883cf9984bcb87a51bb608b2bd13",
            "placeholder": "​",
            "style": "IPY_MODEL_221e081afee145a7a7b614839ca435e3",
            "value": "100%"
          }
        },
        "61052ff7e4444f099292f4f47a358343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4bfce08049b413e96bd4bb6d9d22f64",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_924195f43be541189c02137a1af5b825",
            "value": 576
          }
        },
        "613e24bda50946e8aaf27db78c4ab612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "640f306faa694e85ad3158761d1d1e98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64165ae28252483497538155a866b8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64bf220824874e4b847f1b4d44d1e2df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6578b120201544ee8a66bed3feb0bbe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6619eeed7b9e42a18e0b3cf0e01c4c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6671902ae8264b9e88336a2b31ac5620": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668b6eb8baee4dbfa0f18a2eea34bd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec2c3186b8541fcbb60655f93ec8f3d",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19fcce698cc44c4596176e6cb014edee",
            "value": 144
          }
        },
        "67eead90e3c94d4fba4d02aec123fe6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68402970bb9245f1bc6ac074203782c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685b49164dba4e8ca2f11c6049b75b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697ab55a43cb419985e020529472d1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69f4c7d6f7ce440eafc4a682d07663c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a26142236414fca9cb7c1c94785bf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1491269f004a0cac9873db08e5cbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06c49b944e4c46fe8b8ac2c302a0d3e6",
            "placeholder": "​",
            "style": "IPY_MODEL_d21f4672f2ac40219f72cb30185d2af9",
            "value": " 576/576 [06:26&lt;00:00,  1.99it/s]"
          }
        },
        "6d59c9774cc7485c9b613823a8868369": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3719e0431f5b42e0af7d126e560afd7b",
            "placeholder": "​",
            "style": "IPY_MODEL_5d9f2e8b26914c03a8c84b0cf0b6b8b6",
            "value": "100%"
          }
        },
        "6ec9a6e2f5934a16a916ccf914d92b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "703ce41302b34274b3d4c40fff38dc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dd05963fd5d4a46a18278ac08eb85a5",
              "IPY_MODEL_e881009e6c144c5c98471106e64ee2b1",
              "IPY_MODEL_83dbb4e7f0ca4065ba7437a7df6ef5ab"
            ],
            "layout": "IPY_MODEL_a26ecbf28d4f427aaaa582d197fe0af1"
          }
        },
        "70e8b231ebf140d5bfaffb139321a680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713613c9214043bfacdc749e1fcf2f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7193a948f3c249e2950a20fcc635d86b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a1bb516a6e43a1b51878278a948276": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2888b901d070495ebc440e421aef9c71",
              "IPY_MODEL_d780da5a1359436aa043cdac58b379cd",
              "IPY_MODEL_ad91e66b25fd42f3bb1a28a12940aab8"
            ],
            "layout": "IPY_MODEL_8cb49115ff534fd1b7697ce7eef7cf1a"
          }
        },
        "71d8404cdac54ffcaf6854ffd9f99d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7334a11d35514608b188159e69717f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734ad33a8b75459e904fca7a98b721de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f27439c5caa443e0896680ee73df53b4",
              "IPY_MODEL_9bb69baa4e7149b2bb7a10a20b4da106",
              "IPY_MODEL_a52271797dc64efa9c5105fccadf7b1e"
            ],
            "layout": "IPY_MODEL_5ee2001475ae40afa828cb7c8a7e0fef"
          }
        },
        "73bd6c045e0d4a4b9c47bcba844518eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5532eafa03741dfa7d7eaf875b113fe",
            "placeholder": "​",
            "style": "IPY_MODEL_2051ec926c534139a12c9dafaa5f1d87",
            "value": "100%"
          }
        },
        "73ea1317f8d34abcb0320fd0ef5705a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ed57a18a6d448d95e1a48bae73d35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ff811bfbfa456f9f5dd9e278e49b02",
            "placeholder": "​",
            "style": "IPY_MODEL_a65622ef572146bf9aec836a67fc8b89",
            "value": "100%"
          }
        },
        "74b7bae2715b44a781690941156269e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b905aaa4ba145429cd5a54775380173",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ec3f6b1ed1e444bb5adeb8b1893d9ef",
            "value": 576
          }
        },
        "757370b677b940b7a6e7cc5aed1d806f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5983abd90c1547f995c60d718ad3a6df",
            "placeholder": "​",
            "style": "IPY_MODEL_11737bff28844599822a9a007a4b26a8",
            "value": "100%"
          }
        },
        "75917b51db3c431889f9baa853dba16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ff46994997c48d280730b6df0859635",
            "placeholder": "​",
            "style": "IPY_MODEL_57f774aa94834e5c97700f48425dc1af",
            "value": "100%"
          }
        },
        "76b7e0d1a6bd4996b750afbf6abf5c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6978627fef3491abde4a317153b5f7a",
              "IPY_MODEL_5606bb10da6b4d58bf5f9f62cb996866",
              "IPY_MODEL_8dab1cad717441ad9cc6b424623159c4"
            ],
            "layout": "IPY_MODEL_cf86f2d4ce28492eb45a693755dd2dab"
          }
        },
        "76dd588e3d4e4bfa90ccf8b8acfa4b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e665328c115742b7b49b18631f133bb9",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9ad0cc9c942408bb0f769dc7f44156f",
            "value": 144
          }
        },
        "76edfd6216be4d35b19fa8428ec0acb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ee3b342fa546e9909f8388b8f97485",
            "placeholder": "​",
            "style": "IPY_MODEL_404d5634fb8c4b578d3ddca94f03110c",
            "value": " 144/144 [00:33&lt;00:00,  4.64it/s]"
          }
        },
        "7775149182ed48129fd95c7612ec6fce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f2a62ec87542de93972988ce42b981": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7861853570324c2d9d2ad0695a5dd349": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2596f01929e44f1a719e4bf05950bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_c52af901cfa545f68f4fceb88078c5d9",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "7921db96833a42a09799bf27287d8d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5e74da0606748719acf0c65d10924d6",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc6a007ad9b04127bc640dd22f67d86c",
            "value": 144
          }
        },
        "79845f08dc9e4ba1921dca78b81921d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a10c2c1797e4435943322254d1f6b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aca5025639464af1b575c9659025d601",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0818931c8ac245ae843a05885527b289",
            "value": 576
          }
        },
        "7af5b13480bf4d7db24296421c8b1231": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c89cd3e549455b865c974364af1fbb",
            "placeholder": "​",
            "style": "IPY_MODEL_3120ef27193e4f8ea5795a7e2430a6b7",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "7c5b7f11c35b4af9a4affa52e2337656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8adb0232cc42488afb3b895d5ce172": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df2af09391c40baa6dc6b9d015cbb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ec2c3186b8541fcbb60655f93ec8f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f47f60f07354b04a28fe601849a6f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc285808c19458092dc068dc1ef1e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8050272b18db47bc86e63846f952c7e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817115e98021460eb06299e6b29d3004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "823ca920814142a1a28f13a558f85f42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b6dad1322b4413a58fc98d27197bea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8366e96e7e864dac8abdc17f1c8440c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83dbb4e7f0ca4065ba7437a7df6ef5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1e50ffeb4fd4b4dac866aaf6552623f",
            "placeholder": "​",
            "style": "IPY_MODEL_be43b6c01fe74cefbda9e159947d824d",
            "value": " 144/144 [00:33&lt;00:00,  4.63it/s]"
          }
        },
        "8501b75f202642a0aa57db3f7d0396ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8565465369f84b90ad366695c8c96e33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e2050e14be41dea404ebdaafcd9aee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "861307b80f344f5cbbbf3a18576d9d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae86f781772f4da4ae0dbc156e28960d",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_697ab55a43cb419985e020529472d1f1",
            "value": 144
          }
        },
        "87482c46c88441339045f9782ffff869": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87b265fdfca745dd95a10e7f58293255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88df3a51a6b54871aa6d723e3c88acb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b5ddb8e66d4c3a82b7b75b12bfcde4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89fa0a48474f4af9be7f69681df0a470": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4c9a1807224276b6e4c1aee2ae7a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67eead90e3c94d4fba4d02aec123fe6c",
            "placeholder": "​",
            "style": "IPY_MODEL_4080035014494caeb4159afd14728437",
            "value": "100%"
          }
        },
        "8a5efa2c7bfa477884a62780a66adfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d433deb855466cb715d998392a60dc",
            "placeholder": "​",
            "style": "IPY_MODEL_c862a11a18144793a2ec8e74a1df9762",
            "value": "100%"
          }
        },
        "8a896aa4d40146eb916942590c14681e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d8c302064b147aaa863ae401d4a0ccd",
              "IPY_MODEL_ddaf61393fc74048b08c5ed8e6169ba3",
              "IPY_MODEL_f9e89d1611b74a16bbce7d1681e7b59a"
            ],
            "layout": "IPY_MODEL_1af356108c274cc4b412c2cb54de0a03"
          }
        },
        "8b11f7768d2048c281c84c222d505b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bcbf5353ddb41f58b3498e00fe31bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c7a20f35ed8420499f9ac11d1bd897a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7cd32caea0b499d9f9d620a8dc3a4fe",
            "placeholder": "​",
            "style": "IPY_MODEL_54385da4fe534cc1a107bea3946c0d4d",
            "value": "100%"
          }
        },
        "8cb49115ff534fd1b7697ce7eef7cf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dab1cad717441ad9cc6b424623159c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116ba1d2272f4428a5fbb7e688fcb753",
            "placeholder": "​",
            "style": "IPY_MODEL_9c370cd769f64d7b9ed84eceb1f8a542",
            "value": " 144/144 [00:33&lt;00:00,  4.58it/s]"
          }
        },
        "8dd05963fd5d4a46a18278ac08eb85a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc948194294401ca6e545da71cf3af9",
            "placeholder": "​",
            "style": "IPY_MODEL_613e24bda50946e8aaf27db78c4ab612",
            "value": "100%"
          }
        },
        "8e284d1e4ea74db39f571641d60a46c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7c2efa0d9a4125b0a3332da823ed9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c2f76aaa5447188a4f7c5ad012baac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f219d8ca104cae888b1c524037f93b",
            "placeholder": "​",
            "style": "IPY_MODEL_6619eeed7b9e42a18e0b3cf0e01c4c8b",
            "value": "100%"
          }
        },
        "924195f43be541189c02137a1af5b825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9266c67eb01f4c5aa818511aa70ce9e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931dd9c46f6148879b234414a490edc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93977f619a844e6db041b44aeca987b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943724d81a1143e3861263c37e4ab8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20627e86c90f47ceaaae0fd8c82659c7",
              "IPY_MODEL_0f6db49db25b4a608ae6f920bd4e11b1",
              "IPY_MODEL_f66d95e3735c44e1bfa528c797b04e5a"
            ],
            "layout": "IPY_MODEL_7f47f60f07354b04a28fe601849a6f12"
          }
        },
        "953cc4dbe6cc4a64b68444c42f25302c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a5efa2c7bfa477884a62780a66adfb6",
              "IPY_MODEL_668b6eb8baee4dbfa0f18a2eea34bd38",
              "IPY_MODEL_44cb42f2b4504329abcfd6afd8975691"
            ],
            "layout": "IPY_MODEL_b15d2d72797b4c71912a093e77a870f1"
          }
        },
        "9653cd35b2c242b8a3805fe278f8f4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e3d768b14c4ee4987df3c201b35812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f2a62ec87542de93972988ce42b981",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec0fd14d74f04df4b8c73cfa70623886",
            "value": 144
          }
        },
        "97c8baf0686b4609a3e8020bfa0b723e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97e2d0233d3c48428038fac4d94edf38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a52bd5909b4c36be2dcf3b9403ebeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b6dfa311f84ef5aa432f21b66ae3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ba6ddc2564941f4b357e8067617478b",
              "IPY_MODEL_fafc7ae4d88a4f64ba0ce20b226b8907",
              "IPY_MODEL_be3975587ec54ad682b9d2fc215150ad"
            ],
            "layout": "IPY_MODEL_be5c46e07ed44b378731cc891f9347b2"
          }
        },
        "9967b64021b5495e8a55931695d60911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "999d407a2c45438c8ba048caf6ec7af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89fa0a48474f4af9be7f69681df0a470",
            "placeholder": "​",
            "style": "IPY_MODEL_71d8404cdac54ffcaf6854ffd9f99d1b",
            "value": " 576/576 [06:22&lt;00:00,  1.98it/s]"
          }
        },
        "9b4aa1105f6e4f35a88caac2a20d305a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bb69baa4e7149b2bb7a10a20b4da106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f620a120078348e8ae0ff571ab1d785f",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21f3be4e06a54038a671777d39932cac",
            "value": 144
          }
        },
        "9c2157f5cf114377ae90f889a8f8f6bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c259640dde74b108880ff64bb3af738": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c370cd769f64d7b9ed84eceb1f8a542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e139e9c256b4ea6b70cbb1776926fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e5d606be54e431c9fdde3fcf181b7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_573ab861f8914350b46a6a5e488e2eb7",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49a8fce01db64ae1923e04078dac974b",
            "value": 576
          }
        },
        "9ef86f8d144f48148270f3cea7aefab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f784b2777e341d587d67f6b1f1bfdb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_757370b677b940b7a6e7cc5aed1d806f",
              "IPY_MODEL_96e3d768b14c4ee4987df3c201b35812",
              "IPY_MODEL_58ec95d0eb2f4855bc8701f4e66be41a"
            ],
            "layout": "IPY_MODEL_50f71735ccfc4d49b68a94ed64a6f0db"
          }
        },
        "9fa6e171beea4283a53be22dd3b22cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c259640dde74b108880ff64bb3af738",
            "placeholder": "​",
            "style": "IPY_MODEL_060777b420c541a79d53f68296b169b4",
            "value": " 144/144 [00:33&lt;00:00,  4.61it/s]"
          }
        },
        "9fff7c46fc9b48218d5814591fc52720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75917b51db3c431889f9baa853dba16b",
              "IPY_MODEL_19f7af4954d54264949563d05727b8e6",
              "IPY_MODEL_6b1491269f004a0cac9873db08e5cbaf"
            ],
            "layout": "IPY_MODEL_8565465369f84b90ad366695c8c96e33"
          }
        },
        "a09804bf03934a5cac3cb70369d2864a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6075a088e8ec4a32a9779a35ec7d1fa2",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4080cc07e9d64f43943cefd02a079fa8",
            "value": 576
          }
        },
        "a13476fe43b34ff19c9eb40464f5eeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a52bd5909b4c36be2dcf3b9403ebeb",
            "placeholder": "​",
            "style": "IPY_MODEL_7df2af09391c40baa6dc6b9d015cbb45",
            "value": "100%"
          }
        },
        "a1933ab0570a4372bb6e6459b89cb3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba52436da25c487381dbcbdde0e5e4e8",
            "placeholder": "​",
            "style": "IPY_MODEL_08924d4dba8446d6ac55983d6a59a979",
            "value": " 144/144 [00:33&lt;00:00,  4.60it/s]"
          }
        },
        "a1f26aef797447b09f502a6bdccc5438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a26ecbf28d4f427aaaa582d197fe0af1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36392dcfb04404caac7fc1535c8da14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b4daa450c6c40dea1dea0073949a217",
              "IPY_MODEL_c51180644662452a8f70dc47210190b3",
              "IPY_MODEL_999d407a2c45438c8ba048caf6ec7af7"
            ],
            "layout": "IPY_MODEL_9b4aa1105f6e4f35a88caac2a20d305a"
          }
        },
        "a39b87a9637f4f2fa93af837b1d660fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbdc19ce8d794af88c1cab00c755bdd3",
            "placeholder": "​",
            "style": "IPY_MODEL_97c8baf0686b4609a3e8020bfa0b723e",
            "value": "100%"
          }
        },
        "a3ee3b342fa546e9909f8388b8f97485": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a450c4de019141b6943be618186820f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_823ca920814142a1a28f13a558f85f42",
            "placeholder": "​",
            "style": "IPY_MODEL_419c859c49374c718ae907fd3855f027",
            "value": " 144/144 [00:33&lt;00:00,  4.60it/s]"
          }
        },
        "a4d09608760b48deb7934871ee6e11b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a52271797dc64efa9c5105fccadf7b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4180025e6f41468192a6229a66808606",
            "placeholder": "​",
            "style": "IPY_MODEL_272be9695d0147b595c83cea40c4a0a5",
            "value": " 144/144 [00:33&lt;00:00,  4.63it/s]"
          }
        },
        "a65622ef572146bf9aec836a67fc8b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a682cf6ee29f4102ab136f630ca59964": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49a8196824e04bfd9ee0bb70bca8854a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c7021ef16a646fb95227c51014bf5ab",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "a6a790caa05149e68918d29e1264c2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97e2d0233d3c48428038fac4d94edf38",
            "placeholder": "​",
            "style": "IPY_MODEL_28aac017ffca4c1b92fef83a9827c7f9",
            "value": "100%"
          }
        },
        "a6cafc71727e41f6a8e3e6f49969479d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74c3e5a57364e5495f775469fd76fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abac16e958ff42cfb023719e8e310a17",
            "placeholder": "​",
            "style": "IPY_MODEL_0e05e3d70f3b4bfab0f92e8ff08aecb6",
            "value": "100%"
          }
        },
        "a76e534ed8784f54861ad4de857db319": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88d7de22bc44d5c9be9e448ae189230": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8f219d8ca104cae888b1c524037f93b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ad0cc9c942408bb0f769dc7f44156f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab265d28a61e405caba635138b902cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab97790aca584fb6bcb88cd9143c2567": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abac16e958ff42cfb023719e8e310a17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca5025639464af1b575c9659025d601": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaf625134fa4861abe9afb8afbf292e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acc626cc4f8346a0819e776961df96e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad91e66b25fd42f3bb1a28a12940aab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_204ffa7119ac4179be97d4c8cfea73a3",
            "placeholder": "​",
            "style": "IPY_MODEL_7fc285808c19458092dc068dc1ef1e8a",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "adad5d798a73433091849f5c0cc3272b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae86f781772f4da4ae0dbc156e28960d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae95688d91d54d19a1f8d3ebfe3b3468": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6671902ae8264b9e88336a2b31ac5620",
            "placeholder": "​",
            "style": "IPY_MODEL_bc64a6d8fd4c4356b7785c1544d25248",
            "value": " 144/144 [00:33&lt;00:00,  4.63it/s]"
          }
        },
        "af23384ffcc54386b3839964f1df01d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b24b64b59829474e8649d7745e135197",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b11f7768d2048c281c84c222d505b5a",
            "value": 576
          }
        },
        "aff6af6e08a04e86a5da811a805e818a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1308fa4a951401d89072e60b61c316f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b14f4f87d55042399421a54eaf026b16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b15d2d72797b4c71912a093e77a870f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20e948a56144e63b563668d0311c016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b22b471bfe91493082160211f8ddfcb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_685b49164dba4e8ca2f11c6049b75b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_931dd9c46f6148879b234414a490edc7",
            "value": "100%"
          }
        },
        "b24b64b59829474e8649d7745e135197": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e6d2fbf61f4783b2b6c1e22e203e78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b37530d30f7b46dba8d8105decca6d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b37560efa87a4499ab4f3a1f85b562b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b417e386209a4fb980706841d902f6ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b135938593488a8a494680f39e4969": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51a078097f446738f377756597ee609": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5427a090db54b09bc6e2072cd17e576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a751a2a2b64c7d9eb5447c84334604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6520da42b254a2bbb7f192726defc78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66068b5d54d4898a6dbcf1a203cff38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_291d5237a3b7419fabea1f7cadf554f2",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d97c4522f72246bab603477177652ebd",
            "value": 144
          }
        },
        "b7cd32caea0b499d9f9d620a8dc3a4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8220975bff7491a830f2308fdd45d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97353006eaa4cb4a27268d3cbbf014b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_460b35ef15904ed8ba693c9a2f7b6625",
              "IPY_MODEL_861307b80f344f5cbbbf3a18576d9d2b",
              "IPY_MODEL_76edfd6216be4d35b19fa8428ec0acb8"
            ],
            "layout": "IPY_MODEL_d9e4f14c4fd34fec859a90fb264ba26a"
          }
        },
        "b9b466feadaf4e899e2e965c37ad46b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba52436da25c487381dbcbdde0e5e4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc64a6d8fd4c4356b7785c1544d25248": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd0296a4f45547d6a0df66dcca13a20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c93720adf5b4ae49c5845659862ec27",
            "placeholder": "​",
            "style": "IPY_MODEL_4279391673a84f6693ff0c6335caf26e",
            "value": "100%"
          }
        },
        "be3975587ec54ad682b9d2fc215150ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_542748c5145648afab8d065ce93f1e6d",
            "placeholder": "​",
            "style": "IPY_MODEL_dde482430e4d4ef1a491de2019816032",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "be43b6c01fe74cefbda9e159947d824d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be5c46e07ed44b378731cc891f9347b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef23c7fb87646f6848929f6c1fecf4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf16ab2065b449c6bbdfde8e18806b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0579052144147a780734481b5560bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115a8cc8a6bd451aa3b0ad4c31fa10d9",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69f4c7d6f7ce440eafc4a682d07663c1",
            "value": 144
          }
        },
        "c105ca9f1cc0450092ae65a59d3138d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c14c91d9ada94bbabdcaf42c7fd87b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5b7f11c35b4af9a4affa52e2337656",
            "placeholder": "​",
            "style": "IPY_MODEL_d426fed1597a4305a79eb88dc30b9c8f",
            "value": "100%"
          }
        },
        "c1c5a3ad42954f3583bada6fac7ba761": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c7a20f35ed8420499f9ac11d1bd897a",
              "IPY_MODEL_f833af37ec424e5b929970812520f5b5",
              "IPY_MODEL_f00389de389c468d93c154495ebba366"
            ],
            "layout": "IPY_MODEL_817115e98021460eb06299e6b29d3004"
          }
        },
        "c26cfffbd68840409f70786b2d22d077": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f24418eefbf549daba2b960e27d0da6e",
            "placeholder": "​",
            "style": "IPY_MODEL_9967b64021b5495e8a55931695d60911",
            "value": " 144/144 [00:33&lt;00:00,  4.61it/s]"
          }
        },
        "c2fa5988320b4d34815a88cb959ff23a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d6705ff4874ef1a896369132d27351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d59c9774cc7485c9b613823a8868369",
              "IPY_MODEL_7a10c2c1797e4435943322254d1f6b17",
              "IPY_MODEL_a682cf6ee29f4102ab136f630ca59964"
            ],
            "layout": "IPY_MODEL_5abdd043cd1a4e93a2ddd4d968ffc263"
          }
        },
        "c3dcb699c7364c659da94b2067cf34c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73bd6c045e0d4a4b9c47bcba844518eb",
              "IPY_MODEL_eefd9845fb3347b6915e7998869e6626",
              "IPY_MODEL_2bc545952c334e86811ab2afb753d68c"
            ],
            "layout": "IPY_MODEL_b2e6d2fbf61f4783b2b6c1e22e203e78"
          }
        },
        "c453ad3af78143a4b9d8900c54ec0d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4ef2485141948978983839d1c4995bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f781dadcd34ec0990956ad12ac629d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51180644662452a8f70dc47210190b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a21eebafe44d85b73db64a07a2bbf7",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1f26aef797447b09f502a6bdccc5438",
            "value": 576
          }
        },
        "c52af901cfa545f68f4fceb88078c5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5d8e1ab5cdb4f1992d6442964f151cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5e1719cb7b945918d0d40a245464735": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c862a11a18144793a2ec8e74a1df9762": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cca81033bcf04ecaac09970d2c5b1f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee4279fc0c44e97a0783ecdcccdc0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8050272b18db47bc86e63846f952c7e0",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_013b1300b7d0453d9a44e5474ef7b599",
            "value": 144
          }
        },
        "cf86f2d4ce28492eb45a693755dd2dab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d06ef49d80284d4e857d3de70ab4e42a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c63e1c791f4875afa05fbafb7bf6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bd67a7d339f4d3d808952704a6de9df",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f854ed5b43e43ab883b3e95bb64f79a",
            "value": 144
          }
        },
        "d10460db9e654a03a55366d9bd5650e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8220975bff7491a830f2308fdd45d5b",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acaf625134fa4861abe9afb8afbf292e",
            "value": 576
          }
        },
        "d187ec9b7715432e8bb61def9a97f191": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d21f4672f2ac40219f72cb30185d2af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2596f01929e44f1a719e4bf05950bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d426fed1597a4305a79eb88dc30b9c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45e7cefc7ec4cdaa7661439a3ad88ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5600738377440608359b54f941d8eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a39b87a9637f4f2fa93af837b1d660fd",
              "IPY_MODEL_ec508519c15c43baa87ba078c7624704",
              "IPY_MODEL_7af5b13480bf4d7db24296421c8b1231"
            ],
            "layout": "IPY_MODEL_dbbe53da775b47c699360b5c6b703b6d"
          }
        },
        "d582464602794bb3b4e928179bce33ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61023530b3db4eb7a5c3d0a1586b5e59",
              "IPY_MODEL_4c1444173e0d41a7bc6f2aa133155114",
              "IPY_MODEL_27bbe64e53d647f3b873c21292000da2"
            ],
            "layout": "IPY_MODEL_322446e27fb94835b0c0a6763f7052e7"
          }
        },
        "d5873c57c49d40819ec57bbb98fdc939": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de065bc6691b4e8d9d9e78c73b696f25",
              "IPY_MODEL_74b7bae2715b44a781690941156269e0",
              "IPY_MODEL_4ded12fce21f479480314fcbe50304ef"
            ],
            "layout": "IPY_MODEL_f919bda4e2b64ae4a8d79a493146afdd"
          }
        },
        "d60c96daf993409580da92cee35fee18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d780da5a1359436aa043cdac58b379cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b135938593488a8a494680f39e4969",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e48a352d042b4683ba6c4ad766de9d46",
            "value": 576
          }
        },
        "d8c89cd3e549455b865c974364af1fbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d927883cf9984bcb87a51bb608b2bd13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97c4522f72246bab603477177652ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d99b28d2748049bcbbf35e2bd991ac3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_215620ac6e684ef9a436f1f390331893",
              "IPY_MODEL_eb25f5fe42824db28e5e8734c743aaf0",
              "IPY_MODEL_5c88e00526db4fa8889f0c25391cb11f"
            ],
            "layout": "IPY_MODEL_c4ef2485141948978983839d1c4995bb"
          }
        },
        "d9ba025ca762447f8e1108d8a8ac21ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e4f14c4fd34fec859a90fb264ba26a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4b16e660014d98beca81dd935f2f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbbe53da775b47c699360b5c6b703b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6a007ad9b04127bc640dd22f67d86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcd8f3b4901e46ab901f9481033f3d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddaf61393fc74048b08c5ed8e6169ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2fa5988320b4d34815a88cb959ff23a",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5d8e1ab5cdb4f1992d6442964f151cc",
            "value": 144
          }
        },
        "dde482430e4d4ef1a491de2019816032": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de065bc6691b4e8d9d9e78c73b696f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26383e1a367846dd935553dd05a22c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_333c3c2696ba4686aed6f5337325e5da",
            "value": "100%"
          }
        },
        "de21d5baddac4fb39cc1e60ef1dd267a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_139b2ece68a341dfac81de12f6f6f3c5",
              "IPY_MODEL_cee4279fc0c44e97a0783ecdcccdc0ba",
              "IPY_MODEL_a450c4de019141b6943be618186820f3"
            ],
            "layout": "IPY_MODEL_d60c96daf993409580da92cee35fee18"
          }
        },
        "dea7f410ea664215b26150ccff543916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06ef49d80284d4e857d3de70ab4e42a",
            "placeholder": "​",
            "style": "IPY_MODEL_35b45ccb7cd94303a159515b413d5d4e",
            "value": "100%"
          }
        },
        "df3620ce9be44df89fbb87108b03ba33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6a790caa05149e68918d29e1264c2ad",
              "IPY_MODEL_df6cc5f83a74495ea09ae03eae66b719",
              "IPY_MODEL_32debaf7b0844c718d44d376322ca725"
            ],
            "layout": "IPY_MODEL_73ea1317f8d34abcb0320fd0ef5705a1"
          }
        },
        "df6cc5f83a74495ea09ae03eae66b719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f781dadcd34ec0990956ad12ac629d",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54a2659b81a8425b9b83b6c27c1988c3",
            "value": 144
          }
        },
        "e04f46bc714640eaab9d03d2fd585da6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05885faad1441448e813b8a6a1502ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e275aee5d043058a3615df49beac56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4634fc0487b943f9b51caf2f365ed763",
              "IPY_MODEL_e99987e52ddd48bdbf58dfa49d1f7337",
              "IPY_MODEL_c26cfffbd68840409f70786b2d22d077"
            ],
            "layout": "IPY_MODEL_89b5ddb8e66d4c3a82b7b75b12bfcde4"
          }
        },
        "e12cf108a5e34631a7c5dc406cdf8713": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a74c3e5a57364e5495f775469fd76fc7",
              "IPY_MODEL_0c709a5117ae413c84915bf6de80df9b",
              "IPY_MODEL_e3c078ad1d2f479e9efed2fbcca62beb"
            ],
            "layout": "IPY_MODEL_b14f4f87d55042399421a54eaf026b16"
          }
        },
        "e1481d88aaec476bae81d700176ff07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e50ffeb4fd4b4dac866aaf6552623f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c078ad1d2f479e9efed2fbcca62beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c8adb0232cc42488afb3b895d5ce172",
            "placeholder": "​",
            "style": "IPY_MODEL_5acd05b9556243719a4ef6174391aff0",
            "value": " 576/576 [06:27&lt;00:00,  1.98it/s]"
          }
        },
        "e41ea261063947a29ccd7eda19a830a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e44e4fdabffa4307a7459a2921116356": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48a352d042b4683ba6c4ad766de9d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4ebdcdf2de94bcfa079eac6007bddd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc876d00857742649445719641ae234d",
              "IPY_MODEL_b66068b5d54d4898a6dbcf1a203cff38",
              "IPY_MODEL_1cb24bf2ffc944df959d9fde0dcc1a00"
            ],
            "layout": "IPY_MODEL_37a971f7ffc14fb68b26b89160d64f32"
          }
        },
        "e5532eafa03741dfa7d7eaf875b113fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e74da0606748719acf0c65d10924d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65cbf2d6c7649a984d2a9e646eaf63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90c2f76aaa5447188a4f7c5ad012baac",
              "IPY_MODEL_76dd588e3d4e4bfa90ccf8b8acfa4b25",
              "IPY_MODEL_1f654c9f2c6e431d8f875633bc99c861"
            ],
            "layout": "IPY_MODEL_aff6af6e08a04e86a5da811a805e818a"
          }
        },
        "e665328c115742b7b49b18631f133bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e881009e6c144c5c98471106e64ee2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401c1d403c33491c83f22b5eaa85e3f9",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_287b47e2a1cd4c05b00b75fa94788b2c",
            "value": 144
          }
        },
        "e99987e52ddd48bdbf58dfa49d1f7337": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b417e386209a4fb980706841d902f6ae",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcd8f3b4901e46ab901f9481033f3d70",
            "value": 144
          }
        },
        "eb25f5fe42824db28e5e8734c743aaf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e04f46bc714640eaab9d03d2fd585da6",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0db2b185dd94e86b8f62dcdbe71c852",
            "value": 144
          }
        },
        "ec0fd14d74f04df4b8c73cfa70623886": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec508519c15c43baa87ba078c7624704": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2af37427a877432794a60b060b4db831",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ea405aae134456b8af80bdbfbd7d995",
            "value": 576
          }
        },
        "ecd5c884701444f4887fcf35968d263b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee09f761c90a42ebbe2b0ca265d26926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eefd9845fb3347b6915e7998869e6626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adad5d798a73433091849f5c0cc3272b",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26176e057bc947a19964f85c615a10fb",
            "value": 576
          }
        },
        "ef61101d2e7b4bb19bdbad8b80aedb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3626d0bdd8e14904bd70784f6fadde7c",
              "IPY_MODEL_af23384ffcc54386b3839964f1df01d2",
              "IPY_MODEL_2571eaecd6b74be8b0f40160a23de443"
            ],
            "layout": "IPY_MODEL_7334a11d35514608b188159e69717f9c"
          }
        },
        "efb36c4dc9a543e5906770a0593e0b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a4c9a1807224276b6e4c1aee2ae7a84",
              "IPY_MODEL_a09804bf03934a5cac3cb70369d2864a",
              "IPY_MODEL_0ad034620c0a48a085a8cffd1f8f5d5b"
            ],
            "layout": "IPY_MODEL_b37560efa87a4499ab4f3a1f85b562b4"
          }
        },
        "efc55659dd2446a0a34c32043d84c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efc8a54b25f04a2c8a4f09ace1cdc886": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e2050e14be41dea404ebdaafcd9aee",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25af9f53ac224586ac42212c3ce69d4f",
            "value": 144
          }
        },
        "f00389de389c468d93c154495ebba366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52317606e9bc478d9f76f3142955be28",
            "placeholder": "​",
            "style": "IPY_MODEL_ee09f761c90a42ebbe2b0ca265d26926",
            "value": " 144/144 [00:33&lt;00:00,  4.62it/s]"
          }
        },
        "f0db2b185dd94e86b8f62dcdbe71c852": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f23fae093596484ebf3a2fc299f79a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24418eefbf549daba2b960e27d0da6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27439c5caa443e0896680ee73df53b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0363c8b048ec46509cb6416aee1c04fa",
            "placeholder": "​",
            "style": "IPY_MODEL_ecd5c884701444f4887fcf35968d263b",
            "value": "100%"
          }
        },
        "f299f57658da45d99d5d9fa6995dec8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f480cc1a318d454191e333c259c637ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4bfce08049b413e96bd4bb6d9d22f64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e0009d37454db8b1a615adf92f7238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f620a120078348e8ae0ff571ab1d785f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66d95e3735c44e1bfa528c797b04e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4050fef1b7b94d8eaa60f210dcd46492",
            "placeholder": "​",
            "style": "IPY_MODEL_88df3a51a6b54871aa6d723e3c88acb3",
            "value": " 576/576 [06:26&lt;00:00,  1.98it/s]"
          }
        },
        "f6978627fef3491abde4a317153b5f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c2157f5cf114377ae90f889a8f8f6bf",
            "placeholder": "​",
            "style": "IPY_MODEL_faf7a45f8ad44ffa9bb45e2d6175a9cb",
            "value": "100%"
          }
        },
        "f787c6345cac4988878c348e839d4f32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f833af37ec424e5b929970812520f5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210565db3d154d2da4478a97d27c19b3",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45d140c7af984e8cb533da00f8f76d09",
            "value": 144
          }
        },
        "f919bda4e2b64ae4a8d79a493146afdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e89d1611b74a16bbce7d1681e7b59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358e3a615e8a43148f2356c9c74ae4ff",
            "placeholder": "​",
            "style": "IPY_MODEL_6a26142236414fca9cb7c1c94785bf47",
            "value": " 144/144 [00:33&lt;00:00,  4.60it/s]"
          }
        },
        "faf7a45f8ad44ffa9bb45e2d6175a9cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fafc7ae4d88a4f64ba0ce20b226b8907": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b6dad1322b4413a58fc98d27197bea",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_713613c9214043bfacdc749e1fcf2f47",
            "value": 576
          }
        },
        "fbdc19ce8d794af88c1cab00c755bdd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2671081e90437380ef473b5149d1b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc876d00857742649445719641ae234d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5c64d2e05247a88eef8c050e44ee39",
            "placeholder": "​",
            "style": "IPY_MODEL_4cef287096734ab9be76f2f099fddb90",
            "value": "100%"
          }
        },
        "fcc948194294401ca6e545da71cf3af9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5b648c708340aa9a8e659eda1085cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
